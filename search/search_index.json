{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Vespa python API","text":"<p>Vespa is the scalable open-sourced serving engine to store, compute and rank big data at user serving time. <code>pyvespa</code> provides a python API to Vespa.</p> <p>We aim for complete feature parity with Vespa, and estimate that we cover &gt; 95% of Vespa features, with all most commonly used features supported.</p> <p>If you find a Vespa feature that you are not able to express/use with <code>pyvespa</code>, please open an issue.</p>"},{"location":"index.html#quick-start","title":"Quick start","text":"<p>To get a sense of the most basic functionality, check out the Hybrid Search Quick start: </p> <ul> <li>Hybrid search quick start - Docker</li> <li>Hybrid search quick start - Vespa Cloud</li> </ul>"},{"location":"index.html#overview-of-pyvespa-features","title":"Overview of pyvespa features","text":"<p>Info</p> <p>There are two main interfaces to Vespa:</p> <ol> <li>Control-plane API: Used to deploy and manage Vespa applications.<ul> <li><code>VespaCloud</code>: Control-plane interface to Vespa Cloud.</li> <li><code>VespaDocker</code>: Control-plane iterface to local Vespa instance (docker/podman).</li> </ul> </li> <li>Data-plane API: Used to feed and query data in Vespa applications.<ul> <li><code>Vespa</code></li> </ul> </li> </ol> <p>Note that <code>VespaCloud</code> and <code>Vespa</code> require two separate authentication methods.</p> <p>Refer to the Authenticating to Vespa Cloud for details.</p> <ul> <li>Create and deploy application packages, including schemas, rank profiles, <code>services.xml</code>, query profiles etc.</li> <li>Feed and retrieve documents to/from Vespa, using <code>/document/v1/</code> API.</li> <li>Query Vespa applications, using <code>/search/</code> API.</li> <li>Build complex queries using the <code>QueryBuilder</code> API.</li> <li>Collect training data for ML using <code>VespaFeatureCollector</code>.</li> <li>Evaluate Vespa applications using <code>VespaEvaluator</code>/<code>VespaMatchEvaluator</code>.</li> </ul>"},{"location":"index.html#requirements","title":"Requirements","text":"<p>Install <code>pyvespa</code>:</p> <p>We recommend using <code>uv</code> to manage your python environments:</p> <pre><code>uv add pyvespa\n</code></pre> <p>or using <code>pip</code>:</p> <pre><code>pip install pyvespa\n</code></pre>"},{"location":"index.html#check-out-the-examples","title":"Check out the examples","text":"<p>Check out our wide variety of Examples that demonstrate how to use the Vespa Python API to serve various use cases.</p>"},{"location":"advanced-configuration.html","title":"Advanced Configuration","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install pyvespa and start Docker Daemon, validate minimum 6G available:</p> In\u00a0[1]: Copied! <pre>#!pip3 install pyvespa\n#!docker info | grep \"Total Memory\"\n</pre> #!pip3 install pyvespa #!docker info | grep \"Total Memory\" In\u00a0[2]: Copied! <pre>from vespa.package import Document, Field, Schema, ApplicationPackage\n\napplication_name = \"music\"\nmusic_schema = Schema(\n    name=application_name,\n    document=Document(\n        fields=[\n            Field(\n                name=\"artist\",\n                type=\"string\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n            Field(\n                name=\"timestamp\",\n                type=\"long\",\n                indexing=[\"attribute\", \"summary\"],\n                attribute=[\"fast-access\"],\n            ),\n        ]\n    ),\n)\n</pre> from vespa.package import Document, Field, Schema, ApplicationPackage  application_name = \"music\" music_schema = Schema(     name=application_name,     document=Document(         fields=[             Field(                 name=\"artist\",                 type=\"string\",                 indexing=[\"attribute\", \"summary\"],             ),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"attribute\", \"summary\"],             ),             Field(                 name=\"timestamp\",                 type=\"long\",                 indexing=[\"attribute\", \"summary\"],                 attribute=[\"fast-access\"],             ),         ]     ), ) In\u00a0[3]: Copied! <pre>from vespa.package import ServicesConfiguration\nfrom vespa.configuration.services import (\n    services,\n    container,\n    search,\n    document_api,\n    document_processing,\n    content,\n    redundancy,\n    documents,\n    document,\n    node,\n    nodes,\n)\n\n# Create a ServicesConfiguration with document-expiry set to 1 day (timestamp &gt; now() - 86400)\nservices_config = ServicesConfiguration(\n    application_name=application_name,\n    services_config=services(\n        container(\n            search(),\n            document_api(),\n            document_processing(),\n            id=f\"{application_name}_container\",\n            version=\"1.0\",\n        ),\n        content(\n            redundancy(\"1\"),\n            documents(\n                document(\n                    type=application_name,\n                    mode=\"index\",\n                    # Note that the selection-expression does not need to be escaped, as it will be automatically escaped during xml-serialization\n                    selection=\"music.timestamp &gt; now() - 86400\",\n                ),\n                garbage_collection=\"true\",\n            ),\n            nodes(node(distribution_key=\"0\", hostalias=\"node1\")),\n            id=f\"{application_name}_content\",\n            version=\"1.0\",\n        ),\n    ),\n)\napplication_package = ApplicationPackage(\n    name=application_name,\n    schema=[music_schema],\n    services_config=services_config,\n)\n</pre> from vespa.package import ServicesConfiguration from vespa.configuration.services import (     services,     container,     search,     document_api,     document_processing,     content,     redundancy,     documents,     document,     node,     nodes, )  # Create a ServicesConfiguration with document-expiry set to 1 day (timestamp &gt; now() - 86400) services_config = ServicesConfiguration(     application_name=application_name,     services_config=services(         container(             search(),             document_api(),             document_processing(),             id=f\"{application_name}_container\",             version=\"1.0\",         ),         content(             redundancy(\"1\"),             documents(                 document(                     type=application_name,                     mode=\"index\",                     # Note that the selection-expression does not need to be escaped, as it will be automatically escaped during xml-serialization                     selection=\"music.timestamp &gt; now() - 86400\",                 ),                 garbage_collection=\"true\",             ),             nodes(node(distribution_key=\"0\", hostalias=\"node1\")),             id=f\"{application_name}_content\",             version=\"1.0\",         ),     ), ) application_package = ApplicationPackage(     name=application_name,     schema=[music_schema],     services_config=services_config, ) <p>There are some useful gotchas to keep in mind when constructing the <code>ServicesConfiguration</code> object.</p> <p>First, let's establish a common vocabulary through an example. Consider the following <code>services.xml</code> file, which is what we are actually representing with the <code>ServicesConfiguration</code> object from the previous cell:</p> <pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n&lt;services&gt;\n  &lt;container id=\"music_container\" version=\"1.0\"&gt;\n    &lt;search&gt;&lt;/search&gt;\n    &lt;document-api&gt;&lt;/document-api&gt;\n    &lt;document-processing&gt;&lt;/document-processing&gt;\n  &lt;/container&gt;\n  &lt;content id=\"music_content\" version=\"1.0\"&gt;\n    &lt;redundancy&gt;1&lt;/redundancy&gt;\n    &lt;documents garbage-collection=\"true\"&gt;\n      &lt;document type=\"music\" mode=\"index\" selection=\"music.timestamp &amp;gt; now() - 86400\"&gt;&lt;/document&gt;\n    &lt;/documents&gt;\n    &lt;nodes&gt;\n      &lt;node distribution-key=\"0\" hostalias=\"node1\"&gt;&lt;/node&gt;\n    &lt;/nodes&gt;\n  &lt;/content&gt;\n&lt;/services&gt;\n</pre> <p>In this example, <code>services</code>, <code>container</code>, <code>search</code>, <code>document-api</code>, <code>document-processing</code>, <code>content</code>, <code>redundancy</code>, <code>documents</code>, <code>document</code>, and <code>nodes</code> are tags. The <code>id</code>, <code>version</code>, <code>type</code>, <code>mode</code>, <code>selection</code>, <code>distribution-key</code>, <code>hostalias</code>, and <code>garbage-collection</code> are attributes, with a corresponding value.</p> In\u00a0[4]: Copied! <pre>from vespa.configuration.vt import replace_reserved\n\nreplace_reserved\n</pre> from vespa.configuration.vt import replace_reserved  replace_reserved Out[4]: <pre>{'type': 'type_',\n 'class': 'class_',\n 'for': 'for_',\n 'time': 'time_',\n 'io': 'io_',\n 'from': 'from_',\n 'match': 'match_'}</pre> <p>Only valid tags will be exported by the <code>vespa.configuration.</code> modules.</p> In\u00a0[5]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=application_package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=application_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for application to come up, 0/300 seconds.\nWaiting for application to come up, 5/300 seconds.\nWaiting for application to come up, 10/300 seconds.\nApplication is up!\nFinished deployment.\n</pre> <p><code>app</code> now holds a reference to a Vespa instance. see this notebook for details on authenticating to Vespa Cloud.</p> In\u00a0[6]: Copied! <pre>import time\n\ndocs_to_feed = [\n    {\n        \"id\": \"1\",\n        \"fields\": {\n            \"artist\": \"Snoop Dogg\",\n            \"title\": \"Gin and Juice\",\n            \"timestamp\": int(time.time()) - 86401,\n        },\n    },\n    {\n        \"id\": \"2\",\n        \"fields\": {\n            \"artist\": \"Dr.Dre\",\n            \"title\": \"Still D.R.E\",\n            \"timestamp\": int(time.time()),\n        },\n    },\n]\n</pre> import time  docs_to_feed = [     {         \"id\": \"1\",         \"fields\": {             \"artist\": \"Snoop Dogg\",             \"title\": \"Gin and Juice\",             \"timestamp\": int(time.time()) - 86401,         },     },     {         \"id\": \"2\",         \"fields\": {             \"artist\": \"Dr.Dre\",             \"title\": \"Still D.R.E\",             \"timestamp\": int(time.time()),         },     }, ] In\u00a0[7]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(docs_to_feed, schema=application_name, callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(docs_to_feed, schema=application_name, callback=callback) In\u00a0[8]: Copied! <pre>visit_results = []\nfor slice_ in app.visit(\n    schema=application_name,\n    content_cluster_name=f\"{application_name}_content\",\n    timeout=\"5s\",\n):\n    for response in slice_:\n        visit_results.append(response.json)\nvisit_results\n</pre> visit_results = [] for slice_ in app.visit(     schema=application_name,     content_cluster_name=f\"{application_name}_content\",     timeout=\"5s\", ):     for response in slice_:         visit_results.append(response.json) visit_results Out[8]: <pre>[{'pathId': '/document/v1/music/music/docid/',\n  'documents': [{'id': 'id:music:music::2',\n    'fields': {'artist': 'Dr.Dre',\n     'title': 'Still D.R.E',\n     'timestamp': 1754981413}}],\n  'documentCount': 1}]</pre> <p>We can see that the document with the timestamp of 24 hours ago is not returned by the query, while the document with the current timestamp is returned.</p> In\u00a0[9]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove() In\u00a0[10]: Copied! <pre>from pathlib import Path\nimport requests\nfrom vespa.deployment import VespaDocker\n\n# Download the model if it doesn't exist\nurl = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\"\nlocal_model_path = \"model/model.onnx\"\nif not Path(local_model_path).exists():\n    print(\"Downloading the mxbai-rerank model...\")\n    r = requests.get(url)\n    Path(local_model_path).parent.mkdir(parents=True, exist_ok=True)\n    with open(local_model_path, \"wb\") as f:\n        f.write(r.content)\n        print(f\"Downloaded model to {local_model_path}\")\nelse:\n    print(\"Model already exists, skipping download.\")\n</pre> from pathlib import Path import requests from vespa.deployment import VespaDocker  # Download the model if it doesn't exist url = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\" local_model_path = \"model/model.onnx\" if not Path(local_model_path).exists():     print(\"Downloading the mxbai-rerank model...\")     r = requests.get(url)     Path(local_model_path).parent.mkdir(parents=True, exist_ok=True)     with open(local_model_path, \"wb\") as f:         f.write(r.content)         print(f\"Downloaded model to {local_model_path}\") else:     print(\"Model already exists, skipping download.\") <pre>Model already exists, skipping download.\n</pre> In\u00a0[11]: Copied! <pre>from vespa.package import (\n    OnnxModel,\n    RankProfile,\n    Schema,\n    ApplicationPackage,\n    Field,\n    FieldSet,\n    Function,\n    FirstPhaseRanking,\n    Document,\n)\n\n\napplication_name = \"requestthreads\"\n\n# Define the reranking, as we will use it for two different rank profiles\nreranking = FirstPhaseRanking(\n    keep_rank_count=8,\n    expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\",\n)\n\n# Define the schema\nschema = Schema(\n    name=\"doc\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"body_tokens\",\n                type=\"tensor&lt;float&gt;(d0[512])\",\n                indexing=[\n                    \"input text\",\n                    \"embed tokenizer\",\n                    \"attribute\",\n                    \"summary\",\n                ],\n                is_document_field=False,  # Indicates a synthetic field\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n    models=[\n        OnnxModel(\n            model_name=\"crossencoder\",\n            model_file_path=f\"{local_model_path}\",\n            inputs={\n                \"input_ids\": \"input_ids\",\n                \"attention_mask\": \"attention_mask\",\n            },\n            outputs={\"logits\": \"logits\"},\n        )\n    ],\n    rank_profiles=[\n        RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),\n        RankProfile(\n            name=\"reranking\",\n            inherits=\"default\",\n            inputs=[(\"query(q)\", \"tensor&lt;float&gt;(d0[64])\")],\n            functions=[\n                Function(\n                    name=\"input_ids\",\n                    expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",\n                ),\n                Function(\n                    name=\"attention_mask\",\n                    expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",\n                ),\n            ],\n            first_phase=reranking,\n            summary_features=[\n                \"query(q)\",\n                \"input_ids\",\n                \"attention_mask\",\n                \"onnx(crossencoder).logits\",\n            ],\n        ),\n        RankProfile(\n            name=\"one-thread-profile\",\n            first_phase=reranking,\n            inherits=\"reranking\",\n            num_threads_per_search=1,\n        ),\n    ],\n)\n</pre> from vespa.package import (     OnnxModel,     RankProfile,     Schema,     ApplicationPackage,     Field,     FieldSet,     Function,     FirstPhaseRanking,     Document, )   application_name = \"requestthreads\"  # Define the reranking, as we will use it for two different rank profiles reranking = FirstPhaseRanking(     keep_rank_count=8,     expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\", )  # Define the schema schema = Schema(     name=\"doc\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"body_tokens\",                 type=\"tensor(d0[512])\",                 indexing=[                     \"input text\",                     \"embed tokenizer\",                     \"attribute\",                     \"summary\",                 ],                 is_document_field=False,  # Indicates a synthetic field             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],     models=[         OnnxModel(             model_name=\"crossencoder\",             model_file_path=f\"{local_model_path}\",             inputs={                 \"input_ids\": \"input_ids\",                 \"attention_mask\": \"attention_mask\",             },             outputs={\"logits\": \"logits\"},         )     ],     rank_profiles=[         RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),         RankProfile(             name=\"reranking\",             inherits=\"default\",             inputs=[(\"query(q)\", \"tensor(d0[64])\")],             functions=[                 Function(                     name=\"input_ids\",                     expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",                 ),                 Function(                     name=\"attention_mask\",                     expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",                 ),             ],             first_phase=reranking,             summary_features=[                 \"query(q)\",                 \"input_ids\",                 \"attention_mask\",                 \"onnx(crossencoder).logits\",             ],         ),         RankProfile(             name=\"one-thread-profile\",             first_phase=reranking,             inherits=\"reranking\",             num_threads_per_search=1,         ),     ], ) In\u00a0[12]: Copied! <pre>from vespa.configuration.services import *\nfrom vespa.package import ServicesConfiguration\n\n# Define services configuration with persearch threads set to 4\nservices_config = ServicesConfiguration(\n    application_name=f\"{application_name}\",\n    services_config=services(\n        container(id=f\"{application_name}_default\", version=\"1.0\")(\n            component(\n                model(\n                    url=\"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"\n                ),\n                id=\"tokenizer\",\n                type=\"hugging-face-tokenizer\",\n            ),\n            document_api(),\n            search(),\n        ),\n        content(id=f\"{application_name}\", version=\"1.0\")(\n            min_redundancy(\"1\"),\n            documents(document(type=\"doc\", mode=\"index\")),\n            engine(\n                proton(\n                    tuning(\n                        searchnode(requestthreads(persearch(\"4\"))),\n                    ),\n                ),\n            ),\n        ),\n        version=\"1.0\",\n        minimum_required_vespa_version=\"8.311.28\",\n    ),\n)\n</pre> from vespa.configuration.services import * from vespa.package import ServicesConfiguration  # Define services configuration with persearch threads set to 4 services_config = ServicesConfiguration(     application_name=f\"{application_name}\",     services_config=services(         container(id=f\"{application_name}_default\", version=\"1.0\")(             component(                 model(                     url=\"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"                 ),                 id=\"tokenizer\",                 type=\"hugging-face-tokenizer\",             ),             document_api(),             search(),         ),         content(id=f\"{application_name}\", version=\"1.0\")(             min_redundancy(\"1\"),             documents(document(type=\"doc\", mode=\"index\")),             engine(                 proton(                     tuning(                         searchnode(requestthreads(persearch(\"4\"))),                     ),                 ),             ),         ),         version=\"1.0\",         minimum_required_vespa_version=\"8.311.28\",     ), ) <p>Now, we are ready to deploy our application-package with the defined <code>ServicesConfiguration</code>.</p> In\u00a0[13]: Copied! <pre>app_package = ApplicationPackage(\n    name=f\"{application_name}\",\n    schema=[schema],\n    services_config=services_config,\n)\n</pre> app_package = ApplicationPackage(     name=f\"{application_name}\",     schema=[schema],     services_config=services_config, ) In\u00a0[14]: Copied! <pre>vespa_docker = VespaDocker(port=8089)\napp = vespa_docker.deploy(application_package=app_package)\n</pre> vespa_docker = VespaDocker(port=8089) app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for application to come up, 0/300 seconds.\nWaiting for application to come up, 5/300 seconds.\nWaiting for application to come up, 10/300 seconds.\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[15]: Copied! <pre>sample_docs = [\n    {\"id\": i, \"fields\": {\"text\": text}}\n    for i, text in enumerate(\n        [\n            \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature. The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird'. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n            \"was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961. Jane Austen was an English novelist known primarily for her six major novels, \",\n            \"which interpret, critique and comment upon the British landed gentry at the end of the 18th century. The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, \",\n            \"is among the most popular and critically acclaimed books of the modern era. 'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",\n        ]\n    )\n]\napp.feed_iterable(sample_docs, schema=\"doc\")\n\n# Define the query body\nquery_body = {\n    \"yql\": \"select * from sources * where userQuery();\",\n    \"query\": \"who wrote to kill a mockingbird?\",\n    \"timeout\": \"10s\",\n    \"input.query(q)\": \"embed(tokenizer, @query)\",\n    \"presentation.timing\": \"true\",\n}\n\n# Warm-up query\napp.query(body=query_body)\nquery_body_reranking = {\n    **query_body,\n    \"ranking.profile\": \"reranking\",\n}\n# Query with default persearch threads (set to 4)\nwith app.syncio() as sess:\n    response_default = app.query(body=query_body_reranking)\n\n# Query with num-threads-per-search overridden to 1\nquery_body_one_thread = {\n    **query_body,\n    \"ranking.profile\": \"one-thread-profile\",\n    # \"ranking.matching.numThreadsPerSearch\": 1, Could potentiall also set numThreadsPerSearch in query parameters.\n}\nwith app.syncio() as sess:\n    response_one_thread = sess.query(body=query_body_one_thread)\n\n# Extract query times\ntiming_default = response_default.json[\"timing\"][\"querytime\"]\ntiming_one_thread = response_one_thread.json[\"timing\"][\"querytime\"]\n# Beautifully formatted statement of - num threads and ratio of query times\nprint(f\"Query time with 4 threads: {timing_default:.2f}s\")\nprint(f\"Query time with 1 thread: {timing_one_thread:.2f}s\")\nratio = timing_one_thread / timing_default\nprint(f\"4 threads is approximately {ratio:.2f}x faster than 1 thread\")\n</pre> sample_docs = [     {\"id\": i, \"fields\": {\"text\": text}}     for i, text in enumerate(         [             \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature. The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird'. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",             \"was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961. Jane Austen was an English novelist known primarily for her six major novels, \",             \"which interpret, critique and comment upon the British landed gentry at the end of the 18th century. The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, \",             \"is among the most popular and critically acclaimed books of the modern era. 'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",         ]     ) ] app.feed_iterable(sample_docs, schema=\"doc\")  # Define the query body query_body = {     \"yql\": \"select * from sources * where userQuery();\",     \"query\": \"who wrote to kill a mockingbird?\",     \"timeout\": \"10s\",     \"input.query(q)\": \"embed(tokenizer, @query)\",     \"presentation.timing\": \"true\", }  # Warm-up query app.query(body=query_body) query_body_reranking = {     **query_body,     \"ranking.profile\": \"reranking\", } # Query with default persearch threads (set to 4) with app.syncio() as sess:     response_default = app.query(body=query_body_reranking)  # Query with num-threads-per-search overridden to 1 query_body_one_thread = {     **query_body,     \"ranking.profile\": \"one-thread-profile\",     # \"ranking.matching.numThreadsPerSearch\": 1, Could potentiall also set numThreadsPerSearch in query parameters. } with app.syncio() as sess:     response_one_thread = sess.query(body=query_body_one_thread)  # Extract query times timing_default = response_default.json[\"timing\"][\"querytime\"] timing_one_thread = response_one_thread.json[\"timing\"][\"querytime\"] # Beautifully formatted statement of - num threads and ratio of query times print(f\"Query time with 4 threads: {timing_default:.2f}s\") print(f\"Query time with 1 thread: {timing_one_thread:.2f}s\") ratio = timing_one_thread / timing_default print(f\"4 threads is approximately {ratio:.2f}x faster than 1 thread\") <pre>Query time with 4 threads: 0.73s\nQuery time with 1 thread: 1.24s\n4 threads is approximately 1.69x faster than 1 thread\n</pre> In\u00a0[16]: Copied! <pre>from vespa.package import (\n    QueryProfile,\n    QueryProfileType,\n    QueryTypeField,\n    QueryField,\n)\n\napp_package = ApplicationPackage(\n    name=f\"{application_name}\",\n    schema=[music_schema],\n    query_profile=QueryProfile(\n        fields=[\n            QueryField(\n                name=\"hits\",\n                value=\"30\",\n            )\n        ]\n    ),\n    query_profile_type=QueryProfileType(\n        fields=[\n            QueryTypeField(\n                name=\"ranking.features.query(query_embedding)\",\n                type=\"tensor&lt;float&gt;(x[512])\",\n            )\n        ]\n    ),\n)\n</pre> from vespa.package import (     QueryProfile,     QueryProfileType,     QueryTypeField,     QueryField, )  app_package = ApplicationPackage(     name=f\"{application_name}\",     schema=[music_schema],     query_profile=QueryProfile(         fields=[             QueryField(                 name=\"hits\",                 value=\"30\",             )         ]     ),     query_profile_type=QueryProfileType(         fields=[             QueryTypeField(                 name=\"ranking.features.query(query_embedding)\",                 type=\"tensor(x[512])\",             )         ]     ), ) <p>As you can see from the reference in the Vespa Docs, this makes it impossible to define multiple query profiles or query profile types in the application package, and there are many variants you are unable to express.</p> In\u00a0[17]: Copied! <pre>from vespa.configuration.query_profiles import *\n\n# From https://docs.vespa.ai/en/tutorials/rag-blueprint.html#training-a-first-phase-ranking-model\nqp_hybrid = query_profile(\n    field(\"doc\", name=\"schema\"),\n    field(\"embed(@query)\", name=\"ranking.features.query(embedding)\"),\n    field(\"embed(@query)\", name=\"ranking.features.query(float_embedding)\"),\n    field(-7.798639, name=\"ranking.features.query(intercept)\"),\n    field(\n        13.383840,\n        name=\"ranking.features.query(avg_top_3_chunk_sim_scores_param)\",\n    ),\n    field(\n        0.203145,\n        name=\"ranking.features.query(avg_top_3_chunk_text_scores_param)\",\n    ),\n    field(0.159914, name=\"ranking.features.query(bm25_chunks_param)\"),\n    field(0.191867, name=\"ranking.features.query(bm25_title_param)\"),\n    field(10.067169, name=\"ranking.features.query(max_chunk_sim_scores_param)\"),\n    field(0.153392, name=\"ranking.features.query(max_chunk_text_scores_param)\"),\n    field(\n        \"\"\"select *\n        from %{schema}\n        where userInput(@query) or\n        ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n        ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))\"\"\",\n        name=\"yql\",\n    ),\n    field(10, name=\"hits\"),\n    field(\"learned-linear\", name=\"ranking.profile\"),\n    field(\"top_3_chunks\", name=\"presentation.summary\"),\n    id=\"hybrid\",\n    type=\"hybrid-type\",\n)\n\nqpt_hybrid = query_profile_type(\n    field(\n        name=\"ranking.features.query(embedding)\",\n        type=\"tensor&lt;int8&gt;(x[96])\",\n        mandatory=True,\n        strict=True,\n    ),\n    field(\n        name=\"ranking.features.query(float_embedding)\",\n        type=\"tensor&lt;float&gt;(x[384])\",\n        mandatory=True,\n        strict=True,\n    ),\n    id=\"hybrid-type\",\n)\n</pre> from vespa.configuration.query_profiles import *  # From https://docs.vespa.ai/en/tutorials/rag-blueprint.html#training-a-first-phase-ranking-model qp_hybrid = query_profile(     field(\"doc\", name=\"schema\"),     field(\"embed(@query)\", name=\"ranking.features.query(embedding)\"),     field(\"embed(@query)\", name=\"ranking.features.query(float_embedding)\"),     field(-7.798639, name=\"ranking.features.query(intercept)\"),     field(         13.383840,         name=\"ranking.features.query(avg_top_3_chunk_sim_scores_param)\",     ),     field(         0.203145,         name=\"ranking.features.query(avg_top_3_chunk_text_scores_param)\",     ),     field(0.159914, name=\"ranking.features.query(bm25_chunks_param)\"),     field(0.191867, name=\"ranking.features.query(bm25_title_param)\"),     field(10.067169, name=\"ranking.features.query(max_chunk_sim_scores_param)\"),     field(0.153392, name=\"ranking.features.query(max_chunk_text_scores_param)\"),     field(         \"\"\"select *         from %{schema}         where userInput(@query) or         ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or         ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))\"\"\",         name=\"yql\",     ),     field(10, name=\"hits\"),     field(\"learned-linear\", name=\"ranking.profile\"),     field(\"top_3_chunks\", name=\"presentation.summary\"),     id=\"hybrid\",     type=\"hybrid-type\", )  qpt_hybrid = query_profile_type(     field(         name=\"ranking.features.query(embedding)\",         type=\"tensor(x[96])\",         mandatory=True,         strict=True,     ),     field(         name=\"ranking.features.query(float_embedding)\",         type=\"tensor(x[384])\",         mandatory=True,         strict=True,     ),     id=\"hybrid-type\", ) <p>As you can see below, we get type conversion (<code>True</code> -&gt; <code>true</code>), XML-escaping and correct indentaion of the XML outout.</p> In\u00a0[18]: Copied! <pre>print(qp_hybrid.to_xml())\n</pre> print(qp_hybrid.to_xml()) <pre>&lt;query-profile id=\"hybrid\" type=\"hybrid-type\"&gt;\n  &lt;field name=\"schema\"&gt;doc&lt;/field&gt;\n  &lt;field name=\"ranking.features.query(embedding)\"&gt;embed(@query)&lt;/field&gt;\n  &lt;field name=\"ranking.features.query(float_embedding)\"&gt;embed(@query)&lt;/field&gt;\n  &lt;field name=\"ranking.features.query(intercept)\"&gt;\n-7.798639\n  &lt;/field&gt;\n  &lt;field name=\"ranking.features.query(avg_top_3_chunk_sim_scores_param)\"&gt;\n13.38384\n  &lt;/field&gt;\n  &lt;field name=\"ranking.features.query(avg_top_3_chunk_text_scores_param)\"&gt;\n0.203145\n  &lt;/field&gt;\n  &lt;field name=\"ranking.features.query(bm25_chunks_param)\"&gt;\n0.159914\n  &lt;/field&gt;\n  &lt;field name=\"ranking.features.query(bm25_title_param)\"&gt;\n0.191867\n  &lt;/field&gt;\n  &lt;field name=\"ranking.features.query(max_chunk_sim_scores_param)\"&gt;\n10.067169\n  &lt;/field&gt;\n  &lt;field name=\"ranking.features.query(max_chunk_text_scores_param)\"&gt;\n0.153392\n  &lt;/field&gt;\n  &lt;field name=\"yql\"&gt;select *\n        from %{schema}\n        where userInput(@query) or\n        ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n        ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))&lt;/field&gt;\n  &lt;field name=\"hits\"&gt;1&lt;/field&gt;\n  &lt;field name=\"ranking.profile\"&gt;learned-linear&lt;/field&gt;\n  &lt;field name=\"presentation.summary\"&gt;top_3_chunks&lt;/field&gt;\n&lt;/query-profile&gt;\n\n</pre> In\u00a0[19]: Copied! <pre>print(qpt_hybrid.to_xml())\n</pre> print(qpt_hybrid.to_xml()) <pre>&lt;query-profile-type id=\"hybrid-type\"&gt;\n  &lt;field name=\"ranking.features.query(embedding)\" type=\"tensor&amp;lt;int8&amp;gt;(x[96])\" mandatory=\"true\" strict=\"true\"&gt;&lt;/field&gt;\n  &lt;field name=\"ranking.features.query(float_embedding)\" type=\"tensor&amp;lt;float&amp;gt;(x[384])\" mandatory=\"true\" strict=\"true\"&gt;&lt;/field&gt;\n&lt;/query-profile-type&gt;\n\n</pre> In\u00a0[20]: Copied! <pre>from vespa.configuration.query_profiles import *\n\nqp_variant = query_profile(\n    description(\"Multidimensional query profile\"),\n    dimensions(\"region,model,bucket\"),\n    field(\"My general a value\", name=\"a\"),\n    query_profile(for_=\"us,nokia,test1\")(\n        field(\"My value of the combination us-nokia-test1-a\", name=\"a\"),\n    ),\n    query_profile(for_=\"us\")(\n        field(\"My value of the combination us-a\", name=\"a\"),\n        field(\"My value of the combination us-b\", name=\"b\"),\n    ),\n    query_profile(for_=\"us,nokia,*\")(\n        field(\"My value of the combination us-nokia-a\", name=\"a\"),\n        field(\"My value of the combination us-nokia-b\", name=\"b\"),\n    ),\n    query_profile(for_=\"us,*,test1\")(\n        field(\"My value of the combination us-test1-a\", name=\"a\"),\n        field(\"My value of the combination us-test1-b\", name=\"b\"),\n    ),\n    id=\"multiprofile1\",\n)\n</pre> from vespa.configuration.query_profiles import *  qp_variant = query_profile(     description(\"Multidimensional query profile\"),     dimensions(\"region,model,bucket\"),     field(\"My general a value\", name=\"a\"),     query_profile(for_=\"us,nokia,test1\")(         field(\"My value of the combination us-nokia-test1-a\", name=\"a\"),     ),     query_profile(for_=\"us\")(         field(\"My value of the combination us-a\", name=\"a\"),         field(\"My value of the combination us-b\", name=\"b\"),     ),     query_profile(for_=\"us,nokia,*\")(         field(\"My value of the combination us-nokia-a\", name=\"a\"),         field(\"My value of the combination us-nokia-b\", name=\"b\"),     ),     query_profile(for_=\"us,*,test1\")(         field(\"My value of the combination us-test1-a\", name=\"a\"),         field(\"My value of the combination us-test1-b\", name=\"b\"),     ),     id=\"multiprofile1\", ) In\u00a0[21]: Copied! <pre>from vespa.configuration.query_profiles import *\n\nqpt_alias = query_profile_type(\n    match_(path=\"true\"),  # Match is sanitized due to python keyword\n    field(\n        name=\"ranking.features.query(query_embedding)\",\n        type=\"tensor&lt;float&gt;(x[512])\",\n        alias=\"q_emb query_emb\",\n    ),\n    id=\"queryemb\",\n    inherits=\"native\",\n)\n</pre> from vespa.configuration.query_profiles import *  qpt_alias = query_profile_type(     match_(path=\"true\"),  # Match is sanitized due to python keyword     field(         name=\"ranking.features.query(query_embedding)\",         type=\"tensor(x[512])\",         alias=\"q_emb query_emb\",     ),     id=\"queryemb\",     inherits=\"native\", ) <p>You can pass this configuration to the <code>ApplicationPackage</code> when creating it, and it will be included in the generated <code>services.xml</code> file. Or, you can add it to the <code>ApplicationPackage</code> after it has been created by using the <code>add_query_profile</code> method:</p> In\u00a0[22]: Copied! <pre>app_package.add_query_profile([qp_hybrid, qp_variant, qpt_hybrid, qpt_alias])\n</pre> app_package.add_query_profile([qp_hybrid, qp_variant, qpt_hybrid, qpt_alias]) <p>And by dumping the application package to files, we can see that all query profiles and query profile types are written to the <code>search/query-profiles</code> directory in the application package.</p> In\u00a0[23]: Copied! <pre>import tempfile\nimport os\n\ntemp_dir = tempfile.mkdtemp()\napp_package.to_files(temp_dir)\nprint(f\"Application package files written to {temp_dir}\")\nprint(\"Files in the temporary directory:\")\nprint(os.listdir(temp_dir))\nprint(\"Files in the `search/query-profiles` directory:\")\nprint(os.listdir(os.path.join(temp_dir, \"search\", \"query-profiles\")))\n</pre> import tempfile import os  temp_dir = tempfile.mkdtemp() app_package.to_files(temp_dir) print(f\"Application package files written to {temp_dir}\") print(\"Files in the temporary directory:\") print(os.listdir(temp_dir)) print(\"Files in the `search/query-profiles` directory:\") print(os.listdir(os.path.join(temp_dir, \"search\", \"query-profiles\"))) <pre>Application package files written to /var/folders/vb/ch14y_kn4mqfz75bhc9_g5980000gn/T/tmpyzrfju5a\nFiles in the temporary directory:\n['services.xml', 'models', 'schemas', 'search', 'files']\nFiles in the `search/query-profiles` directory:\n['types', 'multiprofile1.xml', 'hybrid.xml', 'default.xml']\n</pre> <p>Note that this combination of query profiles would not make sense to deploy together in the same application, but the point here is to demonstrate the flexibility of the new <code>query_profile_config</code> parameter, which should enable you to express any query profile or query profile type in python code, and add it to your <code>ApplicationPackage</code>.</p> <p>The following xml-tags are available to construct query profiles and query profile types:</p> In\u00a0[24]: Copied! <pre>queryprofile_tags\n</pre> queryprofile_tags Out[24]: <pre>['query-profile',\n 'query-profile-type',\n 'field',\n 'match',\n 'strict',\n 'description',\n 'dimensions',\n 'ref']</pre> <p>In order to avoid conflicts with Python reserved words, or commonly used objects, the following tags are (optionally) constructed by adding a <code>_</code> at the end of the tag name, or attribute name:</p> In\u00a0[25]: Copied! <pre>from vespa.configuration.vt import restore_reserved\n\nrestore_reserved\n</pre> from vespa.configuration.vt import restore_reserved  restore_reserved Out[25]: <pre>{'type_': 'type',\n 'class_': 'class',\n 'for_': 'for',\n 'time_': 'time',\n 'io_': 'io',\n 'from_': 'from',\n 'match_': 'match'}</pre> <p>Note that we also here must sanitize the names of the <code>match</code> tag to avoid any conflicts with Python keyword, so <code>match</code> should be passed as <code>match_</code>. Additionally, we use the same approach as for the <code>ServicesConfiguration</code> object, so any hyphens in the tag names should be replaced with underscores.</p> <p>The <code>deployment.xml</code> configuration is used to specify how your Vespa application should be deployed across different environments and regions. This only applies to Vespa Cloud deployments, where you can specify deployment targets, regions, and deployment policies. For complete deployment configuration reference, see the Vespa deployment.xml documentation.</p> <p>Similar to services.xml and query profiles, you can now express <code>deployment.xml</code> configuration using Python with the Vespa Tag (VT) syntax.</p> In\u00a0[26]: Copied! <pre>from vespa.configuration.deployment import deployment, prod, region\nfrom vespa.package import ApplicationPackage\n\n# Simple deployment to multiple regions\nsimple_deployment = deployment(\n    prod(region(\"aws-us-east-1c\"), region(\"aws-us-west-2a\")), version=\"1.0\"\n)\n\napp_package = ApplicationPackage(name=\"myapp\", deployment_config=simple_deployment)\n</pre> from vespa.configuration.deployment import deployment, prod, region from vespa.package import ApplicationPackage  # Simple deployment to multiple regions simple_deployment = deployment(     prod(region(\"aws-us-east-1c\"), region(\"aws-us-west-2a\")), version=\"1.0\" )  app_package = ApplicationPackage(name=\"myapp\", deployment_config=simple_deployment) <p>This configuration will generate a <code>deployment.xml</code> file that looks like this:</p> In\u00a0[28]: Copied! <pre>print(app_package.deployment_config.to_xml())\n</pre> print(app_package.deployment_config.to_xml()) <pre>&lt;deployment version=\"1.0\"&gt;\n  &lt;prod&gt;\n    &lt;region&gt;aws-us-east-1c&lt;/region&gt;\n    &lt;region&gt;aws-us-west-2a&lt;/region&gt;\n  &lt;/prod&gt;\n&lt;/deployment&gt;\n</pre> In\u00a0[29]: Copied! <pre>from vespa.configuration.deployment import (\n    deployment,\n    instance,\n    prod,\n    region,\n    block_change,\n    delay,\n    parallel,\n    steps,\n    endpoints,\n    endpoint,\n)\n\n# Complex deployment with multiple instances and advanced policies\ncomplex_deployment = deployment(\n    # Beta instance - simple deployment\n    instance(prod(region(\"aws-us-east-1c\")), id=\"beta\"),\n    # Default instance with advanced configuration\n    instance(\n        # Block changes during specific time windows\n        block_change(\n            revision=\"false\", days=\"mon,wed-fri\", hours=\"16-23\", time_zone=\"UTC\"\n        ),\n        prod(\n            # First region\n            region(\"aws-us-east-1c\"),\n            # Delay before next deployment\n            delay(hours=\"3\", minutes=\"7\", seconds=\"13\"),\n            # Parallel deployment to multiple regions\n            parallel(\n                region(\"aws-us-west-1c\"),\n                # Sequential steps within parallel block\n                steps(region(\"aws-eu-west-1a\"), delay(hours=\"3\")),\n            ),\n        ),\n        # Configure endpoints for this instance\n        endpoints(\n            endpoint(region(\"aws-us-east-1c\"), container_id=\"my-container-service\")\n        ),\n        id=\"default\",\n    ),\n    # Global endpoints across instances\n    endpoints(\n        endpoint(\n            instance(\"beta\", weight=\"1\"),\n            id=\"my-weighted-endpoint\",\n            container_id=\"my-container-service\",\n            region=\"aws-us-east-1c\",\n        )\n    ),\n    version=\"1.0\",\n)\n\napp_package = ApplicationPackage(name=\"myapp\", deployment_config=complex_deployment)\n</pre> from vespa.configuration.deployment import (     deployment,     instance,     prod,     region,     block_change,     delay,     parallel,     steps,     endpoints,     endpoint, )  # Complex deployment with multiple instances and advanced policies complex_deployment = deployment(     # Beta instance - simple deployment     instance(prod(region(\"aws-us-east-1c\")), id=\"beta\"),     # Default instance with advanced configuration     instance(         # Block changes during specific time windows         block_change(             revision=\"false\", days=\"mon,wed-fri\", hours=\"16-23\", time_zone=\"UTC\"         ),         prod(             # First region             region(\"aws-us-east-1c\"),             # Delay before next deployment             delay(hours=\"3\", minutes=\"7\", seconds=\"13\"),             # Parallel deployment to multiple regions             parallel(                 region(\"aws-us-west-1c\"),                 # Sequential steps within parallel block                 steps(region(\"aws-eu-west-1a\"), delay(hours=\"3\")),             ),         ),         # Configure endpoints for this instance         endpoints(             endpoint(region(\"aws-us-east-1c\"), container_id=\"my-container-service\")         ),         id=\"default\",     ),     # Global endpoints across instances     endpoints(         endpoint(             instance(\"beta\", weight=\"1\"),             id=\"my-weighted-endpoint\",             container_id=\"my-container-service\",             region=\"aws-us-east-1c\",         )     ),     version=\"1.0\", )  app_package = ApplicationPackage(name=\"myapp\", deployment_config=complex_deployment) <p>And the generated <code>deployment.xml</code> will include all specified configurations:</p> In\u00a0[30]: Copied! <pre>print(app_package.deployment_config.to_xml())\n</pre> print(app_package.deployment_config.to_xml()) <pre>&lt;deployment version=\"1.0\"&gt;\n  &lt;instance id=\"beta\"&gt;\n    &lt;prod&gt;\n      &lt;region&gt;aws-us-east-1c&lt;/region&gt;\n    &lt;/prod&gt;\n  &lt;/instance&gt;\n  &lt;instance id=\"default\"&gt;\n    &lt;block-change revision=\"false\" days=\"mon,wed-fri\" hours=\"16-23\" time-zone=\"UTC\"&gt;&lt;/block-change&gt;\n    &lt;prod&gt;\n      &lt;region&gt;aws-us-east-1c&lt;/region&gt;\n      &lt;delay hours=\"3\" minutes=\"7\" seconds=\"13\"&gt;&lt;/delay&gt;\n      &lt;parallel&gt;\n        &lt;region&gt;aws-us-west-1c&lt;/region&gt;\n        &lt;steps&gt;\n          &lt;region&gt;aws-eu-west-1a&lt;/region&gt;\n          &lt;delay hours=\"3\"&gt;&lt;/delay&gt;\n        &lt;/steps&gt;\n      &lt;/parallel&gt;\n    &lt;/prod&gt;\n    &lt;endpoints&gt;\n      &lt;endpoint container-id=\"my-container-service\"&gt;\n        &lt;region&gt;aws-us-east-1c&lt;/region&gt;\n      &lt;/endpoint&gt;\n    &lt;/endpoints&gt;\n  &lt;/instance&gt;\n  &lt;endpoints&gt;\n    &lt;endpoint id=\"my-weighted-endpoint\" container-id=\"my-container-service\" region=\"aws-us-east-1c\"&gt;\n      &lt;instance weight=\"1\"&gt;beta&lt;/instance&gt;\n    &lt;/endpoint&gt;\n  &lt;/endpoints&gt;\n&lt;/deployment&gt;\n</pre> <pre>\n</pre> <p>This advanced configuration generates a comprehensive <code>deployment.xml</code> with:</p> <ul> <li>Multiple application instances (beta and default)</li> <li>Upgrade blocking windows to prevent deployments during peak hours</li> <li>Deployment delays and parallel deployment strategies</li> <li>Regional and cross-instance endpoint configurations</li> </ul> <p>To see the available tags for each configuration category, you can print the corresponding tag lists:</p> In\u00a0[31]: Copied! <pre>from vespa.configuration.deployment import deployment_tags\nfrom vespa.configuration.query_profiles import queryprofile_tags\nfrom vespa.configuration.services import services_tags\n\nprint(deployment_tags)\nprint(queryprofile_tags)\nprint(services_tags)\n</pre> from vespa.configuration.deployment import deployment_tags from vespa.configuration.query_profiles import queryprofile_tags from vespa.configuration.services import services_tags  print(deployment_tags) print(queryprofile_tags) print(services_tags) <pre>['deployment', 'instance', 'prod', 'region', 'block-change', 'delay', 'parallel', 'steps', 'endpoints', 'endpoint', 'staging']\n['query-profile', 'query-profile-type', 'field', 'match', 'strict', 'description', 'dimensions', 'ref']\n['abortondocumenterror', 'accesslog', 'admin', 'adminserver', 'age', 'binding', 'bucket-splitting', 'cache', 'certificate', 'chain', 'chunk', 'client', 'clients', 'cluster-controller', 'clustercontroller', 'clustercontrollers', 'component', 'components', 'compression', 'concurrency', 'config', 'configserver', 'configservers', 'conservative', 'container', 'content', 'coverage', 'disk', 'disk-limit-factor', 'diskbloatfactor', 'dispatch', 'dispatch-policy', 'distribution', 'document', 'document-api', 'document-processing', 'document-token-id', 'documentprocessor', 'documents', 'engine', 'environment-variables', 'execution-mode', 'federation', 'feeding', 'filtering', 'flush-on-shutdown', 'flushstrategy', 'gpu', 'gpu-device', 'group', 'groups-allowed-down-ratio', 'handler', 'http', 'ignore-undefined-fields', 'include', 'index', 'init-progress-time', 'initialize', 'interop-threads', 'interval', 'intraop-threads', 'io', 'jvm', 'level', 'lidspace', 'logstore', 'maintenance', 'max-bloat-factor', 'max-concurrent', 'max-document-tokens', 'max-hits-per-partition', 'max-premature-crashes', 'max-query-tokens', 'max-tokens', 'max-wait-after-coverage-factor', 'maxage', 'maxfilesize', 'maxmemorygain', 'maxpendingbytes', 'maxpendingdocs', 'maxsize', 'maxsize-percent', 'mbusport', 'memory', 'memory-limit-factor', 'merges', 'min-active-docs-coverage', 'min-distributor-up-ratio', 'min-node-ratio-per-group', 'min-redundancy', 'min-storage-up-ratio', 'min-wait-after-coverage-factor', 'minimum', 'model', 'model-evaluation', 'models', 'native', 'niceness', 'node', 'nodes', 'onnx', 'onnx-execution-mode', 'onnx-gpu-device', 'onnx-interop-threads', 'onnx-intraop-threads', 'persearch', 'persistence-threads', 'pooling-strategy', 'prepend', 'processing', 'processor', 'proton', 'provider', 'prune', 'query', 'query-timeout', 'query-token-id', 'read', 'redundancy', 'removed-db', 'renderer', 'requestthreads', 'resource-limits', 'resources', 'retrydelay', 'retryenabled', 'route', 'search', 'searchable-copies', 'searcher', 'searchnode', 'secret-store', 'server', 'services', 'slobrok', 'slobroks', 'stable-state-period', 'store', 'summary', 'sync-transactionlog', 'term-score-threshold', 'threadpool', 'threads', 'time', 'timeout', 'token', 'tokenizer-model', 'top-k-probability', 'total', 'tracelevel', 'transactionlog', 'transformer-attention-mask', 'transformer-end-sequence-token', 'transformer-input-ids', 'transformer-mask-token', 'transformer-model', 'transformer-output', 'transformer-pad-token', 'transformer-start-sequence-token', 'transition-time', 'tuning', 'type', 'unpack', 'visibility-delay', 'visitors', 'warmup', 'zookeeper']\n</pre> <p>Note that any attribute can be passed to the tag constructor, with no validation at construction time. You will still get validation at deploy time as usual though.</p> In\u00a0[32]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"advanced-configuration.html#advanced-configuration","title":"Advanced Configuration\u00b6","text":"<p>This notebook demonstrates how to use pyvespa's advanced configuration features to customize Vespa applications beyond the basic settings. You'll learn to express Vespa's XML configuration files using Python code for greater flexibility and control.</p>"},{"location":"advanced-configuration.html#what-youll-learn","title":"What you'll learn\u00b6","text":"<ol> <li><p>services.xml Configuration - Configure <code>services.xml</code> using the <code>ServicesConfiguration</code> object to customize system behavior (document expiry, threading, tuning parameters). Available since <code>pyvespa=0.50.0</code></p> </li> <li><p>Query Profiles Configuration - Define multiple query profiles and query profile types programmatically using the new configuration approach. Available since <code>pyvespa=0.60.0</code></p> </li> <li><p>deployment.xml Configuration - Configure deployment zones, regions and windows to block upgrades. Applicable for Vespa Cloud only. Available since <code>pyvespa=0.60.0</code></p> </li> </ol>"},{"location":"advanced-configuration.html#why","title":"Why?\u00b6","text":"<p>pyvespa has proven to be a preferred framework for deploying and managing Vespa applications. With the legacy configuration methods, not all possible configurations were available. The new approach ensures full feature parity with the XML configuration options.</p>"},{"location":"advanced-configuration.html#configuration-approach","title":"Configuration Approach\u00b6","text":"<p>The <code>vespa.configuration</code> modules in pyvespa provides a Vespa Tag (VT) system that mirrors Vespa's XML configuration structure:</p> <ul> <li>Tags: Python functions representing XML elements (e.g., <code>container()</code>, <code>content()</code>, <code>query_profile()</code>)</li> <li>Attributes: Function parameters that become XML attributes (hyphens become underscores: <code>garbage-collection</code> \u2192 <code>garbage_collection</code>)</li> <li>Values: Automatic type conversion and XML escaping (no manual escaping needed)</li> <li>Structure: Nested function calls create the XML hierarchy</li> </ul> <p>Example: This Python code:</p> <pre>service_config = ServicesConfiguration(\n  name=\"myapp\",\n  container(id=\"myapp_container\", version=\"1.0\")(\n      search(),\n      document_api()\n  )\n)\nservice_config.to_xml()\n</pre> <p>Generates this XML:</p> <pre>&lt;services&gt;\n  &lt;container id=\"myapp_container\" version=\"1.0\"&gt;\n    &lt;search&gt;&lt;/search&gt;\n    &lt;document-api&gt;&lt;/document-api&gt;\n  &lt;/container&gt;\n&lt;/services&gt;\n</pre>"},{"location":"advanced-configuration.html#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>pyvespa installed and Docker running with at least 6GB memory</li> <li>Understanding of basic Vespa concepts (schemas, deployment)</li> </ul> <p>For detailed XML configuration options, refer to:</p> <ul> <li>Vespa services.xml reference</li> <li>Query profiles reference</li> <li>Deployment reference</li> </ul>"},{"location":"advanced-configuration.html#servicesxml-configuration","title":"services.xml Configuration\u00b6","text":""},{"location":"advanced-configuration.html#example-1-configure-document-expiry","title":"Example 1 - Configure document-expiry\u00b6","text":"<p>As an example of a common use case for advanced configuration, we will configure document-expiry. This feature allows you to set a time-to-live for documents in your Vespa application. This is useful when you have documents that are only relevant for a certain period of time, and you want to avoid serving stale data.</p> <p>For reference, see the docs on document-expiry.</p>"},{"location":"advanced-configuration.html#define-a-schema","title":"Define a schema\u00b6","text":"<p>We define a simple schema, with a timestamp field that we will use in the document selection expression to set the document-expiry.</p> <p>Note that the fields that are referenced in the selection expression should be attributes(in-memory).</p> <p>Also, either the fields should be set with <code>fast-access</code> or the number of searchable copies in the content cluster should be the same as the redundancy. Otherwise, the document selection maintenance will be slow and have a major performance impact on the system.</p>"},{"location":"advanced-configuration.html#the-servicesconfiguration-object","title":"The <code>ServicesConfiguration</code> object\u00b6","text":"<p>The <code>ServicesConfiguration</code> object allows you to define any configuration you want in the <code>services.xml</code> file.</p> <p>The syntax is as follows:</p>"},{"location":"advanced-configuration.html#tag-names","title":"Tag names\u00b6","text":"<p>All tags as referenced in the Vespa documentation are available in <code>vespa.configuration.{services,query_profiles,deployment}</code> modules with the following modifications:</p> <ul> <li>All <code>-</code> in the tag names are replaced by <code>_</code> to avoid conflicts with Python syntax.</li> <li>Some tags that are Python reserved words (or commonly used objects) are constructed by adding a <code>_</code> at the end of the tag name. To see which tags are affected, you can check this variable:</li> </ul>"},{"location":"advanced-configuration.html#attributes","title":"Attributes\u00b6","text":"<ul> <li>any attribute can be passed to the tag constructor (no validation at construction time).</li> <li>The attribute name should be the same as in the Vespa documentation, but with <code>-</code> replaced by <code>_</code>. For example, the <code>garbage-collection</code> attribute in the <code>query</code> tag should be passed as <code>garbage_collection</code>.</li> <li>In case the attribute name is a Python reserved word, the same rule as for the tag names applies (add <code>_</code> at the end). An example of this is the <code>global</code> attribute which should be passed as <code>global_</code>.</li> <li>Some attributes, such as <code>id</code>, in the <code>container</code> tag, are mandatory and should be passed as positional arguments to the tag constructor.</li> </ul>"},{"location":"advanced-configuration.html#values","title":"Values\u00b6","text":"<ul> <li>The value of an attribute can be a string, an integer, or a boolean. For types <code>bool</code> and <code>int</code>, the value is converted to a string (lowercased for <code>bool</code>). If you need to pass a float, you should convert it to a string before passing it to the tag constructor, e.g. <code>container(version=\"1.0\")</code>.</li> <li>Note that we are not escaping the values. In the xml file, the value of the <code>selection</code> attribute in the <code>document</code> tag is <code>music.timestamp &amp;gt; now() - 86400</code>. (<code>&amp;gt;</code> is the escaped form of <code>&gt;</code>.) When passing this value to the <code>document</code> tag constructor in python, we should not escape the <code>&gt;</code> character, i.e. <code>document(selection=\"music.timestamp &gt; now() - 86400\")</code>.</li> </ul>"},{"location":"advanced-configuration.html#deploy-the-vespa-application","title":"Deploy the Vespa application\u00b6","text":"<p>Deploy <code>package</code> on the local machine using Docker, without leaving the notebook, by creating an instance of VespaDocker. <code>VespaDocker</code> connects to the local Docker daemon socket and starts the Vespa docker image.</p> <p>If this step fails, please check that the Docker daemon is running, and that the Docker daemon socket can be used by clients (Configurable under advanced settings in Docker Desktop).</p>"},{"location":"advanced-configuration.html#feeding-documents-to-vespa","title":"Feeding documents to Vespa\u00b6","text":"<p>Now, let us feed some documents to Vespa. We will feed one document with a timestamp of 24 hours (+1 sec (86401)) ago and another document with a timestamp of the current time. We will then query the documents to check verify that the document-expiry is working as expected.</p>"},{"location":"advanced-configuration.html#verify-document-expiry-through-visiting","title":"Verify document expiry through visiting\u00b6","text":"<p>Visiting is a feature to efficiently get or process a set of documents, identified by a document selection expression. Here is how you can use visiting in pyvespa:</p>"},{"location":"advanced-configuration.html#clean-up","title":"Clean up\u00b6","text":""},{"location":"advanced-configuration.html#example-2-configuring-requestthreads-per-search","title":"Example 2 - Configuring <code>requestthreads</code> per search\u00b6","text":"<p>In Vespa, there are several configuration options that might be tuned to optimize the serving latency of your application. For an overview, see the Vespa documentation - Vespa Serving Scaling Guide. An example of a configuration that one might want to tune is the <code>requestthreads</code> <code>persearch</code> parameter. This parameter controls the number of search threads that are used to handle each search on the content nodes. The default value is 1.</p> <p>For some applications, where a significant portion of the work per query is linear with the number of documents, increasing the number of <code>requestthreads</code> <code>persearch</code> can improve the serving latency, as it allows more parallelism in the search phase.</p> <p>Examples of potentially expensive work that scales linearly with the number of documents, and thus are likely to benefit from increasing <code>requestthreads</code> <code>persearch</code> are: - Xgboost inference with a large GDBT-model - ONNX inference, e.g with a crossencoder. - MaxSim-operations for late interaction scoring, as in ColBERT and ColPali. - Exact nearest neighbor search.</p> <p>Example of query operators that are less likely to benefit from increasing <code>requestthreads</code> <code>persearch</code> are: - <code>wand</code>/<code>weakAnd</code>, see Using wand with Vespa. - Approximate nearest neighbor search with HNSW.</p> <p>In this example, we will demonstrate an example of configuring <code>requestthreads</code> <code>persearch</code> to 4 for an application where a Crossencoder is used in first-phase ranking. The demo is based on the Cross-encoders for global reranking guide, but here we will use a cross-encoder in first-phase instead of global-phase. First-phase and second-phase ranking are executed on the content nodes, while global-phase ranking is executed on the container node. See Phased ranking for more details.</p>"},{"location":"advanced-configuration.html#download-the-crossencoder-model","title":"Download the crossencoder-model\u00b6","text":""},{"location":"advanced-configuration.html#define-a-schema","title":"Define a schema\u00b6","text":""},{"location":"advanced-configuration.html#define-the-servicesconfiguration","title":"Define the ServicesConfiguration\u00b6","text":"<p>Note that the ServicesConfiguration may be used to define any configuration in the <code>services.xml</code> file. In this example, we are only configuring the <code>requestthreads</code> <code>persearch</code> parameter, but you can use the same approach to configure any other parameter.</p> <p>For a full reference of the available configuration options, see the Vespa documentation - services.xml.</p>"},{"location":"advanced-configuration.html#deploy-the-application-package","title":"Deploy the application package\u00b6","text":""},{"location":"advanced-configuration.html#feed-some-sample-documents","title":"Feed some sample documents\u00b6","text":""},{"location":"advanced-configuration.html#query-profile-configuration","title":"Query-profile Configuration\u00b6","text":"<p>Until pyvespa version 0.60.0, this was the way to add a query profile or query profile type to the application package:</p>"},{"location":"advanced-configuration.html#query-profiles-new-approach","title":"Query-profiles - new approach\u00b6","text":"<p>By importing the tag-functions like this:  <code>from vespa.configuration.query_profiles import *</code>, you can access all supported tags of a query profile or query profile type.</p> <p>Pass these (one or as many as you like) to the <code>query_profile_config</code> parameter of your <code>ApplicationPackage</code>, and they will be added to the application package as query profiles or query profile types.</p> <p>Only two validations are done at construction time:</p> <ol> <li>The <code>id</code> attribute is mandatory for both query profiles and query profile types, as it is used to create the file name in the application package.</li> <li>The top-level tag of each element in the <code>query_profile_config</code> list should be either <code>query_profile</code> or <code>query_profile_type</code>.</li> </ol> <p>By using the new <code>query_profile_config</code> parameter, you can now express any combination of query profile or query profile type in python code, and add it to your <code>ApplicationPackage</code>.</p> <p>Here are some examples:</p>"},{"location":"advanced-configuration.html#query-profile-variant","title":"Query profile variant\u00b6","text":"<p>See Vespa documentation on Query Profile Variants for more details.</p>"},{"location":"advanced-configuration.html#configuring-deploymentxml","title":"Configuring Deployment.xml\u00b6","text":""},{"location":"advanced-configuration.html#simple-deployment-configuration","title":"Simple deployment configuration\u00b6","text":"<p>Here's a basic example that deploys to two production regions:</p>"},{"location":"advanced-configuration.html#advanced-deployment-configuration","title":"Advanced deployment configuration\u00b6","text":"<p>For more complex scenarios, you can configure multiple instances, deployment delays, upgrade blocking windows, and endpoints:</p>"},{"location":"advanced-configuration.html#no-proper-validation-until-deploy-time","title":"No proper validation until deploy time\u00b6","text":""},{"location":"advanced-configuration.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"advanced-configuration.html#next-steps","title":"Next steps\u00b6","text":"<p>This is just an intro into to the advanced configuration options available in Vespa. For more details, see the Vespa documentation.</p>"},{"location":"application-packages.html","title":"Application packages","text":"Refer to troubleshooting     for any problem when running this guide.  In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa\n</pre> !pip3 install pyvespa <p>By exporting to disk, one can see the generated files:</p> In\u00a0[50]: Copied! <pre>import os\nimport tempfile\nfrom pathlib import Path\nfrom vespa.package import ApplicationPackage\n\napp_name = \"myschema\"\napp_package = ApplicationPackage(name=app_name, create_query_profile_by_default=False)\n\ntemp_dir = tempfile.TemporaryDirectory()\napp_package.to_files(temp_dir.name)\n\nfor p in Path(temp_dir.name).rglob(\"*\"):\n    if p.is_file():\n        print(p)\n</pre> import os import tempfile from pathlib import Path from vespa.package import ApplicationPackage  app_name = \"myschema\" app_package = ApplicationPackage(name=app_name, create_query_profile_by_default=False)  temp_dir = tempfile.TemporaryDirectory() app_package.to_files(temp_dir.name)  for p in Path(temp_dir.name).rglob(\"*\"):     if p.is_file():         print(p) <pre>/var/folders/9_/z105jyln7jz8h2vwsrjb7kxh0000gp/T/tmp6geo2dpg/services.xml\n/var/folders/9_/z105jyln7jz8h2vwsrjb7kxh0000gp/T/tmp6geo2dpg/schemas/myschema.sd\n</pre> In\u00a0[51]: Copied! <pre>os.environ[\"TMP_APP_DIR\"] = temp_dir.name\nos.environ[\"APP_NAME\"] = app_name\n\n!cat $TMP_APP_DIR/schemas/$APP_NAME.sd\n</pre> os.environ[\"TMP_APP_DIR\"] = temp_dir.name os.environ[\"APP_NAME\"] = app_name  !cat $TMP_APP_DIR/schemas/$APP_NAME.sd <pre>schema myschema {\r\n    document myschema {\r\n    }\r\n}</pre> <p>Configure the schema with fields, fieldsets and a ranking function:</p> In\u00a0[52]: Copied! <pre>from vespa.package import Field, FieldSet, RankProfile\n\napp_package.schema.add_fields(\n    Field(name=\"id\", type=\"string\", indexing=[\"attribute\", \"summary\"]),\n    Field(\n        name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"body\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n)\n\napp_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"title\", \"body\"]))\n\napp_package.schema.add_rank_profile(\n    RankProfile(name=\"default\", first_phase=\"bm25(title) + bm25(body)\")\n)\n</pre> from vespa.package import Field, FieldSet, RankProfile  app_package.schema.add_fields(     Field(name=\"id\", type=\"string\", indexing=[\"attribute\", \"summary\"]),     Field(         name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ),     Field(         name=\"body\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ), )  app_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"title\", \"body\"]))  app_package.schema.add_rank_profile(     RankProfile(name=\"default\", first_phase=\"bm25(title) + bm25(body)\") ) <p>Export the application package again, show schema:</p> In\u00a0[53]: Copied! <pre>app_package.to_files(temp_dir.name)\n\n!cat $TMP_APP_DIR/schemas/$APP_NAME.sd\n</pre> app_package.to_files(temp_dir.name)  !cat $TMP_APP_DIR/schemas/$APP_NAME.sd <pre>schema myschema {\r\n    document myschema {\r\n        field id type string {\r\n            indexing: attribute | summary\r\n        }\r\n        field title type string {\r\n            indexing: index | summary\r\n            index: enable-bm25\r\n        }\r\n        field body type string {\r\n            indexing: index | summary\r\n            index: enable-bm25\r\n        }\r\n    }\r\n    fieldset default {\r\n        fields: title, body\r\n    }\r\n    rank-profile default {\r\n        first-phase {\r\n            expression {\r\n                bm25(title) + bm25(body)\r\n            }\r\n        }\r\n    }\r\n}</pre> In\u00a0[54]: Copied! <pre>!cat $TMP_APP_DIR/services.xml\n</pre> !cat $TMP_APP_DIR/services.xml <pre>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\r\n&lt;services version=\"1.0\"&gt;\r\n    &lt;container id=\"myschema_container\" version=\"1.0\"&gt;\r\n        &lt;search&gt;&lt;/search&gt;\r\n        &lt;document-api&gt;&lt;/document-api&gt;\r\n    &lt;/container&gt;\r\n    &lt;content id=\"myschema_content\" version=\"1.0\"&gt;\r\n        &lt;redundancy reply-after=\"1\"&gt;1&lt;/redundancy&gt;\r\n        &lt;documents&gt;\r\n            &lt;document type=\"myschema\" mode=\"index\"&gt;&lt;/document&gt;\r\n        &lt;/documents&gt;\r\n        &lt;nodes&gt;\r\n            &lt;node distribution-key=\"0\" hostalias=\"node1\"&gt;&lt;/node&gt;\r\n        &lt;/nodes&gt;\r\n    &lt;/content&gt;\r\n&lt;/services&gt;</pre> <p>Observe:</p> <ul> <li>A content cluster (this is where the index is stored) called <code>myschema_content</code> is created. This is information not normally needed, unless using delete_all_docs to quickly remove all documents from a schema</li> </ul> In\u00a0[55]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_container = VespaDocker()\nvespa_connection = vespa_container.deploy(application_package=app_package)\n</pre> from vespa.deployment import VespaDocker  vespa_container = VespaDocker() vespa_connection = vespa_container.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nWaiting for application status, 10/300 seconds...\nWaiting for application status, 15/300 seconds...\nWaiting for application status, 20/300 seconds...\nWaiting for application status, 25/300 seconds...\nFinished deployment.\n</pre> In\u00a0[56]: Copied! <pre>%%sh\ncat &lt;&lt; EOF &gt; $TMP_APP_DIR/services.xml\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;services version=\"1.0\"&gt;\n    &lt;container id=\"${APP_NAME}_container\" version=\"1.0\"&gt;\n        &lt;search&gt;&lt;/search&gt;\n        &lt;document-api&gt;&lt;/document-api&gt;\n    &lt;/container&gt;\n    &lt;content id=\"${APP_NAME}_content\" version=\"1.0\"&gt;\n        &lt;redundancy reply-after=\"1\"&gt;1&lt;/redundancy&gt;\n        &lt;documents&gt;\n            &lt;document type=\"${APP_NAME}\" mode=\"index\"&gt;&lt;/document&gt;\n        &lt;/documents&gt;\n        &lt;nodes&gt;\n            &lt;node distribution-key=\"0\" hostalias=\"node1\"&gt;&lt;/node&gt;\n        &lt;/nodes&gt;\n        &lt;tuning&gt;\n            &lt;resource-limits&gt;\n                &lt;disk&gt;0.90&lt;/disk&gt;\n            &lt;/resource-limits&gt;\n        &lt;/tuning&gt;\n    &lt;/content&gt;\n&lt;/services&gt;\nEOF\n</pre> %%sh cat &lt;&lt; EOF &gt; $TMP_APP_DIR/services.xml  1 0.90  EOF <p>The resource-limits in <code>tuning/resource-limits/disk</code> configuration setting allows a higher disk usage.</p> <p>Deploy using the exported files:</p> In\u00a0[57]: Copied! <pre>vespa_connection = vespa_container.deploy_from_disk(\n    application_name=app_name, application_root=temp_dir.name\n)\n</pre> vespa_connection = vespa_container.deploy_from_disk(     application_name=app_name, application_root=temp_dir.name ) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nWaiting for application status, 0/300 seconds...\nWaiting for application status, 5/300 seconds...\nFinished deployment.\n</pre> <p>One can also export a deployable zip-file, which can be deployed using the Vespa Cloud Console:</p> In\u00a0[58]: Copied! <pre>Path.mkdir(Path(temp_dir.name) / \"zip\", exist_ok=True, parents=True)\napp_package.to_zipfile(temp_dir.name + \"/zip/application.zip\")\n\n! find \"$TMP_APP_DIR/zip\" -type f\n</pre> Path.mkdir(Path(temp_dir.name) / \"zip\", exist_ok=True, parents=True) app_package.to_zipfile(temp_dir.name + \"/zip/application.zip\")  ! find \"$TMP_APP_DIR/zip\" -type f <pre>/var/folders/9_/z105jyln7jz8h2vwsrjb7kxh0000gp/T/tmp6geo2dpg/zip/application.zip\r\n</pre> In\u00a0[59]: Copied! <pre>temp_dir.cleanup()\nvespa_container.container.stop()\nvespa_container.container.remove()\n</pre> temp_dir.cleanup() vespa_container.container.stop() vespa_container.container.remove()"},{"location":"application-packages.html#application-packages","title":"Application packages\u00b6","text":"<p>Vespa is configured using an application package. Pyvespa provides an API to generate a deployable application package. An application package has at a minimum a schema and services.xml.</p> <p>NOTE: pyvespa generally does not support all indexing options in Vespa - it is made for easy experimentation. To configure setting an unsupported indexing option (or any other unsupported option), export the application package like above, modify the schema or other files and deploy the application package from the directory, or as a zipped file. Find more details at the end of this notebook.</p>"},{"location":"application-packages.html#schema","title":"Schema\u00b6","text":"<p>A schema is created with the same name as the application package:</p>"},{"location":"application-packages.html#services","title":"Services\u00b6","text":"<p><code>services.xml</code> configures container and content clusters - see the Vespa Overview. This is a file you will normally not change or need to know much about:</p>"},{"location":"application-packages.html#deploy","title":"Deploy\u00b6","text":"<p>After completing the code for the fields and ranking, deploy the application into a Docker container - the container is started by pyvespa:</p>"},{"location":"application-packages.html#deploy-from-modified-files","title":"Deploy from modified files\u00b6","text":"<p>To add configuration the the schema, which is not supported by the pyvespa code, export the files, modify, then deploy by using <code>deploy_from_disk</code>. This example adds custom configuration to the <code>services.xml</code> file above and deploys it:</p>"},{"location":"application-packages.html#cleanup","title":"Cleanup\u00b6","text":"<p>Remove the container resources and temporary application package file export:</p>"},{"location":"application-packages.html#next-step-deploy-feed-and-query","title":"Next step: Deploy, feed and query\u00b6","text":"<p>Once the schema is ready for deployment, decide deployment option and deploy the application package:</p> <ul> <li>Deploy to local container</li> <li>Deploy to Vespa Cloud</li> </ul> <p>Use the guides on the pyvespa site to feed and query data.</p>"},{"location":"authenticating-to-vespa-cloud.html","title":"Authenticating to Vespa Cloud","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Pre-requisite: Create a tenant at cloud.vespa.ai, save the tenant name.</p> <p></p> In\u00a0[1]: Copied! <pre>!pip3 install pyvespa vespacli\n</pre> !pip3 install pyvespa vespacli <p>For background context, it is useful to read the Vespa Cloud Security Guide.</p> Control-plane Data-plane Comments Deploy application \u2705 \u274c Modify application (re-deploy) \u2705 \u274c Add or modify data-plane certs or token(s) \u2705 \u274c Feed data \u274c \u2705 Query data \u274c \u2705 Delete data \u274c \u2705 Visiting \u274c \u2705 Monitoring \u274c \u2705 Get application package \u2705 \u274c vespa auth login \u2705 \u274c Interactive control-plane login in browser vespa auth api-key \u2705 \u274c Headless control-plane authentication with an API key generated in the Vespa Cloud console vespa auth cert \u274c \u2705 Used to generate a certificate for a data-plane connection VespaCloud \u2705 \u274c `VespaCloud` is a control-plane connection to Vespa Cloud VespaDocker \u2705 \u274c `VespaDocker` is a control-plane connection to a Vespa server running in Docker Vespa \u274c \u2705 `Vespa` is a data-plane connection to an existing Vespa application In\u00a0[2]: Copied! <pre># Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n# Replace with your application name (does not need to exist yet)\napplication = \"authnotebook\"\n</pre> # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\" # Replace with your application name (does not need to exist yet) application = \"authnotebook\" In\u00a0[3]: Copied! <pre>from vespa.package import ApplicationPackage, Field, Schema, Document\n\nschema_name = \"doc\"\n\nschema = Schema(\n    name=schema_name,\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"body\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                index=\"enable-bm25\",\n            ),\n        ]\n    ),\n)\n\npackage = ApplicationPackage(name=application, schema=[schema])\n</pre> from vespa.package import ApplicationPackage, Field, Schema, Document  schema_name = \"doc\"  schema = Schema(     name=schema_name,     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"body\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 index=\"enable-bm25\",             ),         ]     ), )  package = ApplicationPackage(name=application, schema=[schema]) In\u00a0[4]: Copied! <pre>from vespa.deployment import VespaCloud\nfrom vespa.application import Vespa\nimport os\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,  # Note that the name cannot contain the characters `-` or `_`.\n    application=application,\n    key_content=key,  # Prefer to use  key_location=\"&lt;path-to-key-file.pem&gt;\"\n    application_package=package,\n)\n</pre> from vespa.deployment import VespaCloud from vespa.application import Vespa import os  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly   vespa_cloud = VespaCloud(     tenant=tenant_name,  # Note that the name cannot contain the characters `-` or `_`.     application=application,     key_content=key,  # Prefer to use  key_location=\"\"     application_package=package, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.authnotebook\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p>When you have authenticated to the control-plane of Vespa Cloud, key/cert for data-plane authentication will be generated automatically for you, if none exists.</p> <p>The <code>data-plane-public-cert.pem</code> will be added to the application package (in <code>/security/clients.pem</code> directory) that will be deployed. You should keep them safe, as any app or users that need data-plane access to your Vespa application will need them.</p> <p>For <code>dev</code>-deployments, we allow redeploying an application with a different key/cert than the previous deployment. For <code>prod</code>-deployments however, this is not allowed, and will require a <code>validation-overrides</code>-specification in the application package.</p> <p>The following will upload the application package to Vespa Cloud Dev Zone (<code>aws-us-east-1c</code>), read more about Vespa Zones. The Vespa Cloud Dev Zone is considered as a sandbox environment where resources are down-scaled and idle deployments are expired automatically. For information about production deployments, see the following docs.</p> <p>Note: Deployments to dev and perf expire after 14 days of inactivity, i.e., 14 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 14 more days.</p> In\u00a0[5]: Copied! <pre>app: Vespa = vespa_cloud.deploy()\n</pre> app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for vespa-team.authnotebook. This may take a few minutes the first time.\nINFO    [06:35:26]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:35:27]  Using CA signed certificate version 1\nINFO    [06:35:27]  Using 1 nodes in container cluster 'authnotebook_container'\nINFO    [06:35:30]  Session 309490 for tenant 'vespa-team' prepared, but activation failed: 1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:35:33]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:35:33]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:35:42]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:35:42]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:35:52]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:35:52]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:03]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:03]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:14]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:14]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:22]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:22]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:33]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:33]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:42]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:42]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:36:52]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:36:53]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:03]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:03]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:12]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:12]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:22]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:22]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:33]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:33]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:43]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:43]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:37:53]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:37:54]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:03]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:03]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:12]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:12]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:22]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:22]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:33]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:34]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:42]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:43]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:38:52]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:38:53]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:39:02]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:39:03]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:39:12]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:39:13]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:39:22]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:39:22]  1/2 application hosts and 2/2 admin hosts for vespa-team.authnotebook have completed provisioning and bootstrapping, still waiting for h98840.dev.us-east-1c.aws.vespa-cloud.net\nINFO    [06:39:34]  Deploying platform version 8.408.12 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [06:39:35]  Session 309490 for vespa-team.authnotebook.default activated\nINFO    [06:39:56]  ######## Details for all nodes ########\nINFO    [06:39:56]  h98612b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:39:56]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:39:56]  --- storagenode on port 19102 has not started \nINFO    [06:39:56]  --- searchnode on port 19107 has not started \nINFO    [06:39:56]  --- distributor on port 19111 has not started \nINFO    [06:39:56]  --- metricsproxy-container on port 19092 has not started \nINFO    [06:39:56]  h97566a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:39:56]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:39:56]  --- logserver-container on port 4080 has not started \nINFO    [06:39:56]  --- metricsproxy-container on port 19092 has not started \nINFO    [06:39:56]  h98840a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:39:56]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:39:56]  --- container on port 4080 has not started \nINFO    [06:39:56]  --- metricsproxy-container on port 19092 has not started \nINFO    [06:39:56]  h98621d.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:39:56]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:39:56]  --- container-clustercontroller on port 19050 has not started \nINFO    [06:39:56]  --- metricsproxy-container on port 19092 has not started \nINFO    [06:40:33]  Found endpoints:\nINFO    [06:40:33]  - dev.aws-us-east-1c\nINFO    [06:40:33]   |-- https://ea8555a9.c6970ada.z.vespa-app.cloud/ (cluster 'authnotebook_container')\nINFO    [06:40:33]  Deployment complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for authnotebook_container\nURL: https://ea8555a9.c6970ada.z.vespa-app.cloud/\nApplication is up!\n</pre> <p>If the deployment failed, it is possible you forgot to add the key in the Vespa Cloud Console in the <code>vespa auth api-key</code> step above.</p> <p>If you can authenticate, you should see lines like the following</p> <pre><code> Deployment started in run 1 of dev-aws-us-east-1c for mytenant.authdemo.\n</code></pre> <p>The deployment takes a few minutes the first time while Vespa Cloud sets up the resources for your Vespa application</p> <p><code>app</code> now holds a reference to a Vespa instance. We can access the mTLS protected endpoint name using the control-plane (vespa_cloud) instance. This endpoint we can query and feed to (data plane access) using the mTLS certificate generated in previous steps.</p> In\u00a0[6]: Copied! <pre>mtls_endpoint = vespa_cloud.get_mtls_endpoint()\nmtls_endpoint\n</pre> mtls_endpoint = vespa_cloud.get_mtls_endpoint() mtls_endpoint <pre>Found mtls endpoint for authnotebook_container\nURL: https://ea8555a9.c6970ada.z.vespa-app.cloud/\n</pre> Out[6]: <pre>'https://ea8555a9.c6970ada.z.vespa-app.cloud/'</pre> In\u00a0[7]: Copied! <pre>from vespa.package import AuthClient, Parameter\n\nCLIENT_TOKEN_ID = \"pyvespa_integration\"\n# Same as token name from the Vespa Cloud Console\nauth_clients = [\n    AuthClient(\n        id=\"mtls\",  # Note that you still need to include the mtls client.\n        permissions=[\"read\", \"write\"],\n        parameters=[Parameter(\"certificate\", {\"file\": \"security/clients.pem\"})],\n    ),\n    AuthClient(\n        id=\"token\",\n        permissions=[\"read\"],\n        parameters=[Parameter(\"token\", {\"id\": CLIENT_TOKEN_ID})],\n    ),\n]\n\napp_package = ApplicationPackage(\n    name=application, schema=[schema], auth_clients=auth_clients\n)\n</pre> from vespa.package import AuthClient, Parameter  CLIENT_TOKEN_ID = \"pyvespa_integration\" # Same as token name from the Vespa Cloud Console auth_clients = [     AuthClient(         id=\"mtls\",  # Note that you still need to include the mtls client.         permissions=[\"read\", \"write\"],         parameters=[Parameter(\"certificate\", {\"file\": \"security/clients.pem\"})],     ),     AuthClient(         id=\"token\",         permissions=[\"read\"],         parameters=[Parameter(\"token\", {\"id\": CLIENT_TOKEN_ID})],     ), ]  app_package = ApplicationPackage(     name=application, schema=[schema], auth_clients=auth_clients ) <p>Notice that we added the <code>read</code> and <code>write</code> permissions to mtls clients, and only <code>read</code> to the token client.</p> <p>Make sure to restrict the permissions to suit your needs.</p> <p>Now, we can deploy a new instance of the application package with the new auth-client added:</p>      See Tenants, apps, instances     for details on terminology for Vespa Cloud.  In\u00a0[8]: Copied! <pre>instance = \"token\"\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    key_content=key,\n    application_package=app_package,\n)\napp = vespa_cloud.deploy(instance=instance)\n</pre> instance = \"token\"  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     key_content=key,     application_package=app_package, ) app = vespa_cloud.deploy(instance=instance) <pre>Setting application...\nRunning: vespa config set application vespa-team.authnotebook\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\nDeployment started in run 60 of dev-aws-us-east-1c for vespa-team.authnotebook.token. This may take a few minutes the first time.\nINFO    [06:40:38]  Deploying platform version 8.408.12 and application dev build 54 for dev-aws-us-east-1c of token ...\nINFO    [06:40:39]  Using CA signed certificate version 1\nINFO    [06:40:39]  Using 1 nodes in container cluster 'authnotebook_container'\nWARNING [06:40:41]  Auto-overriding validation which would be disallowed in production: certificate-removal: Data plane certificate(s) from cluster 'authnotebook_container' is removed (removed certificates: [CN=cloud.vespa.example]) This can cause client connection issues.. To allow this add &lt;allow until='yyyy-mm-dd'&gt;certificate-removal&lt;/allow&gt; to validation-overrides.xml, see https://docs.vespa.ai/en/reference/validation-overrides.html\nINFO    [06:40:42]  Session 309492 for tenant 'vespa-team' prepared and activated.\nINFO    [06:40:43]  ######## Details for all nodes ########\nINFO    [06:40:43]  h97526a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:40:43]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:40:43]  --- storagenode on port 19102 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- searchnode on port 19107 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- distributor on port 19111 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- metricsproxy-container on port 19092 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  h97566b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:40:43]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:40:43]  --- logserver-container on port 4080 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- metricsproxy-container on port 19092 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  h97538e.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:40:43]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:40:43]  --- container-clustercontroller on port 19050 has config generation 309492, wanted is 309492\nINFO    [06:40:43]  --- metricsproxy-container on port 19092 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  h97567a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:40:43]  --- platform vespa/cloud-tenant-rhel8:8.408.12\nINFO    [06:40:43]  --- container on port 4080 has config generation 309488, wanted is 309492\nINFO    [06:40:43]  --- metricsproxy-container on port 19092 has config generation 309488, wanted is 309492\nINFO    [06:40:53]  Found endpoints:\nINFO    [06:40:53]  - dev.aws-us-east-1c\nINFO    [06:40:53]   |-- https://ab50e0c2.c6970ada.z.vespa-app.cloud/ (cluster 'authnotebook_container')\nINFO    [06:40:53]  Deployment of new application complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for authnotebook_container\nURL: https://ab50e0c2.c6970ada.z.vespa-app.cloud/\nApplication is up!\n</pre> <p>Note that the connection that will be returned by default, will be the mTLS connection. If you want to get a connection using token-based authentication, you can do it like this:</p> In\u00a0[9]: Copied! <pre>token_app = vespa_cloud.get_application(\n    instance=instance,\n    endpoint_type=\"token\",\n    vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\"),\n)\n</pre> token_app = vespa_cloud.get_application(     instance=instance,     endpoint_type=\"token\",     vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\"), ) <pre>Only region: aws-us-east-1c available in dev environment.\nFound token endpoint for authnotebook_container\nURL: https://c7f94a93.c6970ada.z.vespa-app.cloud/\nApplication is up!\n</pre> In\u00a0[10]: Copied! <pre>token_app.get_application_status()\n</pre> token_app.get_application_status() Out[10]: <pre>&lt;Response [200]&gt;</pre> <p>Note that a Vespa application creates a separate URL endpoint for each auth-client added. Here is how you can retrieve the URL for the token endpoint:</p> In\u00a0[11]: Copied! <pre>token_endpoint = vespa_cloud.get_token_endpoint(instance=instance)\ntoken_endpoint\n</pre> token_endpoint = vespa_cloud.get_token_endpoint(instance=instance) token_endpoint <pre>Found token endpoint for authnotebook_container\nURL: https://c7f94a93.c6970ada.z.vespa-app.cloud/\n</pre> Out[11]: <pre>'https://c7f94a93.c6970ada.z.vespa-app.cloud/'</pre> In\u00a0[12]: Copied! <pre>import os\n\n# Get user home directory\nhome = os.path.expanduser(\"~\")\n# Vespa key/cert directory\napp_dir = f\"{home}/.vespa/{tenant_name}.{application}.default/\"\n\ncert_path = f\"{app_dir}/data-plane-public-cert.pem\"\nkey_path = f\"{app_dir}/data-plane-private-key.pem\"\n</pre> import os  # Get user home directory home = os.path.expanduser(\"~\") # Vespa key/cert directory app_dir = f\"{home}/.vespa/{tenant_name}.{application}.default/\"  cert_path = f\"{app_dir}/data-plane-public-cert.pem\" key_path = f\"{app_dir}/data-plane-private-key.pem\" In\u00a0[13]: Copied! <pre>from vespa.application import Vespa\n\napp = Vespa(url=mtls_endpoint, cert=cert_path, key=key_path)\napp.get_application_status()\n</pre> from vespa.application import Vespa  app = Vespa(url=mtls_endpoint, cert=cert_path, key=key_path) app.get_application_status() Out[13]: <pre>&lt;Response [200]&gt;</pre> In\u00a0[14]: Copied! <pre>import requests\n\nsession = requests.Session()\nsession.cert = (cert_path, key_path)\nurl = f\"{mtls_endpoint}/document/v1/doc/doc/docid/1\"\ndata = {\n    \"fields\": {\n        \"id\": \"id:doc:doc::1\",\n        \"title\": \"the title\",\n        \"body\": \"the body\",\n    }\n}\nresp = session.post(url, json=data).json()\nresp\n</pre> import requests  session = requests.Session() session.cert = (cert_path, key_path) url = f\"{mtls_endpoint}/document/v1/doc/doc/docid/1\" data = {     \"fields\": {         \"id\": \"id:doc:doc::1\",         \"title\": \"the title\",         \"body\": \"the body\",     } } resp = session.post(url, json=data).json() resp Out[14]: <pre>{'pathId': '/document/v1/doc/doc/docid/1', 'id': 'id:doc:doc::1'}</pre> In\u00a0[15]: Copied! <pre>app = Vespa(\n    url=token_endpoint, vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\")\n)\napp.get_application_status()\n</pre> app = Vespa(     url=token_endpoint, vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\") ) app.get_application_status() Out[15]: <pre>&lt;Response [200]&gt;</pre> <p>Token authentication provides an even more convenient way to authenticate to the data-plane, as you do not need to handle key/cert files, and can just add the token to the HTTP header, as shown in the example below.</p> <pre>curl -H \"Authorization: Bearer $TOKEN\" https://{endpoint}/document/v1/{document-type}/{document-id}\n</pre> In\u00a0[16]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-team.authnotebook in dev.aws-us-east-1c\nDeleted instance vespa-team.authnotebook.default\n</pre>"},{"location":"authenticating-to-vespa-cloud.html#authenticating-to-vespa-cloud","title":"Authenticating to Vespa Cloud\u00b6","text":"<p>Security is a top priority for the Vespa Team. We understand that as a newcomer to Vespa, the different authentication methods may not always be immediately clear.</p> <p>This notebook is intended to provide some clarity on the different authentication methods needed when interacting with Vespa Cloud for different purposes.</p>"},{"location":"authenticating-to-vespa-cloud.html#install","title":"Install\u00b6","text":"<p>Install pyvespa &gt;= 0.45 and the Vespa CLI.</p>"},{"location":"authenticating-to-vespa-cloud.html#control-plane-vs-data-plane","title":"Control-plane vs Data-plane\u00b6","text":"<p>This may be self-explanatory for some, but it is worth mentioning that Vespa Cloud has two main components: the control-plane and the data-plane, which provide access to different functionalities.</p>"},{"location":"authenticating-to-vespa-cloud.html#defining-your-application","title":"Defining your application\u00b6","text":"<p>To initialize a connection to Vespa Cloud, you need to define your tenant name and application name.</p>"},{"location":"authenticating-to-vespa-cloud.html#defining-your-application-package","title":"Defining your application package\u00b6","text":"<p>An application package is the whole Vespa application configuration. It can either be constructed directly from python (as we will do below) or initalized from a path, for example by cloning a sample application from the Vespa sample apps.</p>      Tip: You can use the command vespa clone album-recommendation my-app to clone a single sample app if you have the Vespa CLI installed.  <p>For this guide, we will create a minimal application package. See other guides for more complex examples.</p>"},{"location":"authenticating-to-vespa-cloud.html#control-plane-authentication","title":"Control-plane authentication\u00b6","text":"<p>Next, we need to authenticate to the Vespa Cloud control-plane. There are two ways to authenticate to the control-plane:</p>"},{"location":"authenticating-to-vespa-cloud.html#1-interactive-login","title":"1. Interactive login:\u00b6","text":"<p>This is the recommended way to authenticate to the control-plane. It opens a browser window for you to authenticate with either google or github.</p> <p>This method does not work on windows, currently. You can run <code>vespa auth login</code> in a terminal to authenticate first, and then use this method (which will then reuse the generated token).</p> <p>(We will not run this method here, as the notebook is run in CI, but you should run it in your local environment)</p> <pre>from vespa.deployment import VespaCloud\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    application_package=package, # Could also initialize from application_root (path to application package)\n)\n</pre> <p>You should see something similar to this:</p> <pre><code>log\nChecking for access token in auth.json...\nAccess token expired. Please re-authenticate.\nYour Device Confirmation code is: DRDT-ZZDC\nAutomatically open confirmation page in your default browser? [Y/n] y\nOpened link in your browser: https://vespa.auth0.com/activate?user_code=DRDT-ZZDC\nWaiting for login to complete in browser ... done;1m\u28ef\nSuccess: Logged in\n auth.json created at /Users/thomas/.vespa/auth.json\nSuccessfully obtained access token for control plane access.\n</code></pre>"},{"location":"authenticating-to-vespa-cloud.html#2-api-key-authentication","title":"2. API-key authentication\u00b6","text":"<p>This is a headless way to authenticate to the control-plane.</p> <p>Note that the key must be generated, either with <code>vespa auth api-key</code> or in the Vespa Cloud console directly.</p>"},{"location":"authenticating-to-vespa-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>The app is now defined and ready to deploy to Vespa Cloud.</p> <p>Deploy <code>package</code> to Vespa Cloud, by creating an instance of VespaCloud:</p>"},{"location":"authenticating-to-vespa-cloud.html#data-plane-authentication","title":"Data-plane authentication\u00b6","text":"<p>As we have mentioned, there are two ways to authenticate to the data-plane:</p>"},{"location":"authenticating-to-vespa-cloud.html#1-mtls-certificate-authentication","title":"1. mTLS - Certificate authentication\u00b6","text":"<p>This is the default way to authenticate to the data-plane. It uses the certificate which was added to the application package upon deployment.</p>"},{"location":"authenticating-to-vespa-cloud.html#2-token-based-authentication","title":"2. Token-based authentication\u00b6","text":"<p>A more convenient way to authenticate to the data-plane is to use a token. A token must be generated in the Vespa Cloud console. For more details, see the Security Guide</p> <p></p> <p>Set a reasonable expiry, and copy the token to a safe place, such as for instance a passwordmanager. You will not be able to see it again.</p> <p>After the token is generated, you need to add it as an auth-client to the application you want to access.</p> <p>In pyvespa, this is done by adding the AuthClients to the application package:</p> <p>NB! - The method below applies to <code>dev</code></p> <p>The approach described above applies to <code>dev</code>-deployments. For <code>prod</code>-deployments, it is a little more complex, and you need to add the <code>AuthClients</code> to your application package like this:</p> <pre>from vespa.package import ContainerCluster\n\nauth_clients = [\n            AuthClient(\n                id=\"mtls\",\n                permissions=[\"read\"],\n                parameters=[Parameter(\"certificate\", {\"file\": \"security/clients.pem\"})],\n            ),\n            AuthClient(\n                id=\"token\",\n                permissions=[\"read\"], # Set the permissions you need\n                parameters=[Parameter(\"token\", {\"id\": CLIENT_TOKEN_ID})],\n            ),\n        ]\n# Add prod deployment config\nprod_region = \"aws-us-east-1c\"\nclusters = [\n    ContentCluster(\n        id=f\"{schema_name}_content\",\n        nodes=Nodes(count=\"2\"),\n        document_name=schema_name,\n        min_redundancy=\"2\",\n    ),\n    ContainerCluster(\n        id=f\"{schema_name}_container\",\n        nodes=Nodes(count=\"2\"),\n        auth_clients=auth_clients, # Note that the auth_clients are added here for prod deployments\n    ),\n]\nprod_region = \"aws-us-east-1c\"\ndeployment_config = DeploymentConfiguration(\n    environment=\"prod\", regions=[prod_region]\n)\napp_package = ApplicationPackage(name=application, schema=[schema], clusters=clusters, deployment=deployment_config)\n</pre> <p>See Application Package reference for more details.</p>"},{"location":"authenticating-to-vespa-cloud.html#re-connecting-to-a-deployed-application","title":"Re-connecting to a deployed application\u00b6","text":"<p>To connect to a deployed application, you can use the <code>Vespa</code> class, which is a data-plane connection to an existing Vespa application.</p> <p>The <code>Vespa</code> class requires the endpoint URL.</p> <p>Note that this class can also be instantiated without authentication, typically used if connecting to an instance running in Docker, see VespaDocker.</p>"},{"location":"authenticating-to-vespa-cloud.html#connecting-using-mtls","title":"Connecting using mTLS\u00b6","text":"<p>To connect to the Vespa application using mTLS, you must pass <code>key</code> and <code>cert</code> to the <code>Vespa</code> class. Both should be a path to the respective files, matching the cert that was added to the application package upon deployment.</p> <p>A common error is to try to regenerate the key/cert after deployment, causing a mismatch between the key/cert you are trying to authenticate with, and the cert added to the application package.</p>"},{"location":"authenticating-to-vespa-cloud.html#using-requests","title":"Using <code>requests</code>\u00b6","text":"<p>It is often overlooked that all interactions with Vespa are through HTTP-api calls, so you are free to use any HTTP client you like.</p> <p>Below is an example of how to use the <code>requests</code> library to interact with Vespa, using <code>key</code> and <code>cert</code> for authentication, and the /document/v1/ endpoint to feed data to Vespa.</p>"},{"location":"authenticating-to-vespa-cloud.html#connecting-using-token","title":"Connecting using token\u00b6","text":"<p>To connect to the Vespa application using a token, you must pass the token value to the <code>Vespa</code> class as <code>vespa_cloud_secret_token</code>.</p>"},{"location":"authenticating-to-vespa-cloud.html#using-curl","title":"Using cURL\u00b6","text":""},{"location":"authenticating-to-vespa-cloud.html#next-steps","title":"Next steps\u00b6","text":"<p>This was a guide to the different authentication methods when interacting with Vespa Cloud for different purposes.</p> <p>Try to deploy a frontend as interface to your Vespa application.</p> <p>Example of some providers are:</p> <ul> <li>Cloudflare Workers, see also https://cloud.vespa.ai/en/security/cloudflare-workers.html</li> <li>Vercel</li> <li>Railway etc.</li> </ul>"},{"location":"authenticating-to-vespa-cloud.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"evaluating-vespa-application-cloud.html","title":"Evaluating a Vespa Application","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Pre-requisite: Create a tenant at cloud.vespa.ai, save the tenant name.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa vespacli datasets pandas\n</pre> !pip3 install pyvespa vespacli datasets pandas In\u00a0[1]: Copied! <pre># Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n# Replace with your application name (does not need to exist yet)\napplication = \"evaluation\"\nschema_name = \"doc\"\n</pre> # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\" # Replace with your application name (does not need to exist yet) application = \"evaluation\" schema_name = \"doc\" In\u00a0[2]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    Component,\n    Parameter,\n    FieldSet,\n    GlobalPhaseRanking,\n    Function,\n)\nimport pandas as pd\n\n\npackage = ApplicationPackage(\n    name=application,\n    schema=[\n        Schema(\n            name=schema_name,\n            document=Document(\n                fields=[\n                    # Note that we need an id field as attribute to be able to do evaluation\n                    # Vespa internal query document id is used as fallback, but have some limitations, see https://docs.vespa.ai/en/document-v1-api-guide.html#query-result-id\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n                    Field(\n                        name=\"text\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                        bolding=True,\n                    ),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;float&gt;(x[384])\",\n                        indexing=[\n                            \"input text\",\n                            \"embed\",  # uses default model\n                            \"index\",\n                            \"attribute\",\n                        ],\n                        ann=HNSW(distance_metric=\"angular\"),\n                        is_document_field=False,\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n            rank_profiles=[\n                RankProfile(\n                    name=\"match-only\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"random\",  # TODO: Remove when pyvespa supports empty first_phase\n                ),\n                RankProfile(\n                    name=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[Function(name=\"bm25text\", expression=\"bm25(text)\")],\n                    first_phase=\"bm25text\",\n                    match_features=[\"bm25text\"],\n                ),\n                RankProfile(\n                    name=\"semantic\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[\n                        Function(\n                            name=\"cos_sim\", expression=\"closeness(field, embedding)\"\n                        )\n                    ],\n                    first_phase=\"cos_sim\",\n                    match_features=[\"cos_sim\"],\n                ),\n                RankProfile(\n                    name=\"fusion\",\n                    inherits=\"bm25\",\n                    functions=[\n                        Function(\n                            name=\"cos_sim\", expression=\"closeness(field, embedding)\"\n                        )\n                    ],\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"cos_sim\",\n                    global_phase=GlobalPhaseRanking(\n                        expression=\"reciprocal_rank_fusion(bm25text, closeness(field, embedding))\",\n                        rerank_count=1000,\n                    ),\n                    match_features=[\"cos_sim\", \"bm25text\"],\n                ),\n                RankProfile(\n                    name=\"atan_norm\",\n                    inherits=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[\n                        Function(\n                            name=\"scale\",\n                            args=[\"val\"],\n                            expression=\"2*atan(val)/(3.14159)\",\n                        ),\n                        Function(\n                            name=\"normalized_bm25\", expression=\"scale(bm25(text))\"\n                        ),\n                        Function(\n                            name=\"cos_sim\", expression=\"closeness(field, embedding)\"\n                        ),\n                    ],\n                    first_phase=\"normalized_bm25\",\n                    global_phase=GlobalPhaseRanking(\n                        expression=\"normalize_linear(normalized_bm25) + normalize_linear(cos_sim)\",\n                        rerank_count=1000,\n                    ),\n                    match_features=[\"cos_sim\", \"normalized_bm25\"],\n                ),\n            ],\n        )\n    ],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"model-id\": \"e5-small-v2\"\n                    },  # in vespa cloud, we can use the model-id for selected models, see https://cloud.vespa.ai/en/model-hub\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\"model-id\": \"e5-base-v2-vocab\"},\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     Component,     Parameter,     FieldSet,     GlobalPhaseRanking,     Function, ) import pandas as pd   package = ApplicationPackage(     name=application,     schema=[         Schema(             name=schema_name,             document=Document(                 fields=[                     # Note that we need an id field as attribute to be able to do evaluation                     # Vespa internal query document id is used as fallback, but have some limitations, see https://docs.vespa.ai/en/document-v1-api-guide.html#query-result-id                     Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),                     Field(                         name=\"text\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                         bolding=True,                     ),                     Field(                         name=\"embedding\",                         type=\"tensor(x[384])\",                         indexing=[                             \"input text\",                             \"embed\",  # uses default model                             \"index\",                             \"attribute\",                         ],                         ann=HNSW(distance_metric=\"angular\"),                         is_document_field=False,                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],             rank_profiles=[                 RankProfile(                     name=\"match-only\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"random\",  # TODO: Remove when pyvespa supports empty first_phase                 ),                 RankProfile(                     name=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[Function(name=\"bm25text\", expression=\"bm25(text)\")],                     first_phase=\"bm25text\",                     match_features=[\"bm25text\"],                 ),                 RankProfile(                     name=\"semantic\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[                         Function(                             name=\"cos_sim\", expression=\"closeness(field, embedding)\"                         )                     ],                     first_phase=\"cos_sim\",                     match_features=[\"cos_sim\"],                 ),                 RankProfile(                     name=\"fusion\",                     inherits=\"bm25\",                     functions=[                         Function(                             name=\"cos_sim\", expression=\"closeness(field, embedding)\"                         )                     ],                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"cos_sim\",                     global_phase=GlobalPhaseRanking(                         expression=\"reciprocal_rank_fusion(bm25text, closeness(field, embedding))\",                         rerank_count=1000,                     ),                     match_features=[\"cos_sim\", \"bm25text\"],                 ),                 RankProfile(                     name=\"atan_norm\",                     inherits=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[                         Function(                             name=\"scale\",                             args=[\"val\"],                             expression=\"2*atan(val)/(3.14159)\",                         ),                         Function(                             name=\"normalized_bm25\", expression=\"scale(bm25(text))\"                         ),                         Function(                             name=\"cos_sim\", expression=\"closeness(field, embedding)\"                         ),                     ],                     first_phase=\"normalized_bm25\",                     global_phase=GlobalPhaseRanking(                         expression=\"normalize_linear(normalized_bm25) + normalize_linear(cos_sim)\",                         rerank_count=1000,                     ),                     match_features=[\"cos_sim\", \"normalized_bm25\"],                 ),             ],         )     ],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"model-id\": \"e5-small-v2\"                     },  # in vespa cloud, we can use the model-id for selected models, see https://cloud.vespa.ai/en/model-hub                 ),                 Parameter(                     \"tokenizer-model\",                     {\"model-id\": \"e5-base-v2-vocab\"},                 ),             ],         )     ], ) <p>Note that the name cannot have <code>-</code> or <code>_</code>.</p> In\u00a0[3]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    key_content=os.getenv(\n        \"VESPA_TEAM_API_KEY\", None\n    ),  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Key is only used for CI/CD. Can be removed if logging in interactively  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     key_content=os.getenv(         \"VESPA_TEAM_API_KEY\", None     ),  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=package, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.evaluation.default\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p>For more details on different authentication options and methods, see authenticating-to-vespa-cloud.</p> <p>The following will upload the application package to Vespa Cloud Dev Zone (<code>aws-us-east-1c</code>), read more about Vespa Zones. The Vespa Cloud Dev Zone is considered as a sandbox environment where resources are down-scaled and idle deployments are expired automatically. For information about production deployments, see the following method.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p> <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up. (Applications that for example refer to large onnx-models may take a bit longer.)</p> In\u00a0[4]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 52 of dev-aws-us-east-1c for vespa-team.evaluation. This may take a few minutes the first time.\nINFO    [06:52:41]  Deploying platform version 8.586.25 and application dev build 50 for dev-aws-us-east-1c of default ...\nINFO    [06:52:41]  Using CA signed certificate version 9\nINFO    [06:52:42]  Using 1 nodes in container cluster 'evaluation_container'\nINFO    [06:52:44]  Session 379645 for tenant 'vespa-team' prepared and activated.\nINFO    [06:52:44]  ######## Details for all nodes ########\nINFO    [06:52:45]  h125699b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:52:45]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [06:52:45]  --- storagenode on port 19102 has config generation 379643, wanted is 379645\nINFO    [06:52:45]  --- searchnode on port 19107 has config generation 379643, wanted is 379645\nINFO    [06:52:45]  --- distributor on port 19111 has config generation 379645, wanted is 379645\nINFO    [06:52:45]  --- metricsproxy-container on port 19092 has config generation 379645, wanted is 379645\nINFO    [06:52:45]  h119183e.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:52:45]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [06:52:45]  --- container-clustercontroller on port 19050 has config generation 379643, wanted is 379645\nINFO    [06:52:45]  --- metricsproxy-container on port 19092 has config generation 379645, wanted is 379645\nINFO    [06:52:45]  h125689a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:52:45]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [06:52:45]  --- container on port 4080 has config generation 379643, wanted is 379645\nINFO    [06:52:45]  --- metricsproxy-container on port 19092 has config generation 379643, wanted is 379645\nINFO    [06:52:45]  h97530b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [06:52:45]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [06:52:45]  --- logserver-container on port 4080 has config generation 379643, wanted is 379645\nINFO    [06:52:45]  --- metricsproxy-container on port 19092 has config generation 379643, wanted is 379645\nINFO    [06:52:56]  Found endpoints:\nINFO    [06:52:56]  - dev.aws-us-east-1c\nINFO    [06:52:56]   |-- https://f4f49447.ccc9bd09.z.vespa-app.cloud/ (cluster 'evaluation_container')\nINFO    [06:52:56]  Deployment of new application revision complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for evaluation_container\nURL: https://f4f49447.ccc9bd09.z.vespa-app.cloud/\nApplication is up!\n</pre> <p>If the deployment failed, it is possible you forgot to add the key in the Vespa Cloud Console in the <code>vespa auth api-key</code> step above.</p> <p>If you can authenticate, you should see lines like the following</p> <pre><code> Deployment started in run 1 of dev-aws-us-east-1c for mytenant.hybridsearch.\n</code></pre> <p>The deployment takes a few minutes the first time while Vespa Cloud sets up the resources for your Vespa application</p> <p><code>app</code> now holds a reference to a Vespa instance. We can access the mTLS protected endpoint name using the control-plane (vespa_cloud) instance. This endpoint we can query and feed to (data plane access) using the mTLS certificate generated in previous steps.</p> <p>See Authenticating to Vespa Cloud for details on using token authentication instead of mTLS.</p> In\u00a0[5]: Copied! <pre>from datasets import load_dataset\n\ndataset_id = \"zeta-alpha-ai/NanoMSMARCO\"\n\ndataset = load_dataset(dataset_id, \"corpus\", split=\"train\", streaming=True)\nvespa_feed = dataset.map(\n    lambda x: {\n        \"id\": x[\"_id\"],\n        \"fields\": {\"text\": x[\"text\"], \"id\": x[\"_id\"]},\n    }\n)\n</pre> from datasets import load_dataset  dataset_id = \"zeta-alpha-ai/NanoMSMARCO\"  dataset = load_dataset(dataset_id, \"corpus\", split=\"train\", streaming=True) vespa_feed = dataset.map(     lambda x: {         \"id\": x[\"_id\"],         \"fields\": {\"text\": x[\"text\"], \"id\": x[\"_id\"]},     } ) <p>Note that we are only evaluating rank strategies here, we consider it OK to use the <code>train</code> split for evaluation. If we were to make changes to our ranking strategies, such as adding weighting terms, or training ml models for ranking, we would suggest to adopt a <code>train</code>, <code>validation</code>, <code>test</code> split approach to avoid overfitting.</p> In\u00a0[6]: Copied! <pre>query_ds = load_dataset(dataset_id, \"queries\", split=\"train\")\nqrels = load_dataset(dataset_id, \"qrels\", split=\"train\")\n</pre> query_ds = load_dataset(dataset_id, \"queries\", split=\"train\") qrels = load_dataset(dataset_id, \"qrels\", split=\"train\") In\u00a0[7]: Copied! <pre>ids_to_query = dict(zip(query_ds[\"_id\"], query_ds[\"text\"]))\n</pre> ids_to_query = dict(zip(query_ds[\"_id\"], query_ds[\"text\"])) <p>Let us print the first 5 queries:</p> In\u00a0[8]: Copied! <pre>for idx, (qid, q) in enumerate(ids_to_query.items()):\n    print(f\"qid: {qid}, query: {q}\")\n    if idx == 5:\n        break\n</pre> for idx, (qid, q) in enumerate(ids_to_query.items()):     print(f\"qid: {qid}, query: {q}\")     if idx == 5:         break <pre>qid: 994479, query: which health care system provides all citizens or residents with equal access to health care services\nqid: 1009388, query: what's right in health care\nqid: 1088332, query: weather in oran\nqid: 265729, query: how long keep financial records\nqid: 1099433, query: how do hoa fees work\nqid: 200600, query: heels or heal\n</pre> In\u00a0[9]: Copied! <pre>relevant_docs = dict(zip(qrels[\"query-id\"], qrels[\"corpus-id\"]))\n</pre> relevant_docs = dict(zip(qrels[\"query-id\"], qrels[\"corpus-id\"])) <p>Let us print the first 5 query ids and their relevant documents:</p> In\u00a0[10]: Copied! <pre>for idx, (qid, doc_id) in enumerate(relevant_docs.items()):\n    print(f\"qid: {qid}, doc_id: {doc_id}\")\n    if idx == 5:\n        break\n</pre> for idx, (qid, doc_id) in enumerate(relevant_docs.items()):     print(f\"qid: {qid}, doc_id: {doc_id}\")     if idx == 5:         break <pre>qid: 994479, doc_id: 7275120\nqid: 1009388, doc_id: 7248824\nqid: 1088332, doc_id: 7094398\nqid: 265729, doc_id: 7369987\nqid: 1099433, doc_id: 7255675\nqid: 200600, doc_id: 7929603\n</pre> <p>We can see that this dataset only has one relevant document per query. The <code>VespaEvaluator</code> class handles this just fine, but you could also provide a set of relevant documents per query if there are multiple relevant docs.</p> <pre># multiple relevant docs per query\nqrels = {\n    \"q1\": {\"doc1\", \"doc2\"},\n    \"q2\": {\"doc3\", \"doc4\"},\n    # etc. \n}\n</pre> <p>Now we can feed to Vespa using <code>feed_iterable</code> which accepts any <code>Iterable</code> and an optional callback function where we can check the outcome of each operation. The application is configured to use embedding functionality, that produce a vector embedding using a concatenation of the title and the body input fields. This step may be resource intensive, depending on the model size.</p> <p>Read more about embedding inference in Vespa in the Accelerating Transformer-based Embedding Retrieval with Vespa blog post.</p> <p>Default node resources in Vespa Cloud have 2 v-cpu for the Dev Zone.</p> In\u00a0[11]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback) <p>There are two separate classes provided for doing evaluations:</p> <ol> <li><code>VespaMatchEvaluator</code>, which is intended to evaluate only the retrieval (or match-phase), and should not do any ranking. This is useful to evaluate whether your relevant documents are retrieved (and thus exposed to ranking). It only computes recall, total matched documents per query as well as <code>searchtime</code>.</li> <li><code>VespaEvaluator</code> is intended to evaluate a complete ranking strategy, across several common IR metrics.</li> </ol> <p>Both API's are inspired by SentenceTransformers <code>InformationRetrievalEvaluator</code> class.</p> <p>The difference it that <code>VespaMatchEvaluator</code> evaluates only the retrieval phase, while <code>VespaEvaluator</code> evaluates your whole retrieval and ranking system (Vespa application) as opposed to a single model. Your application should be fed with the document corpus in advance, instead of taking in the document corpus.</p> <p>We now have created the app, the queries, and the relevant documents. The only thing missing before we can initialize the <code>VespaMatchEvaluator</code> is a set of functions that defines the Vespa queries. Each of them is passed as <code>vespa_query_fn</code>.</p> <p>We will use the <code>vespa.querybuilder</code> module to create the queries. See reference doc and example notebook for more details on usage.</p> <p>This module is a Python wrapper around the Vespa Query Language (YQL), which is an alternative to providing the YQL query as a string directly.</p> In\u00a0[12]: Copied! <pre>import vespa.querybuilder as qb\n\n\ndef match_weakand_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(qb.select(\"*\").from_(schema_name).where(qb.userQuery(query_text))),\n        \"query\": query_text,\n        \"ranking\": \"match-only\",\n        \"input.query(q)\": f\"embed({query_text})\",\n    }\n\n\ndef match_hybrid_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(schema_name)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"embedding\",\n                    query_vector=\"q\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.userQuery(\n                    query_text,\n                )\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"match-only\",\n        \"input.query(q)\": f\"embed({query_text})\",\n    }\n\n\ndef match_semantic_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(schema_name)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"embedding\",\n                    query_vector=\"q\",\n                    annotations={\"targetHits\": 100},\n                )\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"match-only\",\n        \"input.query(q)\": f\"embed({query_text})\",\n    }\n</pre> import vespa.querybuilder as qb   def match_weakand_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(qb.select(\"*\").from_(schema_name).where(qb.userQuery(query_text))),         \"query\": query_text,         \"ranking\": \"match-only\",         \"input.query(q)\": f\"embed({query_text})\",     }   def match_hybrid_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(schema_name)             .where(                 qb.nearestNeighbor(                     field=\"embedding\",                     query_vector=\"q\",                     annotations={\"targetHits\": 100},                 )                 | qb.userQuery(                     query_text,                 )             )         ),         \"query\": query_text,         \"ranking\": \"match-only\",         \"input.query(q)\": f\"embed({query_text})\",     }   def match_semantic_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(schema_name)             .where(                 qb.nearestNeighbor(                     field=\"embedding\",                     query_vector=\"q\",                     annotations={\"targetHits\": 100},                 )             )         ),         \"query\": query_text,         \"ranking\": \"match-only\",         \"input.query(q)\": f\"embed({query_text})\",     } <p>Now, let us run the evaluator:</p> In\u00a0[13]: Copied! <pre>from vespa.evaluation import VespaMatchEvaluator\n\n\nmatch_results = {}\nfor evaluator_name, query_fn in [\n    (\"semantic\", match_semantic_query_fn),\n    (\"weakand\", match_weakand_query_fn),\n    (\"hybrid\", match_hybrid_query_fn),\n]:\n    print(f\"Evaluating {evaluator_name}...\")\n\n    match_evaluator = VespaMatchEvaluator(\n        queries=ids_to_query,\n        relevant_docs=relevant_docs,\n        vespa_query_fn=query_fn,\n        app=app,\n        name=\"test-run\",\n        id_field=\"id\",  # specify the id field used in the relevant_docs\n        write_csv=True,\n        write_verbose=True,  # optionally write verbose metrics to CSV\n    )\n\n    results = match_evaluator()\n    match_results[evaluator_name] = results\n    print(f\"Results for {evaluator_name}:\")\n    print(results)\n</pre> from vespa.evaluation import VespaMatchEvaluator   match_results = {} for evaluator_name, query_fn in [     (\"semantic\", match_semantic_query_fn),     (\"weakand\", match_weakand_query_fn),     (\"hybrid\", match_hybrid_query_fn), ]:     print(f\"Evaluating {evaluator_name}...\")      match_evaluator = VespaMatchEvaluator(         queries=ids_to_query,         relevant_docs=relevant_docs,         vespa_query_fn=query_fn,         app=app,         name=\"test-run\",         id_field=\"id\",  # specify the id field used in the relevant_docs         write_csv=True,         write_verbose=True,  # optionally write verbose metrics to CSV     )      results = match_evaluator()     match_results[evaluator_name] = results     print(f\"Results for {evaluator_name}:\")     print(results) <pre>Evaluating semantic...\nResults for semantic:\n{'match_recall': 1.0, 'avg_recall_per_query': 1.0, 'total_relevant_docs': 50, 'total_matched_relevant': 50, 'avg_matched_per_query': 100.0, 'total_queries': 50, 'searchtime_avg': 0.0535, 'searchtime_q50': 0.053, 'searchtime_q90': 0.0786, 'searchtime_q95': 0.08700000000000001}\nEvaluating weakand...\nResults for weakand:\n{'match_recall': 0.98, 'avg_recall_per_query': 0.98, 'total_relevant_docs': 50, 'total_matched_relevant': 49, 'avg_matched_per_query': 809.86, 'total_queries': 50, 'searchtime_avg': 0.04391999999999998, 'searchtime_q50': 0.043000000000000003, 'searchtime_q90': 0.058300000000000005, 'searchtime_q95': 0.06665}\nEvaluating hybrid...\nResults for hybrid:\n{'match_recall': 1.0, 'avg_recall_per_query': 1.0, 'total_relevant_docs': 50, 'total_matched_relevant': 50, 'avg_matched_per_query': 833.18, 'total_queries': 50, 'searchtime_avg': 0.03699999999999999, 'searchtime_q50': 0.037, 'searchtime_q90': 0.0531, 'searchtime_q95': 0.058299999999999984}\n</pre> <p>By setting <code>write_csv=True</code> and <code>verbose=True</code>, we can save a CSV-file for each query to inspect further the queries that were not matched. This is important to understand how you could improve recall if some relevant documents were not matched.</p> In\u00a0[14]: Copied! <pre>results = pd.DataFrame(match_results)\nresults\n</pre> results = pd.DataFrame(match_results) results Out[14]: semantic weakand hybrid match_recall 1.0000 0.98000 1.0000 avg_recall_per_query 1.0000 0.98000 1.0000 total_relevant_docs 50.0000 50.00000 50.0000 total_matched_relevant 50.0000 49.00000 50.0000 avg_matched_per_query 100.0000 809.86000 833.1800 total_queries 50.0000 50.00000 50.0000 searchtime_avg 0.0535 0.04392 0.0370 searchtime_q50 0.0530 0.04300 0.0370 searchtime_q90 0.0786 0.05830 0.0531 searchtime_q95 0.0870 0.06665 0.0583 <p>Here, we can see that all retrieval strategies actually match all the relevant documents. To tune number of documents retrieved and latency, we could tune the <code>targetHits</code>-parameter for both the <code>nearestNeighbor</code>-operator and <code>weakAnd</code>-parameter (our <code>userInput</code> is converted to <code>weakAnd</code>, see docs), as well as several additional <code>weakAnd</code>-parameters. See Vespa blog for details. We will not go in detail of this in this notebook.</p> <p>Now, we will move on to demonstrate how to evaluate the ranking strategies. For that, we will use the <code>VespaEvaluator</code>-class. Its interface is very similar to <code>VespaMatchEvaluator</code>, with the difference that it has much more metrics available. Also note that number of <code>hits</code> will affect the number of documents considered for evaluation.</p> In\u00a0[15]: Copied! <pre>from vespa.evaluation import VespaEvaluator\n\n?VespaEvaluator\n</pre> from vespa.evaluation import VespaEvaluator  ?VespaEvaluator <pre>Init signature:\nVespaEvaluator(\n    queries: 'Dict[str, str]',\n    relevant_docs: 'Union[Dict[str, Union[Set[str], Dict[str, float]]], Dict[str, str]]',\n    vespa_query_fn: 'Callable[[str, int, Optional[str]], dict]',\n    app: 'Vespa',\n    name: 'str' = '',\n    id_field: 'str' = '',\n    accuracy_at_k: 'List[int]' = [1, 3, 5, 10],\n    precision_recall_at_k: 'List[int]' = [1, 3, 5, 10],\n    mrr_at_k: 'List[int]' = [10],\n    ndcg_at_k: 'List[int]' = [10],\n    map_at_k: 'List[int]' = [100],\n    write_csv: 'bool' = False,\n    csv_dir: 'Optional[str]' = None,\n)\nDocstring:     \nEvaluate retrieval performance on a Vespa application.\n\nThis class:\n\n- Iterates over queries and issues them against your Vespa application.\n- Retrieves top-k documents per query (with k = max of your IR metrics).\n- Compares the retrieved documents with a set of relevant document ids.\n- Computes IR metrics: Accuracy@k, Precision@k, Recall@k, MRR@k, NDCG@k, MAP@k.\n- Logs vespa search times for each query.\n- Logs/returns these metrics.\n- Optionally writes out to CSV.\n\nNote: The 'id_field' needs to be marked as an attribute in your Vespa schema, so filtering can be done on it.\n\n\nExample usage:\n    ```python\n    from vespa.application import Vespa\n    from vespa.evaluation import VespaEvaluator\n\n    queries = {\n        \"q1\": \"What is the best GPU for gaming?\",\n        \"q2\": \"How to bake sourdough bread?\",\n        # ...\n    }\n    relevant_docs = {\n        \"q1\": {\"d12\", \"d99\"},\n        \"q2\": {\"d101\"},\n        # ...\n    }\n    # relevant_docs can also be a dict of query_id =&gt; single relevant doc_id\n    # relevant_docs = {\n    #     \"q1\": \"d12\",\n    #     \"q2\": \"d101\",\n    #     # ...\n    # }\n    # Or, relevant_docs can be a dict of query_id =&gt; map of doc_id =&gt; relevance\n    # relevant_docs = {\n    #     \"q1\": {\"d12\": 1, \"d99\": 0.1},\n    #     \"q2\": {\"d101\": 0.01},\n    #     # ...\n    # Note that for non-binary relevance, the relevance values should be in [0, 1], and that\n    # only the nDCG metric will be computed.\n\n    def my_vespa_query_fn(query_text: str, top_k: int) -&gt; dict:\n        return {\n            \"yql\": 'select * from sources * where userInput(\"' + query_text + '\");',\n            \"hits\": top_k,\n            \"ranking\": \"your_ranking_profile\",\n        }\n\n    app = Vespa(url=\"http://localhost\", port=8080)\n\n    evaluator = VespaEvaluator(\n        queries=queries,\n        relevant_docs=relevant_docs,\n        vespa_query_fn=my_vespa_query_fn,\n        app=app,\n        name=\"test-run\",\n        accuracy_at_k=[1, 3, 5],\n        precision_recall_at_k=[1, 3, 5],\n        mrr_at_k=[10],\n        ndcg_at_k=[10],\n        map_at_k=[100],\n        write_csv=True\n    )\n\n    results = evaluator()\n    print(\"Primary metric:\", evaluator.primary_metric)\n    print(\"All results:\", results)\n    ```\n\nArgs:\n    queries (Dict[str, str]): A dictionary where keys are query IDs and values are query strings.\n    relevant_docs (Union[Dict[str, Union[Set[str], Dict[str, float]]], Dict[str, str]]):\n        A dictionary mapping query IDs to their relevant document IDs.\n        Can be a set of doc IDs for binary relevance, a dict of doc_id to relevance score (float between 0 and 1)\n        for graded relevance, or a single doc_id string.\n    vespa_query_fn (Callable[[str, int, Optional[str]], dict]): A function that takes a query string,\n        the number of hits to retrieve (top_k), and an optional query_id, and returns a Vespa query body dictionary.\n    app (Vespa): An instance of the Vespa application.\n    name (str, optional): A name for this evaluation run. Defaults to \"\".\n    id_field (str, optional): The field name in the Vespa hit that contains the document ID.\n        If empty, it tries to infer the ID from the 'id' field or 'fields.id'. Defaults to \"\".\n    accuracy_at_k (List[int], optional): List of k values for which to compute Accuracy@k.\n        Defaults to [1, 3, 5, 10].\n    precision_recall_at_k (List[int], optional): List of k values for which to compute Precision@k and Recall@k.\n        Defaults to [1, 3, 5, 10].\n    mrr_at_k (List[int], optional): List of k values for which to compute MRR@k. Defaults to [10].\n    ndcg_at_k (List[int], optional): List of k values for which to compute NDCG@k. Defaults to [10].\n    map_at_k (List[int], optional): List of k values for which to compute MAP@k. Defaults to [100].\n    write_csv (bool, optional): Whether to write the evaluation results to a CSV file. Defaults to False.\n    csv_dir (Optional[str], optional): Directory to save the CSV file. Defaults to None (current directory).\nFile:           ~/Repos/pyvespa/vespa/evaluation.py\nType:           ABCMeta\nSubclasses:     </pre> In\u00a0[16]: Copied! <pre>def semantic_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(schema_name)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"embedding\",\n                    query_vector=\"q\",\n                    annotations={\"targetHits\": 100},\n                )\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"semantic\",\n        \"input.query(q)\": f\"embed({query_text})\",\n        \"hits\": top_k,\n    }\n\n\ndef bm25_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": \"select * from sources * where userQuery();\",  # provide the yql directly as a string\n        \"query\": query_text,\n        \"ranking\": \"bm25\",\n        \"hits\": top_k,\n    }\n\n\ndef fusion_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(schema_name)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"embedding\",\n                    query_vector=\"q\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.userQuery(query_text)\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"fusion\",\n        \"input.query(q)\": f\"embed({query_text})\",\n        \"hits\": top_k,\n    }\n\n\ndef atan_norm_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(schema_name)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"embedding\",\n                    query_vector=\"q\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.userQuery(query_text)\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"atan_norm\",\n        \"input.query(q)\": f\"embed({query_text})\",\n        \"hits\": top_k,\n    }\n</pre> def semantic_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(schema_name)             .where(                 qb.nearestNeighbor(                     field=\"embedding\",                     query_vector=\"q\",                     annotations={\"targetHits\": 100},                 )             )         ),         \"query\": query_text,         \"ranking\": \"semantic\",         \"input.query(q)\": f\"embed({query_text})\",         \"hits\": top_k,     }   def bm25_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": \"select * from sources * where userQuery();\",  # provide the yql directly as a string         \"query\": query_text,         \"ranking\": \"bm25\",         \"hits\": top_k,     }   def fusion_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(schema_name)             .where(                 qb.nearestNeighbor(                     field=\"embedding\",                     query_vector=\"q\",                     annotations={\"targetHits\": 100},                 )                 | qb.userQuery(query_text)             )         ),         \"query\": query_text,         \"ranking\": \"fusion\",         \"input.query(q)\": f\"embed({query_text})\",         \"hits\": top_k,     }   def atan_norm_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(schema_name)             .where(                 qb.nearestNeighbor(                     field=\"embedding\",                     query_vector=\"q\",                     annotations={\"targetHits\": 100},                 )                 | qb.userQuery(query_text)             )         ),         \"query\": query_text,         \"ranking\": \"atan_norm\",         \"input.query(q)\": f\"embed({query_text})\",         \"hits\": top_k,     } In\u00a0[23]: Copied! <pre>all_results = {}\nfor evaluator_name, query_fn in [\n    (\"semantic\", semantic_query_fn),\n    (\"bm25\", bm25_query_fn),\n    (\"fusion\", fusion_query_fn),\n    (\"atan_norm\", atan_norm_query_fn),\n]:\n    print(f\"Evaluating {evaluator_name}...\")\n    evaluator = VespaEvaluator(\n        queries=ids_to_query,\n        relevant_docs=relevant_docs,\n        vespa_query_fn=query_fn,\n        app=app,\n        name=evaluator_name,\n        write_csv=True,  # optionally write metrics to CSV\n    )\n\n    results = evaluator.run()\n    all_results[evaluator_name] = results\n</pre> all_results = {} for evaluator_name, query_fn in [     (\"semantic\", semantic_query_fn),     (\"bm25\", bm25_query_fn),     (\"fusion\", fusion_query_fn),     (\"atan_norm\", atan_norm_query_fn), ]:     print(f\"Evaluating {evaluator_name}...\")     evaluator = VespaEvaluator(         queries=ids_to_query,         relevant_docs=relevant_docs,         vespa_query_fn=query_fn,         app=app,         name=evaluator_name,         write_csv=True,  # optionally write metrics to CSV     )      results = evaluator.run()     all_results[evaluator_name] = results <pre>Evaluating semantic...\nEvaluating bm25...\nEvaluating fusion...\nEvaluating atan_norm...\n</pre> In\u00a0[24]: Copied! <pre>results = pd.DataFrame(all_results)\n</pre> results = pd.DataFrame(all_results) In\u00a0[25]: Copied! <pre># take out all rows with \"searchtime\" to a separate dataframe\nsearchtime = results[results.index.str.contains(\"searchtime\")]\nresults = results[~results.index.str.contains(\"searchtime\")]\n\n\n# Highlight the maximum value in each row\ndef highlight_max(s):\n    is_max = s == s.max()\n    return [\"background-color: lightgreen; color: black;\" if v else \"\" for v in is_max]\n\n\n# Style the DataFrame: Highlight max values and format numbers to 4 decimals\nstyled_df = results.style.apply(highlight_max, axis=1).format(\"{:.4f}\")\nstyled_df\n</pre> # take out all rows with \"searchtime\" to a separate dataframe searchtime = results[results.index.str.contains(\"searchtime\")] results = results[~results.index.str.contains(\"searchtime\")]   # Highlight the maximum value in each row def highlight_max(s):     is_max = s == s.max()     return [\"background-color: lightgreen; color: black;\" if v else \"\" for v in is_max]   # Style the DataFrame: Highlight max values and format numbers to 4 decimals styled_df = results.style.apply(highlight_max, axis=1).format(\"{:.4f}\") styled_df Out[25]: semantic bm25 fusion atan_norm accuracy@1 0.3800 0.3000 0.4400 0.4400 accuracy@3 0.6400 0.6000 0.6800 0.7000 accuracy@5 0.7200 0.6600 0.7200 0.7400 accuracy@10 0.8200 0.7400 0.8000 0.8000 precision@1 0.3800 0.3000 0.4400 0.4400 recall@1 0.3800 0.3000 0.4400 0.4400 precision@3 0.2133 0.2000 0.2267 0.2333 recall@3 0.6400 0.6000 0.6800 0.7000 precision@5 0.1440 0.1320 0.1440 0.1480 recall@5 0.7200 0.6600 0.7200 0.7400 precision@10 0.0820 0.0740 0.0800 0.0800 recall@10 0.8200 0.7400 0.8000 0.8000 mrr@10 0.5309 0.4501 0.5529 0.5738 ndcg@10 0.6007 0.5206 0.6126 0.6296 map@100 0.5393 0.4594 0.5630 0.5838 <p>We can see that for this particular dataset, the hybrid strategy <code>atan_norm</code> is the best across all metrics.</p> In\u00a0[26]: Copied! <pre>results.plot(kind=\"bar\", figsize=(12, 6))\n</pre> results.plot(kind=\"bar\", figsize=(12, 6)) Out[26]: <pre>&lt;Axes: &gt;</pre> In\u00a0[27]: Copied! <pre># plot search time, add (ms) to the y-axis\n# convert to ms\nsearchtime = searchtime * 1000\nsearchtime.plot(kind=\"bar\", figsize=(12, 6)).set(ylabel=\"time (ms)\")\n</pre> # plot search time, add (ms) to the y-axis # convert to ms searchtime = searchtime * 1000 searchtime.plot(kind=\"bar\", figsize=(12, 6)).set(ylabel=\"time (ms)\") Out[27]: <pre>[Text(0, 0.5, 'time (ms)')]</pre> <p>We can see that both hybrid strategies, <code>fusion</code> and <code>atan_norm</code> strategy is a bit slower on average than pure <code>bm25</code> or <code>semantic</code>, as expected.</p> <p>Depending on the latency budget of your application, this is likely still an attractive trade-off.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"evaluating-vespa-application-cloud.html#evaluating-a-vespa-application","title":"Evaluating a Vespa Application\u00b6","text":"<p>We are often asked by users and customers what is the best retrieval and ranking strategy for a given use case. And even though we might sometimes have an intuition, we always recommend to set up experiments and do a proper quantitative evaluation.</p> <p>Models are temporary; Evals are forever.</p> <p>-Eugene Yan</p> <p>Without a proper evaluation setup, you run the risk of settling for <code>lgtm@10</code> (looks good to me @ 10).</p> <p>Then, if you deploy your application to users, you can be sure that you will get feedback of queries that does not produce relevant results. If you then try to optimize for that without knowing whether your tweaks are actually improving the overall quality of your search, you might end up with a system that is worse than the one you started with.</p> <p></p>"},{"location":"evaluating-vespa-application-cloud.html#so-what-can-you-do","title":"So, what can you do?\u00b6","text":"<p>You can set up a proper evaluation pipeline, where you can test different ranking strategies, and see how they perform on a set of evaluation queries that act as a proxy for your real users' queries. This way, you can make informed decisions about what works best for your use case. If you collect real user interactions, it could be even better, but it is important to also keep the evaluation pipeline light enough so that you can run it both during development and in CI pipelines (possibly at different scales).</p> <p>This guide will show how you easily can evaluate a Vespa application using pyvespa's <code>VespaMatchEvaluator</code> and <code>VespaEvaluator</code> class.</p>"},{"location":"evaluating-vespa-application-cloud.html#evaluate-match-phase-retrieval-for-recall","title":"Evaluate match-phase (retrieval) for recall\u00b6","text":"<p>The match-phase (or retrieval phase) in Vespa is perform to retrieve candidate documents to rank. Here, what we care about is that all possibly relevant documents are retrieved fast, without matching too many documents. If we match too many documents, latency will suffer, as all retrieved docs will be exposed to ranking. For an introduction to phased retrieval in Vespa, see the docs</p> <p>For this tutorial, we will evaluate and compare <code>weakAnd</code>, <code>nearestNeighbor</code>, as well as the combination of the two (using <code>OR</code>-operator).</p>"},{"location":"evaluating-vespa-application-cloud.html#evaluate-ranking","title":"Evaluate ranking\u00b6","text":"<p>We will define and compare 4 different ranking strategies in this guide:</p> <ol> <li><code>bm25</code> - Keyword-based retrieval and ranking - The solid baseline.</li> <li><code>semantic</code> - Vector search using cosine similarity (using https://huggingface.co/intfloat/e5-small-v2 for embeddings)</li> <li><code>fusion</code>- Hybrid search (semantic+keyword). Combining BM25 and Semantic with reciprocal rank fusion</li> <li><code>atan_norm</code> - Hybrid search, combining BM25 and Semantic with atan normalization as described in Aapo Tanskanen's Guidebook to the State-of-the-Art Embeddings and Information Retrieval (Originally proposed by Seo et al. (2022))</li> </ol>"},{"location":"evaluating-vespa-application-cloud.html#install","title":"Install\u00b6","text":"<p>Install pyvespa &gt;= 0.53.0 and the Vespa CLI. The Vespa CLI is used for data and control plane key management (Vespa Cloud Security Guide).</p>"},{"location":"evaluating-vespa-application-cloud.html#configure-application","title":"Configure application\u00b6","text":""},{"location":"evaluating-vespa-application-cloud.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files - create one from scratch:</p>"},{"location":"evaluating-vespa-application-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>The app is now defined and ready to deploy to Vespa Cloud.</p> <p>Deploy <code>package</code> to Vespa Cloud, by creating an instance of VespaCloud:</p>"},{"location":"evaluating-vespa-application-cloud.html#getting-your-evaluation-data","title":"Getting your evaluation data\u00b6","text":"<p>For evaluating information retrieval methods, in addition to the document corpus, we also need a set of queries and a mapping from queries to relevant documents.</p> <p>For this guide, we will use the NanoMSMARCO dataset, made available on huggingface by Zeta Alpha.</p> <p>This dataset is a subset of their \ud83c\udf7aNanoBEIR-collection, with 50 queries and up to 10K documents each.</p> <p>This is a great dataset for testing and evaluating information retrieval methods quickly, as it is small and easy to work with.</p> <p>Note that for almost any real-world use case, we would recommend you to create your own evaluation dataset. See Vespa blog post on how you can get help from an LLM for this.</p> <p>Note that creating 20-50 queries and annotating relevant documents for each query can be a good start and well worth the effort.</p>"},{"location":"evaluating-vespa-application-cloud.html#evaluate-match-phase","title":"Evaluate match-phase\u00b6","text":""},{"location":"evaluating-vespa-application-cloud.html#evaluate-ranking","title":"Evaluate ranking\u00b6","text":""},{"location":"evaluating-vespa-application-cloud.html#vespaevaluator","title":"VespaEvaluator\u00b6","text":"<p>Let us take a look at its API and documentation:</p>"},{"location":"evaluating-vespa-application-cloud.html#looking-at-the-results","title":"Looking at the results\u00b6","text":""},{"location":"evaluating-vespa-application-cloud.html#looking-at-searchtimes","title":"Looking at searchtimes\u00b6","text":"<p>Ranking quality is not the only thing that matters. For many applications, search time is equally important.</p>"},{"location":"evaluating-vespa-application-cloud.html#conclusion-and-next-steps","title":"Conclusion and next steps\u00b6","text":"<p>We have shown how you can evaluate a Vespa application on two different levels.</p> <ol> <li>Evaluate retrieval (match-phase) using the <code>VespaMatchEvaluator</code> class. Here we checked recall, and</li> <li>Evaluate ranking strategies using <code>VespaEvaluator</code> class. We have defined and compared 4 different ranking strategies in terms of both ranking quality and searchtime latency.</li> </ol> <p>We hope this can provide you with a good starting point for evaluating your own Vespa application.</p> <p>If you are ready to advance, you can try to optimize the ranking strategies further, by for example weighing each of the terms in the <code>atan_norm</code> strategy differently (<code>a * normalize_linear(normalized_bm25) + (1-a) * normalize_linear(cos_sim)</code>) , or by adding a crossencoder for re-ranking the top-k results.</p>"},{"location":"evaluating-vespa-application-cloud.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"getting-started-pyvespa-cloud.html","title":"Hybrid Search - Quickstart on Vespa Cloud","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Pre-requisite: Create a tenant at cloud.vespa.ai, save the tenant name.</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa vespacli\n</pre> !pip3 install pyvespa vespacli In\u00a0[2]: Copied! <pre># Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n# Replace with your application name (does not need to exist yet)\napplication = \"hybridsearch\"\n</pre> # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\" # Replace with your application name (does not need to exist yet) application = \"hybridsearch\" In\u00a0[\u00a0]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    Component,\n    Parameter,\n    FieldSet,\n    GlobalPhaseRanking,\n    Function,\n)\n\npackage = ApplicationPackage(\n    name=application,\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(\n                        name=\"title\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                    ),\n                    Field(\n                        name=\"body\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                        bolding=True,\n                    ),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;float&gt;(x[384])\",\n                        indexing=[\n                            'input title . \" \" . input body',\n                            \"embed\",\n                            \"index\",\n                            \"attribute\",\n                        ],\n                        ann=HNSW(distance_metric=\"angular\"),\n                        is_document_field=False,\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],\n            rank_profiles=[\n                RankProfile(\n                    name=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[\n                        Function(name=\"bm25sum\", expression=\"bm25(title) + bm25(body)\")\n                    ],\n                    first_phase=\"bm25sum\",\n                ),\n                RankProfile(\n                    name=\"semantic\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                ),\n                RankProfile(\n                    name=\"fusion\",\n                    inherits=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                    global_phase=GlobalPhaseRanking(\n                        expression=\"reciprocal_rank_fusion(bm25sum, closeness(field, embedding))\",\n                        rerank_count=1000,\n                    ),\n                ),\n            ],\n        )\n    ],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"url\": \"https://data.vespa-cloud.com/sample-apps-data/e5-small-v2-int8/e5-small-v2-int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\n                        \"url\": \"https://data.vespa-cloud.com/sample-apps-data/e5-small-v2-int8/tokenizer.json\"\n                    },\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     Component,     Parameter,     FieldSet,     GlobalPhaseRanking,     Function, )  package = ApplicationPackage(     name=application,     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(                         name=\"title\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                     ),                     Field(                         name=\"body\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                         bolding=True,                     ),                     Field(                         name=\"embedding\",                         type=\"tensor(x[384])\",                         indexing=[                             'input title . \" \" . input body',                             \"embed\",                             \"index\",                             \"attribute\",                         ],                         ann=HNSW(distance_metric=\"angular\"),                         is_document_field=False,                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],             rank_profiles=[                 RankProfile(                     name=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[                         Function(name=\"bm25sum\", expression=\"bm25(title) + bm25(body)\")                     ],                     first_phase=\"bm25sum\",                 ),                 RankProfile(                     name=\"semantic\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"closeness(field, embedding)\",                 ),                 RankProfile(                     name=\"fusion\",                     inherits=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"closeness(field, embedding)\",                     global_phase=GlobalPhaseRanking(                         expression=\"reciprocal_rank_fusion(bm25sum, closeness(field, embedding))\",                         rerank_count=1000,                     ),                 ),             ],         )     ],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"url\": \"https://data.vespa-cloud.com/sample-apps-data/e5-small-v2-int8/e5-small-v2-int8.onnx\"                     },                 ),                 Parameter(                     \"tokenizer-model\",                     {                         \"url\": \"https://data.vespa-cloud.com/sample-apps-data/e5-small-v2-int8/tokenizer.json\"                     },                 ),             ],         )     ], ) <p>Note that the name cannot have <code>-</code> or <code>_</code>.</p> In\u00a0[4]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=package, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.hybridsearch\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p>For more details on different authentication options and methods, see authenticating-to-vespa-cloud.</p> <p>The following will upload the application package to Vespa Cloud Dev Zone (<code>aws-us-east-1c</code>), read more about Vespa Zones. The Vespa Cloud Dev Zone is considered as a sandbox environment where resources are down-scaled and idle deployments are expired automatically. For information about production deployments, see the following method.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p> <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up. (Applications that for example refer to large onnx-models may take a bit longer.)</p> In\u00a0[5]: Copied! <pre>app = vespa_cloud.deploy()\n</pre> app = vespa_cloud.deploy() <pre>Deployment started in run 7 of dev-aws-us-east-1c for vespa-team.hybridsearch. This may take a few minutes the first time.\nINFO    [07:04:51]  Deploying platform version 8.367.14 and application dev build 6 for dev-aws-us-east-1c of default ...\nINFO    [07:04:51]  Using CA signed certificate version 3\nINFO    [07:04:52]  Using 1 nodes in container cluster 'hybridsearch_container'\nINFO    [07:04:53]  Validating Onnx models memory usage for container cluster 'hybridsearch_container', percentage of available memory too low (10 &lt; 15) to avoid restart, consider a flavor with more memory to avoid this\nWARNING [07:04:53]  Auto-overriding validation which would be disallowed in production: certificate-removal: Data plane certificate(s) from cluster 'hybridsearch_container' is removed (removed certificates: [CN=cloud.vespa.example]) This can cause client connection issues.. To allow this add &lt;allow until='yyyy-mm-dd'&gt;certificate-removal&lt;/allow&gt; to validation-overrides.xml, see https://docs.vespa.ai/en/reference/validation-overrides.html\nINFO    [07:04:55]  Session 298587 for tenant 'vespa-team' prepared and activated.\nINFO    [07:04:55]  ######## Details for all nodes ########\nINFO    [07:04:55]  h94416a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:04:55]  --- platform vespa/cloud-tenant-rhel8:8.367.14\nINFO    [07:04:55]  --- container on port 4080 has config generation 298580, wanted is 298587\nINFO    [07:04:55]  --- metricsproxy-container on port 19092 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  h94249f.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:04:55]  --- platform vespa/cloud-tenant-rhel8:8.367.14\nINFO    [07:04:55]  --- container-clustercontroller on port 19050 has config generation 298580, wanted is 298587\nINFO    [07:04:55]  --- metricsproxy-container on port 19092 has config generation 298580, wanted is 298587\nINFO    [07:04:55]  h93394a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:04:55]  --- platform vespa/cloud-tenant-rhel8:8.367.14\nINFO    [07:04:55]  --- logserver-container on port 4080 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  --- metricsproxy-container on port 19092 has config generation 298580, wanted is 298587\nINFO    [07:04:55]  h94419a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:04:55]  --- platform vespa/cloud-tenant-rhel8:8.367.14\nINFO    [07:04:55]  --- storagenode on port 19102 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  --- searchnode on port 19107 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  --- distributor on port 19111 has config generation 298587, wanted is 298587\nINFO    [07:04:55]  --- metricsproxy-container on port 19092 has config generation 298587, wanted is 298587\nINFO    [07:05:02]  Found endpoints:\nINFO    [07:05:02]  - dev.aws-us-east-1c\nINFO    [07:05:02]   |-- https://f7f73182.eb1181f2.z.vespa-app.cloud/ (cluster 'hybridsearch_container')\nINFO    [07:05:02]  Deployment of new application complete!\nFound mtls endpoint for hybridsearch_container\nURL: https://f7f73182.eb1181f2.z.vespa-app.cloud/\nConnecting to https://f7f73182.eb1181f2.z.vespa-app.cloud/\nUsing mtls_key_cert Authentication against endpoint https://f7f73182.eb1181f2.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>If the deployment failed, it is possible you forgot to add the key in the Vespa Cloud Console in the <code>vespa auth api-key</code> step above.</p> <p>If you can authenticate, you should see lines like the following</p> <pre><code> Deployment started in run 1 of dev-aws-us-east-1c for mytenant.hybridsearch.\n</code></pre> <p>The deployment takes a few minutes the first time while Vespa Cloud sets up the resources for your Vespa application</p> <p><code>app</code> now holds a reference to a Vespa instance. We can access the mTLS protected endpoint name using the control-plane (vespa_cloud) instance. This endpoint we can query and feed to (data plane access) using the mTLS certificate generated in previous steps.</p> In\u00a0[6]: Copied! <pre>endpoint = vespa_cloud.get_mtls_endpoint()\nendpoint\n</pre> endpoint = vespa_cloud.get_mtls_endpoint() endpoint <pre>Found mtls endpoint for hybridsearch_container\nURL: https://f7f73182.eb1181f2.z.vespa-app.cloud/\n</pre> Out[6]: <pre>'https://f7f73182.eb1181f2.z.vespa-app.cloud/'</pre> In\u00a0[7]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True)\nvespa_feed = dataset.map(\n    lambda x: {\n        \"id\": x[\"_id\"],\n        \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},\n    }\n)\n</pre> from datasets import load_dataset  dataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True) vespa_feed = dataset.map(     lambda x: {         \"id\": x[\"_id\"],         \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},     } ) <p>Now we can feed to Vespa using <code>feed_iterable</code> which accepts any <code>Iterable</code> and an optional callback function where we can check the outcome of each operation. The application is configured to use embedding functionality, that produce a vector embedding using a concatenation of the title and the body input fields. This step is resource intensive.</p> <p>Read more about embedding inference in Vespa in the Accelerating Transformer-based Embedding Retrieval with Vespa blog post.</p> <p>Default node resources in Vespa Cloud have 2 v-cpu for the Dev Zone.</p> In\u00a0[8]: Copied! <pre>from vespa.io import VespaResponse, VespaQueryResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback)\n</pre> from vespa.io import VespaResponse, VespaQueryResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback) <pre>Using mtls_key_cert Authentication against endpoint https://f7f73182.eb1181f2.z.vespa-app.cloud//ApplicationStatus\n</pre> In\u00a0[9]: Copied! <pre>import pandas as pd\n\n\ndef display_hits_as_df(response: VespaQueryResponse, fields) -&gt; pd.DataFrame:\n    records = []\n    for hit in response.hits:\n        record = {}\n        for field in fields:\n            record[field] = hit[\"fields\"][field]\n        records.append(record)\n    return pd.DataFrame(records)\n</pre> import pandas as pd   def display_hits_as_df(response: VespaQueryResponse, fields) -&gt; pd.DataFrame:     records = []     for hit in response.hits:         record = {}         for field in fields:             record[field] = hit[\"fields\"][field]         records.append(record)     return pd.DataFrame(records) In\u00a0[10]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where userQuery() limit 5\",\n        query=query,\n        ranking=\"bm25\",\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where userQuery() limit 5\",         query=query,         ranking=\"bm25\",     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-2450  Protective effect of fruits, vegetables and th...\n1  MED-2464  Low vegetable intake is associated with allerg...\n2  MED-1162  Pesticide residues in imported, organic, and \"...\n3  MED-2461  The association of diet with respiratory sympt...\n4  MED-2085  Antiplatelet, anticoagulant, and fibrinolytic ...\n</pre> In\u00a0[11]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where ({targetHits:5}nearestNeighbor(embedding,q)) limit 5\",\n        query=query,\n        ranking=\"semantic\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where ({targetHits:5}nearestNeighbor(embedding,q)) limit 5\",         query=query,         ranking=\"semantic\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-5072  Lycopene-rich treatments modify noneosinophili...\n1  MED-2472  Vegan regimen with reduced medication in the t...\n2  MED-2464  Low vegetable intake is associated with allerg...\n3  MED-2458  Manipulating antioxidant intake in asthma: a r...\n4  MED-2450  Protective effect of fruits, vegetables and th...\n</pre> In\u00a0[12]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where userQuery() or ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where userQuery() or ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-2464  Low vegetable intake is associated with allerg...\n1  MED-2450  Protective effect of fruits, vegetables and th...\n2  MED-2458  Manipulating antioxidant intake in asthma: a r...\n3  MED-2461  The association of diet with respiratory sympt...\n4  MED-5072  Lycopene-rich treatments modify noneosinophili...\n</pre> In\u00a0[13]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5\",\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5\",         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-2464  Low vegetable intake is associated with allerg...\n1  MED-2450  Protective effect of fruits, vegetables and th...\n2  MED-2458  Manipulating antioxidant intake in asthma: a r...\n3  MED-2461  The association of diet with respiratory sympt...\n4  MED-5072  Lycopene-rich treatments modify noneosinophili...\n</pre> In\u00a0[14]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql='select * from sources * where title contains \"vegetable\" and rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5',\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql='select * from sources * where title contains \"vegetable\" and rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5',         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <pre>         id                                              title\n0  MED-2464  Low vegetable intake is associated with allerg...\n1  MED-2450  Protective effect of fruits, vegetables and th...\n2  MED-3199  Potential risks resulting from fruit/vegetable...\n3  MED-2085  Antiplatelet, anticoagulant, and fibrinolytic ...\n4  MED-4496  The effect of fruit and vegetable intake on ri...\n</pre> <p>Set up a dataplane connection using the cert/key pair:</p> In\u00a0[15]: Copied! <pre>import requests\n\ncert_path = app.cert\nkey_path = app.key\nsession = requests.Session()\nsession.cert = (cert_path, key_path)\n</pre> import requests  cert_path = app.cert key_path = app.key session = requests.Session() session.cert = (cert_path, key_path) <p>Get a document from the endpoint returned when we deployed to Vespa Cloud above. PyVespa wraps the Vespa document api internally and in these examples we use the document api directly, but with the mTLS key/cert pair we used when deploying the app.</p> In\u00a0[16]: Copied! <pre>url = \"{0}/document/v1/{1}/{2}/docid/{3}\".format(endpoint, \"tutorial\", \"doc\", \"MED-10\")\ndoc = session.get(url).json()\ndoc\n</pre> url = \"{0}/document/v1/{1}/{2}/docid/{3}\".format(endpoint, \"tutorial\", \"doc\", \"MED-10\") doc = session.get(url).json() doc Out[16]: <pre>{'pathId': '/document/v1/tutorial/doc/docid/MED-10',\n 'id': 'id:tutorial:doc::MED-10',\n 'fields': {'body': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995\u20132003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08\u20139.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38\u20130.55 and HR 0.54, 95% CI 0.44\u20130.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins\u2019 effect on survival in breast cancer patients.',\n  'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland',\n  'id': 'MED-10'}}</pre> <p>Update the title and post the new version:</p> In\u00a0[17]: Copied! <pre>doc[\"fields\"][\"title\"] = \"Can you eat lobster?\"\nresponse = session.post(url, json=doc).json()\nresponse\n</pre> doc[\"fields\"][\"title\"] = \"Can you eat lobster?\" response = session.post(url, json=doc).json() response Out[17]: <pre>{'pathId': '/document/v1/tutorial/doc/docid/MED-10',\n 'id': 'id:tutorial:doc::MED-10'}</pre> <p>Get the doc again to see the new title:</p> In\u00a0[18]: Copied! <pre>doc = session.get(url).json()\ndoc\n</pre> doc = session.get(url).json() doc Out[18]: <pre>{'pathId': '/document/v1/tutorial/doc/docid/MED-10',\n 'id': 'id:tutorial:doc::MED-10',\n 'fields': {'body': 'Recent studies have suggested that statins, an established drug group in the prevention of cardiovascular mortality, could delay or prevent breast cancer recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of breast cancer death among statin users in a population-based cohort of breast cancer patients. The study cohort included all newly diagnosed breast cancer patients in Finland during 1995\u20132003 (31,236 cases), identified from the Finnish Cancer Registry. Information on statin use before and after the diagnosis was obtained from a national prescription database. We used the Cox proportional hazards regression method to estimate mortality among statin users with statin use as time-dependent variable. A total of 4,151 participants had used statins. During the median follow-up of 3.25 years after the diagnosis (range 0.08\u20139.0 years) 6,011 participants died, of which 3,619 (60.2%) was due to breast cancer. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic statin use were associated with lowered risk of breast cancer death (HR 0.46, 95% CI 0.38\u20130.55 and HR 0.54, 95% CI 0.44\u20130.67, respectively). The risk decrease by post-diagnostic statin use was likely affected by healthy adherer bias; that is, the greater likelihood of dying cancer patients to discontinue statin use as the association was not clearly dose-dependent and observed already at low-dose/short-term use. The dose- and time-dependence of the survival benefit among pre-diagnostic statin users suggests a possible causal effect that should be evaluated further in a clinical trial testing statins\u2019 effect on survival in breast cancer patients.',\n  'title': 'Can you eat lobster?',\n  'id': 'MED-10'}}</pre> In\u00a0[19]: Copied! <pre># cert_path = \"/Users/me/.vespa/mytenant.hybridsearch.default/data-plane-public-cert.pem\"\n# key_path  = \"/Users/me/.vespa/mytenant.hybridsearch.default/data-plane-private-key.pem\"\n\nfrom vespa.application import Vespa\n\nthe_app = Vespa(endpoint, cert=cert_path, key=key_path)\n\nres = the_app.query(\n    yql=\"select documentid, id, title from sources * where userQuery()\",\n    query=\"Can you eat lobster?\",\n    ranking=\"bm25\",\n)\nres.hits[0]\n</pre> # cert_path = \"/Users/me/.vespa/mytenant.hybridsearch.default/data-plane-public-cert.pem\" # key_path  = \"/Users/me/.vespa/mytenant.hybridsearch.default/data-plane-private-key.pem\"  from vespa.application import Vespa  the_app = Vespa(endpoint, cert=cert_path, key=key_path)  res = the_app.query(     yql=\"select documentid, id, title from sources * where userQuery()\",     query=\"Can you eat lobster?\",     ranking=\"bm25\", ) res.hits[0] <pre>Using mtls_key_cert Authentication against endpoint https://f7f73182.eb1181f2.z.vespa-app.cloud//ApplicationStatus\n</pre> Out[19]: <pre>{'id': 'id:tutorial:doc::MED-10',\n 'relevance': 25.27992205160453,\n 'source': 'hybridsearch_content',\n 'fields': {'documentid': 'id:tutorial:doc::MED-10',\n  'id': 'MED-10',\n  'title': 'Can you eat lobster?'}}</pre> <p>A common problem is a cert mismatch - the cert/key pair used when deployed is different than the pair used when making requests against Vespa. This will cause 40x errors.</p> <p>Make sure it is the same pair / re-create with <code>vespa auth cert -f</code> AND redeploy.</p> <p>If you re-generate a mTLS certificate pair, and use that when connecting to Vespa cloud endpoint, it will fail until you have updaded the deployment with the new public certificate.</p> In\u00a0[20]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-team.hybridsearch in dev.aws-us-east-1c\nDeleted instance vespa-team.hybridsearch.default\n</pre>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search-quickstart-on-vespa-cloud","title":"Hybrid Search - Quickstart on Vespa Cloud\u00b6","text":"<p>This is the same guide as getting-started-pyvespa, deploying to Vespa Cloud.</p>"},{"location":"getting-started-pyvespa-cloud.html#install","title":"Install\u00b6","text":"<p>Install pyvespa &gt;= 0.45 and the Vespa CLI. The Vespa CLI is used for data and control plane key management (Vespa Cloud Security Guide).</p>"},{"location":"getting-started-pyvespa-cloud.html#configure-application","title":"Configure application\u00b6","text":""},{"location":"getting-started-pyvespa-cloud.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files - create one from scratch:</p>"},{"location":"getting-started-pyvespa-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>The app is now defined and ready to deploy to Vespa Cloud.</p> <p>Deploy <code>package</code> to Vespa Cloud, by creating an instance of VespaCloud:</p>"},{"location":"getting-started-pyvespa-cloud.html#feeding-documents-to-vespa","title":"Feeding documents to Vespa\u00b6","text":"<p>In this example we use the HF Datasets library to stream the BeIR/nfcorpus dataset and index in our newly deployed Vespa instance. Read more about the NFCorpus:</p> <p>NFCorpus is a full-text English retrieval data set for Medical Information Retrieval.</p> <p>The following uses the stream option of datasets to stream the data without downloading all the contents locally. The <code>map</code> functionality allows us to convert the dataset fields into the expected feed format for <code>pyvespa</code> which expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p>"},{"location":"getting-started-pyvespa-cloud.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Using the Vespa Query language we can query the indexed data.</p> <ul> <li>Using a context manager <code>with app.syncio() as session</code> to handle connection pooling (best practices)</li> <li>The query method accepts any valid Vespa query api parameter in <code>**kwargs</code></li> <li>Vespa api parameter names that contains <code>.</code> must be sent as <code>dict</code> parameters in the <code>body</code> method argument</li> </ul> <p>The following searches for <code>How Fruits and Vegetables Can Treat Asthma?</code> using different retrieval and ranking strategies.</p> <p>Query the text search app using the Vespa Query language by sending the parameters to the body argument of Vespa.query.</p> <p>First we define a simple routine that will return a dataframe of the results for prettier display in the notebook.</p>"},{"location":"getting-started-pyvespa-cloud.html#plain-keyword-search","title":"Plain Keyword search\u00b6","text":"<p>The following uses plain keyword search functionality with bm25 ranking, the <code>bm25</code> rank-profile was configured in the application package to use a linear combination of the bm25 score of the query terms against the title and the body field.</p>"},{"location":"getting-started-pyvespa-cloud.html#plain-semantic-search","title":"Plain Semantic Search\u00b6","text":"<p>The following uses dense vector representations of the query and the document and matching is performed and accelerated by Vespa's support for approximate nearest neighbor search. The vector embedding representation of the text is obtained using Vespa's embedder functionality.</p>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search","title":"Hybrid Search\u00b6","text":"<p>This is one approach to combine the two retrieval strategies and where we use Vespa's support for cross-hits feature normalization and reciprocal rank fusion. This functionality is exposed in the context of <code>global</code> re-ranking, after the distributed query retrieval execution which might span 1000s of nodes.</p>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search-with-the-or-query-operator","title":"Hybrid search with the OR query operator\u00b6","text":"<p>This combines the two methods using logical disjunction (OR). Note that the first-phase expression in our <code>fusion</code> expression is only using the semantic score, this because usually semantic search provides better recall than sparse keyword search alone.</p>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search-with-the-rank-query-operator","title":"Hybrid search with the RANK query operator\u00b6","text":"<p>This combines the two methods using the rank query operator. In this case we express that we want to retrieve the top-1000 documents using vector search, and then have sparse features like BM25 calculated as well (second operand of the rank operator). Finally the hits are re-ranked using the reciprocal rank fusion</p>"},{"location":"getting-started-pyvespa-cloud.html#hybrid-search-with-filters","title":"Hybrid search with filters\u00b6","text":"<p>In this example we add another query term to the yql, restricting the nearest neighbor search to only consider documents that have vegetable in the title.</p>"},{"location":"getting-started-pyvespa-cloud.html#next-steps","title":"Next steps\u00b6","text":"<p>This is just an intro into the capabilities of Vespa and pyvespa. Browse the site to learn more about schemas, feeding and queries - find more complex applications in examples.</p>"},{"location":"getting-started-pyvespa-cloud.html#example-document-operations-using-certkey-pair","title":"Example: Document operations using cert/key pair\u00b6","text":"<p>Above, we deployed to Vespa Cloud, and as part of that, generated a data-plane mTLS cert/key pair.</p> <p>This pair can be used to access the dataplane for reads/writes to documents and running queries from many different clients. The following demonstrates that using the <code>requests</code> library.</p>"},{"location":"getting-started-pyvespa-cloud.html#example-reconnect-pyvespa-using-certkey-pair","title":"Example: Reconnect pyvespa using cert/key pair\u00b6","text":"<p>Above, we stored the dataplane credentials for later use. Deployment of an application usually happens when the schema changes, whereas accessing the dataplane is for document updates and user queries.</p> <p>One only needs to know the endpoint and the cert/key pair to enable a connection to a Vespa Cloud application:</p>"},{"location":"getting-started-pyvespa-cloud.html#delete-application","title":"Delete application\u00b6","text":"<p>The following will delete the application and data from the dev environment.</p>"},{"location":"getting-started-pyvespa.html","title":"Hybrid Search - Quickstart","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install pyvespa and start Docker Daemon, validate minimum 6G available:</p> In\u00a0[1]: Copied! <pre>!pip3 install pyvespa\n!docker info | grep \"Total Memory\"\n</pre> !pip3 install pyvespa !docker info | grep \"Total Memory\" In\u00a0[\u00a0]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    Component,\n    Parameter,\n    FieldSet,\n    GlobalPhaseRanking,\n    Function,\n)\n\npackage = ApplicationPackage(\n    name=\"hybridsearch\",\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(\n                        name=\"title\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                    ),\n                    Field(\n                        name=\"body\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                        bolding=True,\n                    ),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;float&gt;(x[384])\",\n                        indexing=[\n                            'input title . \" \" . input body',\n                            \"embed\",\n                            \"index\",\n                            \"attribute\",\n                        ],\n                        ann=HNSW(distance_metric=\"angular\"),\n                        is_document_field=False,\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],\n            rank_profiles=[\n                RankProfile(\n                    name=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    functions=[\n                        Function(name=\"bm25sum\", expression=\"bm25(title) + bm25(body)\")\n                    ],\n                    first_phase=\"bm25sum\",\n                ),\n                RankProfile(\n                    name=\"semantic\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                ),\n                RankProfile(\n                    name=\"fusion\",\n                    inherits=\"bm25\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                    global_phase=GlobalPhaseRanking(\n                        expression=\"reciprocal_rank_fusion(bm25sum, closeness(field, embedding))\",\n                        rerank_count=1000,\n                    ),\n                ),\n            ],\n        )\n    ],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"url\": \"https://data.vespa-cloud.com/sample-apps-data/e5-small-v2-int8/e5-small-v2-int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\n                        \"url\": \"https://data.vespa-cloud.com/sample-apps-data/e5-small-v2-int8/tokenizer.json\"\n                    },\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     Component,     Parameter,     FieldSet,     GlobalPhaseRanking,     Function, )  package = ApplicationPackage(     name=\"hybridsearch\",     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(                         name=\"title\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                     ),                     Field(                         name=\"body\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                         bolding=True,                     ),                     Field(                         name=\"embedding\",                         type=\"tensor(x[384])\",                         indexing=[                             'input title . \" \" . input body',                             \"embed\",                             \"index\",                             \"attribute\",                         ],                         ann=HNSW(distance_metric=\"angular\"),                         is_document_field=False,                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],             rank_profiles=[                 RankProfile(                     name=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     functions=[                         Function(name=\"bm25sum\", expression=\"bm25(title) + bm25(body)\")                     ],                     first_phase=\"bm25sum\",                 ),                 RankProfile(                     name=\"semantic\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"closeness(field, embedding)\",                 ),                 RankProfile(                     name=\"fusion\",                     inherits=\"bm25\",                     inputs=[(\"query(q)\", \"tensor(x[384])\")],                     first_phase=\"closeness(field, embedding)\",                     global_phase=GlobalPhaseRanking(                         expression=\"reciprocal_rank_fusion(bm25sum, closeness(field, embedding))\",                         rerank_count=1000,                     ),                 ),             ],         )     ],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"url\": \"https://data.vespa-cloud.com/sample-apps-data/e5-small-v2-int8/e5-small-v2-int8.onnx\"                     },                 ),                 Parameter(                     \"tokenizer-model\",                     {                         \"url\": \"https://data.vespa-cloud.com/sample-apps-data/e5-small-v2-int8/tokenizer.json\"                     },                 ),             ],         )     ], ) <p>Note that the name cannot have <code>-</code> or <code>_</code>.</p> In\u00a0[1]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=package) <p><code>app</code> now holds a reference to a Vespa instance.</p> In\u00a0[1]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True)\nvespa_feed = dataset.map(\n    lambda x: {\n        \"id\": x[\"_id\"],\n        \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},\n    }\n)\n</pre> from datasets import load_dataset  dataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True) vespa_feed = dataset.map(     lambda x: {         \"id\": x[\"_id\"],         \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},     } ) <p>Now we can feed to Vespa using <code>feed_iterable</code> which accepts any <code>Iterable</code> and an optional callback function where we can check the outcome of each operation. The application is configured to use embedding functionality, that produce a vector embedding using a concatenation of the title and the body input fields. This step is computionally expensive. Read more about embedding inference in Vespa in the Accelerating Transformer-based Embedding Retrieval with Vespa.</p> In\u00a0[1]: Copied! <pre>from vespa.io import VespaResponse, VespaQueryResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback)\n</pre> from vespa.io import VespaResponse, VespaQueryResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback) In\u00a0[1]: Copied! <pre>import pandas as pd\n\n\ndef display_hits_as_df(response: VespaQueryResponse, fields) -&gt; pd.DataFrame:\n    records = []\n    for hit in response.hits:\n        record = {}\n        for field in fields:\n            record[field] = hit[\"fields\"][field]\n        records.append(record)\n    return pd.DataFrame(records)\n</pre> import pandas as pd   def display_hits_as_df(response: VespaQueryResponse, fields) -&gt; pd.DataFrame:     records = []     for hit in response.hits:         record = {}         for field in fields:             record[field] = hit[\"fields\"][field]         records.append(record)     return pd.DataFrame(records) In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where userQuery() limit 5\",\n        query=query,\n        ranking=\"bm25\",\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where userQuery() limit 5\",         query=query,         ranking=\"bm25\",     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",\n        query=query,\n        ranking=\"semantic\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",         query=query,         ranking=\"semantic\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where userQuery() or ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where userQuery() or ({targetHits:1000}nearestNeighbor(embedding,q)) limit 5\",         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) <p>This combines the two methods using the rank query operator. In this case we express that we want to retrieve the top-1000 documents using vector search, and then have sparse features like BM25 calculated as well (second operand of the rank operator). Finally the hits are re-ranked using the reciprocal rank fusion</p> In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from sources * where rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5\",\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql=\"select * from sources * where rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5\",         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) In\u00a0[1]: Copied! <pre>with app.syncio(connections=1) as session:\n    query = \"How Fruits and Vegetables Can Treat Asthma?\"\n    response: VespaQueryResponse = session.query(\n        yql='select * from sources * where title contains \"vegetable\" and rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5',\n        query=query,\n        ranking=\"fusion\",\n        body={\"input.query(q)\": f\"embed({query})\"},\n    )\n    assert response.is_successful()\n    print(display_hits_as_df(response, [\"id\", \"title\"]))\n</pre> with app.syncio(connections=1) as session:     query = \"How Fruits and Vegetables Can Treat Asthma?\"     response: VespaQueryResponse = session.query(         yql='select * from sources * where title contains \"vegetable\" and rank({targetHits:1000}nearestNeighbor(embedding,q), userQuery()) limit 5',         query=query,         ranking=\"fusion\",         body={\"input.query(q)\": f\"embed({query})\"},     )     assert response.is_successful()     print(display_hits_as_df(response, [\"id\", \"title\"])) In\u00a0[1]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"getting-started-pyvespa.html#hybrid-search-quickstart","title":"Hybrid Search - Quickstart\u00b6","text":"<p>This tutorial creates a hybrid text search application combining traditional keyword matching with semantic vector search (dense retrieval). It also demonstrates using Vespa native embedder functionality.</p>"},{"location":"getting-started-pyvespa.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files - create one from scratch:</p>"},{"location":"getting-started-pyvespa.html#deploy-the-vespa-application","title":"Deploy the Vespa application\u00b6","text":"<p>Deploy <code>package</code> on the local machine using Docker, without leaving the notebook, by creating an instance of VespaDocker. <code>VespaDocker</code> connects to the local Docker daemon socket and starts the Vespa docker image.</p> <p>If this step fails, please check that the Docker daemon is running, and that the Docker daemon socket can be used by clients (Configurable under advanced settings in Docker Desktop).</p>"},{"location":"getting-started-pyvespa.html#feeding-documents-to-vespa","title":"Feeding documents to Vespa\u00b6","text":"<p>In this example we use the HF Datasets library to stream the BeIR/nfcorpus dataset and index in our newly deployed Vespa instance. Read more about the NFCorpus:</p> <p>NFCorpus is a full-text English retrieval data set for Medical Information Retrieval.</p> <p>The following uses the stream option of datasets to stream the data without downloading all the contents locally. The <code>map</code> functionality allows us to convert the dataset fields into the expected feed format for <code>pyvespa</code> which expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p>"},{"location":"getting-started-pyvespa.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Using the Vespa Query language we can query the indexed data.</p> <ul> <li>Using a context manager <code>with app.syncio() as session</code> to handle connection pooling (best practices)</li> <li>The query method accepts any valid Vespa query api parameter in <code>**kwargs</code></li> <li>Vespa api parameter names that contains <code>.</code> must be sent as <code>dict</code> parameters in the <code>body</code> method argument</li> </ul> <p>The following searches for <code>How Fruits and Vegetables Can Treat Asthma?</code> using different retrieval and ranking strategies.</p>"},{"location":"getting-started-pyvespa.html#plain-keyword-search","title":"Plain Keyword search\u00b6","text":"<p>The following uses plain keyword search functionality with bm25 ranking, the <code>bm25</code> rank-profile was configured in the application package to use a linear combination of the bm25 score of the query terms against the title and the body field.</p>"},{"location":"getting-started-pyvespa.html#plain-semantic-search","title":"Plain Semantic Search\u00b6","text":"<p>The following uses dense vector representations of the query and the document and matching is performed and accelerated by Vespa's support for approximate nearest neighbor search. The vector embedding representation of the text is obtained using Vespa's embedder functionality.</p>"},{"location":"getting-started-pyvespa.html#hybrid-search","title":"Hybrid Search\u00b6","text":"<p>This is one approach to combine the two retrieval strategies and where we use Vespa's support for cross-hits feature normalization and reciprocal rank fusion. This functionality is exposed in the context of <code>global</code> re-ranking, after the distributed query retrieval execution which might span 1000s of nodes.</p>"},{"location":"getting-started-pyvespa.html#hybrid-search-with-the-or-query-operator","title":"Hybrid search with the OR query operator\u00b6","text":"<p>This combines the two methods using logical disjunction (OR). Note that the first-phase expression in our <code>fusion</code> expression is only using the semantic score, this because usually semantic search provides better recall than sparse keyword search alone.</p>"},{"location":"getting-started-pyvespa.html#hybrid-search-with-the-rank-query-operator","title":"Hybrid search with the RANK query operator\u00b6","text":""},{"location":"getting-started-pyvespa.html#hybrid-search-with-filters","title":"Hybrid search with filters\u00b6","text":"<p>In this example we add another query term to the yql, restricting the nearest neighbor search to only consider documents that have vegetable in the title.</p>"},{"location":"getting-started-pyvespa.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"getting-started-pyvespa.html#next-steps","title":"Next steps\u00b6","text":"<p>This is just an intro into the capabilities of Vespa and pyvespa. Browse the site to learn more about schemas, feeding and queries - find more complex applications in examples.</p>"},{"location":"query.html","title":"Querying Vespa","text":"Refer to troubleshooting     for any problem when running this guide.  <p>You can run this tutorial in Google Colab:</p> <p></p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa\n</pre> !pip3 install pyvespa <p>Let us first just deploy and get a connection to a Vespa instance.</p> In\u00a0[2]: Copied! <pre>from vespa.application import Vespa\nfrom vespa.deployment import VespaDocker\nfrom vespa.io import VespaQueryResponse\nfrom vespa.exceptions import VespaError\nfrom vespa.package import sample_package\n\nvespa_docker = VespaDocker()\napp: Vespa = vespa_docker.deploy(sample_package)\n</pre> from vespa.application import Vespa from vespa.deployment import VespaDocker from vespa.io import VespaQueryResponse from vespa.exceptions import VespaError from vespa.package import sample_package  vespa_docker = VespaDocker() app: Vespa = vespa_docker.deploy(sample_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for configuration server, 5/60 seconds...\nWaiting for application to come up, 0/300 seconds.\nWaiting for application to come up, 5/300 seconds.\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[3]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True)\nvespa_feed = dataset.map(\n    lambda x: {\n        \"id\": x[\"_id\"],\n        \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},\n    }\n).take(100)\n</pre> from datasets import load_dataset  dataset = load_dataset(\"BeIR/nfcorpus\", \"corpus\", split=\"corpus\", streaming=True) vespa_feed = dataset.map(     lambda x: {         \"id\": x[\"_id\"],         \"fields\": {\"title\": x[\"title\"], \"body\": x[\"text\"], \"id\": x[\"_id\"]},     } ).take(100) In\u00a0[4]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error when feeding document {id}: {response.get_json()}\")\n\n\napp.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error when feeding document {id}: {response.get_json()}\")   app.feed_iterable(vespa_feed, schema=\"doc\", namespace=\"tutorial\", callback=callback) <p>See the Vespa query language for Vespa query api request parameters.</p> <p>The YQL userQuery() operator uses the query read from <code>query</code>. The query also specifies to use the app-specific documentation rank profile. The code uses context manager <code>with session</code> statement to make sure that connection pools are released. If you attempt to make multiple queries, this is important as each query will not have to set up new connections.</p> In\u00a0[5]: Copied! <pre>with app.syncio() as session:\n    response: VespaQueryResponse = session.query(\n        yql=\"select title, body from doc where userQuery()\",\n        hits=1,\n        query=\"Is statin use connected to breast cancer?\",\n        ranking=\"bm25\",\n    )\n    print(response.is_successful())\n    print(response.url)\n</pre> with app.syncio() as session:     response: VespaQueryResponse = session.query(         yql=\"select title, body from doc where userQuery()\",         hits=1,         query=\"Is statin use connected to breast cancer?\",         ranking=\"bm25\",     )     print(response.is_successful())     print(response.url) <pre>True\nhttp://localhost:8080/search/?yql=select+title%2C+body+from+doc+where+userQuery%28%29&amp;hits=1&amp;query=Is+statin+use+connected+to+breast+cancer%3F&amp;ranking=bm25\n</pre> <p>Alternatively, if the native Vespa query parameter contains \".\", which cannot be used as a <code>kwarg</code>, the parameters can be sent as HTTP POST with the <code>body</code> argument. In this case, <code>ranking</code> is an alias of <code>ranking.profile</code>, but using <code>ranking.profile</code> as a <code>**kwargs</code> argument is not allowed in python. This will combine HTTP parameters with an HTTP POST body.</p> In\u00a0[6]: Copied! <pre>with app.syncio() as session:\n    response: VespaQueryResponse = session.query(\n        body={\n            \"yql\": \"select title, body from doc where userQuery()\",\n            \"query\": \"Is statin use connected to breast cancer?\",\n            \"ranking\": \"bm25\",\n            \"presentation.timing\": True,\n        },\n    )\n    print(response.is_successful())\n</pre> with app.syncio() as session:     response: VespaQueryResponse = session.query(         body={             \"yql\": \"select title, body from doc where userQuery()\",             \"query\": \"Is statin use connected to breast cancer?\",             \"ranking\": \"bm25\",             \"presentation.timing\": True,         },     )     print(response.is_successful()) <pre>True\n</pre> <p>The query specified that we wanted one hit:</p> In\u00a0[7]: Copied! <pre>response.hits\n</pre> response.hits Out[7]: <pre>[{'id': 'index:sample_content/0/2deca9d7029f3a77c092dfeb',\n  'relevance': 21.850306796449487,\n  'source': 'sample_content',\n  'fields': {'body': 'Recent studies have suggested that &lt;hi&gt;statins&lt;/hi&gt;, an established drug group in the prevention of cardiovascular mortality, could delay or prevent &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; death among &lt;hi&gt;statin&lt;/hi&gt; users in a population-based cohort of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients. The study cohort included all newly diagnosed &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients in Finland during 1995\u20132003 (31,236 cases), identified from the Finnish &lt;hi&gt;Cancer&lt;/hi&gt; Registry. Information on &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; before and after the diagnosis was obtained from a national prescription database. We &lt;hi&gt;used&lt;/hi&gt; the Cox proportional hazards regression method &lt;hi&gt;to&lt;/hi&gt; estimate mortality among &lt;hi&gt;statin&lt;/hi&gt; users with &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; as time-dependent variable. A total of 4,151 participants had &lt;hi&gt;used&lt;/hi&gt; &lt;hi&gt;statins&lt;/hi&gt;. During the median follow-up of 3.25 years after the diagnosis (range 0.08\u20139.0 years) 6,011 participants died, of which 3,619 (60.2%) was due &lt;hi&gt;to&lt;/hi&gt; &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; were associated with lowered risk of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; death (HR 0.46, 95% CI 0.38\u20130.55 and HR 0.54, 95% CI 0.44\u20130.67, respectively). The risk decrease by post-diagnostic &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; was likely affected by healthy adherer bias; that &lt;hi&gt;is&lt;/hi&gt;, the greater likelihood of dying &lt;hi&gt;cancer&lt;/hi&gt; patients &lt;hi&gt;to&lt;/hi&gt; discontinue &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; as the association was not clearly dose-dependent and observed already at low-dose/short-term &lt;hi&gt;use&lt;/hi&gt;. The dose- and time-dependence of the survival benefit among pre-diagnostic &lt;hi&gt;statin&lt;/hi&gt; users suggests a possible causal effect that should be evaluated further in a clinical trial testing &lt;hi&gt;statins&lt;/hi&gt;\u2019 effect on survival in &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients.',\n   'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}},\n {'id': 'index:sample_content/0/d2f48bedc26e3838b2fc40d8',\n  'relevance': 20.71331154820049,\n  'source': 'sample_content',\n  'fields': {'body': 'BACKGROUND: Preclinical studies have shown that &lt;hi&gt;statins&lt;/hi&gt;, particularly simvastatin, can prevent growth in &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; cell lines and animal models. We investigated whether &lt;hi&gt;statins&lt;/hi&gt; &lt;hi&gt;used&lt;/hi&gt; after &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; diagnosis reduced the risk of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;-specific, or all-cause, mortality in a large cohort of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients. METHODS: A cohort of 17,880 &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients, newly diagnosed between 1998 and 2009, was identified from English &lt;hi&gt;cancer&lt;/hi&gt; registries (from the National &lt;hi&gt;Cancer&lt;/hi&gt; Data Repository). This cohort was linked &lt;hi&gt;to&lt;/hi&gt; the UK Clinical Practice Research Datalink, providing prescription records, and &lt;hi&gt;to&lt;/hi&gt; the Office of National Statistics mortality data (up &lt;hi&gt;to&lt;/hi&gt; 2013), identifying 3694 deaths, including 1469 deaths attributable &lt;hi&gt;to&lt;/hi&gt; &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;. Unadjusted and adjusted hazard ratios (HRs) for &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;-specific, and all-cause, mortality in &lt;hi&gt;statin&lt;/hi&gt; users after &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; diagnosis were calculated &lt;hi&gt;using&lt;/hi&gt; time-dependent Cox regression models. Sensitivity analyses were conducted &lt;hi&gt;using&lt;/hi&gt; multiple imputation methods, propensity score methods and a case-control approach. RESULTS: There was some evidence that &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; after a diagnosis of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; had reduced mortality due &lt;hi&gt;to&lt;/hi&gt; &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; and all causes (fully adjusted HR = 0.84 [95% confidence interval = 0.68-1.04] and 0.84 [0.72-0.97], respectively). These associations were more marked for simvastatin 0.79 (0.63-1.00) and 0.81 (0.70-0.95), respectively. CONCLUSIONS: In this large population-based &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; cohort, there was some evidence of reduced mortality in &lt;hi&gt;statin&lt;/hi&gt; users after &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; diagnosis. However, these associations were weak in magnitude and were attenuated in some sensitivity analyses.',\n   'title': 'Statin use after diagnosis of breast cancer and survival: a population-based cohort study.'}},\n {'id': 'index:sample_content/0/abb8c59b326f35dc406e914d',\n  'relevance': 10.546049129391914,\n  'source': 'sample_content',\n  'fields': {'body': 'BACKGROUND: Although high soy consumption may be associated with lower &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk in Asian populations, findings from epidemiological studies have been inconsistent. OBJECTIVE: We investigated the effects of soy intake on &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk among Korean women according &lt;hi&gt;to&lt;/hi&gt; their menopausal and hormone receptor status. METHODS: We conducted a case-control study with 358 incident &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients and 360 age-matched controls with no history of malignant neoplasm. Dietary consumption of soy products was examined &lt;hi&gt;using&lt;/hi&gt; a 103-item food frequency questionnaire. RESULTS: The estimated mean intakes of total soy and isoflavones from this study population were 76.5 g per day and 15.0 mg per day, respectively. &lt;hi&gt;Using&lt;/hi&gt; a multivariate logistic regression model, we found a significant inverse association between soy intake and &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk, with a dose-response relationship (odds ratios (OR) (95% confidence interval (CI)) for the highest vs the lowest intake quartile: 0.36 (0.20-0.64)). When the data were stratified by menopausal status, the protective effect was observed only among postmenopausal women (OR (95% CI) for the highest vs the lowest intake quartile: 0.08 (0.03-0.22)). The association between soy and &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk did not differ according &lt;hi&gt;to&lt;/hi&gt; estrogen receptor (ER)/progesterone receptor (PR) status, but the estimated intake of soy isoflavones showed an inverse association only among postmenopausal women with ER+/PR+ tumors. CONCLUSIONS: Our findings suggest that high consumption of soy might be related &lt;hi&gt;to&lt;/hi&gt; lower risk of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; and that the effect of soy intake could vary depending on several factors.',\n   'title': 'Effect of dietary soy intake on breast cancer risk according to menopause and hormone receptor status.'}},\n {'id': 'index:sample_content/0/3b8b700c0db54b8272a2da54',\n  'relevance': 7.360259724997032,\n  'source': 'sample_content',\n  'fields': {'body': 'Docosahexaenoic acid (DHA) &lt;hi&gt;is&lt;/hi&gt; an omega-3 fatty acid that comprises 22 carbons and 6 alternative double bonds in its hydrocarbon chain (22:6omega3). Previous studies have shown that DHA from fish oil controls the growth and development of different &lt;hi&gt;cancers&lt;/hi&gt;; however, safety issues have been raised repeatedly about contamination of toxins in fish oil that makes it no longer a clean and safe source of the fatty acid. We investigated the cell growth inhibition of DHA from the cultured microalga Crypthecodinium cohnii (algal DHA [aDHA]) in human &lt;hi&gt;breast&lt;/hi&gt; carcinoma MCF-7 cells. aDHA exhibited growth inhibition on &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; cells dose-dependently by 16.0% &lt;hi&gt;to&lt;/hi&gt; 59.0% of the control level after 72-h incubations with 40 &lt;hi&gt;to&lt;/hi&gt; 160 microM of the fatty acid. DNA flow cytometry shows that aDHA induced sub-G(1) cells, or apoptotic cells, by 64.4% &lt;hi&gt;to&lt;/hi&gt; 171.3% of the control levels after incubations with 80 mM of the fatty acid for 24, 48, and 72 h. Western blot studies further show that aDHA did not modulate the expression of proapoptotic Bax protein but induced the downregulation of anti-apoptotic Bcl-2 expression time-dependently, causing increases of Bax/Bcl-2 ratio by 303.4% and 386.5% after 48- and 72-h incubations respectively with the fatty acid. Results from this study suggest that DHA from the cultured microalga &lt;hi&gt;is&lt;/hi&gt; also effective in controlling &lt;hi&gt;cancer&lt;/hi&gt; cell growth and that downregulation of antiapoptotic Bcl-2 &lt;hi&gt;is&lt;/hi&gt; an important step in the induced apoptosis.',\n   'title': 'Docosahexaenoic acid from a cultured microalga inhibits cell growth and induces apoptosis by upregulating Bax/Bcl-2 ratio in human breast carcinoma...'}},\n {'id': 'index:sample_content/0/9c2d39bb63ce85fcda9bfe6c',\n  'relevance': 5.441906201913548,\n  'source': 'sample_content',\n  'fields': {'body': 'Background Based on the hypothesized protective effect, we examined the effect of soy foods on estrogens in nipple aspirate fluid (NAF) and serum, possible indicators of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk. Methods In a cross-over design, we randomized 96 women who produced \u226510 \u03bcL NAF &lt;hi&gt;to&lt;/hi&gt; a high- or low-soy diet for 6-months. During the high-soy diet, participants consumed 2 soy servings of soy milk, tofu, or soy nuts (approximately 50 mg of isoflavones/day); during the low-soy diet, they maintained their usual diet. Six NAF samples were obtained &lt;hi&gt;using&lt;/hi&gt; a FirstCyte\u00a9 Aspirator. Estradiol (E2) and estrone sulfate (E1S) were assessed in NAF and estrone (E1) in serum only &lt;hi&gt;using&lt;/hi&gt; highly sensitive radioimmunoassays. Mixed-effects regression models accounting for repeated measures and left-censoring limits were applied. Results Mean E2 and E1S were lower during the high-soy than the low-soy diet (113 vs. 313 pg/mL and 46 vs. 68 ng/mL, respectively) without reaching significance (p=0.07); the interaction between group and diet and was not significant. There was no effect of the soy treatment on serum E2 (p=0.76), E1 (p=0.86), or E1S (p=0.56). Within individuals, NAF and serum levels of E2 (rs=0.37; p&lt;0.001) but not E1S (rs=0.004; p=0.97) were correlated. E2 and E1S in NAF and serum were strongly associated (rs=0.78 and rs=0.48; p&lt;0.001). Conclusions Soy foods in amounts consumed by Asians did not significantly modify estrogen levels in NAF and serum. Impact The trend towards lower estrogens in NAF during the high-soy diet counters concerns about adverse effects of soy foods on &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk.',\n   'title': 'Estrogen levels in nipple aspirate fluid and serum during a randomized soy trial'}},\n {'id': 'index:sample_content/0/449eccc1b30615316ab136bc',\n  'relevance': 5.241472721415711,\n  'source': 'sample_content',\n  'fields': {'body': 'The relation between various types of fiber and oral, pharyngeal and esophageal &lt;hi&gt;cancer&lt;/hi&gt; was investigated &lt;hi&gt;using&lt;/hi&gt; data from a case-control study conducted between 1992 and 1997 in Italy. Cases were 271 hospital patients with incident, histologically confirmed oral &lt;hi&gt;cancer&lt;/hi&gt;, 327 with pharyngeal &lt;hi&gt;cancer&lt;/hi&gt; and 304 with esophageal &lt;hi&gt;cancer&lt;/hi&gt;. Controls were 1,950 subjects admitted &lt;hi&gt;to&lt;/hi&gt; the same network of hospitals as the cases for acute, nonneoplastic diseases. Cases and controls were interviewed during their hospital stay &lt;hi&gt;using&lt;/hi&gt; a validated food frequency questionnaire. Odds ratios (OR) were computed after allowance for age, sex, and other potential confounding factors, including alcohol, tobacco consumption, and energy intake. The ORs for the highest vs. the lowest quintile of intake of oral, pharyngeal and esophageal &lt;hi&gt;cancer&lt;/hi&gt; combined were 0.40 for total (Englyst) fiber, 0.37 for soluble fiber, 0.52 for cellulose, 0.48 for insoluble non cellulose polysaccharide, 0.33 for total insoluble fiber and 0.38 for lignin. The inverse relation were similar for vegetable fiber (OR = 0.51), fruit fiber (OR = 0.60) and grain fiber (OR = 0.56), and were somewhat stronger for oral and pharyngeal &lt;hi&gt;cancer&lt;/hi&gt; than for esophageal &lt;hi&gt;cancer&lt;/hi&gt;. The ORs were similar for the two sexes and strata of age, education, alcohol and tobacco consumption, and total non-alcohol energy intake. Our study indicates that fiber intake may have a protective role on oral, pharyngeal and esophageal &lt;hi&gt;cancer&lt;/hi&gt;.',\n   'title': 'Fiber intake and the risk of oral, pharyngeal and esophageal cancer.'}},\n {'id': 'index:sample_content/0/c4cb3b969a89b81a3da71e9d',\n  'relevance': 5.0658599969730735,\n  'source': 'sample_content',\n  'fields': {'body': 'BACKGROUND &amp; AIMS: Increasing evidence suggests that a low folate intake and impaired folate metabolism may be implicated in the development of gastrointestinal &lt;hi&gt;cancers&lt;/hi&gt;. We conducted a systematic review with meta-analysis of epidemiologic studies evaluating the association of folate intake or genetic polymorphisms in 5,10-methylenetetrahydrofolate reductase (MTHFR), a central enzyme in folate metabolism, with risk of esophageal, gastric, or pancreatic &lt;hi&gt;cancer&lt;/hi&gt;. METHODS: A literature search was performed &lt;hi&gt;using&lt;/hi&gt; MEDLINE for studies published through March 2006. Study-specific relative risks were weighted by the inverse of their variance &lt;hi&gt;to&lt;/hi&gt; obtain random-effects summary estimates. RESULTS: The summary relative risks for the highest versus the lowest category of dietary folate intake were 0.66 (95% confidence interval [CI], 0.53-0.83) for esophageal squamous cell carcinoma (4 case-control), 0.50 (95% CI, 0.39-0.65) for esophageal adenocarcinoma (3 case-control), and 0.49 (95% CI, 0.35-0.67) for pancreatic &lt;hi&gt;cancer&lt;/hi&gt; (1 case-control, 4 cohort); there was no heterogeneity among studies. Results on dietary folate intake and risk of gastric &lt;hi&gt;cancer&lt;/hi&gt; (9 case-control, 2 cohort) were inconsistent. In most studies, the MTHFR 677TT (variant) genotype, which &lt;hi&gt;is&lt;/hi&gt; associated with reduced enzyme activity, was associated with an increased risk of esophageal squamous cell carcinoma, gastric cardia adenocarcinoma, noncardia gastric &lt;hi&gt;cancer&lt;/hi&gt;, gastric &lt;hi&gt;cancer&lt;/hi&gt; (all subsites), and pancreatic &lt;hi&gt;cancer&lt;/hi&gt;; all but one of 22 odds ratios were &gt;1, of which 13 estimates were statistically significant. Studies of the MTHFR A1298C polymorphism were limited and inconsistent. CONCLUSIONS: These findings support the hypothesis that folate may play a role in carcinogenesis of the esophagus, stomach, and pancreas.',\n   'title': 'Folate intake, MTHFR polymorphisms, and risk of esophageal, gastric, and pancreatic cancer: a meta-analysis.'}},\n {'id': 'index:sample_content/0/bb0fe2bd511527ef78587e95',\n  'relevance': 4.780565525377517,\n  'source': 'sample_content',\n  'fields': {'body': 'Individual-based studies that investigated the relation between dietary alpha-linolenic acid (ALA) intake and prostate &lt;hi&gt;cancer&lt;/hi&gt; risk have shown inconsistent results. We carried out a meta-analysis of prospective studies &lt;hi&gt;to&lt;/hi&gt; examine this association. We systematically searched studies published up &lt;hi&gt;to&lt;/hi&gt; December 2008. Log relative risks (RRs) were weighted by the inverse of their variances &lt;hi&gt;to&lt;/hi&gt; obtain a pooled estimate with its 95% confidence interval (CI). We identified five prospective studies that met our inclusion criteria and reported risk estimates by categories of ALA intake. Comparing the highest &lt;hi&gt;to&lt;/hi&gt; the lowest ALA intake category, the pooled RR was 0.97 (95% CI:0.86-1.10) but the association was heterogeneous. &lt;hi&gt;Using&lt;/hi&gt; the reported numbers of cases and non-cases in each category of ALA intake, we found that subjects who consumed more than 1.5 g/day of ALA compared with subjects who consumed less than 1.5 g/day had a significant decreased risk of prostate &lt;hi&gt;cancer&lt;/hi&gt;: RR = 0.95 (95% CI:0.91-0.99). Divergences in results could partly be explained by differences in sample sizes and adjustment but they also highlight limits in dietary ALA assessment in such prospective studies. Our findings support a weak protective association between dietary ALA intake and prostate &lt;hi&gt;cancer&lt;/hi&gt; risk but further research &lt;hi&gt;is&lt;/hi&gt; needed &lt;hi&gt;to&lt;/hi&gt; conclude on this question.',\n   'title': 'Prospective studies of dietary alpha-linolenic acid intake and prostate cancer risk: a meta-analysis.'}},\n {'id': 'index:sample_content/0/90efd2c6652f323a8244690d',\n  'relevance': 4.7044749035958535,\n  'source': 'sample_content',\n  'fields': {'body': 'High serum levels of testosterone and estradiol, the bioavailability of which may be increased by Western dietary habits, seem &lt;hi&gt;to&lt;/hi&gt; be important risk factors for postmenopausal &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;. We hypothesized that an ad libitum diet low in animal fat and refined carbohydrates and rich in low-glycemic-index foods, monounsaturated and n-3 polyunsaturated fatty acids, and phytoestrogens, might favorably modify the hormonal profile of postmenopausal women. One hundred and four postmenopausal women selected from 312 healthy volunteers on the basis of high serum testosterone levels were randomized &lt;hi&gt;to&lt;/hi&gt; dietary intervention or control. The intervention included intensive dietary counseling and specially prepared group meals twice a week over 4.5 months. Changes in serum levels of testosterone, estradiol, and sex hormone-binding globulin were the main outcome measures. In the intervention group, sex hormone-binding globulin increased significantly (from 36.0 &lt;hi&gt;to&lt;/hi&gt; 45.1 nmol/liter) compared with the control group (25 versus 4%,; P &lt; 0.0001) and serum testosterone decreased (from 0.41 &lt;hi&gt;to&lt;/hi&gt; 0.33 ng/ml; -20 versus -7% in control group; P = 0.0038). Serum estradiol also decreased, but the change was not significant. The dietary intervention group also significantly decreased body weight (4.06 kg versus 0.54 kg in the control group), waist:hip ratio, total cholesterol, fasting glucose level, and area under insulin curve after oral glucose tolerance test. A radical modification in diet designed &lt;hi&gt;to&lt;/hi&gt; reduce insulin resistance and also involving increased phytoestrogen intake decreases the bioavailability of serum sex hormones in hyperandrogenic postmenopausal women. Additional studies are needed &lt;hi&gt;to&lt;/hi&gt; determine whether such effects can reduce the risk of developing &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;.',\n   'title': 'Reducing bioavailable sex hormones through a comprehensive change in diet: the diet and androgens (DIANA) randomized trial.'}},\n {'id': 'index:sample_content/0/9b56be58163850a7b2ee2425',\n  'relevance': 3.896398317302996,\n  'source': 'sample_content',\n  'fields': {'body': '&lt;hi&gt;Breast&lt;/hi&gt; pain &lt;hi&gt;is&lt;/hi&gt; a common condition affecting most women at some stage in their reproductive life. Mastalgia &lt;hi&gt;is&lt;/hi&gt; resistant &lt;hi&gt;to&lt;/hi&gt; treatment in 6% of cyclical and 26% non-cyclical patients. Surgery &lt;hi&gt;is&lt;/hi&gt; not widely &lt;hi&gt;used&lt;/hi&gt; &lt;hi&gt;to&lt;/hi&gt; treat this condition and only considered in patients with severe mastalgia resistant &lt;hi&gt;to&lt;/hi&gt; medication. The aims of this study were &lt;hi&gt;to&lt;/hi&gt; audit the efficacy of surgery in severe treatment resistant mastalgia and &lt;hi&gt;to&lt;/hi&gt; assess patient satisfaction following surgery. This &lt;hi&gt;is&lt;/hi&gt; a retrospective review of the medical records of all patients seen in mastalgia clinic in the University Hospital of Wales, Cardiff since 1973. A postal questionnaire was distributed &lt;hi&gt;to&lt;/hi&gt; all patients who had undergone surgery. Results showed that of the 1054 patients seen in mastalgia clinic, 12 (1.2%) had undergone surgery. Surgery included 8 subcutaneous mastectomies with implants (3 bilateral, 5 unilateral), 1 bilateral simple mastectomy and 3 quadrantectomies (1 having a further simple mastectomy). The median duration of symptoms was 6.5 years (range 2-16 years). Five patients (50%) were pain free following surgery, 3 developed capsular contractures and 2 wound infections with dehiscence. Pain persisted in both patients undergoing quadrantectomy. We conclude that surgery for mastalgia should only be considered in a minority of patients. Patients should be informed of possible complications inherent of reconstructive surgery and warned that in 50% cases their pain will not be improved.',\n   'title': 'Is there a role for surgery in the treatment of mastalgia?'}}]</pre> <p>Example of iterating over the returned hits obtained from <code>response.hits</code>, extracting the <code>title</code> field:</p> In\u00a0[8]: Copied! <pre>[hit[\"fields\"][\"title\"] for hit in response.hits]\n</pre> [hit[\"fields\"][\"title\"] for hit in response.hits] Out[8]: <pre>['Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland',\n 'Statin use after diagnosis of breast cancer and survival: a population-based cohort study.',\n 'Effect of dietary soy intake on breast cancer risk according to menopause and hormone receptor status.',\n 'Docosahexaenoic acid from a cultured microalga inhibits cell growth and induces apoptosis by upregulating Bax/Bcl-2 ratio in human breast carcinoma...',\n 'Estrogen levels in nipple aspirate fluid and serum during a randomized soy trial',\n 'Fiber intake and the risk of oral, pharyngeal and esophageal cancer.',\n 'Folate intake, MTHFR polymorphisms, and risk of esophageal, gastric, and pancreatic cancer: a meta-analysis.',\n 'Prospective studies of dietary alpha-linolenic acid intake and prostate cancer risk: a meta-analysis.',\n 'Reducing bioavailable sex hormones through a comprehensive change in diet: the diet and androgens (DIANA) randomized trial.',\n 'Is there a role for surgery in the treatment of mastalgia?']</pre> <p>Access the full JSON response in the Vespa default JSON result format:</p> In\u00a0[9]: Copied! <pre>response.json\n</pre> response.json Out[9]: <pre>{'timing': {'querytime': 0.004,\n  'summaryfetchtime': 0.005,\n  'searchtime': 0.011},\n 'root': {'id': 'toplevel',\n  'relevance': 1.0,\n  'fields': {'totalCount': 97},\n  'coverage': {'coverage': 100,\n   'documents': 100,\n   'full': True,\n   'nodes': 1,\n   'results': 1,\n   'resultsFull': 1},\n  'children': [{'id': 'index:sample_content/0/2deca9d7029f3a77c092dfeb',\n    'relevance': 21.850306796449487,\n    'source': 'sample_content',\n    'fields': {'body': 'Recent studies have suggested that &lt;hi&gt;statins&lt;/hi&gt;, an established drug group in the prevention of cardiovascular mortality, could delay or prevent &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; recurrence but the effect on disease-specific mortality remains unclear. We evaluated risk of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; death among &lt;hi&gt;statin&lt;/hi&gt; users in a population-based cohort of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients. The study cohort included all newly diagnosed &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients in Finland during 1995\u20132003 (31,236 cases), identified from the Finnish &lt;hi&gt;Cancer&lt;/hi&gt; Registry. Information on &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; before and after the diagnosis was obtained from a national prescription database. We &lt;hi&gt;used&lt;/hi&gt; the Cox proportional hazards regression method &lt;hi&gt;to&lt;/hi&gt; estimate mortality among &lt;hi&gt;statin&lt;/hi&gt; users with &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; as time-dependent variable. A total of 4,151 participants had &lt;hi&gt;used&lt;/hi&gt; &lt;hi&gt;statins&lt;/hi&gt;. During the median follow-up of 3.25 years after the diagnosis (range 0.08\u20139.0 years) 6,011 participants died, of which 3,619 (60.2%) was due &lt;hi&gt;to&lt;/hi&gt; &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;. After adjustment for age, tumor characteristics, and treatment selection, both post-diagnostic and pre-diagnostic &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; were associated with lowered risk of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; death (HR 0.46, 95% CI 0.38\u20130.55 and HR 0.54, 95% CI 0.44\u20130.67, respectively). The risk decrease by post-diagnostic &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; was likely affected by healthy adherer bias; that &lt;hi&gt;is&lt;/hi&gt;, the greater likelihood of dying &lt;hi&gt;cancer&lt;/hi&gt; patients &lt;hi&gt;to&lt;/hi&gt; discontinue &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; as the association was not clearly dose-dependent and observed already at low-dose/short-term &lt;hi&gt;use&lt;/hi&gt;. The dose- and time-dependence of the survival benefit among pre-diagnostic &lt;hi&gt;statin&lt;/hi&gt; users suggests a possible causal effect that should be evaluated further in a clinical trial testing &lt;hi&gt;statins&lt;/hi&gt;\u2019 effect on survival in &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients.',\n     'title': 'Statin Use and Breast Cancer Survival: A Nationwide Cohort Study from Finland'}},\n   {'id': 'index:sample_content/0/d2f48bedc26e3838b2fc40d8',\n    'relevance': 20.71331154820049,\n    'source': 'sample_content',\n    'fields': {'body': 'BACKGROUND: Preclinical studies have shown that &lt;hi&gt;statins&lt;/hi&gt;, particularly simvastatin, can prevent growth in &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; cell lines and animal models. We investigated whether &lt;hi&gt;statins&lt;/hi&gt; &lt;hi&gt;used&lt;/hi&gt; after &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; diagnosis reduced the risk of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;-specific, or all-cause, mortality in a large cohort of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients. METHODS: A cohort of 17,880 &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients, newly diagnosed between 1998 and 2009, was identified from English &lt;hi&gt;cancer&lt;/hi&gt; registries (from the National &lt;hi&gt;Cancer&lt;/hi&gt; Data Repository). This cohort was linked &lt;hi&gt;to&lt;/hi&gt; the UK Clinical Practice Research Datalink, providing prescription records, and &lt;hi&gt;to&lt;/hi&gt; the Office of National Statistics mortality data (up &lt;hi&gt;to&lt;/hi&gt; 2013), identifying 3694 deaths, including 1469 deaths attributable &lt;hi&gt;to&lt;/hi&gt; &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;. Unadjusted and adjusted hazard ratios (HRs) for &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;-specific, and all-cause, mortality in &lt;hi&gt;statin&lt;/hi&gt; users after &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; diagnosis were calculated &lt;hi&gt;using&lt;/hi&gt; time-dependent Cox regression models. Sensitivity analyses were conducted &lt;hi&gt;using&lt;/hi&gt; multiple imputation methods, propensity score methods and a case-control approach. RESULTS: There was some evidence that &lt;hi&gt;statin&lt;/hi&gt; &lt;hi&gt;use&lt;/hi&gt; after a diagnosis of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; had reduced mortality due &lt;hi&gt;to&lt;/hi&gt; &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; and all causes (fully adjusted HR = 0.84 [95% confidence interval = 0.68-1.04] and 0.84 [0.72-0.97], respectively). These associations were more marked for simvastatin 0.79 (0.63-1.00) and 0.81 (0.70-0.95), respectively. CONCLUSIONS: In this large population-based &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; cohort, there was some evidence of reduced mortality in &lt;hi&gt;statin&lt;/hi&gt; users after &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; diagnosis. However, these associations were weak in magnitude and were attenuated in some sensitivity analyses.',\n     'title': 'Statin use after diagnosis of breast cancer and survival: a population-based cohort study.'}},\n   {'id': 'index:sample_content/0/abb8c59b326f35dc406e914d',\n    'relevance': 10.546049129391914,\n    'source': 'sample_content',\n    'fields': {'body': 'BACKGROUND: Although high soy consumption may be associated with lower &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk in Asian populations, findings from epidemiological studies have been inconsistent. OBJECTIVE: We investigated the effects of soy intake on &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk among Korean women according &lt;hi&gt;to&lt;/hi&gt; their menopausal and hormone receptor status. METHODS: We conducted a case-control study with 358 incident &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; patients and 360 age-matched controls with no history of malignant neoplasm. Dietary consumption of soy products was examined &lt;hi&gt;using&lt;/hi&gt; a 103-item food frequency questionnaire. RESULTS: The estimated mean intakes of total soy and isoflavones from this study population were 76.5 g per day and 15.0 mg per day, respectively. &lt;hi&gt;Using&lt;/hi&gt; a multivariate logistic regression model, we found a significant inverse association between soy intake and &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk, with a dose-response relationship (odds ratios (OR) (95% confidence interval (CI)) for the highest vs the lowest intake quartile: 0.36 (0.20-0.64)). When the data were stratified by menopausal status, the protective effect was observed only among postmenopausal women (OR (95% CI) for the highest vs the lowest intake quartile: 0.08 (0.03-0.22)). The association between soy and &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk did not differ according &lt;hi&gt;to&lt;/hi&gt; estrogen receptor (ER)/progesterone receptor (PR) status, but the estimated intake of soy isoflavones showed an inverse association only among postmenopausal women with ER+/PR+ tumors. CONCLUSIONS: Our findings suggest that high consumption of soy might be related &lt;hi&gt;to&lt;/hi&gt; lower risk of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; and that the effect of soy intake could vary depending on several factors.',\n     'title': 'Effect of dietary soy intake on breast cancer risk according to menopause and hormone receptor status.'}},\n   {'id': 'index:sample_content/0/3b8b700c0db54b8272a2da54',\n    'relevance': 7.360259724997032,\n    'source': 'sample_content',\n    'fields': {'body': 'Docosahexaenoic acid (DHA) &lt;hi&gt;is&lt;/hi&gt; an omega-3 fatty acid that comprises 22 carbons and 6 alternative double bonds in its hydrocarbon chain (22:6omega3). Previous studies have shown that DHA from fish oil controls the growth and development of different &lt;hi&gt;cancers&lt;/hi&gt;; however, safety issues have been raised repeatedly about contamination of toxins in fish oil that makes it no longer a clean and safe source of the fatty acid. We investigated the cell growth inhibition of DHA from the cultured microalga Crypthecodinium cohnii (algal DHA [aDHA]) in human &lt;hi&gt;breast&lt;/hi&gt; carcinoma MCF-7 cells. aDHA exhibited growth inhibition on &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; cells dose-dependently by 16.0% &lt;hi&gt;to&lt;/hi&gt; 59.0% of the control level after 72-h incubations with 40 &lt;hi&gt;to&lt;/hi&gt; 160 microM of the fatty acid. DNA flow cytometry shows that aDHA induced sub-G(1) cells, or apoptotic cells, by 64.4% &lt;hi&gt;to&lt;/hi&gt; 171.3% of the control levels after incubations with 80 mM of the fatty acid for 24, 48, and 72 h. Western blot studies further show that aDHA did not modulate the expression of proapoptotic Bax protein but induced the downregulation of anti-apoptotic Bcl-2 expression time-dependently, causing increases of Bax/Bcl-2 ratio by 303.4% and 386.5% after 48- and 72-h incubations respectively with the fatty acid. Results from this study suggest that DHA from the cultured microalga &lt;hi&gt;is&lt;/hi&gt; also effective in controlling &lt;hi&gt;cancer&lt;/hi&gt; cell growth and that downregulation of antiapoptotic Bcl-2 &lt;hi&gt;is&lt;/hi&gt; an important step in the induced apoptosis.',\n     'title': 'Docosahexaenoic acid from a cultured microalga inhibits cell growth and induces apoptosis by upregulating Bax/Bcl-2 ratio in human breast carcinoma...'}},\n   {'id': 'index:sample_content/0/9c2d39bb63ce85fcda9bfe6c',\n    'relevance': 5.441906201913548,\n    'source': 'sample_content',\n    'fields': {'body': 'Background Based on the hypothesized protective effect, we examined the effect of soy foods on estrogens in nipple aspirate fluid (NAF) and serum, possible indicators of &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk. Methods In a cross-over design, we randomized 96 women who produced \u226510 \u03bcL NAF &lt;hi&gt;to&lt;/hi&gt; a high- or low-soy diet for 6-months. During the high-soy diet, participants consumed 2 soy servings of soy milk, tofu, or soy nuts (approximately 50 mg of isoflavones/day); during the low-soy diet, they maintained their usual diet. Six NAF samples were obtained &lt;hi&gt;using&lt;/hi&gt; a FirstCyte\u00a9 Aspirator. Estradiol (E2) and estrone sulfate (E1S) were assessed in NAF and estrone (E1) in serum only &lt;hi&gt;using&lt;/hi&gt; highly sensitive radioimmunoassays. Mixed-effects regression models accounting for repeated measures and left-censoring limits were applied. Results Mean E2 and E1S were lower during the high-soy than the low-soy diet (113 vs. 313 pg/mL and 46 vs. 68 ng/mL, respectively) without reaching significance (p=0.07); the interaction between group and diet and was not significant. There was no effect of the soy treatment on serum E2 (p=0.76), E1 (p=0.86), or E1S (p=0.56). Within individuals, NAF and serum levels of E2 (rs=0.37; p&lt;0.001) but not E1S (rs=0.004; p=0.97) were correlated. E2 and E1S in NAF and serum were strongly associated (rs=0.78 and rs=0.48; p&lt;0.001). Conclusions Soy foods in amounts consumed by Asians did not significantly modify estrogen levels in NAF and serum. Impact The trend towards lower estrogens in NAF during the high-soy diet counters concerns about adverse effects of soy foods on &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt; risk.',\n     'title': 'Estrogen levels in nipple aspirate fluid and serum during a randomized soy trial'}},\n   {'id': 'index:sample_content/0/449eccc1b30615316ab136bc',\n    'relevance': 5.241472721415711,\n    'source': 'sample_content',\n    'fields': {'body': 'The relation between various types of fiber and oral, pharyngeal and esophageal &lt;hi&gt;cancer&lt;/hi&gt; was investigated &lt;hi&gt;using&lt;/hi&gt; data from a case-control study conducted between 1992 and 1997 in Italy. Cases were 271 hospital patients with incident, histologically confirmed oral &lt;hi&gt;cancer&lt;/hi&gt;, 327 with pharyngeal &lt;hi&gt;cancer&lt;/hi&gt; and 304 with esophageal &lt;hi&gt;cancer&lt;/hi&gt;. Controls were 1,950 subjects admitted &lt;hi&gt;to&lt;/hi&gt; the same network of hospitals as the cases for acute, nonneoplastic diseases. Cases and controls were interviewed during their hospital stay &lt;hi&gt;using&lt;/hi&gt; a validated food frequency questionnaire. Odds ratios (OR) were computed after allowance for age, sex, and other potential confounding factors, including alcohol, tobacco consumption, and energy intake. The ORs for the highest vs. the lowest quintile of intake of oral, pharyngeal and esophageal &lt;hi&gt;cancer&lt;/hi&gt; combined were 0.40 for total (Englyst) fiber, 0.37 for soluble fiber, 0.52 for cellulose, 0.48 for insoluble non cellulose polysaccharide, 0.33 for total insoluble fiber and 0.38 for lignin. The inverse relation were similar for vegetable fiber (OR = 0.51), fruit fiber (OR = 0.60) and grain fiber (OR = 0.56), and were somewhat stronger for oral and pharyngeal &lt;hi&gt;cancer&lt;/hi&gt; than for esophageal &lt;hi&gt;cancer&lt;/hi&gt;. The ORs were similar for the two sexes and strata of age, education, alcohol and tobacco consumption, and total non-alcohol energy intake. Our study indicates that fiber intake may have a protective role on oral, pharyngeal and esophageal &lt;hi&gt;cancer&lt;/hi&gt;.',\n     'title': 'Fiber intake and the risk of oral, pharyngeal and esophageal cancer.'}},\n   {'id': 'index:sample_content/0/c4cb3b969a89b81a3da71e9d',\n    'relevance': 5.0658599969730735,\n    'source': 'sample_content',\n    'fields': {'body': 'BACKGROUND &amp; AIMS: Increasing evidence suggests that a low folate intake and impaired folate metabolism may be implicated in the development of gastrointestinal &lt;hi&gt;cancers&lt;/hi&gt;. We conducted a systematic review with meta-analysis of epidemiologic studies evaluating the association of folate intake or genetic polymorphisms in 5,10-methylenetetrahydrofolate reductase (MTHFR), a central enzyme in folate metabolism, with risk of esophageal, gastric, or pancreatic &lt;hi&gt;cancer&lt;/hi&gt;. METHODS: A literature search was performed &lt;hi&gt;using&lt;/hi&gt; MEDLINE for studies published through March 2006. Study-specific relative risks were weighted by the inverse of their variance &lt;hi&gt;to&lt;/hi&gt; obtain random-effects summary estimates. RESULTS: The summary relative risks for the highest versus the lowest category of dietary folate intake were 0.66 (95% confidence interval [CI], 0.53-0.83) for esophageal squamous cell carcinoma (4 case-control), 0.50 (95% CI, 0.39-0.65) for esophageal adenocarcinoma (3 case-control), and 0.49 (95% CI, 0.35-0.67) for pancreatic &lt;hi&gt;cancer&lt;/hi&gt; (1 case-control, 4 cohort); there was no heterogeneity among studies. Results on dietary folate intake and risk of gastric &lt;hi&gt;cancer&lt;/hi&gt; (9 case-control, 2 cohort) were inconsistent. In most studies, the MTHFR 677TT (variant) genotype, which &lt;hi&gt;is&lt;/hi&gt; associated with reduced enzyme activity, was associated with an increased risk of esophageal squamous cell carcinoma, gastric cardia adenocarcinoma, noncardia gastric &lt;hi&gt;cancer&lt;/hi&gt;, gastric &lt;hi&gt;cancer&lt;/hi&gt; (all subsites), and pancreatic &lt;hi&gt;cancer&lt;/hi&gt;; all but one of 22 odds ratios were &gt;1, of which 13 estimates were statistically significant. Studies of the MTHFR A1298C polymorphism were limited and inconsistent. CONCLUSIONS: These findings support the hypothesis that folate may play a role in carcinogenesis of the esophagus, stomach, and pancreas.',\n     'title': 'Folate intake, MTHFR polymorphisms, and risk of esophageal, gastric, and pancreatic cancer: a meta-analysis.'}},\n   {'id': 'index:sample_content/0/bb0fe2bd511527ef78587e95',\n    'relevance': 4.780565525377517,\n    'source': 'sample_content',\n    'fields': {'body': 'Individual-based studies that investigated the relation between dietary alpha-linolenic acid (ALA) intake and prostate &lt;hi&gt;cancer&lt;/hi&gt; risk have shown inconsistent results. We carried out a meta-analysis of prospective studies &lt;hi&gt;to&lt;/hi&gt; examine this association. We systematically searched studies published up &lt;hi&gt;to&lt;/hi&gt; December 2008. Log relative risks (RRs) were weighted by the inverse of their variances &lt;hi&gt;to&lt;/hi&gt; obtain a pooled estimate with its 95% confidence interval (CI). We identified five prospective studies that met our inclusion criteria and reported risk estimates by categories of ALA intake. Comparing the highest &lt;hi&gt;to&lt;/hi&gt; the lowest ALA intake category, the pooled RR was 0.97 (95% CI:0.86-1.10) but the association was heterogeneous. &lt;hi&gt;Using&lt;/hi&gt; the reported numbers of cases and non-cases in each category of ALA intake, we found that subjects who consumed more than 1.5 g/day of ALA compared with subjects who consumed less than 1.5 g/day had a significant decreased risk of prostate &lt;hi&gt;cancer&lt;/hi&gt;: RR = 0.95 (95% CI:0.91-0.99). Divergences in results could partly be explained by differences in sample sizes and adjustment but they also highlight limits in dietary ALA assessment in such prospective studies. Our findings support a weak protective association between dietary ALA intake and prostate &lt;hi&gt;cancer&lt;/hi&gt; risk but further research &lt;hi&gt;is&lt;/hi&gt; needed &lt;hi&gt;to&lt;/hi&gt; conclude on this question.',\n     'title': 'Prospective studies of dietary alpha-linolenic acid intake and prostate cancer risk: a meta-analysis.'}},\n   {'id': 'index:sample_content/0/90efd2c6652f323a8244690d',\n    'relevance': 4.7044749035958535,\n    'source': 'sample_content',\n    'fields': {'body': 'High serum levels of testosterone and estradiol, the bioavailability of which may be increased by Western dietary habits, seem &lt;hi&gt;to&lt;/hi&gt; be important risk factors for postmenopausal &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;. We hypothesized that an ad libitum diet low in animal fat and refined carbohydrates and rich in low-glycemic-index foods, monounsaturated and n-3 polyunsaturated fatty acids, and phytoestrogens, might favorably modify the hormonal profile of postmenopausal women. One hundred and four postmenopausal women selected from 312 healthy volunteers on the basis of high serum testosterone levels were randomized &lt;hi&gt;to&lt;/hi&gt; dietary intervention or control. The intervention included intensive dietary counseling and specially prepared group meals twice a week over 4.5 months. Changes in serum levels of testosterone, estradiol, and sex hormone-binding globulin were the main outcome measures. In the intervention group, sex hormone-binding globulin increased significantly (from 36.0 &lt;hi&gt;to&lt;/hi&gt; 45.1 nmol/liter) compared with the control group (25 versus 4%,; P &lt; 0.0001) and serum testosterone decreased (from 0.41 &lt;hi&gt;to&lt;/hi&gt; 0.33 ng/ml; -20 versus -7% in control group; P = 0.0038). Serum estradiol also decreased, but the change was not significant. The dietary intervention group also significantly decreased body weight (4.06 kg versus 0.54 kg in the control group), waist:hip ratio, total cholesterol, fasting glucose level, and area under insulin curve after oral glucose tolerance test. A radical modification in diet designed &lt;hi&gt;to&lt;/hi&gt; reduce insulin resistance and also involving increased phytoestrogen intake decreases the bioavailability of serum sex hormones in hyperandrogenic postmenopausal women. Additional studies are needed &lt;hi&gt;to&lt;/hi&gt; determine whether such effects can reduce the risk of developing &lt;hi&gt;breast&lt;/hi&gt; &lt;hi&gt;cancer&lt;/hi&gt;.',\n     'title': 'Reducing bioavailable sex hormones through a comprehensive change in diet: the diet and androgens (DIANA) randomized trial.'}},\n   {'id': 'index:sample_content/0/9b56be58163850a7b2ee2425',\n    'relevance': 3.896398317302996,\n    'source': 'sample_content',\n    'fields': {'body': '&lt;hi&gt;Breast&lt;/hi&gt; pain &lt;hi&gt;is&lt;/hi&gt; a common condition affecting most women at some stage in their reproductive life. Mastalgia &lt;hi&gt;is&lt;/hi&gt; resistant &lt;hi&gt;to&lt;/hi&gt; treatment in 6% of cyclical and 26% non-cyclical patients. Surgery &lt;hi&gt;is&lt;/hi&gt; not widely &lt;hi&gt;used&lt;/hi&gt; &lt;hi&gt;to&lt;/hi&gt; treat this condition and only considered in patients with severe mastalgia resistant &lt;hi&gt;to&lt;/hi&gt; medication. The aims of this study were &lt;hi&gt;to&lt;/hi&gt; audit the efficacy of surgery in severe treatment resistant mastalgia and &lt;hi&gt;to&lt;/hi&gt; assess patient satisfaction following surgery. This &lt;hi&gt;is&lt;/hi&gt; a retrospective review of the medical records of all patients seen in mastalgia clinic in the University Hospital of Wales, Cardiff since 1973. A postal questionnaire was distributed &lt;hi&gt;to&lt;/hi&gt; all patients who had undergone surgery. Results showed that of the 1054 patients seen in mastalgia clinic, 12 (1.2%) had undergone surgery. Surgery included 8 subcutaneous mastectomies with implants (3 bilateral, 5 unilateral), 1 bilateral simple mastectomy and 3 quadrantectomies (1 having a further simple mastectomy). The median duration of symptoms was 6.5 years (range 2-16 years). Five patients (50%) were pain free following surgery, 3 developed capsular contractures and 2 wound infections with dehiscence. Pain persisted in both patients undergoing quadrantectomy. We conclude that surgery for mastalgia should only be considered in a minority of patients. Patients should be informed of possible complications inherent of reconstructive surgery and warned that in 50% cases their pain will not be improved.',\n     'title': 'Is there a role for surgery in the treatment of mastalgia?'}}]}}</pre> In\u00a0[10]: Copied! <pre>with app.syncio(connections=12) as session:\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body={\n            \"yql\": \"select title, body from doc where userQuery()\",\n            \"query\": \"Is statin use connected to breast cancer?\",\n            \"ranking\": \"bm25\",\n            \"presentation.timing\": True,\n        },\n    )\n    print(response.is_successful())\n</pre> with app.syncio(connections=12) as session:     response: VespaQueryResponse = session.query(         hits=1,         body={             \"yql\": \"select title, body from doc where userQuery()\",             \"query\": \"Is statin use connected to breast cancer?\",             \"ranking\": \"bm25\",             \"presentation.timing\": True,         },     )     print(response.is_successful()) <pre>True\n</pre> In\u00a0[11]: Copied! <pre>import time\n\n# Will not compress the request, as body is less than 1024 bytes\nwith app.syncio(connections=1, compress=\"auto\") as session:\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body={\n            \"yql\": \"select title, body from doc where userQuery()\",\n            \"query\": \"Is statin use connected to breast cancer?\",\n            \"ranking\": \"bm25\",\n            \"presentation.timing\": True,\n        },\n    )\n    print(response.is_successful())\n\n# Will compress, as the size of the body exceeds 1024 bytes.\nlarge_body = {\n    \"yql\": \"select title, body from doc where userQuery()\",\n    \"query\": \"Is statin use connected to breast cancer?\",\n    \"input.query(q)\": \"asdf\" * 10000,\n    \"ranking\": \"bm25\",\n    \"presentation.timing\": True,\n}\ncompress_time = {}\n\nwith app.syncio(connections=1, compress=True) as session:\n    start_time = time.time()\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body=large_body,\n    )\n    end_time = time.time()\n    compress_time[\"force_compression\"] = end_time - start_time\n    print(response.is_successful())\n\nwith app.syncio(connections=1, compress=\"auto\") as session:\n    start_time = time.time()\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body=large_body,\n    )\n    end_time = time.time()\n    compress_time[\"auto\"] = end_time - start_time\n    print(response.is_successful())\n\n# Force no compression\nwith app.syncio(compress=False) as session:\n    start_time = time.time()\n    response: VespaQueryResponse = session.query(\n        hits=1,\n        body=large_body,\n        timeout=\"5s\",\n    )\n    end_time = time.time()\n    compress_time[\"no_compression\"] = end_time - start_time\n    print(response.is_successful())\n</pre> import time  # Will not compress the request, as body is less than 1024 bytes with app.syncio(connections=1, compress=\"auto\") as session:     response: VespaQueryResponse = session.query(         hits=1,         body={             \"yql\": \"select title, body from doc where userQuery()\",             \"query\": \"Is statin use connected to breast cancer?\",             \"ranking\": \"bm25\",             \"presentation.timing\": True,         },     )     print(response.is_successful())  # Will compress, as the size of the body exceeds 1024 bytes. large_body = {     \"yql\": \"select title, body from doc where userQuery()\",     \"query\": \"Is statin use connected to breast cancer?\",     \"input.query(q)\": \"asdf\" * 10000,     \"ranking\": \"bm25\",     \"presentation.timing\": True, } compress_time = {}  with app.syncio(connections=1, compress=True) as session:     start_time = time.time()     response: VespaQueryResponse = session.query(         hits=1,         body=large_body,     )     end_time = time.time()     compress_time[\"force_compression\"] = end_time - start_time     print(response.is_successful())  with app.syncio(connections=1, compress=\"auto\") as session:     start_time = time.time()     response: VespaQueryResponse = session.query(         hits=1,         body=large_body,     )     end_time = time.time()     compress_time[\"auto\"] = end_time - start_time     print(response.is_successful())  # Force no compression with app.syncio(compress=False) as session:     start_time = time.time()     response: VespaQueryResponse = session.query(         hits=1,         body=large_body,         timeout=\"5s\",     )     end_time = time.time()     compress_time[\"no_compression\"] = end_time - start_time     print(response.is_successful()) <pre>True\nTrue\nTrue\nTrue\n</pre> In\u00a0[12]: Copied! <pre>compress_time\n</pre> compress_time Out[12]: <pre>{'force_compression': 0.02625894546508789,\n 'auto': 0.013608932495117188,\n 'no_compression': 0.009457826614379883}</pre> <p>The differences will be more significant the larger the size of the body, and the slower the network. It might be beneficial to perform a proper benchmarking if performance is critical for your application.</p> In\u00a0[13]: Copied! <pre># This cell is necessary when running async code in Jupyter Notebooks, as it already runs an event loop\nimport nest_asyncio\n\nnest_asyncio.apply()\n</pre> # This cell is necessary when running async code in Jupyter Notebooks, as it already runs an event loop import nest_asyncio  nest_asyncio.apply() In\u00a0[14]: Copied! <pre>import time\n\n\nquery = {\n    \"yql\": \"select title, body from doc where userQuery()\",\n    \"query\": \"Is statin use connected to breast cancer?\",\n    \"ranking\": \"bm25\",\n    \"presentation.timing\": True,\n}\n\n# List of queries with hits from 1 to 100\nqueries = [{**query, \"hits\": hits} for hits in range(1, 51)]\n\n# Run the queries concurrently\nstart_time = time.time()\nresponses = await app.query_many_async(queries=queries)\nend_time = time.time()\nprint(f\"Total time: {end_time - start_time:.2f} seconds\")\n# Print QPS\nprint(f\"QPS: {len(queries) / (end_time - start_time):.2f}\")\n</pre> import time   query = {     \"yql\": \"select title, body from doc where userQuery()\",     \"query\": \"Is statin use connected to breast cancer?\",     \"ranking\": \"bm25\",     \"presentation.timing\": True, }  # List of queries with hits from 1 to 100 queries = [{**query, \"hits\": hits} for hits in range(1, 51)]  # Run the queries concurrently start_time = time.time() responses = await app.query_many_async(queries=queries) end_time = time.time() print(f\"Total time: {end_time - start_time:.2f} seconds\") # Print QPS print(f\"QPS: {len(queries) / (end_time - start_time):.2f}\") <pre>Total time: 0.68 seconds\nQPS: 73.49\n</pre> In\u00a0[15]: Copied! <pre>dict_responses = [response.json for response in responses]\n</pre> dict_responses = [response.json for response in responses] In\u00a0[16]: Copied! <pre># Create a pandas DataFrame with the responses\nimport pandas as pd\n\ndf = pd.DataFrame(\n    [\n        {\n            \"hits\": len(\n                response.get(\"root\", {}).get(\"children\", [])\n            ),  # Some responses may not have 'children'\n            \"search_time\": response[\"timing\"][\"searchtime\"],\n            \"query_time\": response[\"timing\"][\"querytime\"],\n            \"summary_time\": response[\"timing\"][\"summaryfetchtime\"],\n        }\n        for response in dict_responses\n    ]\n)\ndf\n</pre> # Create a pandas DataFrame with the responses import pandas as pd  df = pd.DataFrame(     [         {             \"hits\": len(                 response.get(\"root\", {}).get(\"children\", [])             ),  # Some responses may not have 'children'             \"search_time\": response[\"timing\"][\"searchtime\"],             \"query_time\": response[\"timing\"][\"querytime\"],             \"summary_time\": response[\"timing\"][\"summaryfetchtime\"],         }         for response in dict_responses     ] ) df Out[16]: hits search_time query_time summary_time 0 1 0.006 0.003 0.002 1 2 0.014 0.006 0.006 2 3 0.046 0.024 0.019 3 4 0.037 0.015 0.010 4 5 0.468 0.035 0.422 5 6 0.199 0.014 0.177 6 7 0.018 0.008 0.009 7 8 0.041 0.012 0.025 8 9 0.103 0.018 0.082 9 10 0.288 0.022 0.265 10 11 0.568 0.015 0.544 11 12 0.507 0.026 0.480 12 13 0.470 0.012 0.457 13 14 0.566 0.025 0.535 14 15 0.566 0.027 0.534 15 16 0.213 0.018 0.194 16 17 0.564 0.010 0.549 17 18 0.543 0.025 0.516 18 19 0.545 0.016 0.520 19 20 0.329 0.017 0.308 20 21 0.413 0.010 0.396 21 22 0.088 0.010 0.078 22 23 0.418 0.019 0.382 23 24 0.401 0.021 0.379 24 25 0.348 0.013 0.334 25 26 0.554 0.020 0.527 26 27 0.532 0.204 0.322 27 28 0.550 0.023 0.524 28 29 0.211 0.005 0.202 29 30 0.524 0.312 0.208 30 31 0.440 0.016 0.422 31 32 0.537 0.459 0.075 32 33 0.532 0.285 0.232 33 34 0.397 0.024 0.371 34 35 0.398 0.046 0.345 35 36 0.555 0.036 0.512 36 37 0.545 0.009 0.525 37 38 0.145 0.018 0.116 38 39 0.418 0.022 0.394 39 40 0.373 0.013 0.359 40 41 0.426 0.044 0.381 41 0 0.446 0.446 0.000 42 43 0.292 0.014 0.267 43 44 0.383 0.027 0.344 44 45 0.422 0.012 0.409 45 46 0.515 0.034 0.475 46 47 0.518 0.039 0.475 47 48 0.504 0.007 0.493 48 49 0.505 0.012 0.488 49 50 0.517 0.007 0.500 In\u00a0[17]: Copied! <pre>with app.syncio(connections=12) as session:\n    try:\n        response: VespaQueryResponse = session.query(\n            hits=1,\n            body={\n                \"yql\": \"select * from doc where userQuery()\",\n                \"query\": \"Is statin use connected to breast cancer?\",\n                \"timeout\": \"1ms\",\n            },\n        )\n        print(response.is_successful())\n    except VespaError as e:\n        print(str(e))\n</pre> with app.syncio(connections=12) as session:     try:         response: VespaQueryResponse = session.query(             hits=1,             body={                 \"yql\": \"select * from doc where userQuery()\",                 \"query\": \"Is statin use connected to breast cancer?\",                 \"timeout\": \"1ms\",             },         )         print(response.is_successful())     except VespaError as e:         print(str(e)) <pre>[{'code': 12, 'summary': 'Timed out', 'message': 'No time left after waiting for 1ms to execute query'}]\n</pre> <p>In the following example, we forgot to include the <code>query</code> parameter but still reference it in the yql. This causes a bad client request response (400):</p> In\u00a0[18]: Copied! <pre>with app.syncio(connections=12) as session:\n    try:\n        response: VespaQueryResponse = session.query(\n            hits=1, body={\"yql\": \"select * from doc where userQuery()\"}\n        )\n        print(response.is_successful())\n    except VespaError as e:\n        print(str(e))\n</pre> with app.syncio(connections=12) as session:     try:         response: VespaQueryResponse = session.query(             hits=1, body={\"yql\": \"select * from doc where userQuery()\"}         )         print(response.is_successful())     except VespaError as e:         print(str(e)) <pre>[{'code': 3, 'summary': 'Illegal query', 'source': 'sample_content', 'message': 'No query'}]\n</pre> In\u00a0[20]: Copied! <pre># Example using QueryBuilder with our sample app\nimport vespa.querybuilder as qb\nfrom vespa.querybuilder import QueryField\n\ntitle = QueryField(\"title\")\nbody = QueryField(\"body\")\n\n# Build a query to find documents containing \"asthma\" in title or body\nq = (\n    qb.select([\"title\", \"body\"])\n    .from_(\"doc\")\n    .where(title.contains(\"asthma\") | body.contains(\"asthma\"))\n    .set_limit(5)\n)\n\nprint(f\"Query: {q}\")\nwith app.syncio() as session:\n    resp = session.query(yql=q, ranking=\"bm25\")\n    print(f\"Found {len(resp.hits)} documents\")\n    for hit in resp.hits:\n        print(f\"- {hit['fields']['title']}\")\n</pre> # Example using QueryBuilder with our sample app import vespa.querybuilder as qb from vespa.querybuilder import QueryField  title = QueryField(\"title\") body = QueryField(\"body\")  # Build a query to find documents containing \"asthma\" in title or body q = (     qb.select([\"title\", \"body\"])     .from_(\"doc\")     .where(title.contains(\"asthma\") | body.contains(\"asthma\"))     .set_limit(5) )  print(f\"Query: {q}\") with app.syncio() as session:     resp = session.query(yql=q, ranking=\"bm25\")     print(f\"Found {len(resp.hits)} documents\")     for hit in resp.hits:         print(f\"- {hit['fields']['title']}\") <pre>Query: select title, body from doc where title contains \"asthma\" or body contains \"asthma\" limit 5\nFound 0 documents\n</pre> In\u00a0[24]: Copied! <pre>app = Vespa(url=\"https://api.search.vespa.ai\")\n</pre> app = Vespa(url=\"https://api.search.vespa.ai\") In\u00a0[25]: Copied! <pre>import vespa.querybuilder as qb\nfrom vespa.querybuilder import QueryField\n\nnamespace = QueryField(\"namespace\")\nq = (\n    qb.select([\"title\", \"path\", \"term_count\"])\n    .from_(\"doc\")\n    .where(\n        namespace.matches(\"pyvespa\")\n    )  # matches is regex-match, see https://docs.vespa.ai/en/reference/query-language-reference.html#matches\n    .order_by(\"term_count\", ascending=False)\n    .set_limit(10)\n)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q)\nresults = [hit[\"fields\"] for hit in resp.hits]\ndf = pd.DataFrame(results)\ndf\n</pre> import vespa.querybuilder as qb from vespa.querybuilder import QueryField  namespace = QueryField(\"namespace\") q = (     qb.select([\"title\", \"path\", \"term_count\"])     .from_(\"doc\")     .where(         namespace.matches(\"pyvespa\")     )  # matches is regex-match, see https://docs.vespa.ai/en/reference/query-language-reference.html#matches     .order_by(\"term_count\", ascending=False)     .set_limit(10) ) print(f\"Query: {q}\") resp = app.query(yql=q) results = [hit[\"fields\"] for hit in resp.hits] df = pd.DataFrame(results) df <pre>Query: select title, path, term_count from doc where namespace matches \"pyvespa\" order by term_count desc limit 10\n</pre> Out[25]: path title term_count 0 /examples/feed_performance.html Feeding performance\u00b6 76669 1 /examples/simplified-retrieval-with-colpali-vl... Scaling ColPALI (VLM) Retrieval\u00b6 14393 2 /examples/pdf-retrieval-with-ColQwen2-vlm_Vesp... PDF-Retrieval using ColQWen2 (ColPali) with Ve... 14309 3 /examples/colpali-document-retrieval-vision-la... Vespa \ud83e\udd1d ColPali: Efficient Document Retrieval ... 13996 4 /examples/colpali-benchmark-vqa-vlm_Vespa-clou... ColPali Ranking Experiments on DocVQA\u00b6 13692 5 /examples/visual_pdf_rag_with_vespa_colpali_cl... Visual PDF RAG with Vespa - ColPali demo appli... 8237 6 /examples/billion-scale-vector-search-with-coh... Billion-scale vector search with Cohere binary... 7880 7 /examples/video_search_twelvelabs_cloud.html Video Search and Retrieval with Vespa and Twel... 7605 8 /examples/chat_with_your_pdfs_using_colbert_la... Chat with your pdfs with ColBERT, langchain, a... 7501 9 /api/vespa/package.html Package 6059 In\u00a0[26]: Copied! <pre>import vespa.querybuilder as qb\nfrom vespa.querybuilder import QueryField\nfrom datetime import datetime\n\nqueryterm = \"embedding\"\n\n# We need to instantiate a QueryField for fields that we want to call methods on\nlast_updated = QueryField(\"last_updated\")\ntitle = QueryField(\"title\")\nheaders = QueryField(\"headers\")\npath = QueryField(\"path\")\nnamespace = QueryField(\"namespace\")\ncontent = QueryField(\"content\")\n\nfrom_ts = int(datetime(2024, 1, 1).timestamp())\nto_ts = int(datetime.now().timestamp())\nprint(f\"From: {from_ts}, To: {to_ts}\")\nq = (\n    qb.select(\n        [title, last_updated, content]\n    )  # Select takes either a list of QueryField or strings, (or '*' for all fields)\n    .from_(\"doc\")\n    .where(\n        namespace.matches(\"op.*\")\n        &amp; last_updated.in_range(from_ts, to_ts)  # could also use &gt; and &lt;\n        &amp; qb.weakAnd(\n            title.contains(queryterm),\n            content.contains(queryterm),\n            headers.contains(queryterm),\n            path.contains(queryterm),\n        )\n    )\n    .set_limit(3)\n)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q, ranking=\"documentation\")\n</pre> import vespa.querybuilder as qb from vespa.querybuilder import QueryField from datetime import datetime  queryterm = \"embedding\"  # We need to instantiate a QueryField for fields that we want to call methods on last_updated = QueryField(\"last_updated\") title = QueryField(\"title\") headers = QueryField(\"headers\") path = QueryField(\"path\") namespace = QueryField(\"namespace\") content = QueryField(\"content\")  from_ts = int(datetime(2024, 1, 1).timestamp()) to_ts = int(datetime.now().timestamp()) print(f\"From: {from_ts}, To: {to_ts}\") q = (     qb.select(         [title, last_updated, content]     )  # Select takes either a list of QueryField or strings, (or '*' for all fields)     .from_(\"doc\")     .where(         namespace.matches(\"op.*\")         &amp; last_updated.in_range(from_ts, to_ts)  # could also use &gt; and &lt;         &amp; qb.weakAnd(             title.contains(queryterm),             content.contains(queryterm),             headers.contains(queryterm),             path.contains(queryterm),         )     )     .set_limit(3) ) print(f\"Query: {q}\") resp = app.query(yql=q, ranking=\"documentation\") <pre>From: 1704063600, To: 1749803562\nQuery: select title, last_updated, content from doc where namespace matches \"op.*\" and range(last_updated, 1704063600, 1749803562) and weakAnd(title contains \"embedding\", content contains \"embedding\", headers contains \"embedding\", path contains \"embedding\") limit 3\n</pre> In\u00a0[27]: Copied! <pre>df = pd.DataFrame([hit[\"fields\"] | hit for hit in resp.hits])\ndf = pd.concat(\n    [\n        df.drop([\"matchfeatures\", \"fields\"], axis=1),\n        pd.json_normalize(df[\"matchfeatures\"]),\n    ],\n    axis=1,\n)\ndf.T\n</pre> df = pd.DataFrame([hit[\"fields\"] | hit for hit in resp.hits]) df = pd.concat(     [         df.drop([\"matchfeatures\", \"fields\"], axis=1),         pd.json_normalize(df[\"matchfeatures\"]),     ],     axis=1, ) df.T Out[27]: 0 1 2 content &lt;sep /&gt;similar data by finding nearby points i... Reference configuration for &lt;hi&gt;embedders&lt;/hi&gt;... &lt;sep /&gt; basic news search application - applic... title Embedding Embedding Reference News search and recommendation tutorial - embe... last_updated 1749727838 1749727838 1749727839 id index:documentation/0/5d6e77ca20d4e8ee29716747 index:documentation/1/a03c4aef22fcde916804d3d9 index:documentation/1/ad44f35cbd7b8214f88963e3 relevance 23.259617 22.075122 16.505077 source documentation documentation documentation bm25(content) 2.385057 2.352575 2.384316 bm25(headers) 7.476571 8.106656 5.46136 bm25(keywords) 0.0 0.0 0.0 bm25(path) 3.990027 3.349312 3.100325 bm25(title) 4.703981 4.133289 2.779538 fieldLength(content) 3825.0 2031.0 3273.0 fieldLength(title) 1.0 2.0 6.0 fieldMatch(content) 0.915753 0.892113 0.915871 fieldMatch(content).matches 1.0 1.0 1.0 fieldMatch(title) 1.0 0.933869 0.842758 query(contentWeight) 1.0 1.0 1.0 query(headersWeight) 1.0 1.0 1.0 query(pathWeight) 1.0 1.0 1.0 query(titleWeight) 2.0 2.0 2.0 In\u00a0[28]: Copied! <pre>from vespa.querybuilder import Grouping as G\n\ngrouping = G.all(\n    G.group(\"customer\"),\n    G.each(G.output(G.sum(\"price\"))),\n)\nq = qb.select(\"*\").from_(\"purchase\").where(True).set_limit(0).groupby(grouping)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q)\ngroup = resp.hits[0][\"children\"][0][\"children\"]\n# get value and sum(price) into a DataFrame\ndf = pd.DataFrame([hit[\"fields\"] | hit for hit in group])\ndf = df.loc[:, [\"value\", \"sum(price)\"]]\ndf\n</pre> from vespa.querybuilder import Grouping as G  grouping = G.all(     G.group(\"customer\"),     G.each(G.output(G.sum(\"price\"))), ) q = qb.select(\"*\").from_(\"purchase\").where(True).set_limit(0).groupby(grouping) print(f\"Query: {q}\") resp = app.query(yql=q) group = resp.hits[0][\"children\"][0][\"children\"] # get value and sum(price) into a DataFrame df = pd.DataFrame([hit[\"fields\"] | hit for hit in group]) df = df.loc[:, [\"value\", \"sum(price)\"]] df <pre>Query: select * from purchase where true limit 0 | all(group(customer) each(output(sum(price))))\n</pre> Out[28]: value sum(price) 0 Brown 20537 1 Jones 39816 2 Smith 19484 In\u00a0[29]: Copied! <pre>from vespa.querybuilder import Grouping as G\n\n# First, we construct the grouping expression:\ngrouping = G.all(\n    G.group(\"customer\"),\n    G.each(\n        G.group(G.time_date(\"date\")),\n        G.each(\n            G.output(G.sum(\"price\")),\n        ),\n    ),\n)\n# Then, we construct the query:\nq = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q)\ngroup_data = resp.hits[0][\"children\"][0][\"children\"]\nrecords = [\n    {\n        \"GroupId\": group[\"value\"],\n        \"Date\": date_entry[\"value\"],\n        \"Sum(price)\": date_entry[\"fields\"].get(\"sum(price)\", 0),\n    }\n    for group in group_data\n    for date_group in group.get(\"children\", [])\n    for date_entry in date_group.get(\"children\", [])\n]\n\n# Create DataFrame\ndf = pd.DataFrame(records)\ndf\n</pre> from vespa.querybuilder import Grouping as G  # First, we construct the grouping expression: grouping = G.all(     G.group(\"customer\"),     G.each(         G.group(G.time_date(\"date\")),         G.each(             G.output(G.sum(\"price\")),         ),     ), ) # Then, we construct the query: q = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping) print(f\"Query: {q}\") resp = app.query(yql=q) group_data = resp.hits[0][\"children\"][0][\"children\"] records = [     {         \"GroupId\": group[\"value\"],         \"Date\": date_entry[\"value\"],         \"Sum(price)\": date_entry[\"fields\"].get(\"sum(price)\", 0),     }     for group in group_data     for date_group in group.get(\"children\", [])     for date_entry in date_group.get(\"children\", []) ]  # Create DataFrame df = pd.DataFrame(records) df <pre>Query: select * from purchase where true | all(group(customer) each(group(time.date(date)) each(output(sum(price)))))\n</pre> Out[29]: GroupId Date Sum(price) 0 Brown 2006-9-10 7540 1 Brown 2006-9-11 1597 2 Brown 2006-9-8 8000 3 Brown 2006-9-9 3400 4 Jones 2006-9-10 8900 5 Jones 2006-9-11 20816 6 Jones 2006-9-8 8000 7 Jones 2006-9-9 2100 8 Smith 2006-9-10 6100 9 Smith 2006-9-11 2584 10 Smith 2006-9-6 1000 11 Smith 2006-9-7 3000 12 Smith 2006-9-9 6800 In\u00a0[30]: Copied! <pre>from vespa.querybuilder import Grouping as G\n\ngrouping = G.all(\n    G.group(G.mod(G.div(\"date\", G.mul(60, 60)), 24)),\n    G.order(-G.sum(\"price\")),\n    G.each(G.output(G.sum(\"price\"))),\n)\nq = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping)\nprint(f\"Query: {q}\")\nresp = app.query(yql=q)\ngroup_data = resp.hits[0][\"children\"][0][\"children\"]\ndf = pd.DataFrame([hit[\"fields\"] | hit for hit in group_data])\ndf = df.loc[:, [\"value\", \"sum(price)\"]]\ndf\n</pre> from vespa.querybuilder import Grouping as G  grouping = G.all(     G.group(G.mod(G.div(\"date\", G.mul(60, 60)), 24)),     G.order(-G.sum(\"price\")),     G.each(G.output(G.sum(\"price\"))), ) q = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping) print(f\"Query: {q}\") resp = app.query(yql=q) group_data = resp.hits[0][\"children\"][0][\"children\"] df = pd.DataFrame([hit[\"fields\"] | hit for hit in group_data]) df = df.loc[:, [\"value\", \"sum(price)\"]] df <pre>Query: select * from purchase where true | all(group(mod(div(date, mul(60, 60)),24)) order(-sum(price)) each(output(sum(price))))\n</pre> Out[30]: value sum(price) 0 10 26181 1 9 23524 2 8 22367 3 11 6765 4 7 1000"},{"location":"query.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>This guide goes through how to query a Vespa instance using the Query API and https://search.vespa.ai/ app as an example.</p>"},{"location":"query.html#query-performance","title":"Query Performance\u00b6","text":"<p>There are several things that impact end-to-end query performance:</p> <ul> <li>HTTP layer performance, connecting handling, mututal TLS handshake and network round-trip latency<ul> <li>Make sure to re-use connections using context manager <code>with vespa.app.syncio():</code> to avoid setting up new connections for every unique query. See http best practises</li> <li>The size of the fields and the number of hits requested also greatly impact network performance; a larger payload means higher latency.</li> <li>By adding <code>\"presentation.timing\": True</code> as a request parameter, the Vespa response includes the server-side processing (also including reading the query from the network, but not delivering the result over the network). This can be handy for debugging latency.</li> </ul> </li> <li>Vespa performance, the features used inside the Vespa instance.</li> </ul>"},{"location":"query.html#compressing-queries","title":"Compressing queries\u00b6","text":"<p>The <code>VespaSync</code> class has a <code>compress</code> argument that can be used to compress the query before sending it to Vespa. This can be useful when the query is large and/or the network is slow. The compression is done using <code>gzip</code>, and is supported by Vespa.</p> <p>By default, the <code>compress</code> argument is set to <code>\"auto\"</code>, which means that the query will be compressed if the size of the query is larger than 1024 bytes. The <code>compress</code> argument can also be set to <code>True</code> or <code>False</code> to force the query to be compressed or not, respectively.</p> <p>The compression will be applied to both queries and feed operations. (HTTP POST or PUT requests).</p>"},{"location":"query.html#running-queries-asynchronously","title":"Running Queries asynchronously\u00b6","text":"<p>If you want to benchmark the capacity of a Vespa application, we suggest using vespa-fbench, which is a load generator tool that lets you measure throughput and latency with a predefined number of clients. Vespa-fbench is not Vespa-specific, and can be used to benchmark any HTTP service.</p> <p>Another option is to use the Open Source k6 load testing tool.</p> <p>If you want to run multiple queries from pyvespa, we suggest using the convenience method <code>Vespa.query_many_async()</code>, which allows you to run multiple queries in parallel using the async client. Below, we will demonstrate a simple example of running 100 queries in parallel, and capture the server-reported times and the client-reported time (including network latency).</p>"},{"location":"query.html#error-handling","title":"Error handling\u00b6","text":"<p>Vespa's default query timeout is 500ms; Pyvespa will by default retry up to 3 times for queries that return response codes like 429, 500,503 and 504. A <code>VespaError</code> is raised if retries did not end up with success. In the following example, we set a very low timeout of <code>1ms</code> which will cause Vespa to time out the request, and it returns a 504 http error code. The underlying error is wrapped in a <code>VespaError</code> with the payload error message returned from Vespa:</p>"},{"location":"query.html#using-the-querybuilder-dsl-api","title":"Using the Querybuilder DSL API\u00b6","text":"<p>From <code>pyvespa&gt;=0.52.0</code>, we provide a Domain Specific Language (DSL) that allows you to build queries programmatically in the <code>vespa.querybuilder</code>-module. See reference for full details. There are also many examples in our tests:</p> <ul> <li>https://github.com/vespa-engine/pyvespa/blob/master/tests/unit/test_grouping.py</li> <li>https://github.com/vespa-engine/pyvespa/blob/master/tests/unit/test_qb.py</li> <li>https://github.com/vespa-engine/pyvespa/blob/master/tests/integration/test_integration_grouping.py</li> <li>https://github.com/vespa-engine/pyvespa/blob/master/tests/integration/test_integration_queries.py</li> </ul> <p>This section demonstrates common query patterns using the querybuilder DSL. All features of the Vespa Query Language are supported by the querybuilder DSL.</p>      Using the Querybuilder DSL is completely optional, and you can always use the Vespa Query Language directly by passing the query as a string, which might be more convenient for simple queries.  <p>We will use the Vespa documentation search app for some advanced examples that require specific schemas. For basic examples, our local sample app works well.</p>"},{"location":"query.html#advanced-querybuilder-examples","title":"Advanced QueryBuilder Examples\u00b6","text":"<p>For the following advanced examples, we'll switch to using Vespa's documentation search app which has more complex schemas. First, let's clean up our sample app:</p>"},{"location":"query.html#example-1-matches-order-by-and-limit","title":"Example 1 - matches, order by and limit\u00b6","text":"<p>We want to find the 10 documents with the most terms in the 'pyvespa'-namespace (the documentation search has a 'namespace'-field, which refers to the source of the documentation). Note that the documentation search operates on the 'paragraph'-schema, but for demo purposes, we will use the 'document'-schema.</p>"},{"location":"query.html#example-2-timestamp-range-contains","title":"Example 2 - timestamp range, contains\u00b6","text":"<p>We want to find the documents where one of the indexed fields contains the query term <code>embedding</code>,is updated after Jan 1st 2024 and the current timestamp, and have the documents ranked the 'documentation' rank profile. See https://github.com/vespa-cloud/vespa-documentation-search/blob/main/src/main/application/schemas/doc.sd.</p>"},{"location":"query.html#example-3-basic-grouping","title":"Example 3 - Basic grouping\u00b6","text":"<p>Vespa supports grouping and aggregation of matches through the Vespa grouping language. For an introduction to grouping, see https://docs.vespa.ai/en/grouping.html.</p> <p>We will use purchase schema that is also deployed in the documentation search app.</p>"},{"location":"query.html#example-4-nested-grouping","title":"Example 4 - Nested grouping\u00b6","text":"<p>Let's find out how much each customer has spent per day by grouping on customer, then date:</p>"},{"location":"query.html#example-5-grouping-with-expressions","title":"Example 5 - Grouping with expressions\u00b6","text":"<p>Instead of just grouping on some attribute value, the group clause may contain arbitrarily complex expressions - see Grouping reference for exhaustive list.</p> <p>Examples:</p> <ul> <li>Select the minimum or maximum of sub-expressions</li> <li>Addition, subtraction, multiplication, division, and even modulo of - sub-expressions</li> <li>Bitwise operations on sub-expressions</li> <li>Concatenation of the results of sub-expressions</li> </ul> <p>Let's use some of these expressions to get the sum the prices of purchases on a per-hour-of-day basis.</p>"},{"location":"reads-writes.html","title":"Read and write operations","text":"Refer to troubleshooting     for any problem when running this guide.  In\u00a0[1]: Copied! <pre>!docker info | grep \"Total Memory\"\n</pre> !docker info | grep \"Total Memory\" <p>Define a simple application package with five fields</p> In\u00a0[1]: Copied! <pre>from vespa.application import ApplicationPackage\nfrom vespa.package import Schema, Document, Field, FieldSet, HNSW, RankProfile\n\napp_package = ApplicationPackage(\n    name=\"vector\",\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"attribute\", \"summary\"]),\n                    Field(\n                        name=\"title\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                    ),\n                    Field(\n                        name=\"body\",\n                        type=\"string\",\n                        indexing=[\"index\", \"summary\"],\n                        index=\"enable-bm25\",\n                    ),\n                    Field(\n                        name=\"popularity\",\n                        type=\"float\",\n                        indexing=[\"attribute\", \"summary\"],\n                    ),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;bfloat16&gt;(x[1536])\",\n                        indexing=[\"attribute\", \"summary\", \"index\"],\n                        ann=HNSW(\n                            distance_metric=\"innerproduct\",\n                            max_links_per_node=16,\n                            neighbors_to_explore_at_insert=128,\n                        ),\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],\n            rank_profiles=[\n                RankProfile(\n                    name=\"default\",\n                    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[1536])\")],\n                    first_phase=\"closeness(field, embedding)\",\n                )\n            ],\n        )\n    ],\n)\n</pre> from vespa.application import ApplicationPackage from vespa.package import Schema, Document, Field, FieldSet, HNSW, RankProfile  app_package = ApplicationPackage(     name=\"vector\",     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"attribute\", \"summary\"]),                     Field(                         name=\"title\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                     ),                     Field(                         name=\"body\",                         type=\"string\",                         indexing=[\"index\", \"summary\"],                         index=\"enable-bm25\",                     ),                     Field(                         name=\"popularity\",                         type=\"float\",                         indexing=[\"attribute\", \"summary\"],                     ),                     Field(                         name=\"embedding\",                         type=\"tensor(x[1536])\",                         indexing=[\"attribute\", \"summary\", \"index\"],                         ann=HNSW(                             distance_metric=\"innerproduct\",                             max_links_per_node=16,                             neighbors_to_explore_at_insert=128,                         ),                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"body\"])],             rank_profiles=[                 RankProfile(                     name=\"default\",                     inputs=[(\"query(q)\", \"tensor(x[1536])\")],                     first_phase=\"closeness(field, embedding)\",                 )             ],         )     ], ) In\u00a0[2]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=app_package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for configuration server, 5/60 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\n    \"KShivendu/dbpedia-entities-openai-1M\", split=\"train\", streaming=True\n).take(1000)\n</pre> from datasets import load_dataset  dataset = load_dataset(     \"KShivendu/dbpedia-entities-openai-1M\", split=\"train\", streaming=True ).take(1000) In\u00a0[4]: Copied! <pre>pyvespa_feed_format = dataset.map(\n    lambda x: {\"id\": x[\"_id\"], \"fields\": {\"id\": x[\"_id\"], \"embedding\": x[\"openai\"]}}\n)\n</pre> pyvespa_feed_format = dataset.map(     lambda x: {\"id\": x[\"_id\"], \"fields\": {\"id\": x[\"_id\"], \"embedding\": x[\"openai\"]}} ) <p>Feed using feed_iterable which accepts an <code>Iterable</code>. <code>feed_iterable</code> accepts a callback callable routine that is called for every single data operation so we can check the result. If the result <code>is_successful()</code> the operation is persisted and applied in Vespa.</p> In\u00a0[5]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[6]: Copied! <pre>app.feed_iterable(\n    iter=pyvespa_feed_format,\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    max_queue_size=4000,\n    max_workers=16,\n    max_connections=16,\n)\n</pre> app.feed_iterable(     iter=pyvespa_feed_format,     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     max_queue_size=4000,     max_workers=16,     max_connections=16, ) In\u00a0[7]: Copied! <pre>def my_generator() -&gt; dict:\n    for i in range(1000):\n        yield {\n            \"id\": str(i),\n            \"fields\": {\n                \"id\": str(i),\n                \"title\": \"title\",\n                \"body\": \"this is body\",\n                \"popularity\": 1.0,\n            },\n        }\n</pre> def my_generator() -&gt; dict:     for i in range(1000):         yield {             \"id\": str(i),             \"fields\": {                 \"id\": str(i),                 \"title\": \"title\",                 \"body\": \"this is body\",                 \"popularity\": 1.0,             },         } In\u00a0[8]: Copied! <pre>app.feed_iterable(\n    iter=my_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    max_queue_size=4000,\n    max_workers=12,\n    max_connections=12,\n)\n</pre> app.feed_iterable(     iter=my_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     max_queue_size=4000,     max_workers=12,     max_connections=12, ) In\u00a0[9]: Copied! <pre>all_docs = []\nfor slice in app.visit(\n    content_cluster_name=\"vector_content\",\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    selection=\"true\",  # Document selection - see https://docs.vespa.ai/en/reference/document-select-language.html\n    slices=4,\n    wanted_document_count=300,\n):\n    for response in slice:\n        print(response.number_documents_retrieved)\n        all_docs.extend(response.documents)\n</pre> all_docs = [] for slice in app.visit(     content_cluster_name=\"vector_content\",     schema=\"doc\",     namespace=\"benchmark\",     selection=\"true\",  # Document selection - see https://docs.vespa.ai/en/reference/document-select-language.html     slices=4,     wanted_document_count=300, ):     for response in slice:         print(response.number_documents_retrieved)         all_docs.extend(response.documents) <pre>300\n196\n303\n185\n309\n191\n303\n213\n</pre> In\u00a0[10]: Copied! <pre>len(all_docs)\n</pre> len(all_docs) Out[10]: <pre>2000</pre> In\u00a0[11]: Copied! <pre>for slice in app.visit(\n    content_cluster_name=\"vector_content\", wanted_document_count=1000\n):\n    for response in slice:\n        print(response.number_documents_retrieved)\n</pre> for slice in app.visit(     content_cluster_name=\"vector_content\", wanted_document_count=1000 ):     for response in slice:         print(response.number_documents_retrieved) <pre>190\n189\n226\n205\n184\n214\n202\n181\n217\n192\n</pre> In\u00a0[12]: Copied! <pre>def my_update_generator() -&gt; dict:\n    for i in range(1000):\n        yield {\"id\": str(i), \"fields\": {\"popularity\": 2.0}}\n</pre> def my_update_generator() -&gt; dict:     for i in range(1000):         yield {\"id\": str(i), \"fields\": {\"popularity\": 2.0}} In\u00a0[13]: Copied! <pre>app.feed_iterable(\n    iter=my_update_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    operation_type=\"update\",\n    max_queue_size=4000,\n    max_workers=12,\n    max_connections=12,\n)\n</pre> app.feed_iterable(     iter=my_update_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     operation_type=\"update\",     max_queue_size=4000,     max_workers=12,     max_connections=12, ) In\u00a0[14]: Copied! <pre>def my_increment_generator() -&gt; dict:\n    for i in range(1000):\n        yield {\"id\": str(i), \"fields\": {\"popularity\": {\"increment\": 1.0}}}\n</pre> def my_increment_generator() -&gt; dict:     for i in range(1000):         yield {\"id\": str(i), \"fields\": {\"popularity\": {\"increment\": 1.0}}} In\u00a0[15]: Copied! <pre>app.feed_iterable(\n    iter=my_increment_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    operation_type=\"update\",\n    max_queue_size=4000,\n    max_workers=12,\n    max_connections=12,\n    auto_assign=False,\n)\n</pre> app.feed_iterable(     iter=my_increment_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     operation_type=\"update\",     max_queue_size=4000,     max_workers=12,     max_connections=12,     auto_assign=False, ) <p>We can now query the data, notice how we use a context manager <code>with</code> to close connection after query This avoids resource leakage and allows for reuse of connections. In this case, we only do a single query and there is no need for having more than one connection. Setting more connections will just increase connection level overhead.</p> In\u00a0[16]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nwith app.syncio(connections=1):\n    response: VespaQueryResponse = app.query(\n        yql=\"select id from doc where popularity &gt; 2.5\", hits=0\n    )\n    print(response.number_documents_retrieved)\n</pre> from vespa.io import VespaQueryResponse  with app.syncio(connections=1):     response: VespaQueryResponse = app.query(         yql=\"select id from doc where popularity &gt; 2.5\", hits=0     )     print(response.number_documents_retrieved) <pre>1000\n</pre> In\u00a0[16]: Copied! <pre>def my_delete_generator() -&gt; dict:\n    for i in range(1000):\n        yield {\"id\": str(i)}\n\n\napp.feed_iterable(\n    iter=my_delete_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    operation_type=\"delete\",\n    max_queue_size=5000,\n    max_workers=48,\n    max_connections=48,\n)\n</pre> def my_delete_generator() -&gt; dict:     for i in range(1000):         yield {\"id\": str(i)}   app.feed_iterable(     iter=my_delete_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     operation_type=\"delete\",     max_queue_size=5000,     max_workers=48,     max_connections=48, ) In\u00a0[17]: Copied! <pre># Dump some operation to a jsonl file, we store it in the format expected by pyvespa\n# This to demonstrate feeding from a file in the next section.\nimport json\n\nwith open(\"documents.jsonl\", \"w\") as f:\n    for doc in dataset:\n        d = {\"id\": doc[\"_id\"], \"fields\": {\"id\": doc[\"_id\"], \"embedding\": doc[\"openai\"]}}\n        f.write(json.dumps(d) + \"\\n\")\n</pre> # Dump some operation to a jsonl file, we store it in the format expected by pyvespa # This to demonstrate feeding from a file in the next section. import json  with open(\"documents.jsonl\", \"w\") as f:     for doc in dataset:         d = {\"id\": doc[\"_id\"], \"fields\": {\"id\": doc[\"_id\"], \"embedding\": doc[\"openai\"]}}         f.write(json.dumps(d) + \"\\n\") <p>Define the file generator that will yield one line at a time</p> In\u00a0[18]: Copied! <pre>import json\n\n\ndef from_file_generator() -&gt; dict:\n    with open(\"documents.jsonl\") as f:\n        for line in f:\n            yield json.loads(line)\n</pre> import json   def from_file_generator() -&gt; dict:     with open(\"documents.jsonl\") as f:         for line in f:             yield json.loads(line) In\u00a0[19]: Copied! <pre>app.feed_iterable(\n    iter=from_file_generator(),\n    schema=\"doc\",\n    namespace=\"benchmark\",\n    callback=callback,\n    operation_type=\"feed\",\n    max_queue_size=4000,\n    max_workers=32,\n    max_connections=32,\n)\n</pre> app.feed_iterable(     iter=from_file_generator(),     schema=\"doc\",     namespace=\"benchmark\",     callback=callback,     operation_type=\"feed\",     max_queue_size=4000,     max_workers=32,     max_connections=32, ) In\u00a0[20]: Copied! <pre>with app.syncio(connections=1):\n    response: VespaResponse = app.feed_data_point(\n        schema=\"doc\",\n        namespace=\"benchmark\",\n        data_id=\"1\",\n        fields={\n            \"id\": \"1\",\n            \"title\": \"title\",\n            \"body\": \"this is body\",\n            \"popularity\": 1.0,\n        },\n    )\n    print(response.is_successful())\n    print(response.get_json())\n</pre> with app.syncio(connections=1):     response: VespaResponse = app.feed_data_point(         schema=\"doc\",         namespace=\"benchmark\",         data_id=\"1\",         fields={             \"id\": \"1\",             \"title\": \"title\",             \"body\": \"this is body\",             \"popularity\": 1.0,         },     )     print(response.is_successful())     print(response.get_json()) <pre>True\n{'pathId': '/document/v1/benchmark/doc/docid/1', 'id': 'id:benchmark:doc::1'}\n</pre> <p>Get the same document, try also to change data_id to a document that does not exist which will raise a 404 http error.</p> In\u00a0[21]: Copied! <pre>with app.syncio(connections=1):\n    response: VespaResponse = app.get_data(\n        schema=\"doc\",\n        namespace=\"benchmark\",\n        data_id=\"1\",\n    )\n    print(response.is_successful())\n    print(response.get_json())\n</pre> with app.syncio(connections=1):     response: VespaResponse = app.get_data(         schema=\"doc\",         namespace=\"benchmark\",         data_id=\"1\",     )     print(response.is_successful())     print(response.get_json()) <pre>True\n{'pathId': '/document/v1/benchmark/doc/docid/1', 'id': 'id:benchmark:doc::1', 'fields': {'body': 'this is body', 'title': 'title', 'popularity': 1.0, 'id': '1'}}\n</pre> In\u00a0[22]: Copied! <pre>with app.syncio(connections=1):\n    response: VespaResponse = app.update_data(\n        schema=\"doc\",\n        namespace=\"benchmark\",\n        data_id=\"does-not-exist\",\n        fields={\"popularity\": 3.0},\n        create=True,\n    )\n    print(response.is_successful())\n    print(response.get_json())\n</pre> with app.syncio(connections=1):     response: VespaResponse = app.update_data(         schema=\"doc\",         namespace=\"benchmark\",         data_id=\"does-not-exist\",         fields={\"popularity\": 3.0},         create=True,     )     print(response.is_successful())     print(response.get_json()) <pre>True\n{'pathId': '/document/v1/benchmark/doc/docid/does-not-exist', 'id': 'id:benchmark:doc::does-not-exist'}\n</pre> In\u00a0[23]: Copied! <pre>with app.syncio(connections=1):\n    response: VespaResponse = app.get_data(\n        schema=\"doc\",\n        namespace=\"benchmark\",\n        data_id=\"does-not-exist\",\n    )\n    print(response.is_successful())\n    print(response.get_json())\n</pre> with app.syncio(connections=1):     response: VespaResponse = app.get_data(         schema=\"doc\",         namespace=\"benchmark\",         data_id=\"does-not-exist\",     )     print(response.is_successful())     print(response.get_json()) <pre>True\n{'pathId': '/document/v1/benchmark/doc/docid/does-not-exist', 'id': 'id:benchmark:doc::does-not-exist', 'fields': {'popularity': 3.0}}\n</pre> In\u00a0[24]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"reads-writes.html#read-and-write-operations","title":"Read and write operations\u00b6","text":"<p>This notebook documents ways to feed, get, update and delete data:</p> <ul> <li>Using context manager with <code>with</code> for efficiently managing resources</li> <li>Feeding streams of data using <code>feed_iter</code> which can feed from streams, Iterables, Lists and files by the use of generators</li> </ul>"},{"location":"reads-writes.html#deploy-a-sample-application","title":"Deploy a sample application\u00b6","text":"<p>Install pyvespa and start Docker, validate minimum 4G available:</p>"},{"location":"reads-writes.html#feed-data-by-streaming-over-iterable-type","title":"Feed data by streaming over Iterable type\u00b6","text":"<p>This example notebook uses the dbpedia-entities-openai-1M dataset (1M OpenAI Embeddings (1536 dimensions) from June 2023).</p> <p>The <code>streaming=True</code> option allow paging the data on-demand from HF S3. This is extremely useful for large datasets, where the data does not fit in memory and downloading the entire dataset is not needed. Read more about datasets stream.</p>"},{"location":"reads-writes.html#converting-to-dataset-field-names-to-vespa-schema-field-names","title":"Converting to dataset field names to Vespa schema field names\u00b6","text":"<p>We need to convert the dataset field names to the configured Vespa schema field names, we do this with a simple lambda function.</p> <p>The map function does not page the data, the map step is performed lazily if we start iterating over the dataset. This allows chaining of map operations where the lambda is yielding the next document.</p>"},{"location":"reads-writes.html#feeding-with-generators","title":"Feeding with generators\u00b6","text":"<p>The above handled streaming data from a remote repo, we can also use generators or just List. In this example, we generate synthetic data using a generator function.</p>"},{"location":"reads-writes.html#visiting","title":"Visiting\u00b6","text":"<p>Visiting is a feature to efficiently get or process a set of documents, identified by a document selection expression. Visit yields multiple slices (run concurrently) each yielding responses (depending on number of documents in each slice). This allows for custom handling of each response.</p> <p>Visiting can be useful for exporting data, for example for ML training or for migrating a vespa application.</p>"},{"location":"reads-writes.html#updates","title":"Updates\u00b6","text":"<p>Using a similar generator we can update the fake data we added. This performs partial updates, assigning the <code>popularity</code> field to have the value <code>2.0</code>.</p>"},{"location":"reads-writes.html#other-update-operations","title":"Other update operations\u00b6","text":"<p>We can also perform other update operations, see Vespa docs on reads and writes. To achieve this we need to set the <code>auto_assign</code> parameter to <code>False</code> in the <code>feed_iterable</code> method (which will pass this to <code>update_data_point</code>-method).</p>"},{"location":"reads-writes.html#deleting","title":"Deleting\u00b6","text":"<p>Delete all the synthetic data with a custom generator. Now we don't need the <code>fields</code> key.</p>"},{"location":"reads-writes.html#feeding-operations-from-a-file","title":"Feeding operations from a file\u00b6","text":"<p>This demonstrates how we can use <code>feed_iter</code> to feed from a large file without reading the entire file, this also uses a generator.</p> <p>First we dump some operations to the file and peak at the first line:</p>"},{"location":"reads-writes.html#get-and-feed-individual-data-points","title":"Get and Feed individual data points\u00b6","text":"<p>Feed a single data point to Vespa</p>"},{"location":"reads-writes.html#upsert","title":"Upsert\u00b6","text":"<p>The following sends an update operation, if the document exist, the popularity field will be updated to take the value 3.0, and if the document does not exist, it's created and where the popularity value is 3.0.</p>"},{"location":"reads-writes.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"reads-writes.html#next-steps","title":"Next steps\u00b6","text":"<p>Read more on writing to Vespa in reads-and-writes.</p>"},{"location":"troubleshooting.html","title":"Troubleshooting","text":"<p>Also see the Vespa FAQ and Vespa support for more help resources.</p>"},{"location":"troubleshooting.html#vespaai-and-pyvespa","title":"Vespa.ai and pyvespa","text":"<p>Both Vespa and pyvespa APIs change regularly - make sure to use the latest version of vespaengine/vespa by running <code>docker pull vespaengine/vespa</code> and install pyvespa.</p> <p>To check the current version, run:</p> <pre><code>python3 -m pip show pyvespa\n</code></pre>"},{"location":"troubleshooting.html#docker-memory","title":"Docker Memory","text":"<p>pyvespa will start a Docker container with 4G memory by default - make sure Docker settings have at least this. Use the Docker Desktop settings or <code>docker info | grep \"Total Memory\"</code> or <code>podman info | grep \"memTotal\"</code> to validate.</p>"},{"location":"troubleshooting.html#port-conflicts-docker","title":"Port conflicts / Docker","text":"<p>Some of the notebooks run a Docker container. Make sure to stop running Docker containers before (re)running pyvespa notebooks - run <code>docker ps</code> and <code>docker ps -a -q -f status=exited</code> to list containers.</p>"},{"location":"troubleshooting.html#deployment","title":"Deployment","text":"<p>Vespa has safeguards for incompatible deployments, and will warn with validation-override or INVALID_APPLICATION_PACKAGE in the deploy output. See validation-overrides. This is most often due to pyvespa reusing a Docker container instance. The fix is to list (<code>docker ps</code>) and remove (<code>docker rm -f &lt;container id&gt;</code>) the existing Docker containers. Alternatively, use the Docker Dashboard application. Then deploy again.</p> <p>After deployment, validate status:</p> <ul> <li>Config server state: http://localhost:19071/state/v1/health</li> <li>Container state: http://localhost:8080/state/v1/health</li> </ul> <p>Look for <code>\"status\" : { \"code\" : \"up\"}</code> - both URLs must work before feeding or querying.</p>"},{"location":"troubleshooting.html#full-disk","title":"Full disk","text":"<p>Make sure to allocate enough disk space for Docker in Docker settings. If writes/queries fail or return no results, look in the <code>vespa.log</code> (output in the Docker dashboard):</p> <pre><code>WARNING searchnode\nproton.proton.server.disk_mem_usage_filter   Write operations are now blocked:\n'diskLimitReached: { action: \"add more content nodes\",\nreason: \"disk used (0.939172) &gt; disk limit (0.9)\",\nstats: { capacity: 50406772736, used: 47340617728, diskUsed: 0.939172, diskLimit: 0.9}}'\n</code></pre> <p>Future pyvespa versions might throw an exception in these cases. See Feed block - Vespa stops writes before the disk goes full. Add more disk space, clean up, or follow the example to reconfigure for higher usage.</p>"},{"location":"troubleshooting.html#check-number-of-indexed-documents","title":"Check number of indexed documents","text":"<p>For query errors, check the number of documents indexed before debugging further:</p> <pre><code>app.query(yql='select * from sources * where true').number_documents_indexed\n</code></pre> <p>If this is zero, check that the deployment of the application worked, and that the subsequent feeding step completed successfully.</p>"},{"location":"troubleshooting.html#too-many-open-files-during-batch-feeding","title":"Too many open files during batch feeding","text":"<p>This is an OS-related issue. There are two options to solve the problem:</p> <ol> <li> <p>Reduce the number of connections via the <code>connections</code> parameter:    <pre><code>with app.syncio(connections=12):\n</code></pre></p> </li> <li> <p>Increase the open file limit: <code>ulimit -n 10000</code>. Check if the limit was increased with <code>ulimit -Sn</code>.</p> </li> </ol>"},{"location":"troubleshooting.html#data-export","title":"Data export","text":"<p><code>vespa visit</code> exports data from Vespa - see Vespa CLI. Use this to validate data feeding and troubleshoot query issues.</p>"},{"location":"api/summary.html","title":"API Reference","text":"<ul> <li>vespa<ul> <li>application</li> <li>deployment</li> <li>evaluation</li> <li>exceptions</li> <li>io</li> <li>package</li> <li>querybuilder<ul> <li>builder<ul> <li>builder</li> </ul> </li> <li>grouping<ul> <li>grouping</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"api/vespa/index.html","title":"Index","text":""},{"location":"api/vespa/index.html#vespa","title":"<code>vespa</code>","text":""},{"location":"api/vespa/application.html","title":"Application","text":""},{"location":"api/vespa/application.html#vespa.application","title":"<code>vespa.application</code>","text":""},{"location":"api/vespa/application.html#vespa.application.Vespa","title":"<code>Vespa(url, port=None, deployment_message=None, cert=None, key=None, vespa_cloud_secret_token=None, output_file=sys.stdout, application_package=None, additional_headers=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Establish a connection with an existing Vespa application.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Vespa endpoint URL.</p> required <code>port</code> <code>int</code> <p>Vespa endpoint port.</p> <code>None</code> <code>deployment_message</code> <code>str</code> <p>Message returned by Vespa engine after deployment. Used internally by deploy methods.</p> <code>None</code> <code>cert</code> <code>str</code> <p>Path to data plane certificate and key file in case the 'key' parameter is None. If 'key' is not None, this should be the path of the certificate file. Typically generated by Vespa-cli with 'vespa auth cert'.</p> <code>None</code> <code>key</code> <code>str</code> <p>Path to the data plane key file. Typically generated by Vespa-cli with 'vespa auth cert'.</p> <code>None</code> <code>vespa_cloud_secret_token</code> <code>str</code> <p>Vespa Cloud data plane secret token.</p> <code>None</code> <code>output_file</code> <code>str</code> <p>Output file to write output messages.</p> <code>stdout</code> <code>application_package</code> <code>str</code> <p>Application package definition used to deploy the application.</p> <code>None</code> <code>additional_headers</code> <code>dict</code> <p>Additional headers to be sent to the Vespa application.</p> <code>None</code> Example usage <pre><code>Vespa(url=\"https://cord19.vespa.ai\")   # doctest: +SKIP\n\nVespa(url=\"http://localhost\", port=8080)\nVespa(http://localhost, 8080)\n\nVespa(url=\"https://token-endpoint..z.vespa-app.cloud\", vespa_cloud_secret_token=\"your_token\")  # doctest: +SKIP\n\nVespa(url=\"https://mtls-endpoint..z.vespa-app.cloud\", cert=\"/path/to/cert.pem\", key=\"/path/to/key.pem\")  # doctest: +SKIP\n\nVespa(url=\"https://mtls-endpoint..z.vespa-app.cloud\", cert=\"/path/to/cert.pem\", key=\"/path/to/key.pem\", additional_headers={\"X-Custom-Header\": \"test\"})  # doctest: +SKIP\n</code></pre>"},{"location":"api/vespa/application.html#vespa.application.Vespa.application_package","title":"<code>application_package</code>  <code>property</code>","text":"<p>Get application package definition, if available.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.asyncio","title":"<code>asyncio(connections=1, total_timeout=None, timeout=httpx.Timeout(5), client=None, **kwargs)</code>","text":"<p>Access Vespa asynchronous connection layer. Should be used as a context manager.</p> Example usage <pre><code>async with app.asyncio() as async_app:\n    response = await async_app.query(body=body)\n\n# passing kwargs\nlimits = httpx.Limits(max_keepalive_connections=5, max_connections=5, keepalive_expiry=15)\ntimeout = httpx.Timeout(connect=3, read=4, write=2, pool=5)\nasync with app.asyncio(connections=5, timeout=timeout, limits=limits) as async_app:\n    response = await async_app.query(body=body)\n</code></pre> <p>See <code>VespaAsync</code> for more details on the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>int</code> <p>Number of maximum_keepalive_connections.</p> <code>1</code> <code>total_timeout</code> <code>int</code> <p>Deprecated. Will be ignored. Use timeout instead.</p> <code>None</code> <code>timeout</code> <code>Timeout</code> <p>httpx.Timeout object. See Timeouts. Defaults to 5 seconds.</p> <code>Timeout(5)</code> <code>client</code> <code>AsyncClient</code> <p>Reusable httpx.AsyncClient to use instead of creating a new one. When provided, the caller is responsible for closing the client.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the httpx.AsyncClient.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>VespaAsync</code> <code>VespaAsync</code> <p>Instance of Vespa asynchronous layer.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_async_session","title":"<code>get_async_session(connections=1, total_timeout=None, timeout=httpx.Timeout(5), **kwargs)</code>","text":"<p>Return a configured <code>httpx.AsyncClient</code> for reuse.</p> <p>The client is created with the same configuration as <code>VespaAsync</code> and is HTTP/2 enabled by default. Callers are responsible for closing the client via <code>await client.aclose()</code> when finished.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>int</code> <p>Number of logical connections to keep alive.</p> <code>1</code> <code>timeout</code> <code>Timeout | int</code> <p>Timeout configuration for the client.</p> <code>Timeout(5)</code> <code>**kwargs</code> <p>Additional keyword arguments forwarded to <code>httpx.AsyncClient</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncClient</code> <p>httpx.AsyncClient: Configured asynchronous HTTP client.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.syncio","title":"<code>syncio(connections=8, compress='auto', session=None)</code>","text":"<p>Access Vespa synchronous connection layer. Should be used as a context manager.</p> <p>Example usage:</p> <pre><code>```python\nwith app.syncio() as sync_app:\n    response = sync_app.query(body=body)\n```\n</code></pre> <p>See  for more details. <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>int</code> <p>Number of allowed concurrent connections.</p> <code>8</code> <code>total_timeout</code> <code>float</code> <p>Total timeout in seconds.</p> required <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <code>session</code> <code>Session</code> <p>Reusable requests session to utilise for all requests made within the context manager. When provided, the caller is responsible for closing the session.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>VespaAsyncLayer</code> <code>VespaSync</code> <p>Instance of Vespa asynchronous layer.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_sync_session","title":"<code>get_sync_session(connections=8, compress='auto')</code>","text":"<p>Return a configured <code>requests.Session</code> for reuse.</p> <p>The returned session is configured with the same headers, authentication, and connection pooling behaviour as the <code>VespaSync</code> context manager. Callers are responsible for closing the session when it is no longer needed.</p> <p>Parameters:</p> Name Type Description Default <code>connections</code> <code>int</code> <p>Number of allowed concurrent connections.</p> <code>8</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress request bodies.</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>Session</code> <code>Session</code> <p>Configured requests session using <code>CustomHTTPAdapter</code> pooling.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.wait_for_application_up","title":"<code>wait_for_application_up(max_wait=300)</code>","text":"<p>Wait for application endpoint ready (/ApplicationStatus).</p> <p>Parameters:</p> Name Type Description Default <code>max_wait</code> <code>int</code> <p>Seconds to wait for the application endpoint.</p> <code>300</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If not able to reach endpoint within <code>max_wait</code> or the client fails to authenticate.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_application_status","title":"<code>get_application_status()</code>","text":"<p>Get application status (/ApplicationStatus).</p> <p>Returns:</p> Type Description <code>Optional[Response]</code> <p>None</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_model_endpoint","title":"<code>get_model_endpoint(model_id=None)</code>","text":"<p>Get stateless model evaluation endpoints.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.query","title":"<code>query(body=None, groupname=None, streaming=False, **kwargs)</code>","text":"<p>Send a query request to the Vespa application.</p> <p>Send 'body' containing all the request parameters.</p> <p>Parameters:</p> Name Type Description Default <code>body</code> <code>dict</code> <p>Dictionary containing request parameters.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname used with streaming search.</p> <code>None</code> <code>streaming</code> <code>bool</code> <p>Whether to use streaming mode (SSE). Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Extra Vespa Query API parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[VespaQueryResponse, Generator[str, None, None]]</code> <p>VespaQueryResponse when streaming=False, or a generator of decoded lines when streaming=True.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.feed_data_point","title":"<code>feed_data_point(schema, data_id, fields, namespace=None, groupname=None, compress='auto', **kwargs)</code>","text":"<p>Feed a data point to a Vespa app. Will create a new VespaSync with connection overhead.</p> Example usage <pre><code>app = Vespa(url=\"localhost\", port=8080)\ndata_id = \"1\",\nfields = {\n        \"field1\": \"value1\",\n    }\nwith VespaSync(app) as sync_app:\n    response = sync_app.feed_data_point(\n        schema=\"schema_name\",\n        data_id=data_id,\n        fields=fields\n    )\nprint(response)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are sending data to.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>fields</code> <code>dict</code> <p>Dictionary containing all the fields required by the <code>schema</code>.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are sending data to.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname that we are sending data to.</p> <code>None</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <p>Returns:</p> Name Type Description <code>VespaResponse</code> <code>VespaResponse</code> <p>The response of the HTTP POST request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.feed_iterable","title":"<code>feed_iterable(iter, schema=None, namespace=None, callback=None, operation_type='feed', max_queue_size=1000, max_workers=8, max_connections=16, compress='auto', **kwargs)</code>","text":"<p>Feed data from an Iterable of Dict with the keys 'id' and 'fields' to be used in the <code>feed_data_point</code> function.</p> <p>Uses a queue to feed data in parallel with a thread pool. The result of each operation is forwarded to the user-provided callback function that can process the returned <code>VespaResponse</code>.</p> Example usage <pre><code>app = Vespa(url=\"localhost\", port=8080)\ndata = [\n    {\"id\": \"1\", \"fields\": {\"field1\": \"value1\"}},\n    {\"id\": \"2\", \"fields\": {\"field1\": \"value2\"}},\n]\ndef callback(response, id):\n    print(f\"Response for id {id}: {response.status_code}\")\napp.feed_iterable(data, schema=\"schema_name\", callback=callback)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>iter</code> <code>Iterable[dict]</code> <p>An iterable of Dict containing the keys 'id' and 'fields' to be used in the <code>feed_data_point</code>. Note that this 'id' is only the last part of the full document id, which will be generated automatically by pyvespa.</p> required <code>schema</code> <code>str</code> <p>The Vespa schema name that we are sending data to.</p> <code>None</code> <code>namespace</code> <code>str</code> <p>The Vespa document id namespace. If no namespace is provided, the schema is used.</p> <code>None</code> <code>callback</code> <code>function</code> <p>A callback function to be called on each result. Signature <code>callback(response: VespaResponse, id: str)</code>.</p> <code>None</code> <code>operation_type</code> <code>str</code> <p>The operation to perform. Defaults to <code>feed</code>. Valid values are <code>feed</code>, <code>update</code>, or <code>delete</code>.</p> <code>'feed'</code> <code>max_queue_size</code> <code>int</code> <p>The maximum size of the blocking queue and max in-flight operations.</p> <code>1000</code> <code>max_workers</code> <code>int</code> <p>The maximum number of workers in the threadpool executor.</p> <code>8</code> <code>max_connections</code> <code>int</code> <p>The maximum number of persisted connections to the Vespa endpoint.</p> <code>16</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters passed to the respective operation type specific function (<code>_data_point</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.feed_async_iterable","title":"<code>feed_async_iterable(iter, schema=None, namespace=None, callback=None, operation_type='feed', max_queue_size=1000, max_workers=64, max_connections=1, **kwargs)</code>","text":"<p>Feed data asynchronously using httpx.AsyncClient with HTTP/2. Feed from an Iterable of Dict with the keys 'id' and 'fields' to be used in the <code>feed_data_point</code> function. The result of each operation is forwarded to the user-provided callback function that can process the returned <code>VespaResponse</code>. Prefer using this method over <code>feed_iterable</code> when the operation is I/O bound from the client side.</p> Example usage <pre><code>app = Vespa(url=\"localhost\", port=8080)\ndata = [\n    {\"id\": \"1\", \"fields\": {\"field1\": \"value1\"}},\n    {\"id\": \"2\", \"fields\": {\"field1\": \"value2\"}},\n]\ndef callback(response, id):\n    print(f\"Response for id {id}: {response.status_code}\")\napp.feed_async_iterable(data, schema=\"schema_name\", callback=callback)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>iter</code> <code>Iterable[dict]</code> <p>An iterable of Dict containing the keys 'id' and 'fields' to be used in the <code>feed_data_point</code>. Note that this 'id' is only the last part of the full document id, which will be generated automatically by pyvespa.</p> required <code>schema</code> <code>str</code> <p>The Vespa schema name that we are sending data to.</p> <code>None</code> <code>namespace</code> <code>str</code> <p>The Vespa document id namespace. If no namespace is provided, the schema is used.</p> <code>None</code> <code>callback</code> <code>function</code> <p>A callback function to be called on each result. Signature <code>callback(response: VespaResponse, id: str)</code>.</p> <code>None</code> <code>operation_type</code> <code>str</code> <p>The operation to perform. Defaults to <code>feed</code>. Valid values are <code>feed</code>, <code>update</code>, or <code>delete</code>.</p> <code>'feed'</code> <code>max_queue_size</code> <code>int</code> <p>The maximum number of tasks waiting to be processed. Useful to limit memory usage. Default is 1000.</p> <code>1000</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent requests to have in-flight, bound by an asyncio.Semaphore, that needs to be acquired by a submit task. Increase if the server is scaled to handle more requests.</p> <code>64</code> <code>max_connections</code> <code>int</code> <p>The maximum number of connections passed to httpx.AsyncClient to the Vespa endpoint. As HTTP/2 is used, only one connection is needed.</p> <code>1</code> <code>**kwargs</code> <code>dict</code> <p>Additional parameters passed to the respective operation type-specific function (<code>_data_point</code>).</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.query_many_async","title":"<code>query_many_async(queries, num_connections=1, max_concurrent=100, client_kwargs={}, **query_kwargs)</code>  <code>async</code>","text":"<p>Execute many queries asynchronously using httpx.AsyncClient. Number of concurrent requests is controlled by the <code>max_concurrent</code> parameter. Each query will be retried up to 3 times using an exponential backoff strategy.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Iterable[dict]</code> <p>Iterable of query bodies (dictionaries) to be sent.</p> required <code>num_connections</code> <code>int</code> <p>Number of connections to be used in the asynchronous client (uses HTTP/2). Defaults to 1.</p> <code>1</code> <code>max_concurrent</code> <code>int</code> <p>Maximum concurrent requests to be sent. Defaults to 100. Be careful with increasing too much.</p> <code>100</code> <code>client_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the httpx.AsyncClient.</p> <code>{}</code> <code>**query_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the query method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[VespaQueryResponse]</code> <p>List[VespaQueryResponse]: List of <code>VespaQueryResponse</code> objects.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.query_many","title":"<code>query_many(queries, num_connections=1, max_concurrent=100, client_kwargs={}, **query_kwargs)</code>","text":"<p>Execute many queries asynchronously using httpx.AsyncClient. This method is a wrapper around the <code>query_many_async</code> method that uses the asyncio event loop to run the coroutine. Number of concurrent requests is controlled by the <code>max_concurrent</code> parameter. Each query will be retried up to 3 times using an exponential backoff strategy.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Iterable[dict]</code> <p>Iterable of query bodies (dictionaries) to be sent.</p> required <code>num_connections</code> <code>int</code> <p>Number of connections to be used in the asynchronous client (uses HTTP/2). Defaults to 1.</p> <code>1</code> <code>max_concurrent</code> <code>int</code> <p>Maximum concurrent requests to be sent. Defaults to 100. Be careful with increasing too much.</p> <code>100</code> <code>client_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the httpx.AsyncClient.</p> <code>{}</code> <code>**query_kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the query method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[VespaQueryResponse]</code> <p>List[VespaQueryResponse]: List of <code>VespaQueryResponse</code> objects.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.delete_data","title":"<code>delete_data(schema, data_id, namespace=None, groupname=None, **kwargs)</code>","text":"<p>Delete a data point from a Vespa app.</p> Example usage <pre><code>app = Vespa(url=\"localhost\", port=8080)\nresponse = app.delete_data(schema=\"schema_name\", data_id=\"1\")\nprint(response)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are deleting data from.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are deleting data from. If no namespace is provided, the schema is used.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname that we are deleting data from.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the HTTP DELETE request. See Vespa API documentation for more details.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP DELETE request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.delete_all_docs","title":"<code>delete_all_docs(content_cluster_name, schema, namespace=None, slices=1, **kwargs)</code>","text":"<p>Delete all documents associated with the schema. This might block for a long time as it requires sending multiple delete requests to complete.</p> <p>Parameters:</p> Name Type Description Default <code>content_cluster_name</code> <code>str</code> <p>Name of content cluster to GET from, or visit.</p> required <code>schema</code> <code>str</code> <p>The schema that we are deleting data from.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are deleting data from. If no namespace is provided, the schema is used.</p> <code>None</code> <code>slices</code> <code>int</code> <p>Number of slices to use for parallel delete requests. Defaults to 1.</p> <code>1</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the HTTP DELETE request. See Vespa API documentation for more details.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>Response</code> <p>The response of the HTTP DELETE request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.visit","title":"<code>visit(content_cluster_name, schema=None, namespace=None, slices=1, selection='true', wanted_document_count=500, slice_id=None, **kwargs)</code>","text":"<p>Visit all documents associated with the schema and matching the selection.</p> <p>Will run each slice on a separate thread, for each slice yields the response for each page.</p> Example usage <pre><code>for slice in app.visit(schema=\"schema_name\", slices=2):\n    for response in slice:\n        print(response.json)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>content_cluster_name</code> <code>str</code> <p>Name of content cluster to GET from.</p> required <code>schema</code> <code>str</code> <p>The schema that we are visiting data from.</p> <code>None</code> <code>namespace</code> <code>str</code> <p>The namespace that we are visiting data from.</p> <code>None</code> <code>slices</code> <code>int</code> <p>Number of slices to use for parallel GET.</p> <code>1</code> <code>selection</code> <code>str</code> <p>Selection expression to filter documents.</p> <code>'true'</code> <code>wanted_document_count</code> <code>int</code> <p>Best effort number of documents to retrieve for each request. May contain less if there are not enough documents left.</p> <code>500</code> <code>slice_id</code> <code>int</code> <p>Slice id to use for the visit. If None, all slices will be used.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See Vespa API documentation.</p> <code>{}</code> <p>Yields:</p> Type Description <code>Generator[VespaVisitResponse, None, None]</code> <p>Generator[Generator[Response]]: A generator of slices, each containing a generator of responses.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If an HTTP error occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_data","title":"<code>get_data(schema, data_id, namespace=None, groupname=None, raise_on_not_found=False, **kwargs)</code>","text":"<p>Get a data point from a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>schema</code> <code>str</code> <p>The schema that we are getting data from. Will attempt to infer schema name if not provided.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are getting data from. If no namespace is provided, the schema is used.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname that we are getting data from.</p> <code>None</code> <code>raise_on_not_found</code> <code>bool</code> <p>Raise an exception if the data_id is not found. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the HTTP GET request. See Vespa API documentation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP GET request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.update_data","title":"<code>update_data(schema, data_id, fields, create=False, namespace=None, groupname=None, compress='auto', **kwargs)</code>","text":"<p>Update a data point in a Vespa app.</p> Example usage <pre><code>vespa = Vespa(url=\"localhost\", port=8080)\n\nfields = {\"mystringfield\": \"value1\", \"myintfield\": 42}\nresponse = vespa.update_data(schema=\"schema_name\", data_id=\"id1\", fields=fields)\n# or, with partial update, setting auto_assign=False\nfields = {\"myintfield\": {\"increment\": 1}}\nresponse = vespa.update_data(schema=\"schema_name\", data_id=\"id1\", fields=fields, auto_assign=False)\nprint(response.json)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are updating data.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>fields</code> <code>dict</code> <p>Dict containing all the fields you want to update.</p> required <code>create</code> <code>bool</code> <p>If true, updates to non-existent documents will create an empty document to update.</p> <code>False</code> <code>auto_assign</code> <code>bool</code> <p>Assumes <code>fields</code>-parameter is an assignment operation. If set to false, the fields parameter should be a dictionary including the update operation.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are updating data. If no namespace is provided, the schema is used.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname that we are updating data.</p> <code>None</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments to be passed to the HTTP PUT request. See Vespa API documentation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP PUT request.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_model_from_application_package","title":"<code>get_model_from_application_package(model_name)</code>","text":"<p>Get model definition from application package, if available.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.predict","title":"<code>predict(x, model_id, function_name='output_0')</code>","text":"<p>Obtain a stateless model evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>various</code> <p>Input where the format depends on the task that the model is serving.</p> required <code>model_id</code> <code>str</code> <p>The id of the model used to serve the prediction.</p> required <code>function_name</code> <code>str</code> <p>The name of the output function to be evaluated.</p> <code>'output_0'</code> <p>Returns:</p> Name Type Description <code>var</code> <p>Model prediction.</p>"},{"location":"api/vespa/application.html#vespa.application.Vespa.get_document_v1_path","title":"<code>get_document_v1_path(id, schema=None, namespace=None, group=None, number=None)</code>","text":"<p>Convert to document v1 path.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The id of the document.</p> required <code>namespace</code> <code>str</code> <p>The namespace of the document.</p> <code>None</code> <code>schema</code> <code>str</code> <p>The schema of the document.</p> <code>None</code> <code>group</code> <code>str</code> <p>The group of the document.</p> <code>None</code> <code>number</code> <code>int</code> <p>The number of the document.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path to the document v1 endpoint.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync","title":"<code>VespaSync(app, pool_maxsize=10, pool_connections=10, compress='auto', session=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Class to handle synchronous requests to Vespa. This class is intended to be used as a context manager.</p> Example usage <pre><code>with VespaSync(app) as sync_app:\n    response = sync_app.query(body=body)\nprint(response)\n</code></pre> <p>Can also be accessed directly through <code>Vespa.syncio</code>:     <pre><code>app = Vespa(url=\"localhost\", port=8080)\nwith app.syncio() as sync_app:\n    response = sync_app.query(body=body)\n</code></pre></p> <p>Reusing a session across multiple contexts (avoids TLS handshake overhead):     <pre><code># Get a reusable session\nsession = app.get_sync_session()\ntry:\n    # Use it multiple times\n    with app.syncio(session=session) as sync_app:\n        response1 = sync_app.query(body=body1)\n    with app.syncio(session=session) as sync_app:\n        response2 = sync_app.query(body=body2)\nfinally:\n    # User is responsible for closing\n    session.close()\n</code></pre></p> <p>See also <code>Vespa.feed_iterable</code> for a convenient way to feed data synchronously.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Vespa</code> <p>Vespa app object.</p> required <code>pool_maxsize</code> <code>int</code> <p>The maximum number of connections to save in the pool. Defaults to 10.</p> <code>10</code> <code>pool_connections</code> <code>int</code> <p>The number of urllib3 connection pools to cache. Defaults to 10.</p> <code>10</code> <code>compress</code> <code>Union[str, bool]</code> <p>Whether to compress the request body. Defaults to \"auto\", which will compress if the body is larger than 1024 bytes.</p> <code>'auto'</code> <code>session</code> <code>Session</code> <p>An externally managed session to reuse. When provided, the caller is responsible for closing it. Defaults to None.</p> <code>None</code>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.get_model_endpoint","title":"<code>get_model_endpoint(model_id=None)</code>","text":"<p>Get model evaluation endpoints.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.predict","title":"<code>predict(model_id, function_name, encoded_tokens)</code>","text":"<p>Obtain a stateless model evaluation.</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The id of the model used to serve the prediction.</p> required <code>function_name</code> <code>str</code> <p>The name of the output function to be evaluated.</p> required <code>encoded_tokens</code> <code>str</code> <p>URL-encoded input to the model.</p> required <p>Returns:</p> Type Description <p>The model prediction.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.feed_data_point","title":"<code>feed_data_point(schema, data_id, fields, namespace=None, groupname=None, **kwargs)</code>","text":"<p>Feed a data point to a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are sending data to.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>fields</code> <code>dict</code> <p>Dict containing all the fields required by the <code>schema</code>.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are sending data to. If no namespace is provided, the schema is used.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The group that we are sending data to.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>VespaResponse</code> <p>The response of the HTTP POST request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.query","title":"<code>query(body=None, groupname=None, streaming=False, **kwargs)</code>","text":"<p>Send a query request to the Vespa application.</p> <p>Parameters:</p> Name Type Description Default <code>body</code> <code>dict</code> <p>Dict containing all the request parameters.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname used in streaming search.</p> <code>None</code> <code>streaming</code> <code>bool</code> <p>Whether to use streaming mode (SSE). Defaults to False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional valid Vespa HTTP Query API parameters. See: https://docs.vespa.ai/en/reference/query-api-reference.html.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[VespaQueryResponse, Generator[str, None, None]]</code> <p>VespaQueryResponse when streaming=False, or a generator of decoded lines when streaming=True.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.delete_data","title":"<code>delete_data(schema, data_id, namespace=None, groupname=None, **kwargs)</code>","text":"<p>Delete a data point from a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are deleting data from.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are deleting data from.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP DELETE request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.delete_all_docs","title":"<code>delete_all_docs(content_cluster_name, schema, namespace=None, slices=1, **kwargs)</code>","text":"<p>Delete all documents associated with the schema.</p> <p>Parameters:</p> Name Type Description Default <code>content_cluster_name</code> <code>str</code> <p>Name of content cluster to GET from or visit.</p> required <code>schema</code> <code>str</code> <p>The schema that we are deleting data from.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are deleting data from.</p> <code>None</code> <code>slices</code> <code>int</code> <p>Number of slices to use for parallel delete.</p> <code>1</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>None</code> <p>The response of the HTTP DELETE request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.visit","title":"<code>visit(content_cluster_name, schema=None, namespace=None, slices=1, selection='true', wanted_document_count=500, slice_id=None, **kwargs)</code>","text":"<p>Visit all documents associated with the schema and matching the selection.</p> <p>This method will run each slice on a separate thread, yielding the response for each page for each slice.</p> <p>Parameters:</p> Name Type Description Default <code>content_cluster_name</code> <code>str</code> <p>Name of content cluster to GET from.</p> required <code>schema</code> <code>str</code> <p>The schema that we are visiting data from.</p> <code>None</code> <code>namespace</code> <code>str</code> <p>The namespace that we are visiting data from.</p> <code>None</code> <code>slices</code> <code>int</code> <p>Number of slices to use for parallel GET.</p> <code>1</code> <code>wanted_document_count</code> <code>int</code> <p>Best effort number of documents to retrieve for each request. May contain fewer if there are not enough documents left.</p> <code>500</code> <code>selection</code> <code>str</code> <p>Selection expression to use. Defaults to \"true\". See: https://docs.vespa.ai/en/reference/document-select-language.html.</p> <code>'true'</code> <code>slice_id</code> <code>int</code> <p>Slice id to use. Defaults to -1, which means all slices.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>generator</code> <code>None</code> <p>A generator of slices, each containing a generator of responses.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.get_data","title":"<code>get_data(schema, data_id, namespace=None, groupname=None, raise_on_not_found=False, **kwargs)</code>","text":"<p>Get a data point from a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are getting data from.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>namespace</code> <code>str</code> <p>The namespace that we are getting data from.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname used to get data.</p> <code>None</code> <code>raise_on_not_found</code> <code>bool</code> <p>Raise an exception if the document is not found. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP GET request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaSync.update_data","title":"<code>update_data(schema, data_id, fields, create=False, auto_assign=True, namespace=None, groupname=None, **kwargs)</code>","text":"<p>Update a data point in a Vespa app.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The schema that we are updating data in.</p> required <code>data_id</code> <code>str</code> <p>Unique id associated with this data point.</p> required <code>fields</code> <code>dict</code> <p>Dict containing all the fields you want to update.</p> required <code>create</code> <code>bool</code> <p>If true, updates to non-existent documents will create an empty document to update. Default is False.</p> <code>False</code> <code>auto_assign</code> <code>bool</code> <p>Assumes <code>fields</code>-parameter is an assignment operation. If set to False, the fields parameter should include the update operation. Default is True.</p> <code>True</code> <code>namespace</code> <code>str</code> <p>The namespace that we are updating data in.</p> <code>None</code> <code>groupname</code> <code>str</code> <p>The groupname used to update data.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional HTTP request parameters. See: &lt;https://docs.vespa.ai/en/reference/document-v1-api-reference.html#request-parameters.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Response</code> <code>VespaResponse</code> <p>The response of the HTTP PUT request.</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If one occurred.</p>"},{"location":"api/vespa/application.html#vespa.application.VespaAsync","title":"<code>VespaAsync(app, connections=1, total_timeout=None, timeout=httpx.Timeout(5), client=None, **kwargs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Class to handle asynchronous HTTP connections to Vespa.</p> <p>Uses <code>httpx</code> as the async HTTP client, and HTTP/2 by default. This class is intended to be used as a context manager.</p> <p>Basic usage:     <pre><code>async with VespaAsync(app) as async_app:\n    response = await async_app.query(\n        body={\"yql\": \"select * from sources * where title contains 'music';\"}\n    )\n</code></pre></p> <p>Passing custom timeout and limits:     <pre><code>import httpx\n\ntimeout = httpx.Timeout(10.0, connect=5.0)\nlimits = httpx.Limits(max_connections=10, max_keepalive_connections=5)\n\nasync with VespaAsync(app, timeout=timeout, limits=limits) as async_app:\n    response = await async_app.query(\n        body={\"yql\": \"select * from sources * where title contains 'music';\"}\n    )\n</code></pre></p> <p>Using additional kwargs (e.g., proxies):     <pre><code>proxies = \"http://localhost:8080\"\n\nasync with VespaAsync(app, proxies=proxies) as async_app:\n    response = await async_app.query(\n        body={\"yql\": \"select * from sources * where title contains 'music';\"}\n    )\n</code></pre></p> <p>Accessing via <code>Vespa.asyncio</code>:     <pre><code>app = Vespa(url=\"localhost\", port=8080)\nasync with app.asyncio(timeout=timeout, limits=limits) as async_app:\n    response = await async_app.query(\n        body={\"yql\": \"select * from sources * where title contains 'music';\"}\n    )\n</code></pre></p> <p>Reusing a client across multiple contexts (avoids TLS handshake overhead):     <pre><code># Get a reusable client\nclient = app.get_async_session()\ntry:\n    # Use it multiple times\n    async with app.asyncio(client=client) as async_app:\n        response1 = await async_app.query(body=body1)\n    async with app.asyncio(client=client) as async_app:\n        response2 = await async_app.query(body=body2)\nfinally:\n    # User is responsible for closing\n    await client.aclose()\n</code></pre></p> <p>See also <code>Vespa.feed_async_iterable</code> for a convenient interface to async data feeding.</p> <p>Parameters:</p> Name Type Description Default <code>app</code> <code>Vespa</code> <p>Vespa application object.</p> required <code>connections</code> <code>Optional[int]</code> <p>Number of connections. Defaults to 1 as HTTP/2 is multiplexed.</p> <code>1</code> <code>total_timeout</code> <code>int</code> <p>Deprecated. Will be ignored and removed in future versions. Use <code>timeout</code> to pass an <code>httpx.Timeout</code> object instead.</p> <code>None</code> <code>timeout</code> <code>Timeout</code> <p>Timeout settings for the <code>httpx.AsyncClient</code>. Defaults to <code>httpx.Timeout(5)</code>.</p> <code>Timeout(5)</code> <code>client</code> <code>AsyncClient</code> <p>An externally managed async client to reuse. When provided, the caller is responsible for closing it. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to be passed to the <code>httpx.AsyncClient</code>. See HTTPX AsyncClient documentation for more details.</p> <code>{}</code> Note <ul> <li>Passing <code>timeout</code> allows you to configure timeouts for connect, read, write, and overall request time.</li> <li>The <code>limits</code> parameter can be used to control connection pooling behavior, such as the maximum number of concurrent connections.</li> <li>See HTTPX documentation for more information on <code>httpx</code> and its features.</li> </ul>"},{"location":"api/vespa/application.html#vespa.application.raise_for_status","title":"<code>raise_for_status(response, raise_on_not_found=False)</code>","text":"<p>Raises an appropriate error if necessary.</p> <p>If the response contains an error message, <code>VespaError</code> is raised along with <code>HTTPError</code> to provide more details.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Response</code> <p>Response object from the Vespa API.</p> required <code>raise_on_not_found</code> <code>bool</code> <p>If True, raises <code>HTTPError</code> if status_code is 404.</p> <code>False</code> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If status_code is between 400 and 599.</p> <code>VespaError</code> <p>If the response JSON contains an error message.</p>"},{"location":"api/vespa/deployment.html","title":"Deployment","text":""},{"location":"api/vespa/deployment.html#vespa.deployment","title":"<code>vespa.deployment</code>","text":""},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDeployment","title":"<code>VespaDeployment</code>","text":""},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDeployment.read_app_package_from_disk","title":"<code>read_app_package_from_disk(application_root)</code>","text":"<p>Reads the contents of an application package on disk into a zip file.</p> <p>Parameters:</p> Name Type Description Default <code>application_root</code> <code>str</code> <p>The directory root of the application package.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>bytes</code> <p>The zipped application package.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker","title":"<code>VespaDocker(url='http://localhost', port=8080, container_memory=4 * 1024 ** 3, output_file=sys.stdout, container=None, container_image='vespaengine/vespa', volumes=None, cfgsrv_port=19071, debug_port=5005)</code>","text":"<p>               Bases: <code>VespaDeployment</code></p> <p>Manage Docker deployments.</p> <p>Make sure to start the Docker daemon before instantiating this class.</p> Example usage <pre><code>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker(port=8080)\n# or initialize from a running container:\nvespa_docker = VespaDocker('http://localhost', 8080, None, None, 4294967296, 'vespaengine/vespa')\n</code></pre> <p>Note:</p> <p>It is NOT possible to refer to Volume Mounts in your Application Package. This means that for example .onnx-model files that are part of the Application Package must be on your host machine, so that it can be uploaded as part of the Application Package to the Vespa container.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>The port for the container. Default is 8080.</p> <code>8080</code> <code>cfgsrv_port</code> <code>int</code> <p>The Vespa Config Server port. Default is 19071.</p> <code>19071</code> <code>debug_port</code> <code>int</code> <p>The port to connect to for debugging the Vespa container. Default is 5005.</p> <code>5005</code> <code>output_file</code> <code>str</code> <p>The file to write output messages to.</p> <code>stdout</code> <code>container_memory</code> <code>int</code> <p>Memory available to the container in bytes. Default is 4GB.</p> <code>4 * 1024 ** 3</code> <code>container</code> <code>str</code> <p>Used when instantiating <code>VespaDocker</code> from a running container.</p> <code>None</code> <code>volumes</code> <code>list of str</code> <p>A list of volume mount strings, such as <code>['/home/user1/:/mnt/vol2', '/var/www:/mnt/vol1']</code>. The Application Package cannot reference volume mounts.</p> <code>None</code> <code>container_image</code> <code>str</code> <p>The Docker container image to use.</p> <code>'vespaengine/vespa'</code> <code>url</code> <code>str</code> <p>The URL to connect to the Vespa instance. Default is \"http://localhost\".</p> <code>'http://localhost'</code>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.from_container_name_or_id","title":"<code>from_container_name_or_id(name_or_id, output_file=sys.stdout)</code>  <code>staticmethod</code>","text":"<p>Instantiate VespaDocker from a running container.</p> <p>Parameters:</p> Name Type Description Default <code>name_or_id</code> <code>str</code> <p>The name or id of the running container.</p> required <code>output_file</code> <code>str</code> <p>The file to write output messages to.</p> <code>stdout</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified container is not found.</p> <p>Returns:</p> Name Type Description <code>VespaDocker</code> <code>VespaDocker</code> <p>An instance of VespaDocker associated with the running container.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.deploy","title":"<code>deploy(application_package, max_wait_configserver=60, max_wait_deployment=300, max_wait_docker=300, debug=False)</code>","text":"<p>Deploy the application package into a Vespa container.</p> <p>Parameters:</p> Name Type Description Default <code>application_package</code> <code>ApplicationPackage</code> <p>The application package to be deployed.</p> required <code>max_wait_configserver</code> <code>int</code> <p>Maximum seconds to wait for the config server to start.</p> <code>60</code> <code>max_wait_deployment</code> <code>int</code> <p>Maximum seconds to wait for the deployment to complete.</p> <code>300</code> <code>max_wait_docker</code> <code>int</code> <p>Maximum seconds to wait for the Docker container to start.</p> <code>300</code> <code>debug</code> <code>bool</code> <p>If True, adds the configured debug_port to the Docker port mapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>VespaConnection</code> <code>Vespa</code> <p>A Vespa connection instance once the deployment is complete.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.deploy_from_disk","title":"<code>deploy_from_disk(application_name, application_root, max_wait_configserver=60, max_wait_application=300, docker_timeout=300, debug=False)</code>","text":"<p>Deploy from a directory tree.</p> <p>This method is used when making changes to application package files that are not supported by pyvespa. This is why this method is not found in the ApplicationPackage class.</p> <p>Parameters:</p> Name Type Description Default <code>application_name</code> <code>str</code> <p>The name of the application package.</p> required <code>application_root</code> <code>str</code> <p>The root directory of the application package.</p> required <code>debug</code> <code>bool</code> <p>If True, adds the configured debug_port to the Docker port mapping.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>VespaConnection</code> <code>Vespa</code> <p>A Vespa connection instance once the deployment is complete.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.wait_for_config_server_start","title":"<code>wait_for_config_server_start(max_wait=300)</code>","text":"<p>Waits for the Config Server to start inside the Docker image.</p> <p>Parameters:</p> Name Type Description Default <code>max_wait</code> <code>int</code> <p>The maximum number of seconds to wait for the application endpoint to become available.</p> <code>300</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the config server does not start within the specified max_wait time.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.start_services","title":"<code>start_services(max_wait=120)</code>","text":"<p>Start Vespa services inside the Docker image, first waiting for the Config Server, then for other services.</p> <p>Parameters:</p> Name Type Description Default <code>max_wait</code> <code>int</code> <p>The maximum number of seconds to wait for the application endpoint to become available.</p> <code>120</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a container has not been set or the services fail to start within the specified max_wait time.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.stop_services","title":"<code>stop_services()</code>","text":"<p>Stop Vespa services inside the Docker image, first stopping the services, then stopping the Config Server.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a container has not been set or an error occurs while stopping the services.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaDocker.restart_services","title":"<code>restart_services()</code>","text":"<p>Restart Vespa services inside the Docker image. This is equivalent to calling <code>self.stop_services()</code> followed by <code>self.start_services()</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If a container has not been set or an error occurs during the restart process.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud","title":"<code>VespaCloud(tenant, application, application_package=None, key_location=None, key_content=None, auth_client_token_id=None, output_file=sys.stdout, application_root=None, cluster=None, instance='default')</code>","text":"<p>               Bases: <code>VespaDeployment</code></p> <p>Deploy an application to the Vespa Cloud (cloud.vespa.ai).</p> <p>There are several ways to initialize VespaCloud: - Application source: From a Python-defined application package or from the application_root folder. - Control plane access: Using an API key (must be added to Vespa Cloud Console) or an access token, obtained by interactive login. - Data plane access: mTLS is used by default, but Vespa applications can also be configured to use token-based authentication (token must be added to Vespa Cloud Console, and the corresponding auth_token_id must be provided).</p> Example usage <pre><code># 1. Initialize VespaCloud with an application package and existing API key for control plane access.\nvespa_cloud = VespaCloud(\n    tenant=\"my-tenant\",\n    application=\"my-application\",\n    application_package=app_package,\n    key_location=\"/path/to/private-key.pem\",\n)\n\n# 2. Initialize VespaCloud from disk folder by interactive control plane auth.\nvespa_cloud = VespaCloud(\n    tenant=\"my-tenant\",\n    application=\"my-application\",\n    application_root=\"/path/to/application\",\n)\n\n# 3. Initialize VespaCloud with an application package and token-based data plane access.\nvespa_cloud = VespaCloud(\n    tenant=\"my-tenant\",\n    application=\"my-application\",\n    application_package=app_package,\n    auth_client_token_id=\"my-token-id\", # Must be added in Vespa Cloud Console\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>tenant</code> <code>str</code> <p>Tenant name registered in the Vespa Cloud.</p> required <code>application</code> <code>str</code> <p>Application name in the Vespa Cloud.</p> required <code>application_package</code> <code>ApplicationPackage</code> <p>Application package to be deployed. Either this or application_root must be set.</p> <code>None</code> <code>key_location</code> <code>str</code> <p>Location of the control plane key used for signing HTTP requests to the Vespa Cloud.</p> <code>None</code> <code>key_content</code> <code>str</code> <p>Content of the control plane key used for signing HTTP requests to the Vespa Cloud. Use only when the key file is not available.</p> <code>None</code> <code>auth_client_token_id</code> <code>str</code> <p>Token-based data plane authentication. This token name must be configured in the Vespa Cloud Console. It configures Vespa's services.xml, and the token must have read and write permissions.</p> <code>None</code> <code>output_file</code> <code>str</code> <p>Output file to write output messages. Default is sys.stdout.</p> <code>stdout</code> <code>application_root</code> <code>str</code> <p>Directory for the application root (location of services.xml, models/, schemas/, etc.). If the application is packaged with Maven, use the generated <code>&lt;myapp&gt;/target/application</code> directory.</p> <code>None</code> <code>cluster</code> <code>str</code> <p>Name of the cluster to target when retrieving endpoints. This affects which endpoints are used for initializing the :class:<code>Vespa</code> instance in <code>VespaCloud.get_application</code> and <code>VespaCloud.deploy</code>.</p> <code>None</code> <code>instance</code> <code>str</code> <p>Name of the application instance. Default is \"default\".</p> <code>'default'</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If deployment fails.</p> <p>Returns:</p> Name Type Description <code>Vespa</code> <code>None</code> <p>A Vespa connection instance for interacting with the deployed application.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.deploy","title":"<code>deploy(instance='default', disk_folder=None, version=None, max_wait=1800, environment='dev')</code>","text":"<p>Deploy the given application package as the given instance in the Vespa Cloud dev or perf environment.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Name of this instance of the application in the Vespa Cloud.</p> <code>'default'</code> <code>disk_folder</code> <code>str</code> <p>Disk folder to save the required Vespa config files. Defaults to the application name folder within the user's current working directory.</p> <code>None</code> <code>version</code> <code>str</code> <p>Vespa version to use for deployment. Defaults to None, meaning the latest version. Should only be set based on instructions from the Vespa team. Must be a valid Vespa version, e.g., \"8.435.13\".</p> <code>None</code> <code>max_wait</code> <code>int</code> <p>Seconds to wait for the deployment to complete.</p> <code>1800</code> <code>environment</code> <code>Literal['dev', 'perf']</code> <p>Environment to deploy to. Default is \"dev\".</p> <code>'dev'</code> <p>Returns:</p> Name Type Description <code>Vespa</code> <code>Vespa</code> <p>A Vespa connection instance. This instance connects to the mTLS endpoint. To connect to the token endpoint, use <code>VespaCloud.get_application(endpoint_type=\"token\")</code>.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If deployment fails or if there are issues with the deployment process.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.deploy_to_prod","title":"<code>deploy_to_prod(instance='default', application_root=None, source_url='')</code>","text":"<p>Deploy the given application package as the given instance in the Vespa Cloud prod environment. NB! This feature is experimental and may fail in unexpected ways. Expect better support in future releases.</p> <p>If submitting an application that is not yet packaged, tests should be located in /tests. If submitting an application packaged with maven, application_root should refer to the generated /target/application directory. <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Name of this instance of the application in the Vespa Cloud.</p> <code>'default'</code> <code>application_root</code> <code>str</code> <p>Path to either save the required Vespa config files (if initialized with application_package) or read them from (if initialized with application_root).</p> <code>None</code> <code>source_url</code> <code>str</code> <p>Optional source URL (including commit hash) for the deployment. This is a URL to the source code repository, e.g., GitHub, that is used to build the application package. Example: https://github.com/vespa-cloud/vector-search/commit/474d7771bd938d35dc5dcfd407c21c019d15df3c. The source URL will show up in the Vespa Cloud Console next to the build number.</p> <code>''</code> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If deployment fails or if there are issues with the deployment process.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_application","title":"<code>get_application(instance='default', environment='dev', endpoint_type='mtls', vespa_cloud_secret_token=None, region=None, max_wait=60)</code>","text":"<p>Get a connection to the Vespa application instance. Will only work if the application is already deployed.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\napp: Vespa = vespa_cloud.get_application()\n# Feed, query, visit, etc.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Name of this instance of the application in the Vespa Cloud. Default is \"default\".</p> <code>'default'</code> <code>environment</code> <code>str</code> <p>Environment of the application. Default is \"dev\". Options are \"dev\", \"perf\", or \"prod\".</p> <code>'dev'</code> <code>endpoint_type</code> <code>str</code> <p>Type of endpoint to connect to. Default is \"mtls\". Options are \"mtls\" or \"token\".</p> <code>'mtls'</code> <code>vespa_cloud_secret_token</code> <code>str</code> <p>Vespa Cloud Secret Token. Only required if endpoint_type is \"token\".</p> <code>None</code> <code>region</code> <code>str</code> <p>Region of the application in Vespa Cloud, e.g., \"aws-us-east-1c\". If not provided, the first region from the environment will be used.</p> <code>None</code> <code>max_wait</code> <code>int</code> <p>Seconds to wait for the application to be up. Default is 60 seconds.</p> <code>60</code> <p>Returns:</p> Name Type Description <code>Vespa</code> <code>Vespa</code> <p>Vespa application instance.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the application is not yet deployed or there are issues retrieving the connection.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.check_production_build_status","title":"<code>check_production_build_status(build_no)</code>","text":"<p>Check the status of a production build. Useful for example in CI/CD pipelines to check when a build has converged.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\nbuild_no = vespa_cloud.deploy_to_prod()\nstatus = vespa_cloud.check_production_build_status(build_no)\n# This can yield one of three responses:\n# 1. If the revision (build_no), or higher, has successfully converged everywhere, and nothing older has then been deployed on top of that again. Nothing more will happen in this case.\n# {\n#     \"deployed\": True,\n#     \"status\": \"done\"\n# }\n\n# 2. If the revision (build_no), or newer, has not yet converged, but the system is (most likely) still trying to deploy it. There is a point in polling again later when this is the response.\n# {\n#     \"deployed\": False,\n#     \"status\": \"deploying\"\n# }\n# 3. If the revision, or newer, has not yet converged everywhere, and it's never going to, because it was similar to the previous build, or marked obsolete by a user. There is no point in asking again for this revision.\n# {\n#     \"deployed\": False,\n#     \"status\": \"done\"\n# }\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>build_no</code> <code>int</code> <p>The build number to check.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with the aggregated status of all deployment jobs for the given build number. The dictionary contains: - \"deployed\" (bool): Whether the build has successfully converged. - \"status\" (str): The current status of the build (\"done\", \"deploying\").</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If there are issues with retrieving the status of the build.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.wait_for_prod_deployment","title":"<code>wait_for_prod_deployment(build_no=None, max_wait=3600, poll_interval=5)</code>","text":"<p>Wait for a production deployment to finish. Useful for example in CI/CD pipelines to wait for a deployment to finish.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\nbuild_no = vespa_cloud.deploy_to_prod()\nsuccess = vespa_cloud.wait_for_prod_deployment(build_no, max_wait=3600, poll_interval=5)\nprint(success)\n# Output: True\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>build_no</code> <code>int</code> <p>The build number to check.</p> <code>None</code> <code>max_wait</code> <code>int</code> <p>Maximum time to wait for the deployment in seconds. Default is 3600 (1 hour).</p> <code>3600</code> <code>poll_interval</code> <code>int</code> <p>Polling interval in seconds. Default is 5 seconds.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the deployment is done and converged, False if the deployment has failed.</p> <p>Raises:</p> Type Description <code>TimeoutError</code> <p>If the deployment did not finish within <code>max_wait</code> seconds.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.deploy_from_disk","title":"<code>deploy_from_disk(instance, application_root, max_wait=300, version=None, environment='dev')</code>","text":"<p>Deploy to the development or performance environment from a directory tree. This method is used when making changes to application package files that are not supported by pyvespa. Note: Requires a certificate and key to be generated using 'vespa auth cert'.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\nvespa_cloud.deploy_from_disk(\n    instance=\"my-instance\",\n    application_root=\"/path/to/application\",\n    max_wait=3600,\n    version=\"8.435.13\"\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>The name of the instance where the application will be run.</p> required <code>application_root</code> <code>str</code> <p>The root directory of the application package.</p> required <code>max_wait</code> <code>int</code> <p>The maximum number of seconds to wait for the deployment. Default is 3600 (1 hour).</p> <code>300</code> <code>version</code> <code>str</code> <p>The Vespa version to use for the deployment. Default is None, which means the latest version. It must be a valid Vespa version (e.g., \"8.435.13\").</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment to deploy to. Default is \"dev\". Options are \"dev\" or \"perf\".</p> <code>'dev'</code> <p>Returns:</p> Name Type Description <code>Vespa</code> <code>Vespa</code> <p>A Vespa connection instance. This connects to the mtls endpoint. To connect to the token endpoint, use <code>VespaCloud.get_application(endpoint_type=\"token\")</code>.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.delete","title":"<code>delete(instance='default', environment='dev')</code>","text":"<p>Delete the specified instance from the development environment in the Vespa Cloud.</p> <p>To delete a production instance, you must submit a new deployment with <code>deployment-removal</code> added to the 'validation-overrides.xml'. See https://cloud.vespa.ai/en/deleting-applications for more details.</p> Example usage <pre><code>vespa_cloud = VespaCloud(...)\nvespa_cloud.delete_instance(instance=\"my-instance\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>The name of the instance to delete.</p> <code>'default'</code> <code>environment</code> <code>str</code> <p>The environment from which to delete the instance. Must be \"dev\" or \"perf\".</p> <code>'dev'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_all_endpoints","title":"<code>get_all_endpoints(instance='default', region=None, environment='dev')</code>","text":"<p>Get all endpoints for the application instance.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/perf/prod).</p> <code>'dev'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[Dict[str, str]]</code> <p>List of endpoints.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_app_package_contents","title":"<code>get_app_package_contents(instance='default', region=None, environment='dev')</code>","text":"<p>Get all endpoints for the application package content in the specified region and environment.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'. If None, uses the default region for the environment.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/perf/prod). Default is 'dev'.</p> <code>'dev'</code> <p>Returns:</p> Name Type Description <code>list</code> <code>List[str]</code> <p>List of endpoints for the application instance.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_schemas","title":"<code>get_schemas(instance='default', region=None, environment='dev')</code>","text":"<p>Get all schemas for the application instance in the specified environment and region.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'. If None, uses the default region for the environment.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/perf/prod). Default is 'dev'.</p> <code>'dev'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, str]</code> <p>Dictionary with schema name as key and content as value.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.download_app_package_content","title":"<code>download_app_package_content(destination_path, instance='default', region=None, environment='dev')</code>","text":"<p>Download the application package content to a specified destination path.</p> <p>Parameters:</p> Name Type Description Default <code>destination_path</code> <code>str</code> <p>The path where the application package content will be downloaded.</p> required <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'. If None, uses the default region for the environment.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/perf/prod). Default is 'dev'.</p> <code>'dev'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_endpoint_auth_method","title":"<code>get_endpoint_auth_method(url, instance='default', region=None, environment='dev')</code>","text":"<p>Get the authentication method for the given endpoint URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The endpoint URL.</p> required <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/perf/prod).</p> <code>'dev'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The authentication method ('mtls' or 'token').</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_endpoint","title":"<code>get_endpoint(auth_method, instance='default', region=None, environment='dev', cluster=None)</code>","text":"<p>Get the endpoint URL for the application.</p> <p>Tip: See the 'endpoint'-tab in Vespa Cloud Console for available endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>auth_method</code> <code>str</code> <p>Authentication method. Options are 'mtls' or 'token'.</p> required <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name, e.g. 'aws-us-east-1c'.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/perf/prod).</p> <code>'dev'</code> <code>cluster</code> <code>str</code> <p>Specific cluster to get the endpoint for. If None, uses the instance's default cluster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The endpoint URL.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_mtls_endpoint","title":"<code>get_mtls_endpoint(instance='default', region=None, environment='dev', cluster=None)</code>","text":"<p>Get the endpoint URL of a mTLS endpoint for the application. Will return the first mTLS endpoint found if multiple exist. Use <code>VespaCloud.get_all_endpoints</code> to get all endpoints.</p> <p>Tip: See the 'endpoint'-tab in Vespa Cloud Console for available endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/perf/prod).</p> <code>'dev'</code> <code>cluster</code> <code>str</code> <p>Specific cluster to get the endpoint for. If None, uses the instance's default cluster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The endpoint URL.</p>"},{"location":"api/vespa/deployment.html#vespa.deployment.VespaCloud.get_token_endpoint","title":"<code>get_token_endpoint(instance='default', region=None, environment='dev', cluster=None)</code>","text":"<p>Get the endpoint URL of a token endpoint for the application. Will return the first token endpoint found if multiple exist. Use <code>VespaCloud.get_all_endpoints</code> to get all endpoints.</p> <p>Tip: See the 'endpoint'-tab in Vespa Cloud Console for available endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>instance</code> <code>str</code> <p>Application instance name.</p> <code>'default'</code> <code>region</code> <code>str</code> <p>Region name.</p> <code>None</code> <code>environment</code> <code>str</code> <p>Environment (dev/perf/prod).</p> <code>'dev'</code> <code>cluster</code> <code>str</code> <p>Specific cluster to get the endpoint for. If None, uses the instance's default cluster.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The endpoint URL.</p>"},{"location":"api/vespa/evaluation.html","title":"Evaluation","text":""},{"location":"api/vespa/evaluation.html#vespa.evaluation","title":"<code>vespa.evaluation</code>","text":""},{"location":"api/vespa/evaluation.html#vespa.evaluation.RandomHitsSamplingStrategy","title":"<code>RandomHitsSamplingStrategy</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enum for different random hits sampling strategies.</p> <ul> <li>RATIO: Sample random hits as a ratio of relevant docs (e.g., 1.0 = equal number, 2.0 = twice as many)</li> <li>FIXED: Sample a fixed number of random hits per query</li> </ul>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaEvaluatorBase","title":"<code>VespaEvaluatorBase(queries, relevant_docs, vespa_query_fn, app, name='', id_field='', write_csv=False, csv_dir=None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for Vespa evaluators providing initialization and interface.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaEvaluatorBase.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be implemented by subclasses.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaEvaluatorBase.__call__","title":"<code>__call__()</code>","text":"<p>Make the evaluator callable.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaEvaluator","title":"<code>VespaEvaluator(queries, relevant_docs, vespa_query_fn, app, name='', id_field='', accuracy_at_k=[1, 3, 5, 10], precision_recall_at_k=[1, 3, 5, 10], mrr_at_k=[10], ndcg_at_k=[10], map_at_k=[100], write_csv=False, csv_dir=None)</code>","text":"<p>               Bases: <code>VespaEvaluatorBase</code></p> <p>Evaluate retrieval performance on a Vespa application.</p> <p>This class:</p> <ul> <li>Iterates over queries and issues them against your Vespa application.</li> <li>Retrieves top-k documents per query (with k = max of your IR metrics).</li> <li>Compares the retrieved documents with a set of relevant document ids.</li> <li>Computes IR metrics: Accuracy@k, Precision@k, Recall@k, MRR@k, NDCG@k, MAP@k.</li> <li>Logs vespa search times for each query.</li> <li>Logs/returns these metrics.</li> <li>Optionally writes out to CSV.</li> </ul> <p>Note: The 'id_field' needs to be marked as an attribute in your Vespa schema, so filtering can be done on it.</p> Example usage <pre><code>from vespa.application import Vespa\nfrom vespa.evaluation import VespaEvaluator\n\nqueries = {\n    \"q1\": \"What is the best GPU for gaming?\",\n    \"q2\": \"How to bake sourdough bread?\",\n    # ...\n}\nrelevant_docs = {\n    \"q1\": {\"d12\", \"d99\"},\n    \"q2\": {\"d101\"},\n    # ...\n}\n# relevant_docs can also be a dict of query_id =&gt; single relevant doc_id\n# relevant_docs = {\n#     \"q1\": \"d12\",\n#     \"q2\": \"d101\",\n#     # ...\n# }\n# Or, relevant_docs can be a dict of query_id =&gt; map of doc_id =&gt; relevance\n# relevant_docs = {\n#     \"q1\": {\"d12\": 1, \"d99\": 0.1},\n#     \"q2\": {\"d101\": 0.01},\n#     # ...\n# Note that for non-binary relevance, the relevance values should be in [0, 1], and that\n# only the nDCG metric will be computed.\n\ndef my_vespa_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": 'select * from sources * where userInput(\"' + query_text + '\");',\n        \"hits\": top_k,\n        \"ranking\": \"your_ranking_profile\",\n    }\n\napp = Vespa(url=\"http://localhost\", port=8080)\n\nevaluator = VespaEvaluator(\n    queries=queries,\n    relevant_docs=relevant_docs,\n    vespa_query_fn=my_vespa_query_fn,\n    app=app,\n    name=\"test-run\",\n    accuracy_at_k=[1, 3, 5],\n    precision_recall_at_k=[1, 3, 5],\n    mrr_at_k=[10],\n    ndcg_at_k=[10],\n    map_at_k=[100],\n    write_csv=True\n)\n\nresults = evaluator()\nprint(\"Primary metric:\", evaluator.primary_metric)\nprint(\"All results:\", results)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Dict[str, str]</code> <p>A dictionary where keys are query IDs and values are query strings.</p> required <code>relevant_docs</code> <code>Union[Dict[str, Union[Set[str], Dict[str, float]]], Dict[str, str]]</code> <p>A dictionary mapping query IDs to their relevant document IDs. Can be a set of doc IDs for binary relevance, a dict of doc_id to relevance score (float between 0 and 1) for graded relevance, or a single doc_id string.</p> required <code>vespa_query_fn</code> <code>Callable[[str, int, Optional[str]], dict]</code> <p>A function that takes a query string, the number of hits to retrieve (top_k), and an optional query_id, and returns a Vespa query body dictionary.</p> required <code>app</code> <code>Vespa</code> <p>An instance of the Vespa application.</p> required <code>name</code> <code>str</code> <p>A name for this evaluation run. Defaults to \"\".</p> <code>''</code> <code>id_field</code> <code>str</code> <p>The field name in the Vespa hit that contains the document ID. If empty, it tries to infer the ID from the 'id' field or 'fields.id'. Defaults to \"\".</p> <code>''</code> <code>accuracy_at_k</code> <code>List[int]</code> <p>List of k values for which to compute Accuracy@k. Defaults to [1, 3, 5, 10].</p> <code>[1, 3, 5, 10]</code> <code>precision_recall_at_k</code> <code>List[int]</code> <p>List of k values for which to compute Precision@k and Recall@k. Defaults to [1, 3, 5, 10].</p> <code>[1, 3, 5, 10]</code> <code>mrr_at_k</code> <code>List[int]</code> <p>List of k values for which to compute MRR@k. Defaults to [10].</p> <code>[10]</code> <code>ndcg_at_k</code> <code>List[int]</code> <p>List of k values for which to compute NDCG@k. Defaults to [10].</p> <code>[10]</code> <code>map_at_k</code> <code>List[int]</code> <p>List of k values for which to compute MAP@k. Defaults to [100].</p> <code>[100]</code> <code>write_csv</code> <code>bool</code> <p>Whether to write the evaluation results to a CSV file. Defaults to False.</p> <code>False</code> <code>csv_dir</code> <code>Optional[str]</code> <p>Directory to save the CSV file. Defaults to None (current directory).</p> <code>None</code>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaEvaluator.run","title":"<code>run()</code>","text":"<p>Executes the evaluation by running queries and computing IR metrics.</p> <p>This method: 1. Executes all configured queries against the Vespa application. 2. Collects search results and timing information. 3. Computes the configured IR metrics (Accuracy@k, Precision@k, Recall@k, MRR@k, NDCG@k, MAP@k). 4. Records search timing statistics. 5. Logs results and optionally writes them to CSV.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>A dictionary containing: - IR metrics with names like \"accuracy@k\", \"precision@k\", etc. - Search time statistics (\"searchtime_avg\", \"searchtime_q50\", etc.). The values are floats between 0 and 1 for metrics and in seconds for timing.</p> Example <pre><code>{\n    \"accuracy@1\": 0.75,\n    \"ndcg@10\": 0.68,\n    \"searchtime_avg\": 0.0123,\n    ...\n}\n</code></pre>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaMatchEvaluator","title":"<code>VespaMatchEvaluator(queries, relevant_docs, vespa_query_fn, app, id_field, name='', rank_profile='unranked', write_csv=False, write_verbose=False, csv_dir=None)</code>","text":"<p>               Bases: <code>VespaEvaluatorBase</code></p> <p>Evaluate recall in the match-phase over a set of queries for a Vespa application.</p> <p>This class:</p> <ul> <li>Iterates over queries and issues them against your Vespa application.</li> <li>Sends one query with limit 0 to get the number of matched documents.</li> <li>Sends one query with recall-parameter set according to the provided relevant documents.</li> <li>Compares the retrieved documents with a set of relevant document ids.</li> <li>Logs vespa search times for each query.</li> <li>Logs/returns these metrics.</li> <li>Optionally writes out to CSV.</li> </ul> <p>Note: It is recommended to use a rank profile without any first-phase (and second-phase) ranking if you care about speed of evaluation run. If you do so, you need to make sure that the rank profile you use has the same inputs. For example, if you want to evaluate a YQL query including nearestNeighbor-operator, your rank-profile needs to define the corresponding input tensor. You must also either provide the query tensor or define it as input (e.g 'input.query(embedding)=embed(@query)') in your Vespa query function. Also note that the 'id_field' needs to be marked as an attribute in your Vespa schema, so filtering can be done on it. Example usage:     <pre><code>from vespa.application import Vespa\nfrom vespa.evaluation import VespaEvaluator\n\nqueries = {\n    \"q1\": \"What is the best GPU for gaming?\",\n    \"q2\": \"How to bake sourdough bread?\",\n    # ...\n}\nrelevant_docs = {\n    \"q1\": {\"d12\", \"d99\"},\n    \"q2\": {\"d101\"},\n    # ...\n}\n# relevant_docs can also be a dict of query_id =&gt; single relevant doc_id\n# relevant_docs = {\n#     \"q1\": \"d12\",\n#     \"q2\": \"d101\",\n#     # ...\n# }\n# Or, relevant_docs can be a dict of query_id =&gt; map of doc_id =&gt; relevance\n# relevant_docs = {\n#     \"q1\": {\"d12\": 1, \"d99\": 0.1},\n#     \"q2\": {\"d101\": 0.01},\n#     # ...\n\ndef my_vespa_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": 'select * from sources * where userInput(\"' + query_text + '\");',\n        \"hits\": top_k,\n        \"ranking\": \"your_ranking_profile\",\n    }\n\napp = Vespa(url=\"http://localhost\", port=8080)\n\nevaluator = VespaMatchEvaluator(\n    queries=queries,\n    relevant_docs=relevant_docs,\n    vespa_query_fn=my_vespa_query_fn,\n    app=app,\n    name=\"test-run\",\n    id_field=\"id\",\n    write_csv=True,\n    write_verbose=True,\n)\n\nresults = evaluator()\nprint(\"Primary metric:\", evaluator.primary_metric)\nprint(\"All results:\", results)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Dict[str, str]</code> <p>A dictionary where keys are query IDs and values are query strings.</p> required <code>relevant_docs</code> <code>Union[Dict[str, Union[Set[str], Dict[str, float]]], Dict[str, str]]</code> <p>A dictionary mapping query IDs to their relevant document IDs. Can be a set of doc IDs for binary relevance, or a single doc_id string. Graded relevance (dict of doc_id to relevance score) is not supported for match evaluation.</p> required <code>vespa_query_fn</code> <code>Callable[[str, int, Optional[str]], dict]</code> <p>A function that takes a query string, the number of hits to retrieve (top_k), and an optional query_id, and returns a Vespa query body dictionary.</p> required <code>app</code> <code>Vespa</code> <p>An instance of the Vespa application.</p> required <code>name</code> <code>str</code> <p>A name for this evaluation run. Defaults to \"\".</p> <code>''</code> <code>id_field</code> <code>str</code> <p>The field name in the Vespa hit that contains the document ID. If empty, it tries to infer the ID from the 'id' field or 'fields.id'. Defaults to \"\".</p> required <code>write_csv</code> <code>bool</code> <p>Whether to write the summary evaluation results to a CSV file. Defaults to False.</p> <code>False</code> <code>write_verbose</code> <code>bool</code> <p>Whether to write detailed query-level results to a separate CSV file. Defaults to False.</p> <code>False</code> <code>csv_dir</code> <code>Optional[str]</code> <p>Directory to save the CSV files. Defaults to None (current directory).</p> <code>None</code>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaMatchEvaluator.create_grouping_filter","title":"<code>create_grouping_filter(yql, id_field, relevant_ids)</code>  <code>staticmethod</code>","text":"<p>Create a grouping filter to append Vespa YQL queries to limit results to relevant documents. | all( group(id_field) filter(regex(\"\", id_field)) each(output(count()))) <p>Parameters: yql (str): The base YQL query string. id_field (str): The field name in the Vespa hit that contains the document ID. relevant_ids (list[str]): List of relevant document IDs to include in the filter.</p> <p>Returns: str: The modified YQL query string with the grouping filter applied.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaMatchEvaluator.extract_matched_ids","title":"<code>extract_matched_ids(resp, id_field)</code>  <code>staticmethod</code>","text":"<p>Extract matched document IDs from Vespa query response hits. Parameters: resp (VespaQueryResponse): The Vespa query response object. id_field (str): The field name in the Vespa hit that contains the document ID</p> <p>Returns: Set[str]: A set of matched document IDs.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaMatchEvaluator.run","title":"<code>run()</code>","text":"<p>Executes the match-phase recall evaluation.</p> <p>This method: 1. Sends a grouping query to see which of the relevant documents were matched, and get totalCount. 3. Computes recall metrics and match statistics. 4. Logs results and optionally writes them to CSV.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, float]</code> <p>A dictionary containing recall metrics, match statistics, and search time statistics.</p> Example <pre><code>{\n    \"match_recall\": 0.85,\n    \"total_relevant_docs\": 150,\n    \"total_matched_relevant\": 128,\n    \"avg_matched_per_query\": 45.2,\n    \"searchtime_avg\": 0.015,\n    ...\n}\n</code></pre>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaCollectorBase","title":"<code>VespaCollectorBase(queries, relevant_docs, vespa_query_fn, app, id_field, name='', csv_dir=None, random_hits_strategy=RandomHitsSamplingStrategy.RATIO, random_hits_value=1.0, max_random_hits_per_query=None, collect_matchfeatures=True, collect_rankfeatures=False, collect_summaryfeatures=False, write_csv=True)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for Vespa training data collectors providing initialization and interface.</p> <p>Initialize the VespaFeatureCollector.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Dict[str, str]</code> <p>Dictionary mapping query IDs to query strings</p> required <code>relevant_docs</code> <code>Union[Dict[str, Union[Set[str], Dict[str, float]]], Dict[str, str]]</code> <p>Dictionary mapping query IDs to relevant document IDs</p> required <code>vespa_query_fn</code> <code>Callable[[str, int, Optional[str]], dict]</code> <p>Function to generate Vespa query bodies</p> required <code>app</code> <code>Vespa</code> <p>Vespa application instance</p> required <code>id_field</code> <code>str</code> <p>Field name containing document IDs in Vespa hits (must be defined as an attribute in the schema)</p> required <code>name</code> <code>str</code> <p>Name for this collection run</p> <code>''</code> <code>csv_dir</code> <code>Optional[str]</code> <p>Directory to save CSV files</p> <code>None</code> <code>random_hits_strategy</code> <code>Union[RandomHitsSamplingStrategy, str]</code> <p>Strategy for sampling random hits - either \"ratio\" or \"fixed\" - RATIO: Sample random hits as a ratio of relevant docs - FIXED: Sample a fixed number of random hits per query</p> <code>RATIO</code> <code>random_hits_value</code> <code>Union[float, int]</code> <p>Value for the sampling strategy - For RATIO: Ratio value (e.g., 1.0 = equal, 2.0 = twice as many random hits) - For FIXED: Fixed number of random hits per query</p> <code>1.0</code> <code>max_random_hits_per_query</code> <code>Optional[int]</code> <p>Optional maximum limit on random hits per query (only applies when using RATIO strategy to prevent excessive sampling)</p> <code>None</code> <code>collect_matchfeatures</code> <code>bool</code> <p>Whether to collect match features</p> <code>True</code> <code>collect_rankfeatures</code> <code>bool</code> <p>Whether to collect rank features</p> <code>False</code> <code>collect_summaryfeatures</code> <code>bool</code> <p>Whether to collect summary features</p> <code>False</code> <code>write_csv</code> <code>bool</code> <p>Whether to write results to CSV file</p> <code>True</code>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaCollectorBase.collect","title":"<code>collect()</code>  <code>abstractmethod</code>","text":"<p>Abstract method to be implemented by subclasses.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaCollectorBase.__call__","title":"<code>__call__()</code>","text":"<p>Make the collector callable.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaFeatureCollector","title":"<code>VespaFeatureCollector(queries, relevant_docs, vespa_query_fn, app, id_field, name='', csv_dir=None, random_hits_strategy=RandomHitsSamplingStrategy.RATIO, random_hits_value=1.0, max_random_hits_per_query=None, collect_matchfeatures=True, collect_rankfeatures=False, collect_summaryfeatures=False, write_csv=True)</code>","text":"<p>               Bases: <code>VespaCollectorBase</code></p> <p>Collects training data for retrieval tasks from a Vespa application.</p> <p>This class:</p> <ul> <li>Iterates over queries and issues them against your Vespa application.</li> <li>Retrieves top-k documents per query.</li> <li>Samples random hits based on the specified strategy.</li> <li>Compiles a CSV file with query-document pairs and their relevance labels.</li> </ul> <p>Important: If you want to sample random hits, you need to make sure that the rank profile you define in your <code>vespa_query_fn</code> has a ranking expression that reflects this. See docs for example. In this case, be aware that the <code>relevance_score</code> value in the returned results (or CSV) will be of no value. This will only have meaning if you use this to collect features for relevant documents only.</p> Example usage <pre><code>from vespa.application import Vespa\nfrom vespa.evaluation import VespaFeatureCollector\n\nqueries = {\n    \"q1\": \"What is the best GPU for gaming?\",\n    \"q2\": \"How to bake sourdough bread?\",\n    # ...\n}\nrelevant_docs = {\n    \"q1\": {\"d12\", \"d99\"},\n    \"q2\": {\"d101\"},\n    # ...\n}\n\ndef my_vespa_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": 'select * from sources * where userInput(\"' + query_text + '\");',\n        \"hits\": 10,  # Do not make use of top_k here.\n        \"ranking\": \"your_ranking_profile\", # This should have `random` as ranking expression\n    }\n\napp = Vespa(url=\"http://localhost\", port=8080)\n\ncollector = VespaFeatureCollector(\n    queries=queries,\n    relevant_docs=relevant_docs,\n    vespa_query_fn=my_vespa_query_fn,\n    app=app,\n    id_field=\"id\",  # Field in Vespa hit that contains the document ID (must be an attribute)\n    name=\"retrieval-data-collection\",\n    csv_dir=\"/path/to/save/csv\",\n    random_hits_strategy=\"ratio\",  # or RandomHitsSamplingStrategy.RATIO\n    random_hits_value=1.0,  # Sample equal number of random hits to relevant docs\n    max_random_hits_per_query=100,  # Optional: cap random hits per query\n    collect_matchfeatures=True,  # Collect match features from rank profile\n    collect_rankfeatures=False,  # Skip traditional rank features\n    collect_summaryfeatures=False,  # Skip summary features\n)\n\ncollector()\n</code></pre> <p>Alternative Usage Examples:</p> <pre><code># Example 1: Fixed number of random hits per query\ncollector = VespaFeatureCollector(\n    queries=queries,\n    relevant_docs=relevant_docs,\n    vespa_query_fn=my_vespa_query_fn,\n    app=app,\n    id_field=\"id\",  # Required field name\n    random_hits_strategy=\"fixed\",\n    random_hits_value=50,  # Always sample 50 random hits per query\n)\n\n# Example 2: Ratio-based with a cap\ncollector = VespaFeatureCollector(\n    queries=queries,\n    relevant_docs=relevant_docs,\n    vespa_query_fn=my_vespa_query_fn,\n    app=app,\n    id_field=\"id\",  # Required field name\n    random_hits_strategy=\"ratio\",\n    random_hits_value=2.0,  # Sample twice as many random hits as relevant docs\n    max_random_hits_per_query=200,  # But never more than 200 per query\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Dict[str, str]</code> <p>A dictionary where keys are query IDs and values are query strings.</p> required <code>relevant_docs</code> <code>Union[Dict[str, Union[Set[str], Dict[str, float]]], Dict[str, str]]</code> <p>A dictionary mapping query IDs to their relevant document IDs. Can be a set of doc IDs for binary relevance, a dict of doc_id to relevance score (float between 0 and 1) for graded relevance, or a single doc_id string.</p> required <code>vespa_query_fn</code> <code>Callable[[str, int, Optional[str]], dict]</code> <p>A function that takes a query string, the number of hits to retrieve (top_k), and an optional query_id, and returns a Vespa query body dictionary.</p> required <code>app</code> <code>Vespa</code> <p>An instance of the Vespa application.</p> required <code>id_field</code> <code>str</code> <p>The field name in the Vespa hit that contains the document ID. This field must be defined as an attribute in your Vespa schema.</p> required <code>name</code> <code>str</code> <p>A name for this data collection run. Defaults to \"\".</p> <code>''</code> <code>csv_dir</code> <code>Optional[str]</code> <p>Directory to save the CSV file. Defaults to None (current directory).</p> <code>None</code> <code>random_hits_strategy</code> <code>Union[RandomHitsSamplingStrategy, str]</code> <p>Strategy for sampling random hits. Can be \"ratio\" (or RandomHitsSamplingStrategy.RATIO) to sample as a ratio of relevant docs, or \"fixed\" (or RandomHitsSamplingStrategy.FIXED) to sample a fixed number per query. Defaults to \"ratio\".</p> <code>RATIO</code> <code>random_hits_value</code> <code>Union[float, int]</code> <p>Value for the sampling strategy. For RATIO strategy: ratio value (e.g., 1.0 = equal number, 2.0 = twice as many random hits). For FIXED strategy: fixed number of random hits per query. Defaults to 1.0.</p> <code>1.0</code> <code>max_random_hits_per_query</code> <code>Optional[int]</code> <p>Maximum limit on random hits per query. Only applies to RATIO strategy to prevent excessive sampling. Defaults to None (no limit).</p> <code>None</code> <code>collect_matchfeatures</code> <code>bool</code> <p>Whether to collect match features defined in rank profile's match-features section. Defaults to True.</p> <code>True</code> <code>collect_rankfeatures</code> <code>bool</code> <p>Whether to collect rank features using ranking.listFeatures=true. Defaults to False.</p> <code>False</code> <code>collect_summaryfeatures</code> <code>bool</code> <p>Whether to collect summary features from document summaries. Defaults to False.</p> <code>False</code> <code>write_csv</code> <code>bool</code> <p>Whether to write results to CSV file. Defaults to True.</p> <code>True</code>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaFeatureCollector.get_recall_param","title":"<code>get_recall_param(relevant_doc_ids, get_relevant)</code>","text":"<p>Adds the recall parameter to the Vespa query body based on relevant document IDs.</p> <p>Parameters:</p> Name Type Description Default <code>relevant_doc_ids</code> <code>set</code> <p>A set of relevant document IDs.</p> required <code>get_relevant</code> <code>bool</code> <p>Whether to retrieve relevant documents.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The updated Vespa query body with the recall parameter.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaFeatureCollector.calculate_random_hits_count","title":"<code>calculate_random_hits_count(num_relevant_docs)</code>","text":"<p>Calculate the number of random hits to sample based on the configured strategy.</p> <p>Parameters:</p> Name Type Description Default <code>num_relevant_docs</code> <code>int</code> <p>Number of relevant documents for the query</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of random hits to sample</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.VespaFeatureCollector.collect","title":"<code>collect()</code>","text":"<p>Collects training data by executing queries and saving results to CSV.</p> <p>This method: 1. Executes all configured queries against the Vespa application. 2. Collects the top-k document IDs and their relevance labels. 3. Optionally writes the data to a CSV file for training purposes. 4. Returns the collected data as a single dictionary with results.</p> <p>Returns:</p> Type Description <code>Dict[str, List[Dict]]</code> <p>Dict containing:</p> <code>Dict[str, List[Dict]]</code> <ul> <li>'results': List of dictionaries, each containing all data for a query-document pair (query_id, doc_id, relevance_label, relevance_score, and all extracted features)</li> </ul>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.mean","title":"<code>mean(values)</code>","text":"<p>Compute the mean of a list of numbers without using numpy.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.percentile","title":"<code>percentile(values, p)</code>","text":"<p>Compute the p-th percentile of a list of values (0 &lt;= p &lt;= 100). This approximates numpy.percentile's behavior.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.validate_queries","title":"<code>validate_queries(queries)</code>","text":"<p>Validate and normalize queries. Converts query IDs to strings if they are ints.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.validate_qrels","title":"<code>validate_qrels(qrels)</code>","text":"<p>Validate and normalize qrels. Converts query IDs to strings if they are ints.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.validate_vespa_query_fn","title":"<code>validate_vespa_query_fn(fn)</code>","text":"<p>Validates the vespa_query_fn function.</p> The function must be callable and accept either 2 or 3 parameters <ul> <li>(query_text: str, top_k: int)</li> <li>or (query_text: str, top_k: int, query_id: Optional[str])</li> </ul> <p>It must return a dictionary when called with test inputs.</p> <p>Returns True if the function takes a query_id parameter, False otherwise.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.filter_queries","title":"<code>filter_queries(queries, relevant_docs)</code>","text":"<p>Filter out queries that have no relevant docs</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.extract_doc_id_from_hit","title":"<code>extract_doc_id_from_hit(hit, id_field)</code>","text":"<p>Extract document ID from a Vespa hit.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.get_id_field_from_hit","title":"<code>get_id_field_from_hit(hit, id_field)</code>","text":"<p>Get the ID field from a Vespa hit.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.calculate_searchtime_stats","title":"<code>calculate_searchtime_stats(searchtimes)</code>","text":"<p>Calculate search time statistics.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.execute_queries","title":"<code>execute_queries(app, query_bodies)</code>","text":"<p>Execute queries and collect timing information. Returns the responses and a list of search times.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.write_csv","title":"<code>write_csv(metrics, searchtime_stats, csv_file, csv_dir, name)</code>","text":"<p>Write metrics to CSV file.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.log_metrics","title":"<code>log_metrics(name, metrics)</code>","text":"<p>Log metrics with appropriate formatting.</p>"},{"location":"api/vespa/evaluation.html#vespa.evaluation.extract_features_from_hit","title":"<code>extract_features_from_hit(hit, collect_matchfeatures, collect_rankfeatures, collect_summaryfeatures)</code>","text":"<p>Extract features from a Vespa hit based on the collection configuration.</p> <p>Parameters:</p> Name Type Description Default <code>hit</code> <code>dict</code> <p>The Vespa hit dictionary</p> required <code>collect_matchfeatures</code> <code>bool</code> <p>Whether to collect match features</p> required <code>collect_rankfeatures</code> <code>bool</code> <p>Whether to collect rank features</p> required <code>collect_summaryfeatures</code> <code>bool</code> <p>Whether to collect summary features</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict mapping feature names to values</p>"},{"location":"api/vespa/exceptions.html","title":"Exceptions","text":""},{"location":"api/vespa/exceptions.html#vespa.exceptions","title":"<code>vespa.exceptions</code>","text":""},{"location":"api/vespa/exceptions.html#vespa.exceptions.VespaError","title":"<code>VespaError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Vespa returned an error response</p>"},{"location":"api/vespa/io.html","title":"IO","text":""},{"location":"api/vespa/io.html#vespa.io","title":"<code>vespa.io</code>","text":""},{"location":"api/vespa/io.html#vespa.io.VespaResponse","title":"<code>VespaResponse(json, status_code, url, operation_type)</code>","text":"<p>               Bases: <code>object</code></p> <p>Class to represent a Vespa HTTP API response.</p>"},{"location":"api/vespa/io.html#vespa.io.VespaResponse.get_status_code","title":"<code>get_status_code()</code>","text":"<p>Return status code of the response.</p>"},{"location":"api/vespa/io.html#vespa.io.VespaResponse.is_successfull","title":"<code>is_successfull()</code>","text":"<p>[Deprecated] Use is_successful() instead</p>"},{"location":"api/vespa/io.html#vespa.io.VespaResponse.is_successful","title":"<code>is_successful()</code>","text":"<p>True if status code is 200.</p>"},{"location":"api/vespa/io.html#vespa.io.VespaResponse.get_json","title":"<code>get_json()</code>","text":"<p>Return json of the response.</p>"},{"location":"api/vespa/io.html#vespa.io.VespaQueryResponse","title":"<code>VespaQueryResponse(json, status_code, url, request_body=None)</code>","text":"<p>               Bases: <code>VespaResponse</code></p>"},{"location":"api/vespa/io.html#vespa.io.VespaQueryResponse.get_json","title":"<code>get_json()</code>","text":"<p>For debugging when the response does not have hits.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>JSON object with full response</p>"},{"location":"api/vespa/package.html","title":"Package","text":""},{"location":"api/vespa/package.html#vespa.package","title":"<code>vespa.package</code>","text":""},{"location":"api/vespa/package.html#vespa.package.VT","title":"<code>VT(tag, cs, attrs=None, void_=False, replace_underscores=True, **kwargs)</code>","text":"<p>A 'Vespa Tag' structure, containing <code>tag</code>, <code>children</code>, and <code>attrs</code></p>"},{"location":"api/vespa/package.html#vespa.package.VT.sanitize_tag_name","title":"<code>sanitize_tag_name(tag)</code>  <code>staticmethod</code>","text":"<p>Convert invalid tag names (with '-') to valid Python identifiers (with '_')</p>"},{"location":"api/vespa/package.html#vespa.package.VT.restore_tag_name","title":"<code>restore_tag_name()</code>","text":"<p>Restore sanitized tag names back to the original names for XML generation</p>"},{"location":"api/vespa/package.html#vespa.package.Summary","title":"<code>Summary(name=None, type=None, fields=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Configures a summary field.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the summary field. Can be <code>None</code> if used inside a <code>Field</code>, which then uses the name of the <code>Field</code>.</p> <code>None</code> <code>type</code> <code>str</code> <p>The type of the summary field. Can be <code>None</code> if used inside a <code>Field</code>, which then uses the type of the <code>Field</code>.</p> <code>None</code> <code>fields</code> <code>list</code> <p>A list of properties used to configure the summary. These can be single properties (like \"summary: dynamic\", common in <code>Field</code>), or composite values (like \"source: another_field\").</p> <code>None</code> Example <pre><code>    Summary(None, None, [\"dynamic\"])\n    Summary(None, None, ['dynamic'])\n\n    Summary(\n        \"title\",\n        \"string\",\n        [(\"source\", \"title\")]\n    )\n    Summary('title', 'string', [('source', 'title')])\n\n    Summary(\n        \"title\",\n        \"string\",\n        [(\"source\", [\"title\", \"abstract\"])]\n    )\n    Summary('title', 'string', [('source', ['title', 'abstract'])])\n\n    Summary(\n        name=\"artist\",\n        type=\"string\",\n    )\n    Summary('artist', 'string', None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Summary.as_lines","title":"<code>as_lines</code>  <code>property</code>","text":"<p>Returns the object as a list of strings, where each string represents a line of configuration that can be used during schema generation as shown below:</p> Example usage <pre><code>    {% for line in field.summary.as_lines %}\n        {{ line }}\n    {% endfor %}\n</code></pre> Example <pre><code>Summary(None, None, [\"dynamic\"]).as_lines\n['summary: dynamic']\n</code></pre> <pre><code>Summary(\n    \"artist\",\n    \"string\",\n).as_lines\n['summary artist type string {}']\n</code></pre> <pre><code>Summary(\n    \"artist\",\n    \"string\",\n    [(\"bolding\", \"on\"), (\"sources\", \"artist\")],\n).as_lines\n['summary artist type string {', '    bolding: on', '    sources: artist', '}']\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.HNSW","title":"<code>HNSW(distance_metric='euclidean', max_links_per_node=16, neighbors_to_explore_at_insert=200)</code>","text":"<p>               Bases: <code>object</code></p> <p>Configures Vespa HNSW indexes.</p> <p>For more information, check the Vespa documentation.</p> <p>Parameters:</p> Name Type Description Default <code>distance_metric</code> <code>str</code> <p>The distance metric to use when computing distance between vectors. Default is 'euclidean'.</p> <code>'euclidean'</code> <code>max_links_per_node</code> <code>int</code> <p>Specifies how many links per HNSW node to select when building the graph. Default is 16.</p> <code>16</code> <code>neighbors_to_explore_at_insert</code> <code>int</code> <p>Specifies how many neighbors to explore when inserting a document in the HNSW graph. Default is 200.</p> <code>200</code>"},{"location":"api/vespa/package.html#vespa.package.StructField","title":"<code>StructField(name, **kwargs)</code>","text":"<p>Create a Vespa struct-field.</p> <p>For more detailed information about struct-fields, check the Vespa documentation.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the struct-field.</p> required <code>indexing</code> <code>list, tuple, or str</code> <p>Configures how to process data of a struct-field during indexing. - Tuple: renders as <code>indexing { value1; value2; ... }</code> block with each item on a new line, and semicolon at the end. - List: renders as <code>indexing: value1 | value2 | ...</code> - Single string: renders as <code>indexing: value</code></p> required <code>attribute</code> <code>list</code> <p>Specifies a property of an index structure attribute.</p> required <code>match</code> <code>list</code> <p>Set properties that decide how the matching method for this field operates.</p> required <code>query_command</code> <code>list</code> <p>Add configuration for the query-command of the field.</p> required <code>summary</code> <code>Summary</code> <p>Add configuration for the summary of the field.</p> required <code>rank</code> <code>str</code> <p>Specifies the property that defines ranking calculations done for a field.</p> required Example <pre><code>StructField(\n    name = \"first_name\",\n)\nStructField('first_name', None, None, None, None, None, None)\n</code></pre> <pre><code>StructField(\n    name = \"first_name\",\n    indexing = [\"attribute\"],\n    attribute = [\"fast-search\"],\n)\nStructField('first_name', ['attribute'], ['fast-search'], None, None, None, None)\n</code></pre> <pre><code>StructField(\n    name = \"last_name\",\n    match = [\"exact\", (\"exact-terminator\", '\"@%\"')],\n    query_command = ['\"exact %%\"'],\n    summary = Summary(None, None, fields=[\"dynamic\", (\"bolding\", \"on\")])\n)\nStructField('last_name', None, None, ['exact', ('exact-terminator', '\"@%\"')], ['\"exact %%\"'], Summary(None, None, ['dynamic', ('bolding', 'on')]), None)\n</code></pre> <pre><code>StructField(\n    name = \"first_name\",\n    indexing = [\"attribute\"],\n    attribute = [\"fast-search\"],\n    rank = \"filter\",\n)\nStructField('first_name', ['attribute'], ['fast-search'], None, None, None, 'filter')\n</code></pre> <pre><code>StructField(\n    name = \"complex_field\",\n    indexing = ('\"preprocessing\"', [\"attribute\", \"summary\"]),\n    attribute = [\"fast-search\"],\n)\nStructField('complex_field', ('\"preprocessing\"', ['attribute', 'summary']), ['fast-search'], None, None, None, None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.StructField.indexing_as_multiline","title":"<code>indexing_as_multiline</code>  <code>property</code>","text":"<p>Generate multiline indexing statements for tuple-based indexing.</p>"},{"location":"api/vespa/package.html#vespa.package.FieldConfiguration","title":"<code>FieldConfiguration</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>alias (list[str]): Add alias to the field.     Use the format \"component: component_alias\" to add an alias to a field's component.     See Vespa documentation for an example.</p>"},{"location":"api/vespa/package.html#vespa.package.Field","title":"<code>Field(name, type, indexing=None, index=None, attribute=None, ann=None, match=None, weight=None, bolding=None, summary=None, is_document_field=True, **kwargs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa field.</p> <p>For more detailed information about fields, check the Vespa documentation.</p> <p>Once we have an <code>ApplicationPackage</code> instance containing a <code>Schema</code> and a <code>Document</code>, we usually want to add fields so that we can store our data in a structured manner. We can accomplish that by creating <code>Field</code> instances and adding those to the <code>ApplicationPackage</code> instance via <code>Schema</code> and <code>Document</code> methods.</p> Index Configuration Behavior <ul> <li>Single string configuration: uses <code>index: value</code> syntax</li> <li>Single dict or multiple configurations: uses <code>index { ... }</code> block syntax</li> <li>All configurations in a list are consolidated into a single index block</li> </ul> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the field.</p> required <code>type</code> <code>str</code> <p>The data type of the field.</p> required <code>indexing</code> <code>list, tuple, or str</code> <p>Configures how to process data of a field during indexing. - Tuple: renders as <code>indexing { value1; value2; ... }</code> block with each item on a new line, and semicolon at the end. - List: renders as <code>indexing: value1 | value2 | ...</code> - Single string: renders as <code>indexing: value</code></p> <code>None</code> <code>index</code> <code>str, dict, or list</code> <p>Sets index parameters. - Single string (e.g., \"enable-bm25\"): renders as <code>index: enable-bm25</code> - Single dict (e.g., {\"arity\": 2}): renders as <code>index { arity: 2 }</code> - List with multiple items: renders as single <code>index { ... }</code> block containing all configurations Fields with index are normalized and tokenized by default.</p> <code>None</code> <code>attribute</code> <code>list</code> <p>Specifies a property of an index structure attribute.</p> <code>None</code> <code>ann</code> <code>HNSW</code> <p>Add configuration for approximate nearest neighbor.</p> <code>None</code> <code>match</code> <code>list</code> <p>Set properties that decide how the matching method for this field operates.</p> <code>None</code> <code>weight</code> <code>int</code> <p>Sets the weight of the field, used when calculating rank scores.</p> <code>None</code> <code>bolding</code> <code>bool</code> <p>Whether to highlight matching query terms in the summary.</p> <code>None</code> <code>summary</code> <code>Summary</code> <p>Add configuration for the summary of the field.</p> <code>None</code> <code>is_document_field</code> <code>bool</code> <p>Whether the field is a document field or part of the schema. Default is True.</p> <code>True</code> <code>stemming</code> <code>str</code> <p>Add configuration for stemming of the field.</p> required <code>rank</code> <code>str</code> <p>Add configuration for ranking calculations of the field.</p> required <code>query_command</code> <code>list</code> <p>Add configuration for query-command of the field.</p> required <code>struct_fields</code> <code>list</code> <p>Add struct-fields to the field.</p> required <code>alias</code> <code>list</code> <p>Add alias to the field. Use the format \"component: component_alias\" to add an alias to a field's component. See Vespa documentation for an example.</p> required Example <pre><code>Field(name = \"title\", type = \"string\", indexing = [\"index\", \"summary\"], index = \"enable-bm25\")\nField('title', 'string', ['index', 'summary'], 'enable-bm25', None, None, None, None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"title\",\n    type = \"array&lt;string&gt;\",\n    indexing = ('\"en\"', [\"index\", \"summary\"]),\n)\nField('title', 'array&lt;string&gt;', ('\"en\"', ['index', 'summary']), None, None, None, None, None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    indexing = [\"attribute\"],\n    attribute=[\"fast-search\", \"fast-access\"]\n)\nField('abstract', 'string', ['attribute'], None, ['fast-search', 'fast-access'], None, None, None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(name=\"tensor_field\",\n    type=\"tensor&lt;float&gt;(x[128])\",\n    indexing=[\"attribute\"],\n    ann=HNSW(\n        distance_metric=\"euclidean\",\n        max_links_per_node=16,\n        neighbors_to_explore_at_insert=200,\n    ),\n)\nField('tensor_field', 'tensor&lt;float&gt;(x[128])', ['attribute'], None, None, HNSW('euclidean', 16, 200), None, None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    match = [\"exact\", (\"exact-terminator\", '\"@%\"',)],\n)\nField('abstract', 'string', None, None, None, None, ['exact', ('exact-terminator', '\"@%\"')], None, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    weight = 200,\n)\nField('abstract', 'string', None, None, None, None, None, 200, None, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    bolding = True,\n)\nField('abstract', 'string', None, None, None, None, None, None, True, None, True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    summary = Summary(None, None, [\"dynamic\", [\"bolding\", \"on\"]]),\n)\nField('abstract', 'string', None, None, None, None, None, None, None, Summary(None, None, ['dynamic', ['bolding', 'on']]), True, None, None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    stemming = \"shortest\",\n)\nField('abstract', 'string', None, None, None, None, None, None, None, None, True, 'shortest', None, None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    rank = \"filter\",\n)\nField('abstract', 'string', None, None, None, None, None, None, None, None, True, None, 'filter', None, [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    query_command = ['\"exact %%\"'],\n)\nField('abstract', 'string', None, None, None, None, None, None, None, None, True, None, None, ['\"exact %%\"'], [], None)\n</code></pre> <pre><code>Field(\n    name = \"abstract\",\n    type = \"string\",\n    struct_fields = [\n        StructField(\n            name = \"first_name\",\n            indexing = [\"attribute\"],\n            attribute = [\"fast-search\"],\n        ),\n    ],\n)\nField('abstract', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [StructField('first_name', ['attribute'], ['fast-search'], None, None, None, None)], None)\n</code></pre> <pre><code>Field(\n    name = \"artist\",\n    type = \"string\",\n    alias = [\"artist_name\", \"component: component_alias\"],\n)\nField('artist', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], ['artist_name', 'component: component_alias'])\n</code></pre> <pre><code># Single string index - uses simple syntax\nField(name = \"title\", type = \"string\", index = \"enable-bm25\")\n# Renders as: index: enable-bm25\n</code></pre> <pre><code># Single dict index - uses block syntax\nField(name = \"predicate_field\", type = \"predicate\", index = {\"arity\": 2})\n# Renders as: index { arity: 2 }\n</code></pre> <pre><code># Multiple string indices - uses block syntax\nField(name = \"multi\", type = \"string\", index = [\"enable-bm25\", \"another-setting\"])\n# Renders as: index { enable-bm25; another-setting }\n</code></pre> <pre><code># Complex index configurations with multiple parameters\nField(\n    name = \"predicate_field\",\n    type = \"predicate\",\n    indexing = [\"attribute\"],\n    index = {\n        \"arity\": 2,\n        \"lower-bound\": 3,\n        \"upper-bound\": 200,\n        \"dense-posting-list-threshold\": 0.25\n    }\n)\n# Renders as: index { arity: 2; lower-bound: 3; upper-bound: 200; dense-posting-list-threshold: 0.25 }\n</code></pre> <pre><code># Multiple index configurations with mixed types\nField(\n    name = \"complex_field\",\n    type = \"string\",\n    indexing = [\"index\", \"summary\"],\n    index = [\n        \"enable-bm25\",  # Simple index setting\n        {\"arity\": 2, \"lower-bound\": 3},  # Complex index block\n        \"another-setting\"  # Another simple setting\n    ]\n)\n# Renders as single block:\n# index {\n#     enable-bm25\n#     arity: 2\n#     lower-bound: 3\n#     another-setting\n# }\n</code></pre> <pre><code># Parameterless index settings using None values\nField(\n    name = \"taxonomy\",\n    type = \"array&lt;string&gt;\",\n    indexing = [\"index\", \"summary\"],\n    match = [\"text\"],\n    index = {\"enable-bm25\": None}\n)\n# Renders as: index { enable-bm25 } (without \": None\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Field.indexing_as_multiline","title":"<code>indexing_as_multiline</code>  <code>property</code>","text":"<p>Generate multiline indexing statements for tuple-based indexing.</p>"},{"location":"api/vespa/package.html#vespa.package.Field.index_configurations","title":"<code>index_configurations</code>  <code>property</code>","text":"<p>Returns index configurations as a list, normalizing single values to lists. This allows the template to consistently iterate over index configurations.</p>"},{"location":"api/vespa/package.html#vespa.package.Field.use_simple_index_syntax","title":"<code>use_simple_index_syntax</code>  <code>property</code>","text":"<p>Returns True if we should use simple 'index: value' syntax. Simple syntax is used only when there's exactly one string configuration. Otherwise, we use the block syntax 'index { ... }'.</p>"},{"location":"api/vespa/package.html#vespa.package.Field.add_struct_fields","title":"<code>add_struct_fields(*struct_fields)</code>","text":"<p>Add <code>StructField</code>'s to the <code>Field</code>.</p> <p>Parameters:</p> Name Type Description Default <code>struct_fields</code> <code>list</code> <p>A list of <code>StructField</code> objects to be added.</p> <code>()</code>"},{"location":"api/vespa/package.html#vespa.package.ImportedField","title":"<code>ImportedField(name, reference_field, field_to_import)</code>","text":"<p>               Bases: <code>object</code></p> <p>Imported field from a reference document.</p> <p>Useful to implement parent/child relationships.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Field name.</p> required <code>reference_field</code> <code>str</code> <p>A field of type reference that points to the document that contains the field to be imported.</p> required <code>field_to_import</code> <code>str</code> <p>Field name to be imported, as defined in the reference document.</p> required Example <pre><code>ImportedField(\n    name=\"global_category_ctrs\",\n    reference_field=\"category_ctr_ref\",\n    field_to_import=\"ctrs\",\n)\nImportedField('global_category_ctrs', 'category_ctr_ref', 'ctrs')\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Struct","title":"<code>Struct(name, fields=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa struct. A struct defines a composite type. Check the Vespa documentation for more detailed information about structs.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the struct.</p> required <code>fields</code> <code>list</code> <p>List of <code>Field</code> objects to be included in the fieldset.</p> <code>None</code> Example <pre><code>Struct(\"person\")\nStruct('person', None)\n\nStruct(\n    \"person\",\n    [\n        Field(\"first_name\", \"string\"),\n        Field(\"last_name\", \"string\"),\n    ],\n)\nStruct('person', [Field('first_name', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], None), Field('last_name', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], None)])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.DocumentSummary","title":"<code>DocumentSummary(name, inherits=None, summary_fields=None, from_disk=None, omit_summary_features=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Document Summary. Check the Vespa documentation for more detailed information about document-summary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the document-summary.</p> required <code>inherits</code> <code>str</code> <p>Name of another document-summary from which this inherits.</p> <code>None</code> <code>summary_fields</code> <code>list</code> <p>List of <code>Summary</code> objects used in this document-summary.</p> <code>None</code> <code>from_disk</code> <code>bool</code> <p>Marks this document-summary as accessing fields on disk.</p> <code>None</code> <code>omit_summary_features</code> <code>bool</code> <p>Specifies that summary-features should be omitted from this document summary.</p> <code>None</code> Example <pre><code>DocumentSummary(\n    name=\"document-summary\",\n)\nDocumentSummary('document-summary', None, None, None, None)\n\nDocumentSummary(\n    name=\"which-inherits\",\n    inherits=\"base-document-summary\",\n)\nDocumentSummary('which-inherits', 'base-document-summary', None, None, None)\n\nDocumentSummary(\n    name=\"with-field\",\n    summary_fields=[Summary(\"title\", \"string\", [(\"source\", \"title\")])]\n)\nDocumentSummary('with-field', None, [Summary('title', 'string', [('source', 'title')])], None, None)\n\nDocumentSummary(\n    name=\"with-bools\",\n    from_disk=True,\n    omit_summary_features=True,\n)\nDocumentSummary('with-bools', None, None, True, True)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Document","title":"<code>Document(fields=None, inherits=None, structs=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Document.</p> <p>Check the Vespa documentation for more detailed information about documents.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>A list of <code>Field</code> objects to include in the document's schema.</p> <code>None</code> Example <pre><code>Document()\nDocument(None, None, None)\n\nDocument(fields=[Field(name=\"title\", type=\"string\")])\nDocument([Field('title', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], None)], None, None)\n\nDocument(fields=[Field(name=\"title\", type=\"string\")], inherits=\"context\")\nDocument([Field('title', 'string', None, None, None, None, None, None, None, None, True, None, None, None, [], None)], context, None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Document.add_fields","title":"<code>add_fields(*fields)</code>","text":"<p>Add <code>Field</code> objects to the document.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>Fields to be added.</p> <code>()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.Document.add_structs","title":"<code>add_structs(*structs)</code>","text":"<p>Add <code>Struct</code> objects to the document.</p> <p>Parameters:</p> Name Type Description Default <code>structs</code> <code>list</code> <p>Structs to be added.</p> <code>()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.FieldSet","title":"<code>FieldSet(name, fields)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa field set.</p> <p>A fieldset groups fields together for searching. Check the Vespa documentation for more detailed information about field sets.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the fieldset.</p> required <code>fields</code> <code>list</code> <p>Field names to be included in the fieldset.</p> required <p>Returns:</p> Name Type Description <code>FieldSet</code> <code>None</code> <p>A field set instance.</p> Example <pre><code>FieldSet(name=\"default\", fields=[\"title\", \"body\"])\nFieldSet('default', ['title', 'body'])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Function","title":"<code>Function(name, expression, args=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa rank function.</p> <p>Define a named function that can be referenced as a part of the ranking expression, or (if having no arguments) as a feature. Check the Vespa documentation for more detailed information about rank functions.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the function.</p> required <code>expression</code> <code>str</code> <p>String representing a Vespa expression.</p> required <code>args</code> <code>list</code> <p>List of arguments to be used in the function expression. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Function</code> <code>None</code> <p>A rank function instance.</p> Example <pre><code>    Function(\n        name=\"myfeature\",\n        expression=\"fieldMatch(bar) + freshness(foo)\",\n        args=[\"foo\", \"bar\"]\n    )\n    Function('myfeature', 'fieldMatch(bar) + freshness(foo)', ['foo', 'bar'])\n</code></pre> <p>It is possible to define functions with multi-line expressions: <pre><code>    Function(\n        name=\"token_type_ids\",\n        expression=\"tensor&lt;float&gt;(d0[1],d1[128])(\\n\"\n                   \"    if (d1 &lt; question_length,\\n\"\n                   \"        0,\\n\"\n                   \"    if (d1 &lt; question_length + doc_length,\\n\"\n                   \"        1,\\n\"\n                   \"        TOKEN_NONE\\n\"\n                   \"    )))\",\n    )\n    Function('token_type_ids', 'tensor&lt;float&gt;(d0[1],d1[128])(\\n    if (d1 &lt; question_length,\\n        0,\\n    if (d1 &lt; question_length + doc_length,\\n        1,\\n        TOKEN_NONE\\n    )))', None)\n</code></pre></p>"},{"location":"api/vespa/package.html#vespa.package.FirstPhaseRanking","title":"<code>FirstPhaseRanking(expression, keep_rank_count=None, rank_score_drop_limit=None)</code>","text":"<p>Create a Vespa first phase ranking configuration.</p> <p>This is the initial ranking performed on all matching documents. Check the Vespa documentation for more detailed information about first phase ranking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Specify the ranking expression to be used for the first phase of ranking. Check also the Vespa documentation for ranking expressions.</p> required <code>keep_rank_count</code> <code>int</code> <p>How many documents to keep the first phase top rank values for. Default value is 10000.</p> <code>None</code> <code>rank_score_drop_limit</code> <code>float</code> <p>Drop all hits with a first phase rank score less than or equal to this floating point number.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>FirstPhaseRanking</code> <code>None</code> <p>A first phase ranking configuration instance.</p> Example <pre><code>FirstPhaseRanking(\"myFeature * 10\")\nFirstPhaseRanking('myFeature * 10', None, None)\n\nFirstPhaseRanking(expression=\"myFeature * 10\", keep_rank_count=50, rank_score_drop_limit=10)\nFirstPhaseRanking('myFeature * 10', 50, 10)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.SecondPhaseRanking","title":"<code>SecondPhaseRanking(expression, rerank_count=100, rank_score_drop_limit=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa second phase ranking configuration.</p> <p>This is the optional reranking performed on the best hits from the first phase. Check the Vespa documentation for more detailed information about second phase ranking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Specify the ranking expression to be used for the second phase of ranking. Check also the Vespa documentation for ranking expressions.</p> required <code>rerank_count</code> <code>int</code> <p>Specifies the number of hits to be reranked in the second phase. Default value is 100.</p> <code>100</code> <code>rank_score_drop_limit</code> <code>float</code> <p>Drop all hits with a first phase rank score less than or equal to this floating point number.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>SecondPhaseRanking</code> <code>None</code> <p>A second phase ranking configuration instance.</p> Example <pre><code>SecondPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10)\nSecondPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, None)\n\nSecondPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10, rank_score_drop_limit=5)\nSecondPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, 5)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.GlobalPhaseRanking","title":"<code>GlobalPhaseRanking(expression, rerank_count=100, rank_score_drop_limit=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa global phase ranking configuration.</p> <p>This is the optional reranking performed on the best hits from the content nodes phase(s). Check the Vespa documentation for more detailed information about global phase ranking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>Specify the ranking expression to be used for the global phase of ranking. Check also the Vespa documentation for ranking expressions.</p> required <code>rerank_count</code> <code>int</code> <p>Specifies the number of hits to be reranked in the global phase. Default value is 100.</p> <code>100</code> <code>rank_score_drop_limit</code> <code>float</code> <p>Drop all hits with a first phase rank score less than or equal to this floating point number.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>GlobalPhaseRanking</code> <code>None</code> <p>A global phase ranking configuration instance.</p> Example <pre><code>    GlobalPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10)\n    GlobalPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, None)\n\n    GlobalPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10, rank_score_drop_limit=5)\n    GlobalPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, 5)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Mutate","title":"<code>Mutate(on_match, on_first_phase, on_second_phase, on_summary)</code>","text":"<p>               Bases: <code>object</code></p> <p>Enable mutating operations in rank profiles.</p> <p>Check the Vespa documentation for more detailed information about mutable attributes.</p> <p>Parameters:</p> Name Type Description Default <code>on_match</code> <code>dict</code> <p>Dictionary for the on-match phase containing 3 mandatory keys: - <code>attribute</code>: name of the mutable attribute to mutate. - <code>operation_string</code>: operation to perform on the mutable attribute. - <code>operation_value</code>: number to set, add, or subtract to/from the current value of the mutable attribute.</p> required <code>on_first_phase</code> <code>dict</code> <p>Dictionary for the on-first-phase phase containing 3 mandatory keys: - <code>attribute</code>: name of the mutable attribute to mutate. - <code>operation_string</code>: operation to perform on the mutable attribute. - <code>operation_value</code>: number to set, add, or subtract to/from the current value of the mutable attribute.</p> required <code>on_second_phase</code> <code>dict</code> <p>Dictionary for the on-second-phase phase containing 3 mandatory keys: - <code>attribute</code>: name of the mutable attribute to mutate. - <code>operation_string</code>: operation to perform on the mutable attribute. - <code>operation_value</code>: number to set, add, or subtract to/from the current value of the mutable attribute.</p> required <code>on_summary</code> <code>dict</code> <p>Dictionary for the on-summary phase containing 3 mandatory keys: - <code>attribute</code>: name of the mutable attribute to mutate. - <code>operation_string</code>: operation to perform on the mutable attribute. - <code>operation_value</code>: number to set, add, or subtract to/from the current value of the mutable attribute.</p> required Example <pre><code>enable_mutating_operations(\n    on_match={\n        'attribute': 'popularity',\n        'operation_string': 'add',\n        'operation_value': 5\n    },\n    on_first_phase={\n        'attribute': 'score',\n        'operation_string': 'subtract',\n        'operation_value': 3\n    }\n)\nenable_mutating_operations({'attribute': 'popularity', 'operation_string': 'add', 'operation_value': 5},\n                            {'attribute': 'score', 'operation_string': 'subtract', 'operation_value': 3})\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Diversity","title":"<code>Diversity(attribute, min_groups)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa ranking diversity configuration.</p> <p>This is an optional config that is used to guarantee diversity in the different query phases. Check the Vespa documentation for more detailed information about diversity configuration.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>Which attribute to use when deciding diversity. The attribute must be a single-valued numeric, string or reference type.</p> required <code>min_groups</code> <code>int</code> <p>Specifies the minimum number of groups returned from the phase.</p> required <p>Returns:</p> Name Type Description <code>Diversity</code> <code>None</code> <p>A ranking diversity configuration instance.</p> Example <pre><code>Diversity(attribute=\"popularity\", min_groups=5)\nDiversity('popularity', 5)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.MatchPhaseRanking","title":"<code>MatchPhaseRanking(attribute, order, max_hits)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa match phase ranking configuration.</p> <p>This is an optional phase that can be used to quickly select a subset of hits for further ranking. Check the Vespa documentation for more detailed information about match phase ranking configuration.</p> <p>Parameters:</p> Name Type Description Default <code>attribute</code> <code>str</code> <p>The numeric attribute to use for filtering.</p> required <code>order</code> <code>str</code> <p>The sort order, either \"ascending\" or \"descending\".</p> required <code>max_hits</code> <code>int</code> <p>Maximum number of hits to pass to the next phase.</p> required Example <pre><code>MatchPhaseRanking(attribute=\"popularity\", order=\"descending\", max_hits=1000)\nMatchPhaseRanking('popularity', 'descending', 1000)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.RankProfile","title":"<code>RankProfile(name, first_phase=None, inherits=None, constants=None, functions=None, summary_features=None, match_features=None, second_phase=None, global_phase=None, match_phase=None, num_threads_per_search=None, diversity=None, **kwargs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa rank profile.</p> <p>Rank profiles are used to specify an alternative ranking of the same data for different purposes, and to experiment with new rank settings. Check the Vespa documentation for more detailed information about rank profiles.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Rank profile name.</p> required <code>first_phase</code> <code>str</code> <p>The config specifying the first phase of ranking. More info about first phase ranking.</p> <code>None</code> <code>inherits</code> <code>str</code> <p>The inherits attribute is optional. If defined, it contains the name of another rank profile in the same schema. Values not defined in this rank profile will then be inherited.</p> <code>None</code> <code>constants</code> <code>dict</code> <p>Dict of constants available in ranking expressions, resolved and optimized at configuration time. More info about constants.</p> <code>None</code> <code>functions</code> <code>list</code> <p>List of <code>Function</code> objects representing rank functions to be included in the rank profile.</p> <code>None</code> <code>summary_features</code> <code>list</code> <p>List of rank features to be included with each hit. More info about summary features.</p> <code>None</code> <code>match_features</code> <code>list</code> <p>List of rank features to be included with each hit. More info about match features.</p> <code>None</code> <code>second_phase</code> <code>SecondPhaseRanking</code> <p>Config specifying the second phase of ranking. See <code>SecondPhaseRanking</code>.</p> <code>None</code> <code>global_phase</code> <code>GlobalPhaseRanking</code> <p>Config specifying the global phase of ranking. See <code>GlobalPhaseRanking</code>.</p> <code>None</code> <code>match_phase</code> <code>MatchPhaseRanking</code> <p>Config specifying the match phase of ranking. See <code>MatchPhaseRanking</code>.</p> <code>None</code> <code>num_threads_per_search</code> <code>int</code> <p>Overrides the global <code>persearch</code> value for this rank profile to a lower value.</p> <code>None</code> <code>diversity</code> <code>Optional[Diversity]</code> <p>Optional config specifying the diversity of ranking.</p> <code>None</code> <code>weight</code> <code>list</code> <p>A list of tuples containing the field and their weight.</p> required <code>rank_type</code> <code>list</code> <p>A list of tuples containing a field and the rank-type-name. More info about rank-type.</p> required <code>rank_properties</code> <code>list</code> <p>A list of tuples containing a field and its configuration. More info about rank-properties.</p> required <code>mutate</code> <code>Mutate</code> <p>A <code>Mutate</code> object containing attributes to mutate on, mutation operation, and value. More info about mutate operation.</p> required Example <pre><code>RankProfile(name = \"default\", first_phase = \"nativeRank(title, body)\")\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, None, None, None, None, None)\n\nRankProfile(name = \"new\", first_phase = \"BM25(title)\", inherits = \"default\")\nRankProfile('new', 'BM25(title)', 'default', None, None, None, None, None, None, None, None, None, None, None, None)\n\nRankProfile(\n    name = \"new\",\n    first_phase = \"BM25(title)\",\n    inherits = \"default\",\n    constants={\"TOKEN_NONE\": 0, \"TOKEN_CLS\": 101, \"TOKEN_SEP\": 102},\n    summary_features=[\"BM25(title)\"]\n)\nRankProfile('new', 'BM25(title)', 'default', {'TOKEN_NONE': 0, 'TOKEN_CLS': 101, 'TOKEN_SEP': 102}, None, ['BM25(title)'], None, None, None, None, None, None, None, None, None)\n\nRankProfile(\n    name=\"bert\",\n    first_phase=\"bm25(title) + bm25(body)\",\n    second_phase=SecondPhaseRanking(expression=\"1.25 * bm25(title) + 3.75 * bm25(body)\", rerank_count=10),\n    inherits=\"default\",\n    constants={\"TOKEN_NONE\": 0, \"TOKEN_CLS\": 101, \"TOKEN_SEP\": 102},\n    functions=[\n        Function(\n            name=\"question_length\",\n            expression=\"sum(map(query(query_token_ids), f(a)(a &gt; 0)))\"\n        ),\n        Function(\n            name=\"doc_length\",\n            expression=\"sum(map(attribute(doc_token_ids), f(a)(a &gt; 0)))\"\n        )\n    ],\n    summary_features=[\"question_length\", \"doc_length\"]\n)\nRankProfile('bert', 'bm25(title) + bm25(body)', 'default', {'TOKEN_NONE': 0, 'TOKEN_CLS': 101, 'TOKEN_SEP': 102}, [Function('question_length', 'sum(map(query(query_token_ids), f(a)(a &gt; 0)))', None), Function('doc_length', 'sum(map(attribute(doc_token_ids), f(a)(a &gt; 0)))', None)], ['question_length', 'doc_length'], None, SecondPhaseRanking('1.25 * bm25(title) + 3.75 * bm25(body)', 10, None), None, None, None, None, None, None, None)\n\nRankProfile(\n    name = \"default\",\n    first_phase = \"nativeRank(title, body)\",\n    weight = [(\"title\", 200), (\"body\", 100)]\n)\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, None, [('title', 200), ('body', 100)], None, None, None)\n\nRankProfile(\n    name = \"default\",\n    first_phase = \"nativeRank(title, body)\",\n    rank_type = [(\"body\", \"about\")]\n)\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, None, None, [('body', 'about')], None, None)\n\nRankProfile(\n    name = \"default\",\n    first_phase = \"nativeRank(title, body)\",\n    rank_properties = [(\"fieldMatch(title).maxAlternativeSegmentations\", \"10\")]\n)\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, None, None, None, [('fieldMatch(title).maxAlternativeSegmentations', '10')], None)\n\nRankProfile(\n   name = \"default\",\n   first_phase = FirstPhaseRanking(expression=\"nativeRank(title, body)\", keep_rank_count=50)\n)\nRankProfile('default', FirstPhaseRanking('nativeRank(title, body)', 50, None), None, None, None, None, None, None, None, None, None, None, None, None, None)\n\nRankProfile(\n    name = \"default\",\n    first_phase = \"nativeRank(title, body)\",\n    num_threads_per_search = 2\n)\nRankProfile('default', 'nativeRank(title, body)', None, None, None, None, None, None, None, None, 2, None, None, None, None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.OnnxModel","title":"<code>OnnxModel(model_name, model_file_path, inputs, outputs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa ONNX model config.</p> <p>Vespa has support for advanced ranking models through its tensor API. If you have your model in the ONNX format, Vespa can import the models and use them directly. Check the Vespa documentation for more detailed information about field sets.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Unique model name to use as an ID when referencing the model.</p> required <code>model_file_path</code> <code>str</code> <p>ONNX model file path.</p> required <code>inputs</code> <code>dict</code> <p>Dict mapping the ONNX input names as specified in the ONNX file to valid Vespa inputs. These can be a document field (<code>attribute(field_name)</code>), a query parameter (<code>query(query_param)</code>), a constant (<code>constant(name)</code>), or a user-defined function (<code>function_name</code>).</p> required <code>outputs</code> <code>dict</code> <p>Dict mapping the ONNX output names as specified in the ONNX file to the name used in Vespa to specify the output. If omitted, the first output in the ONNX file will be used.</p> required Example <pre><code>OnnxModel(\n    model_name=\"bert\",\n    model_file_path=\"bert.onnx\",\n    inputs={\n        \"input_ids\": \"input_ids\",\n        \"token_type_ids\": \"token_type_ids\",\n        \"attention_mask\": \"attention_mask\",\n    },\n    outputs={\"logits\": \"logits\"},\n)\nOnnxModel('bert', 'bert.onnx', {'input_ids': 'input_ids', 'token_type_ids': 'token_type_ids', 'attention_mask': 'attention_mask'}, {'logits': 'logits'})\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Schema","title":"<code>Schema(name, document, fieldsets=None, rank_profiles=None, models=None, global_document=False, imported_fields=None, document_summaries=None, mode='index', inherits=None, **kwargs)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Schema.</p> <p>Check the Vespa documentation for more detailed information about schemas.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Schema name.</p> required <code>document</code> <code>Document</code> <p>Vespa <code>Document</code> associated with the Schema.</p> required <code>fieldsets</code> <code>list</code> <p>A list of <code>FieldSet</code> associated with the Schema.</p> <code>None</code> <code>rank_profiles</code> <code>list</code> <p>A list of <code>RankProfile</code> associated with the Schema.</p> <code>None</code> <code>models</code> <code>list</code> <p>A list of <code>OnnxModel</code> associated with the Schema.</p> <code>None</code> <code>global_document</code> <code>bool</code> <p>Set to True to copy the documents to all content nodes. Defaults to False.</p> <code>False</code> <code>imported_fields</code> <code>list</code> <p>A list of <code>ImportedField</code> defining fields from global documents to be imported.</p> <code>None</code> <code>document_summaries</code> <code>list</code> <p>A list of <code>DocumentSummary</code> associated with the schema.</p> <code>None</code> <code>mode</code> <code>str</code> <p>Schema mode. Defaults to 'index'. Other options are 'store-only' and 'streaming'.</p> <code>'index'</code> <code>inherits</code> <code>str</code> <p>Schema to inherit from.</p> <code>None</code> <code>stemming</code> <code>str</code> <p>The default stemming setting. Defaults to 'best'.</p> required Example <pre><code>Schema(name=\"schema_name\", document=Document())\nSchema('schema_name', Document(None, None, None), None, None, [], False, None, [], None)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Schema.add_fields","title":"<code>add_fields(*fields)</code>","text":"<p>Add <code>Field</code> to the Schema's <code>Document</code>.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list</code> <p>A list of <code>Field</code> objects to be added to the <code>Document</code>.</p> <code>()</code> Example <pre><code>schema.add_fields([Field(name=\"title\", type=\"string\"), Field(name=\"body\", type=\"text\")])\nschema.add_fields([Field('title', 'string'), Field('body', 'text')])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Schema.add_field_set","title":"<code>add_field_set(field_set)</code>","text":"<p>Add a <code>FieldSet</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>field_set</code> <code>list</code> <p>A list of <code>FieldSet</code> objects to be added to the Schema.</p> required"},{"location":"api/vespa/package.html#vespa.package.Schema.add_rank_profile","title":"<code>add_rank_profile(rank_profile)</code>","text":"<p>Add a <code>RankProfile</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>rank_profile</code> <code>RankProfile</code> <p>The rank profile to be added to the Schema.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.Schema.add_model","title":"<code>add_model(model)</code>","text":"<p>Add an <code>OnnxModel</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>OnnxModel</code> <p>The ONNX model to be added to the Schema.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.Schema.add_imported_field","title":"<code>add_imported_field(imported_field)</code>","text":"<p>Add an <code>ImportedField</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>imported_field</code> <code>ImportedField</code> <p>The imported field to be added to the Schema.</p> required"},{"location":"api/vespa/package.html#vespa.package.Schema.add_document_summary","title":"<code>add_document_summary(document_summary)</code>","text":"<p>Add a <code>DocumentSummary</code> to the Schema.</p> <p>Parameters:</p> Name Type Description Default <code>document_summary</code> <code>DocumentSummary</code> <p>The document summary to be added to the Schema.</p> required"},{"location":"api/vespa/package.html#vespa.package.QueryTypeField","title":"<code>QueryTypeField(name, type)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a field to be included in a <code>QueryProfileType</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Field name.</p> required <code>type</code> <code>str</code> <p>Field type.</p> required Example <pre><code>QueryTypeField(\n    name=\"ranking.features.query(title_bert)\",\n    type=\"tensor&lt;float&gt;(x[768])\"\n)\nQueryTypeField('ranking.features.query(title_bert)', 'tensor&lt;float&gt;(x[768])')\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryProfileType","title":"<code>QueryProfileType(fields=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Query Profile Type.</p> <p>Check the Vespa documentation for more detailed information about query profile types.</p> <p>An <code>ApplicationPackage</code> instance comes with a default <code>QueryProfile</code> named <code>default</code> that is associated with a <code>QueryProfileType</code> named <code>root</code>, meaning that you usually do not need to create those yourself, only add fields to them when required.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list[QueryTypeField]</code> <p>A list of <code>QueryTypeField</code>.</p> <code>None</code> Example <pre><code>QueryProfileType(\n    fields=[\n        QueryTypeField(\n            name=\"ranking.features.query(tensor_bert)\",\n            type=\"tensor&lt;float&gt;(x[768])\"\n        )\n    ]\n)\n# Output: QueryProfileType([QueryTypeField('ranking.features.query(tensor_bert)', 'tensor&lt;float&gt;(x[768])')])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryProfileType.add_fields","title":"<code>add_fields(*fields)</code>","text":"<p>Add <code>QueryTypeField</code> objects to the Query Profile Type.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>QueryTypeField</code> <p>Fields to be added.</p> <code>()</code> Example <pre><code>query_profile_type = QueryProfileType()\nquery_profile_type.add_fields(\n    QueryTypeField(\n        name=\"age\",\n        type=\"integer\"\n    ),\n    QueryTypeField(\n        name=\"profession\",\n        type=\"string\"\n    )\n)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryField","title":"<code>QueryField(name, value)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a field to be included in a <code>QueryProfile</code>.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Field name.</p> required <code>value</code> <code>Any</code> <p>Field value.</p> required Example <pre><code>QueryField(name=\"maxHits\", value=1000)\n# Output: QueryField('maxHits', 1000)\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryProfile","title":"<code>QueryProfile(fields=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Query Profile.</p> <p>Check the Vespa documentation for more detailed information about query profiles.</p> <p>A <code>QueryProfile</code> is a named collection of query request parameters given in the configuration. The query request can specify a query profile whose parameters will be used as parameters of that request. The query profiles may optionally be type-checked. Type checking is turned on by referencing a <code>QueryProfileType</code> from the query profile.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>list[QueryField]</code> <p>A list of <code>QueryField</code>.</p> <code>None</code> Example <pre><code>QueryProfile(fields=[QueryField(name=\"maxHits\", value=1000)])\n# Output: QueryProfile([QueryField('maxHits', 1000)])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.QueryProfile.add_fields","title":"<code>add_fields(*fields)</code>","text":"<p>Add <code>QueryField</code> objects to the Query Profile.</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>QueryField</code> <p>Fields to be added.</p> <code>()</code> Example <pre><code>query_profile = QueryProfile()\nquery_profile.add_fields(QueryField(name=\"maxHits\", value=1000))\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ApplicationConfiguration","title":"<code>ApplicationConfiguration(name, value)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Schema.</p> <p>Check the Config documentation for more detailed information about generic configuration.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Configuration name.</p> required <code>value</code> <code>str | dict</code> <p>Either a string or a dictionary (which may be nested) of values.</p> required Example <pre><code>ApplicationConfiguration(\n    name=\"container.handler.observability.application-userdata\",\n    value={\"version\": \"my-version\"}\n)\n# Output: ApplicationConfiguration(name=\"container.handler.observability.application-userdata\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Parameter","title":"<code>Parameter(name, args=None, children=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa Component configuration parameter.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Parameter name.</p> required <code>args</code> <code>Any</code> <p>Parameter arguments.</p> <code>None</code> <code>children</code> <code>str | list[Parameter]</code> <p>Parameter children. Can be either a string or a list of <code>Parameter</code> objects for nested configs.</p> <code>None</code>"},{"location":"api/vespa/package.html#vespa.package.AuthClient","title":"<code>AuthClient(id, permissions, parameters=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a Vespa AuthClient.</p> <p>Check the Vespa documentation.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>The auth client ID.</p> required <code>permissions</code> <code>list[str]</code> <p>List of permissions.</p> required <code>parameters</code> <code>list[Parameter]</code> <p>List of <code>Parameter</code> objects defining the configuration of the auth client.</p> <code>None</code> Example <pre><code>AuthClient(\n    id=\"token\",\n    permissions=[\"read\", \"write\"],\n    parameters=[Parameter(\"token\", {\"id\": \"my-token-id\"})],\n)\n# Output: AuthClient(id=\"token\", permissions=\"['read', 'write']\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Component","title":"<code>Component(id, cls=None, bundle=None, type=None, parameters=None)</code>","text":"<p>               Bases: <code>object</code></p>"},{"location":"api/vespa/package.html#vespa.package.Nodes","title":"<code>Nodes(count='1', parameters=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Specify node resources for a content or container cluster as part of a <code>ContainerCluster</code> or <code>ContentCluster</code>.</p> <p>Parameters:</p> Name Type Description Default <code>count</code> <code>int</code> <p>Number of nodes in a cluster.</p> <code>'1'</code> <code>parameters</code> <code>list[Parameter]</code> <p>List of <code>Parameter</code> objects defining the configuration of the cluster resources.</p> <code>None</code> Example <pre><code>ContainerCluster(\n    id=\"example_container\",\n    nodes=Nodes(\n        count=\"2\",\n        parameters=[\n            Parameter(\n                \"resources\",\n                {\"vcpu\": \"4.0\", \"memory\": \"16Gb\", \"disk\": \"125Gb\"},\n                children=[Parameter(\"gpu\", {\"count\": \"1\", \"memory\": \"16Gb\"})]\n            ),\n            Parameter(\"node\", {\"hostalias\": \"node1\", \"distribution-key\": \"0\"}),\n        ]\n    )\n)\n# Output: ContainerCluster(id=\"example_container\", version=\"1.0\", nodes=\"Nodes(count='2')\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.Cluster","title":"<code>Cluster(id, version='1.0', nodes=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Base class for a cluster configuration. Should not be instantiated directly. Use subclasses <code>ContainerCluster</code> or <code>ContentCluster</code> instead.</p> <p>Parameters:</p> Name Type Description Default <code>id</code> <code>str</code> <p>Cluster ID.</p> required <code>version</code> <code>str</code> <p>Cluster version.</p> <code>'1.0'</code> <code>nodes</code> <code>Nodes</code> <p><code>Nodes</code> that specifies node resources.</p> <code>None</code>"},{"location":"api/vespa/package.html#vespa.package.Cluster.to_xml","title":"<code>to_xml(root)</code>","text":"<p>Set up XML elements that are used in both container and content clusters.</p>"},{"location":"api/vespa/package.html#vespa.package.ContainerCluster","title":"<code>ContainerCluster(id, version='1.0', nodes=None, components=None, auth_clients=None)</code>","text":"<p>               Bases: <code>Cluster</code></p> <p>Defines the configuration of a container cluster.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>list[Component]</code> <p>List of <code>Component</code> that contains configurations for application components, e.g. embedders.</p> <code>None</code> <code>auth_clients</code> <code>list[AuthClient]</code> <p>List of <code>AuthClient</code> that contains configurations for authentication clients (e.g., mTLS/token).</p> <code>None</code> <code>nodes</code> <code>Nodes</code> <p><code>Nodes</code> that specifies the resources of the cluster.</p> <code>None</code> <p>If <code>ContainerCluster</code> is used, any <code>Component</code>s must be added to the <code>ContainerCluster</code>, rather than to the <code>ApplicationPackage</code>, in order to be included in the generated schema.</p> Example <pre><code>ContainerCluster(\n    id=\"example_container\",\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"}\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"}\n                )\n            ]\n        )\n    ],\n    auth_clients=[AuthClient(id=\"mtls\", permissions=[\"read\", \"write\"])],\n    nodes=Nodes(count=\"2\", parameters=[Parameter(\"resources\", {\"vcpu\": \"4.0\", \"memory\": \"16Gb\", \"disk\": \"125Gb\"})])\n)\n# Output: ContainerCluster(id=\"example_container\", version=\"1.0\", nodes=\"Nodes(count='2')\", components=\"[Component(id='e5', type='hugging-face-embedder')]\", auth_clients=\"[AuthClient(id='mtls', permissions=['read', 'write'])]\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ContentCluster","title":"<code>ContentCluster(id, document_name, version='1.0', nodes=None, min_redundancy='1')</code>","text":"<p>               Bases: <code>Cluster</code></p> <p>Defines the configuration of a content cluster.</p> <p>Parameters:</p> Name Type Description Default <code>document_name</code> <code>str</code> <p>Name of document.</p> required <code>min_redundancy</code> <code>int</code> <p>Minimum redundancy of the content cluster. Must be at least 2 for production deployments.</p> <code>'1'</code> Example <pre><code>ContentCluster(id=\"example_content\", document_name=\"doc\")\n# Output: ContentCluster(id=\"example_content\", version=\"1.0\", document_name=\"doc\")\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ValidationID","title":"<code>ValidationID</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Collection of IDs that can be used in validation-overrides.xml.</p> <p>Taken from ValidationId.java.</p> <p><code>clusterSizeReduction</code> was not added as it will be removed in Vespa 9.</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.indexingChange","title":"<code>indexingChange = 'indexing-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing what tokens are expected and stored in field indexes</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.indexModeChange","title":"<code>indexModeChange = 'indexing-mode-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing the index mode (streaming, indexed, store-only) of documents</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.fieldTypeChange","title":"<code>fieldTypeChange = 'field-type-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Field type changes</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.tensorTypeChange","title":"<code>tensorTypeChange = 'tensor-type-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Tensor type change</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.resourcesReduction","title":"<code>resourcesReduction = 'resources-reduction'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Large reductions in node resources (&gt; 50% of the current max total resources)</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.contentTypeRemoval","title":"<code>contentTypeRemoval = 'schema-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Removal of a schema (causes deletion of all documents)</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.contentClusterRemoval","title":"<code>contentClusterRemoval = 'content-cluster-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Removal (or id change) of content clusters</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.deploymentRemoval","title":"<code>deploymentRemoval = 'deployment-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Removal of production zones from deployment.xml</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.globalDocumentChange","title":"<code>globalDocumentChange = 'global-document-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing global attribute for document types in content clusters</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.configModelVersionMismatch","title":"<code>configModelVersionMismatch = 'config-model-version-mismatch'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Internal use</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.skipOldConfigModels","title":"<code>skipOldConfigModels = 'skip-old-config-models'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Internal use</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.accessControl","title":"<code>accessControl = 'access-control'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Internal use, used in zones where there should be no access-control</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.globalEndpointChange","title":"<code>globalEndpointChange = 'global-endpoint-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing global endpoints</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.zoneEndpointChange","title":"<code>zoneEndpointChange = 'zone-endpoint-change'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Changing zone (possibly private) endpoint settings</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.redundancyIncrease","title":"<code>redundancyIncrease = 'redundancy-increase'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Increasing redundancy - may easily cause feed blocked</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.redundancyOne","title":"<code>redundancyOne = 'redundancy-one'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>redundancy=1 requires a validation override on first deployment</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.pagedSettingRemoval","title":"<code>pagedSettingRemoval = 'paged-setting-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>May cause content nodes to run out of memory</p>"},{"location":"api/vespa/package.html#vespa.package.ValidationID.certificateRemoval","title":"<code>certificateRemoval = 'certificate-removal'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Remove data plane certificates</p>"},{"location":"api/vespa/package.html#vespa.package.Validation","title":"<code>Validation(validation_id, until, comment=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Represents a validation to be overridden on application.</p> <p>Check the Vespa documentation for more detailed information about validations.</p> <p>Parameters:</p> Name Type Description Default <code>validation_id</code> <code>str</code> <p>ID of the validation.</p> required <code>until</code> <code>str</code> <p>The last day this change is allowed, as an ISO-8601-format date in UTC, e.g. 2016-01-30.         Dates may at most be 30 days in the future, but should be as close to now as possible for safety,         while allowing time for review and propagation to all deployed zones. <code>allow-tags</code> with dates in the past are ignored.</p> required <code>comment</code> <code>str</code> <p>Optional text explaining the reason for the change to humans.</p> <code>None</code>"},{"location":"api/vespa/package.html#vespa.package.DeploymentConfiguration","title":"<code>DeploymentConfiguration(environment, regions)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a DeploymentConfiguration, which defines how to generate a deployment.xml file (for use in production deployments).</p> <p>Parameters:</p> Name Type Description Default <code>environment</code> <code>str</code> <p>The environment to deploy to. Currently, only 'prod' is supported.</p> required <code>regions</code> <code>list[str]</code> <p>List of regions to deploy to, e.g. [\"us-east-1\", \"us-west-1\"].                 See Vespa documentation for more information.</p> required Example <pre><code>DeploymentConfiguration(environment=\"prod\", regions=[\"us-east-1\", \"us-west-1\"])\n# Output: DeploymentConfiguration(environment='prod', regions=['us-east-1', 'us-west-1'])\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.EmptyDeploymentConfiguration","title":"<code>EmptyDeploymentConfiguration()</code>","text":"<p>               Bases: <code>DeploymentConfiguration</code></p> <p>Create an EmptyDeploymentConfiguration, which creates an empty deployment.xml, used to delete production deployments.</p>"},{"location":"api/vespa/package.html#vespa.package.ServicesConfiguration","title":"<code>ServicesConfiguration(application_name, schemas=None, configurations=[], stateless_model_evaluation=False, components=[], auth_clients=[], clusters=[], services_config=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create a ServicesConfiguration, adopting the VespaTag (VT) approach, rather than Jinja templates. Intended to be used in ApplicationPackage, to generate services.xml, based on either: - A passed <code>services_config</code> (VT) object, or - A set of configurations, schemas, components, auth_clients, and clusters (equivalent to the old approach).</p> <p>The latter will be done in code by calling <code>build_services_vt()</code> to generate the VT object.</p> <p>Parameters:</p> Name Type Description Default <code>application_name</code> <code>str</code> <p>Application name.</p> required <code>schemas</code> <code>Optional[List[Schema]]</code> <p>List of <code>Schema</code>s of the application.</p> <code>None</code> <code>configurations</code> <code>Optional[List[ApplicationConfiguration]]</code> <p>List of <code>ApplicationConfiguration</code> that contains configurations for the application.</p> <code>[]</code> <code>stateless_model_evaluation</code> <code>Optional[bool]</code> <p>Enable stateless model evaluation. Default is False.</p> <code>False</code> <code>components</code> <code>Optional[List[Component]]</code> <p>List of <code>Component</code> that contains configurations for application components.</p> <code>[]</code> <code>auth_clients</code> <code>Optional[List[AuthClient]]</code> <p>List of <code>AuthClient</code> that contains configurations for authentication clients.</p> <code>[]</code> <code>clusters</code> <code>Optional[List[Cluster]]</code> <p>List of <code>Cluster</code> that contains configurations for content or container clusters.</p> <code>[]</code> <code>services_config</code> <code>Optional[VT]</code> <p><code>VT</code> object that contains the services configuration.</p> <code>None</code> Example <pre><code>config = ServicesConfiguration(\n    application_name=\"myapp\",\n    schemas=[Schema(name=\"myschema\", document=Document())],\n    configurations=[ApplicationConfiguration(name=\"container.handler.observability.application-userdata\", value={\"version\": \"my-version\"})],\n    components=[Component(id=\"hf-embedder\", type=\"huggingface-embedder\")],\n    stateless_model_evaluation=True,\n)\nprint(str(config))\n# Output: &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n# &lt;services version=\"1.0\"&gt;...&lt;/services&gt;\n\nservices_config = ServicesConfiguration(\n    application_name=\"myapp\",\n    services_config=services(\n        container(id=\"myapp_default\", version=\"1.0\")(\n            component(\n                model(url=\"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"),\n                id=\"tokenizer\", type=\"hugging-face-tokenizer\"\n            ),\n            document_api(),\n            search(),\n        ),\n        content(id=\"myapp\", version=\"1.0\")(\n            min_redundancy(\"1\"),\n            documents(document(type=\"doc\", mode=\"index\")),\n            engine(proton(tuning(searchnode(requestthreads(persearch(\"4\"))))))\n        ),\n        version=\"1.0\", minimum_required_vespa_version=\"8.311.28\",\n    ),\n)\nprint(str(services_config))\n# Output: &lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\n# &lt;services version=\"1.0\" minimum-required-vespa-version=\"8.311.28\"&gt;...&lt;/services&gt;\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage","title":"<code>ApplicationPackage(name, schema=None, query_profile=None, query_profile_type=None, stateless_model_evaluation=False, create_schema_by_default=True, create_query_profile_by_default=True, configurations=None, validations=None, components=None, auth_clients=None, clusters=None, deployment_config=None, services_config=None, query_profile_config=None)</code>","text":"<p>               Bases: <code>object</code></p> <p>Create an application package.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Application name. Cannot contain '-' or '_'.</p> required <code>schema</code> <code>list</code> <p>List of Schema objects for the application. If None, a default Schema with the same name as the application will be created. Defaults to None.</p> <code>None</code> <code>query_profile</code> <code>QueryProfile</code> <p>QueryProfile of the application. If None, a default QueryProfile with QueryProfileType 'root' will be created. Defaults to None.</p> <code>None</code> <code>query_profile_type</code> <code>QueryProfileType</code> <p>QueryProfileType of the application. If None, a default QueryProfileType 'root' will be created. Defaults to None.</p> <code>None</code> <code>stateless_model_evaluation</code> <code>bool</code> <p>Enable stateless model evaluation. Defaults to False.</p> <code>False</code> <code>create_schema_by_default</code> <code>bool</code> <p>Include a default Schema if none is provided in the schema argument. Defaults to True.</p> <code>True</code> <code>create_query_profile_by_default</code> <code>bool</code> <p>Include a default QueryProfile and QueryProfileType if not explicitly defined by the user. Defaults to True.</p> <code>True</code> <code>configurations</code> <code>list</code> <p>List of ApplicationConfiguration for the application. Defaults to None.</p> <code>None</code> <code>validations</code> <code>list</code> <p>Optional list of Validation objects to be overridden. Defaults to None.</p> <code>None</code> <code>components</code> <code>list</code> <p>List of Component objects for application components. Defaults to None.</p> <code>None</code> <code>clusters</code> <code>list</code> <p>List of Cluster objects for content or container clusters. If clusters is provided, any Component must be part of a cluster. Defaults to None.</p> <code>None</code> <code>auth_clients</code> <code>list</code> <p>List of AuthClient objects for client authorization. If clusters is passed, pass the auth clients to the ContainerCluster instead. Defaults to None.</p> <code>None</code> <code>deployment_config</code> <code>Union[DeploymentConfiguration, VT]</code> <p>Deployment configuration for the application. Must be either a DeploymentConfiguration object (legacy) or a VT (Vespa Tag) based deployment configuration whose top-level tag must be <code>deployment</code>. Defaults to None.</p> <code>None</code> <code>services_config</code> <code>ServicesConfiguration</code> <p>(Optional) Services configuration for the application. For advanced configuration.  See https://vespa-engine.github.io/pyvespa/advanced-configuration.html</p> <code>None</code> <code>query_profile_config</code> <code>Union[VT, List[VT]]</code> <p>Configuration for query profiles. If provided, will override the query_profile and query_profile_type arguments. Defaults to None. See See https://vespa-engine.github.io/pyvespa/advanced-configuration.html</p> <code>None</code> <p>Example:     To create a default application package:</p> <pre><code>```python\nApplicationPackage(name=\"testapp\")\nApplicationPackage('testapp', [Schema('testapp', Document(None, None, None), None, None, [], False, None, [], None)],\n                QueryProfile(None), QueryProfileType(None))\n```\n</code></pre> <p>This creates a default Schema, QueryProfile, and QueryProfileType, which can be populated with your application's specifics.</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.services_to_text","title":"<code>services_to_text</code>  <code>property</code>","text":"<p>Intention is to only use services_config, but keeping this until 100% compatibility is achieved through tests.</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.add_schema","title":"<code>add_schema(*schemas)</code>","text":"<p>Add Schema's to the application package.</p> <p>Parameters:</p> Name Type Description Default <code>schemas</code> <code>list</code> <p>Schemas to be added.</p> <code>()</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.add_query_profile","title":"<code>add_query_profile(query_profile_item)</code>","text":"<p>Add a query profile item (query-profile or query-profile-type) to the application package.</p> <p>Parameters:</p> Name Type Description Default <code>query_profile_item</code> <code>VT or List[VT]</code> <p>Query profile item(s) to be added.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Example <pre><code>app_package = ApplicationPackage(name=\"testapp\")\nqp = query_profile(\n    field(30, name=\"hits\"),\n    field(3, name=\"trace.level\"),\n)\napp_package.add_query_profile(\n    qp\n)\n# Query profile item is added to the application package.\n# inspect with `app_package.query_profile_config`\n</code></pre>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.to_zip","title":"<code>to_zip()</code>","text":"<p>Return the application package as zipped bytes, to be used in a subsequent deploy.</p> <p>Returns:</p> Name Type Description <code>BytesIO</code> <code>BytesIO</code> <p>A buffer containing the zipped application package.</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.to_zipfile","title":"<code>to_zipfile(zfile)</code>","text":"<p>Export the application package as a deployable zipfile. See application packages for deployment options.</p> <p>Parameters:</p> Name Type Description Default <code>zfile</code> <code>str</code> <p>Filename to export to.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.ApplicationPackage.to_files","title":"<code>to_files(root)</code>","text":"<p>Export the application package as a directory tree.</p> <p>Parameters:</p> Name Type Description Default <code>root</code> <code>str</code> <p>Directory to export files to.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p>"},{"location":"api/vespa/package.html#vespa.package.validate_services","title":"<code>validate_services(xml_input)</code>","text":"<p>Validate an XML input against the RelaxNG schema file for services.xml</p> <p>Parameters:</p> Name Type Description Default <code>xml_input</code> <code>Path or str or Element</code> <p>The XML input to validate.</p> required <p>Returns:     True if the XML input is valid according to the RelaxNG schema, False otherwise.</p>"},{"location":"api/vespa/querybuilder/index.html","title":"Index","text":""},{"location":"api/vespa/querybuilder/index.html#vespa.querybuilder","title":"<code>vespa.querybuilder</code>","text":""},{"location":"api/vespa/querybuilder/builder/index.html","title":"Index","text":""},{"location":"api/vespa/querybuilder/builder/index.html#vespa.querybuilder.builder","title":"<code>vespa.querybuilder.builder</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html","title":"Querybuilder","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder","title":"<code>vespa.querybuilder.builder.builder</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.QueryField","title":"<code>QueryField(name)</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Condition","title":"<code>Condition(expression)</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Condition.all","title":"<code>all(*conditions)</code>  <code>classmethod</code>","text":"<p>Combine multiple conditions using logical AND.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Condition.any","title":"<code>any(*conditions)</code>  <code>classmethod</code>","text":"<p>Combine multiple conditions using logical OR.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query","title":"<code>Query(select_fields, prepend_yql=False)</code>","text":""},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.from_","title":"<code>from_(*sources)</code>","text":"<p>Specify the source schema(s) to query.</p> Example <pre><code>import vespa.querybuilder as qb\nfrom vespa.package import Schema, Document\n\nquery = qb.select(\"*\").from_(\"schema1\", \"schema2\")\nstr(query)\n'select * from schema1, schema2'\nquery = qb.select(\"*\").from_(Schema(name=\"schema1\", document=Document()), Schema(name=\"schema2\", document=Document()))\nstr(query)\n'select * from schema1, schema2'\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>Union[str, Schema]</code> <p>The source schema(s) to query.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>The Query object.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.where","title":"<code>where(condition)</code>","text":"<p>Adds a where clause to filter query results.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#where</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Union[Condition, bool]</code> <p>Filter condition that can be: - Condition object for complex queries - Boolean for simple true/false - QueryField for field-based filters</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <p><pre><code>import vespa.querybuilder as qb\n\n# Using field conditions\nf1 = qb.QueryField(\"f1\")\nquery = qb.select(\"*\").from_(\"sd1\").where(f1.contains(\"v1\"))\nstr(query)\n'select * from sd1 where f1 contains \"v1\"'\n</code></pre> <pre><code># Using boolean\nquery = qb.select(\"*\").from_(\"sd1\").where(True)\nstr(query)\n'select * from sd1 where true'\n</code></pre> <pre><code># Using complex conditions\ncondition = f1.contains(\"v1\") &amp; qb.QueryField(\"f2\").contains(\"v2\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where f1 contains \"v1\" and f2 contains \"v2\"'\n</code></pre></p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.order_by","title":"<code>order_by(field, ascending=True, annotations=None)</code>","text":"<p>Orders results by specified fields.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#order-by</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <p>Field names or QueryField objects to order by</p> required <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations like \"locale\", \"strength\", etc. See https://docs.vespa.ai/en/reference/sorting.html#special-sorting-attributes for details.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\n# Simple ordering\nquery = qb.select(\"*\").from_(\"sd1\").order_by(\"price\")\nstr(query)\n'select * from sd1 order by price asc'\n\n# Multiple fields with annotation\nquery = qb.select(\"*\").from_(\"sd1\").order_by(\n    \"price\", annotations={\"locale\": \"en_US\"}, ascending=False\n).order_by(\"name\", annotations={\"locale\": \"no_NO\"}, ascending=True)\nstr(query)\n'select * from sd1 order by {\"locale\":\"en_US\"}price desc, {\"locale\":\"no_NO\"}name asc'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.orderByAsc","title":"<code>orderByAsc(field, annotations=None)</code>","text":"<p>Convenience method for ordering results by a field in ascending order. See <code>order_by</code> for more information.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.orderByDesc","title":"<code>orderByDesc(field, annotations=None)</code>","text":"<p>Convenience method for ordering results by a field in descending order. See <code>order_by</code> for more information.</p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.set_limit","title":"<code>set_limit(limit)</code>","text":"<p>Sets maximum number of results to return.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#limit-offset</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of hits to return</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1 = qb.QueryField(\"f1\")\nquery = qb.select(\"*\").from_(\"sd1\").where(f1.contains(\"v1\")).set_limit(5)\nstr(query)\n'select * from sd1 where f1 contains \"v1\" limit 5'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.set_offset","title":"<code>set_offset(offset)</code>","text":"<p>Sets number of initial results to skip for pagination.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#limit-offset</p> <p>Parameters:</p> Name Type Description Default <code>offset</code> <code>int</code> <p>Number of results to skip</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1 = qb.QueryField(\"f1\")\nquery = qb.select(\"*\").from_(\"sd1\").where(f1.contains(\"v1\")).set_offset(10)\nstr(query)\n'select * from sd1 where f1 contains \"v1\" offset 10'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.set_timeout","title":"<code>set_timeout(timeout)</code>","text":"<p>Sets query timeout in milliseconds.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#timeout</p> <p>Parameters:</p> Name Type Description Default <code>timeout</code> <code>int</code> <p>Timeout in milliseconds</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1 = qb.QueryField(\"f1\")\nquery = qb.select(\"*\").from_(\"sd1\").where(f1.contains(\"v1\")).set_timeout(500)\nstr(query)\n'select * from sd1 where f1 contains \"v1\" timeout 500'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.add_parameter","title":"<code>add_parameter(key, value)</code>","text":"<p>Adds a query parameter.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#parameter-substitution</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>Any</code> <p>Parameter value</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.userInput(\"@myvar\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).add_parameter(\"myvar\", \"test\")\nstr(query)\n'select * from sd1 where userInput(@myvar)&amp;myvar=test'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.param","title":"<code>param(key, value)</code>","text":"<p>Alias for add_parameter().</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#parameter-substitution</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Parameter name</p> required <code>value</code> <code>Any</code> <p>Parameter value</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>Self for method chaining</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.userInput(\"@animal\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).param(\"animal\", \"panda\")\nstr(query)\n'select * from sd1 where userInput(@animal)&amp;animal=panda'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Query.groupby","title":"<code>groupby(group_expression, continuations=[])</code>","text":"<p>Groups results by specified expression.</p> <p>For more information, see https://docs.vespa.ai/en/grouping.html</p> <p>Also see  for available methods to build group expressions. <p>Parameters:</p> Name Type Description Default <code>group_expression</code> <code>str</code> <p>Grouping expression</p> required <code>continuations</code> <code>List</code> <p>List of continuation tokens (see https://docs.vespa.ai/en/grouping.html#pagination)</p> <code>[]</code> <p>Returns:</p> Type Description <code>Query</code> <p>: Self for method chaining Example <pre><code>import vespa.querybuilder as qb\nfrom vespa.querybuilder import Grouping as G\n\n# Group by customer with sum of price\ngrouping = G.all(\n    G.group(\"customer\"),\n    G.each(G.output(G.sum(\"price\"))),\n)\nstr(grouping)\n'all(group(customer) each(output(sum(price))))'\nquery = qb.select(\"*\").from_(\"sd1\").groupby(grouping)\nstr(query)\n'select * from sd1 | all(group(customer) each(output(sum(price))))'\n\n# Group by year with count\ngrouping = G.all(\n    G.group(\"time.year(a)\"),\n    G.each(G.output(G.count())),\n)\nstr(grouping)\n'all(group(time.year(a)) each(output(count())))'\nquery = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping)\nstr(query)\n'select * from purchase where true | all(group(time.year(a)) each(output(count())))'\n# With continuations\nquery = qb.select(\"*\").from_(\"purchase\").where(True).groupby(grouping, continuations=[\"foo\", \"bar\"])\nstr(query)\n\"select * from purchase where true | { 'continuations':['foo', 'bar'] }all(group(time.year(a)) each(output(count())))\"\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q","title":"<code>Q</code>","text":"<p>Wrapper class for QueryBuilder static methods. Methods are exposed as module-level functions. To use:     <pre><code>import vespa.querybuilder as qb\n\nquery = qb.select(\"*\").from_(\"sd1\") # or any of the other Q class methods\n</code></pre></p>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.select","title":"<code>select(fields)</code>  <code>staticmethod</code>","text":"<p>Creates a new query selecting specified fields.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#select</p> <p>Parameters:</p> Name Type Description Default <code>fields</code> <code>Union[str, List[str], List[QueryField]]</code> <p>Field names or QueryField objects to select</p> required <p>Returns:</p> Name Type Description <code>Query</code> <code>Query</code> <p>New query object</p> Example <pre><code>import vespa.querybuilder as qb\n\nquery = qb.select(\"*\").from_(\"sd1\")\nstr(query)\n'select * from sd1'\n\nquery = qb.select([\"title\", \"url\"])\nstr(query)\n'select title, url from *'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.any","title":"<code>any(*conditions)</code>  <code>staticmethod</code>","text":"<p>\"Combines multiple conditions with OR operator.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#or</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>Condition</code> <p>Variable number of Condition objects to combine with OR</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>Combined condition using OR operators</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1, f2 = qb.QueryField(\"f1\"), qb.QueryField(\"f2\")\ncondition = qb.any(f1 &gt; 10, f2 == \"v2\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where f1 &gt; 10 or f2 = \"v2\"'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.all","title":"<code>all(*conditions)</code>  <code>staticmethod</code>","text":"<p>Combines multiple conditions with AND operator.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#and</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>Condition</code> <p>Variable number of Condition objects to combine with AND</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>Combined condition using AND operators</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1, f2 = qb.QueryField(\"f1\"), qb.QueryField(\"f2\")\ncondition = qb.all(f1 &gt; 10, f2 == \"v2\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where f1 &gt; 10 and f2 = \"v2\"'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.userQuery","title":"<code>userQuery(value='')</code>  <code>staticmethod</code>","text":"<p>Creates a userQuery operator for text search.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#userquery</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>Optional query string. Default is empty string.</p> <code>''</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A userQuery condition</p> Example <pre><code>import vespa.querybuilder as qb\n\n# Basic userQuery\ncondition = qb.userQuery()\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where userQuery()'\n\n# UserQuery with search terms\ncondition = qb.userQuery(\"search terms\")\nquery = qb.select(\"*\").from_(\"documents\").where(condition)\nstr(query)\n'select * from documents where userQuery(\"search terms\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.dotProduct","title":"<code>dotProduct(field, weights, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a dot product calculation condition.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#dotproduct.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field containing vectors</p> required <code>weights</code> <code>Union[List[float], Dict[str, float], str]</code> <p>Either list of numeric weights or dict mapping elements to weights or a parameter substitution string starting with '@'</p> required <code>annotations</code> <code>Optional[Dict]</code> <p>Optional modifiers like label</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A dot product calculation condition</p> Example <pre><code>import vespa.querybuilder as qb\n\n# Using dict weights with annotation\ncondition = qb.dotProduct(\n    \"weightedset_field\",\n    {\"feature1\": 1, \"feature2\": 2},\n    annotations={\"label\": \"myDotProduct\"}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({label:\"myDotProduct\"}dotProduct(weightedset_field, {\"feature1\": 1, \"feature2\": 2}))'\n\n# Using list weights\ncondition = qb.dotProduct(\"weightedset_field\", [0.4, 0.6])\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where dotProduct(weightedset_field, [0.4, 0.6])'\n\n# Using parameter substitution\ncondition = qb.dotProduct(\"weightedset_field\", \"@myweights\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).add_parameter(\"myweights\", [0.4, 0.6])\nstr(query)\n'select * from sd1 where dotProduct(weightedset_field, \"@myweights\")&amp;myweights=[0.4, 0.6]'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.weightedSet","title":"<code>weightedSet(field, weights, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a weighted set condition.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#weightedset.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field containing weighted set data</p> required <code>weights</code> <code>Union[List[float], Dict[str, float], str]</code> <p>Either list of numeric weights or dict mapping elements to weights or a parameter substitution string starting with '@'</p> required <code>annotations</code> <code>Optional[Dict]</code> <p>Optional annotations like targetNumHits</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A weighted set condition</p> Example <pre><code>import vespa.querybuilder as qb\n\n# using map weights\ncondition = qb.weightedSet(\n    \"weightedset_field\",\n    {\"element1\": 1, \"element2\": 2},\n    annotations={\"targetNumHits\": 10}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({targetNumHits:10}weightedSet(weightedset_field, {\"element1\": 1, \"element2\": 2}))'\n\n# using list weights\ncondition = qb.weightedSet(\"weightedset_field\", [0.4, 0.6])\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where weightedSet(weightedset_field, [0.4, 0.6])'\n\n# using parameter substitution\ncondition = qb.weightedSet(\"weightedset_field\", \"@myweights\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).add_parameter(\"myweights\", [0.4, 0.6])\nstr(query)\n'select * from sd1 where weightedSet(weightedset_field, \"@myweights\")&amp;myweights=[0.4, 0.6]'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.nonEmpty","title":"<code>nonEmpty(condition)</code>  <code>staticmethod</code>","text":"<p>Creates a nonEmpty operator to check if a field has content.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#nonempty.</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Union[Condition, QueryField]</code> <p>Field or condition to check</p> required <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A nonEmpty condition</p> Example <pre><code>import vespa.querybuilder as qb\n\nfield = qb.QueryField(\"title\")\ncondition = qb.nonEmpty(field)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where nonEmpty(title)'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.wand","title":"<code>wand(field, weights, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a Weighted AND (WAND) operator for efficient top-k retrieval.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#wand.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field name to search</p> required <code>weights</code> <code>Union[List[float], Dict[str, float], str]</code> <p>Either list of numeric weights or dict mapping terms to weights or a parameter substitution string starting with '@'</p> required <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations like targetHits</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A WAND condition</p> Example <pre><code>import vespa.querybuilder as qb\n\n# Using list weights\ncondition = qb.wand(\"description\", weights=[0.4, 0.6])\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where wand(description, [0.4, 0.6])'\n\n# Using dict weights with annotation\nweights = {\"hello\": 0.3, \"world\": 0.7}\ncondition = qb.wand(\n    \"title\",\n    weights,\n    annotations={\"targetHits\": 100}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({targetHits: 100}wand(title, {\"hello\": 0.3, \"world\": 0.7}))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.weakAnd","title":"<code>weakAnd(*conditions, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a weakAnd operator for less strict AND matching.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#weakand.</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>Condition</code> <p>Variable number of conditions to combine</p> <code>()</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations like targetHits</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A weakAnd condition</p> Example <pre><code>import vespa.querybuilder as qb\n\nf1, f2 = qb.QueryField(\"f1\"), qb.QueryField(\"f2\")\ncondition = qb.weakAnd(f1 == \"v1\", f2 == \"v2\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where weakAnd(f1 = \"v1\", f2 = \"v2\")'\n\n# With annotation\ncondition = qb.weakAnd(\n    f1 == \"v1\",\n    f2 == \"v2\",\n    annotations={\"targetHits\": 100}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({\"targetHits\": 100}weakAnd(f1 = \"v1\", f2 = \"v2\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.geoLocation","title":"<code>geoLocation(field, lat, lng, radius, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a geolocation search condition.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#geolocation.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field containing location data</p> required <code>lat</code> <code>float</code> <p>Latitude coordinate</p> required <code>lon</code> <code>float</code> <p>Longitude coordinate</p> required <code>radius</code> <code>str</code> <p>Search radius (e.g. \"10km\")</p> required <code>annotations</code> <code>Optional[Dict]</code> <p>Optional settings like targetHits</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A geolocation search condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.geoLocation(\n    \"location_field\",\n    37.7749,\n    -122.4194,\n    \"10km\",\n    annotations={\"targetHits\": 100}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({targetHits:100}geoLocation(location_field, 37.7749, -122.4194, \"10km\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.nearestNeighbor","title":"<code>nearestNeighbor(field, query_vector, annotations={'targetHits': 100})</code>  <code>staticmethod</code>","text":"<p>Creates a nearest neighbor search condition.</p> <p>See https://docs.vespa.ai/en/reference/query-language-reference.html#nearestneighbor for more information.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Vector field to search in</p> required <code>query_vector</code> <code>str</code> <p>Query vector to compare against</p> required <code>annotations</code> <code>Dict[str, Any]</code> <p>Optional annotations to modify the behavior. Required annotation: targetHits (default: 10)</p> <code>{'targetHits': 100}</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A nearest neighbor search condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.nearestNeighbor(\n    field=\"dense_rep\",\n    query_vector=\"q_dense\",\n)\nquery = qb.select([\"id, text\"]).from_(\"m\").where(condition)\nstr(query)\n'select id, text from m where ({targetHits:100}nearestNeighbor(dense_rep, q_dense))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.rank","title":"<code>rank(*queries)</code>  <code>staticmethod</code>","text":"<p>Creates a rank condition for combining multiple queries.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#rank</p> <p>Parameters:</p> Name Type Description Default <code>*queries</code> <p>Variable number of Query objects to combine</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A rank condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.rank(\n    qb.nearestNeighbor(\"field\", \"queryVector\"),\n    qb.QueryField(\"a\").contains(\"A\"),\n    qb.QueryField(\"b\").contains(\"B\"),\n    qb.QueryField(\"c\").contains(\"C\"),\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where rank(({targetHits:100}nearestNeighbor(field, queryVector)), a contains \"A\", b contains \"B\", c contains \"C\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.phrase","title":"<code>phrase(*terms, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a phrase search operator for exact phrase matching.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#phrase</p> <p>Parameters:</p> Name Type Description Default <code>*terms</code> <code>str</code> <p>Terms that make up the phrase</p> <code>()</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A phrase condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.phrase(\"new\", \"york\", \"city\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where phrase(\"new\", \"york\", \"city\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.near","title":"<code>near(*terms, distance=None, annotations=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Creates a near search operator for finding terms within a specified distance.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#near</p> <p>Parameters:</p> Name Type Description Default <code>*terms</code> <code>str</code> <p>Terms to search for</p> <code>()</code> <code>distance</code> <code>Optional[int]</code> <p>Maximum word distance between terms. Will default to 2 if not specified.</p> <code>None</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> <code>None</code> <code>**kwargs</code> <p>Additional annotations</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A near condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.near(\"machine\", \"learning\", distance=5)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({distance:5}near(\"machine\", \"learning\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.onear","title":"<code>onear(*terms, distance=None, annotations=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Creates an ordered near operator for ordered proximity search.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#onear</p> <p>Parameters:</p> Name Type Description Default <code>*terms</code> <code>str</code> <p>Terms to search for in order</p> <code>()</code> <code>distance</code> <code>Optional[int]</code> <p>Maximum word distance between terms. Will default to 2 if not specified.</p> <code>None</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>An onear condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.onear(\"deep\", \"learning\", distance=3)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({distance:3}onear(\"deep\", \"learning\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.sameElement","title":"<code>sameElement(*conditions)</code>  <code>staticmethod</code>","text":"<p>Creates a sameElement operator to match conditions in same array element.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#sameelement</p> <p>Parameters:</p> Name Type Description Default <code>*conditions</code> <code>Condition</code> <p>Conditions that must match in same element</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A sameElement condition</p> Example <pre><code>import vespa.querybuilder as qb\n\npersons = qb.QueryField(\"persons\")\nfirst_name = qb.QueryField(\"first_name\")\nlast_name = qb.QueryField(\"last_name\")\nyear_of_birth = qb.QueryField(\"year_of_birth\")\ncondition = persons.contains(\n    qb.sameElement(\n        first_name.contains(\"Joe\"),\n        last_name.contains(\"Smith\"),\n        year_of_birth &lt; 1940,\n    )\n )\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where persons contains sameElement(first_name contains \"Joe\", last_name contains \"Smith\", year_of_birth &lt; 1940)'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.equiv","title":"<code>equiv(*terms)</code>  <code>staticmethod</code>","text":"<p>Creates an equiv operator for matching equivalent terms.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#equiv</p> <p>Parameters:</p> Name Type Description Default <code>terms</code> <code>List[str]</code> <p>List of equivalent terms</p> <code>()</code> <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> required <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>An equiv condition</p> Example <pre><code>import vespa.querybuilder as qb\n\nfieldName = qb.QueryField(\"fieldName\")\ncondition = fieldName.contains(qb.equiv(\"Snoop Dogg\", \"Calvin Broadus\"))\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where fieldName contains equiv(\"Snoop Dogg\", \"Calvin Broadus\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.uri","title":"<code>uri(value, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a uri operator for matching URIs.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#uri</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Field name containing URI</p> required <code>value</code> <code>str</code> <p>URI value to match</p> required <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A uri condition</p> Example <pre><code>import vespa.querybuilder as qb\n\nurl = \"vespa.ai/foo\"\ncondition = qb.uri(url)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where uri(\"vespa.ai/foo\")'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.fuzzy","title":"<code>fuzzy(value, annotations=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Creates a fuzzy operator for approximate string matching.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#fuzzy</p> <p>Parameters:</p> Name Type Description Default <code>term</code> <code>str</code> <p>Term to fuzzy match</p> required <code>annotations</code> <code>Optional[Dict[str, Any]]</code> <p>Optional annotations</p> <code>None</code> <code>**kwargs</code> <p>Optional parameters like maxEditDistance, prefixLength, etc.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A fuzzy condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.fuzzy(\"parantesis\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where fuzzy(\"parantesis\")'\n\n# With annotation\ncondition = qb.fuzzy(\"parantesis\", annotations={\"prefixLength\": 1, \"maxEditDistance\": 2})\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where ({prefixLength:1,maxEditDistance:2}fuzzy(\"parantesis\"))'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.userInput","title":"<code>userInput(value=None, annotations=None)</code>  <code>staticmethod</code>","text":"<p>Creates a userInput operator for query evaluation.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#userinput.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Optional[str]</code> <p>The input variable name, e.g. \"@myvar\"</p> <code>None</code> <code>annotations</code> <code>Optional[Dict]</code> <p>Optional annotations to modify the behavior</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A condition representing the userInput operator</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.userInput(\"@myvar\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where userInput(@myvar)'\n\n# With defaultIndex annotation\ncondition = qb.userInput(\"@myvar\").annotate({\"defaultIndex\": \"text\"})\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where {defaultIndex:\"text\"}userInput(@myvar)'\n\n# With parameter\ncondition = qb.userInput(\"@animal\")\nquery = qb.select(\"*\").from_(\"sd1\").where(condition).param(\"animal\", \"panda\")\nstr(query)\n'select * from sd1 where userInput(@animal)&amp;animal=panda'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.predicate","title":"<code>predicate(field, attributes=None, range_attributes=None)</code>  <code>staticmethod</code>","text":"<p>Creates a predicate condition for filtering documents based on specific attributes.</p> <p>For more information, see https://docs.vespa.ai/en/reference/query-language-reference.html#predicate.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>The predicate field name</p> required <code>attributes</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of attribute key-value pairs</p> <code>None</code> <code>range_attributes</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary of range attribute key-value pairs</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A condition representing the predicate operation</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.predicate(\n    \"predicate_field\",\n    attributes={\"gender\": \"Female\"},\n    range_attributes={\"age\": \"20L\"}\n)\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where predicate(predicate_field,{\"gender\":\"Female\"},{\"age\":20L})'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.true","title":"<code>true()</code>  <code>staticmethod</code>","text":"<p>Creates a condition that is always true.</p> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A true condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.true()\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where true'\n</code></pre>"},{"location":"api/vespa/querybuilder/builder/builder.html#vespa.querybuilder.builder.builder.Q.false","title":"<code>false()</code>  <code>staticmethod</code>","text":"<p>Creates a condition that is always false.</p> <p>Returns:</p> Name Type Description <code>Condition</code> <code>Condition</code> <p>A false condition</p> Example <pre><code>import vespa.querybuilder as qb\n\ncondition = qb.false()\nquery = qb.select(\"*\").from_(\"sd1\").where(condition)\nstr(query)\n'select * from sd1 where false'\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/index.html","title":"Index","text":""},{"location":"api/vespa/querybuilder/grouping/index.html#vespa.querybuilder.grouping","title":"<code>vespa.querybuilder.grouping</code>","text":""},{"location":"api/vespa/querybuilder/grouping/grouping.html","title":"Grouping","text":""},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping","title":"<code>vespa.querybuilder.grouping.grouping</code>","text":""},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping","title":"<code>Grouping</code>","text":"<p>A Pythonic DSL for building Vespa grouping expressions programmatically.</p> <p>This class provides a set of static methods that build grouping syntax strings which can be combined to form a valid Vespa \u201cselect=\u2026\u201d grouping expression.</p> <p>For a guide to grouping in vespa, see https://docs.vespa.ai/en/grouping.html. For the reference docs, see https://docs.vespa.ai/en/reference/grouping-syntax.html.</p> Minimal Example <pre><code>from vespa.querybuilder import Grouping as G\n\n# Build a simple grouping expression which groups on \"my_attribute\"\n# and outputs the count of matching documents under each group:\nexpr = G.all(\n    G.group(\"my_attribute\"),\n    G.each(\n        G.output(G.count())\n    )\n)\nprint(expr)\nall(group(my_attribute) each(output(count())))\n</code></pre> <p>In the above example, the \u201call(...)\u201d wraps the grouping operations at the top level. We first group on \u201cmy_attribute\u201d, then under \u201ceach(...)\u201d we add an output aggregator \u201ccount()\u201d. The \u201cprint\u201d output is the exact grouping expression string you would pass to Vespa in the \u201cselect\u201d query parameter.</p> <p>For multi-level (nested) grouping, you can nest additional calls to \u201cgroup(...)\u201d or \u201ceach(...)\u201d inside. For example:     <pre><code># Nested grouping:\n# 1) Group by 'category'\n# 2) Within each category, group by 'sub_category'\n# 3) Output the count() under each sub-category\nnested_expr = G.all(\n    G.group(\"category\"),\n    G.each(\n        G.group(\"sub_category\"),\n        G.each(\n            G.output(G.count())\n        )\n    )\n)\nprint(nested_expr)\nall(group(category) each(group(sub_category) each(output(count()))))\n</code></pre></p> <p>You may use any of the static methods below to build more advanced groupings, aggregations, or arithmetic/string expressions for sorting, filtering, or bucket definitions. Refer to Vespa documentation for the complete details.</p>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.all","title":"<code>all(*args)</code>  <code>staticmethod</code>","text":"<p>Corresponds to the \u201call(...)\u201d grouping block in Vespa, which means \u201cgroup all documents (no top-level grouping) and then do the enclosed operations\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>Sub-expressions to include within the <code>all(...)</code> block.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.all(G.group(\"my_attribute\"), G.each(G.output(G.count())))\nprint(expr)\nall(group(my_attribute) each(output(count())))\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.each","title":"<code>each(*args)</code>  <code>staticmethod</code>","text":"<p>Corresponds to the \u201ceach(...)\u201d grouping block in Vespa, which means \u201ccreate a group for each unique value and then do the enclosed operations\u201d.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>str</code> <p>Sub-expressions to include within the <code>each(...)</code> block.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.each(\"output(count())\", \"output(avg(price))\")\nprint(expr)\neach(output(count()) output(avg(price)))\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.group","title":"<code>group(field)</code>  <code>staticmethod</code>","text":"<p>Defines a grouping step on a field or expression.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>The field or expression on which to group.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.group(\"my_map.key\")\nprint(expr)\ngroup(my_map.key)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.count","title":"<code>count()</code>  <code>staticmethod</code>","text":"<p>\u201ccount()\u201d aggregator.</p> <p>By default, returns a string 'count()'. Negative ordering or usage can be done by prefixing a minus, e.g.: order(-count()) in Vespa syntax.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'count()' or prefixed version if used with a minus operator.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.count()\nprint(expr)\ncount()\n\nsort_expr = f\"-{expr}\"\nprint(sort_expr)\n-count()\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.sum","title":"<code>sum(value)</code>  <code>staticmethod</code>","text":"<p>\u201csum(...)\u201d aggregator. Sums the given expression or field over all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression to sum.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'sum(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.sum(\"my_numeric_field\")\nprint(expr)\nsum(my_numeric_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.avg","title":"<code>avg(value)</code>  <code>staticmethod</code>","text":"<p>\u201cavg(...)\u201d aggregator. Computes the average of the given expression or field for all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression to average.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'avg(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.avg(\"my_numeric_field\")\nprint(expr)\navg(my_numeric_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.min","title":"<code>min(value)</code>  <code>staticmethod</code>","text":"<p>\u201cmin(...)\u201d aggregator. Keeps the minimum value of the expression or field among all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression to find the minimum of.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'min(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.min(\"some_field\")\nprint(expr)\nmin(some_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.max","title":"<code>max(value)</code>  <code>staticmethod</code>","text":"<p>\u201cmax(...)\u201d aggregator. Keeps the maximum value of the expression or field among all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression to find the maximum of.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'max(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.max(\"relevance()\")\nprint(expr)\nmax(relevance())\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.stddev","title":"<code>stddev(value)</code>  <code>staticmethod</code>","text":"<p>\u201cstddev(...)\u201d aggregator. Computes the population standard deviation for the expression or field among all documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'stddev(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.stddev(\"my_numeric_field\")\nprint(expr)\nstddev(my_numeric_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.xor","title":"<code>xor(value)</code>  <code>staticmethod</code>","text":"<p>\u201cxor(...)\u201d aggregator. XORs all values of the expression or field together over the documents in the group.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, int, float]</code> <p>The field or numeric expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'xor(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.xor(\"my_field\")\nprint(expr)\nxor(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.output","title":"<code>output(*args)</code>  <code>staticmethod</code>","text":"<p>Defines output aggregators to be collected for the grouping level.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Union[str, Expression]</code> <p>Multiple aggregator expressions, e.g., 'count()', 'sum(price)'.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Expression</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'output(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.output(G.count(), G.sum(\"price\"))\nprint(expr)\noutput(count(),sum(price))\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.order","title":"<code>order(*args)</code>  <code>staticmethod</code>","text":"<p>Defines an order(...) clause to sort groups by the given expressions or aggregators.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Union[str, Expression]</code> <p>Multiple expressions or aggregators to order by.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Expression</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'order(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.order(G.sum(G.relevance()), -G.count())\nprint(expr)\norder(sum(relevance()),-count())\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.precision","title":"<code>precision(value)</code>  <code>staticmethod</code>","text":"<p>Sets the \u201cprecision(...)\u201d for the grouping step.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>int</code> <p>Precision value.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'precision(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.precision(1000)\nprint(expr)\nprecision(1000)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.add","title":"<code>add(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cadd(...)\u201d expression. Adds all arguments together in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to be added.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'add(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.add(\"my_field\", \"5\", \"10\")\nprint(expr)\nadd(my_field, 5, 10)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.sub","title":"<code>sub(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201csub(...)\u201d expression. Subtracts each subsequent argument from the first.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions involved in subtraction.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'sub(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.sub(\"my_field\", \"2\")\nprint(expr)\nsub(my_field, 2)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.mul","title":"<code>mul(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cmul(...)\u201d expression. Multiplies all arguments in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to multiply.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'mul(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.mul(\"my_field\", \"2\", \"3\")\nprint(expr)\nmul(my_field, 2, 3)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.div","title":"<code>div(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cdiv(...)\u201d expression. Divides the first argument by the second, etc.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to divide in order.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'div(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.div(\"my_field\", \"2\")\nprint(expr)\ndiv(my_field, 2)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.mod","title":"<code>mod(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cmod(...)\u201d expression. Modulo the first argument by the second, result by the third, etc.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to apply modulo on in order.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'mod(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.mod(\"my_field\", \"100\")\nprint(expr)\nmod(my_field,100)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.and_","title":"<code>and_(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cand(...)\u201d expression. Bitwise AND of the arguments in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to apply bitwise AND.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'and(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.and_(\"fieldA\", \"fieldB\")\nprint(expr)\nand(fieldA, fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.or_","title":"<code>or_(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cor(...)\u201d expression. Bitwise OR of the arguments in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to apply bitwise OR.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'or(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.or_(\"fieldA\", \"fieldB\")\nprint(expr)\nor(fieldA, fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.xor_expr","title":"<code>xor_expr(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cxor(...)\u201d bitwise expression.</p> <p>(Note: For aggregator use, see xor(...) aggregator method above.)</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The expressions to apply bitwise XOR.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'xor(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.xor_expr(\"fieldA\", \"fieldB\")\nprint(expr)\nxor(fieldA, fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.strlen","title":"<code>strlen(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cstrlen(...)\u201d expression. Returns the number of bytes in the string.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The string field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'strlen(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.strlen(\"my_string_field\")\nprint(expr)\nstrlen(my_string_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.strcat","title":"<code>strcat(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201cstrcat(...)\u201d expression. Concatenate all string arguments in order.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The string expressions to concatenate.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'strcat(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.strcat(\"fieldA\", \"_\", \"fieldB\")\nprint(expr)\nstrcat(fieldA,_,fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.todouble","title":"<code>todouble(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctodouble(...)\u201d expression. Convert argument to double.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'todouble(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.todouble(\"my_field\")\nprint(expr)\ntodouble(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.tolong","title":"<code>tolong(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctolong(...)\u201d expression. Convert argument to long.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'tolong(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.tolong(\"my_field\")\nprint(expr)\ntolong(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.tostring","title":"<code>tostring(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctostring(...)\u201d expression. Convert argument to string.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'tostring(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.tostring(\"my_field\")\nprint(expr)\ntostring(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.toraw","title":"<code>toraw(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctoraw(...)\u201d expression. Convert argument to raw data.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'toraw(...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.toraw(\"my_field\")\nprint(expr)\ntoraw(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.cat","title":"<code>cat(*expressions)</code>  <code>staticmethod</code>","text":"<p>\u201ccat(...)\u201d expression. Concatenate the binary representation of arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*expressions</code> <code>str</code> <p>The binary expressions or fields to concatenate.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'cat(expr1, expr2, ...)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.cat(\"fieldA\", \"fieldB\")\nprint(expr)\ncat(fieldA,fieldB)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.md5","title":"<code>md5(expr, width)</code>  <code>staticmethod</code>","text":"<p>\u201cmd5(...)\u201d expression.</p> <p>Does an MD5 over the binary representation of the argument, and keeps the lowest 'width' bits.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to apply MD5 on.</p> required <code>width</code> <code>int</code> <p>The number of bits to keep.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'md5(expr, width)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.md5(\"my_field\", 16)\nprint(expr)\nmd5(my_field, 16)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.xorbit","title":"<code>xorbit(expr, width)</code>  <code>staticmethod</code>","text":"<p>\u201cxorbit(...)\u201d expression.</p> <p>Performs an XOR of 'width' bits over the binary representation of the argument. Width is rounded up to a multiple of 8.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field to apply xorbit on.</p> required <code>width</code> <code>int</code> <p>The number of bits for the XOR operation.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'xorbit(expr, width)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.xorbit(\"my_field\", 16)\nprint(expr)\nxorbit(my_field, 16)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.relevance","title":"<code>relevance()</code>  <code>staticmethod</code>","text":"<p>\u201crelevance()\u201d expression. Returns the computed rank (relevance) of a document.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'relevance()' as a Vespa expression string.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.relevance()\nprint(expr)\nrelevance()\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.array_at","title":"<code>array_at(array_name, index_expr)</code>  <code>staticmethod</code>","text":"<p>\u201carray.at(...)\u201d accessor expression. Returns a single element from the array at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>array_name</code> <code>str</code> <p>The name of the array.</p> required <code>index_expr</code> <code>Union[str, int]</code> <p>The index or expression that evaluates to an index.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'array.at(array_name, index)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.array_at(\"my_array\", 0)\nprint(expr)\narray.at(my_array, 0)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.zcurve_x","title":"<code>zcurve_x(expr)</code>  <code>staticmethod</code>","text":"<p>\u201czcurve.x(...)\u201d expression. Returns the X component of the given zcurve-encoded 2D point.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The zcurve-encoded field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'zcurve.x(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.zcurve_x(\"location_zcurve\")\nprint(expr)\nzcurve.x(location_zcurve)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.zcurve_y","title":"<code>zcurve_y(expr)</code>  <code>staticmethod</code>","text":"<p>\u201czcurve.y(...)\u201d expression. Returns the Y component of the given zcurve-encoded 2D point.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The zcurve-encoded field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'zcurve.y(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.zcurve_y(\"location_zcurve\")\nprint(expr)\nzcurve.y(location_zcurve)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_dayofmonth","title":"<code>time_dayofmonth(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.dayofmonth(...)\u201d expression. Returns the day of month (1-31).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.dayofmonth(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_dayofmonth(\"timestamp_field\")\nprint(expr)\ntime.dayofmonth(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_dayofweek","title":"<code>time_dayofweek(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.dayofweek(...)\u201d expression. Returns the day of week (0-6), Monday = 0.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.dayofweek(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_dayofweek(\"timestamp_field\")\nprint(expr)\ntime.dayofweek(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_dayofyear","title":"<code>time_dayofyear(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.dayofyear(...)\u201d expression. Returns the day of year (0-365).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.dayofyear(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_dayofyear(\"timestamp_field\")\nprint(expr)\ntime.dayofyear(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_hourofday","title":"<code>time_hourofday(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.hourofday(...)\u201d expression. Returns the hour of day (0-23).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.hourofday(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_hourofday(\"timestamp_field\")\nprint(expr)\ntime.hourofday(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_minuteofhour","title":"<code>time_minuteofhour(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.minuteofhour(...)\u201d expression. Returns the minute of hour (0-59).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.minuteofhour(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_minuteofhour(\"timestamp_field\")\nprint(expr)\ntime.minuteofhour(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_monthofyear","title":"<code>time_monthofyear(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.monthofyear(...)\u201d expression. Returns the month of year (1-12).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.monthofyear(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_monthofyear(\"timestamp_field\")\nprint(expr)\ntime.monthofyear(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_secondofminute","title":"<code>time_secondofminute(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.secondofminute(...)\u201d expression. Returns the second of minute (0-59).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.secondofminute(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_secondofminute(\"timestamp_field\")\nprint(expr)\ntime.secondofminute(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_year","title":"<code>time_year(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.year(...)\u201d expression. Returns the full year (e.g. 2009).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.year(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_year(\"timestamp_field\")\nprint(expr)\ntime.year(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.time_date","title":"<code>time_date(expr)</code>  <code>staticmethod</code>","text":"<p>\u201ctime.date(...)\u201d expression. Returns the date (e.g. 2009-01-10).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The timestamp field or expression.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'time.date(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.time_date(\"timestamp_field\")\nprint(expr)\ntime.date(timestamp_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_exp","title":"<code>math_exp(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.exp(...)\u201d expression. Returns e^expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'math.exp(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_exp(\"my_field\")\nprint(expr)\nmath.exp(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_log","title":"<code>math_log(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.log(...)\u201d expression. Returns the natural logarithm of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.log(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_log(\"my_field\")\nprint(expr)\nmath.log(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_log1p","title":"<code>math_log1p(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.log1p(...)\u201d expression. Returns the natural logarithm of (1 + expr).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.log1p(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_log1p(\"my_field\")\nprint(expr)\nmath.log1p(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_log10","title":"<code>math_log10(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.log10(...)\u201d expression. Returns the base-10 logarithm of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.log10(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_log10(\"my_field\")\nprint(expr)\nmath.log10(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_sqrt","title":"<code>math_sqrt(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.sqrt(...)\u201d expression. Returns the square root of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.sqrt(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_sqrt(\"my_field\")\nprint(expr)\nmath.sqrt(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_cbrt","title":"<code>math_cbrt(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.cbrt(...)\u201d expression. Returns the cube root of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.cbrt(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_cbrt(\"my_field\")\nprint(expr)\nmath.cbrt(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_sin","title":"<code>math_sin(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.sin(...)\u201d expression. Returns the sine of expr (argument in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.sin(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_sin(\"my_field\")\nprint(expr)\nmath.sin(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_cos","title":"<code>math_cos(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.cos(...)\u201d expression. Returns the cosine of expr (argument in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.cos(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_cos(\"my_field\")\nprint(expr)\nmath.cos(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_tan","title":"<code>math_tan(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.tan(...)\u201d expression. Returns the tangent of expr (argument in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.tan(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_tan(\"my_field\")\nprint(expr)\nmath.tan(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_asin","title":"<code>math_asin(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.asin(...)\u201d expression. Returns the arcsine of expr (in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.asin(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_asin(\"my_field\")\nprint(expr)\nmath.asin(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_acos","title":"<code>math_acos(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.acos(...)\u201d expression. Returns the arccosine of expr (in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.acos(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_acos(\"my_field\")\nprint(expr)\nmath.acos(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_atan","title":"<code>math_atan(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.atan(...)\u201d expression. Returns the arctangent of expr (in radians).</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.atan(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_atan(\"my_field\")\nprint(expr)\nmath.atan(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_sinh","title":"<code>math_sinh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.sinh(...)\u201d expression. Returns the hyperbolic sine of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.sinh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_sinh(\"my_field\")\nprint(expr)\nmath.sinh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_cosh","title":"<code>math_cosh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.cosh(...)\u201d expression. Returns the hyperbolic cosine of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.cosh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_cosh(\"my_field\")\nprint(expr)\nmath.cosh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_tanh","title":"<code>math_tanh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.tanh(...)\u201d expression. Returns the hyperbolic tangent of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.tanh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_tanh(\"my_field\")\nprint(expr)\nmath.tanh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_asinh","title":"<code>math_asinh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.asinh(...)\u201d expression. Returns the inverse hyperbolic sine of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.asinh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_asinh(\"my_field\")\nprint(expr)\nmath.asinh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_acosh","title":"<code>math_acosh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.acosh(...)\u201d expression. Returns the inverse hyperbolic cosine of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.acosh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_acosh(\"my_field\")\nprint(expr)\nmath.acosh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_atanh","title":"<code>math_atanh(expr)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.atanh(...)\u201d expression. Returns the inverse hyperbolic tangent of expr.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.atanh(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_atanh(\"my_field\")\nprint(expr)\nmath.atanh(my_field)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_pow","title":"<code>math_pow(expr_x, expr_y)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.pow(...)\u201d expression. Returns expr_x^expr_y.</p> <p>Parameters:</p> Name Type Description Default <code>expr_x</code> <code>str</code> <p>The expression or field for the base.</p> required <code>expr_y</code> <code>str</code> <p>The expression or field for the exponent.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.pow(expr_x, expr_y)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_pow(\"my_field\", \"2\")\nprint(expr)\nmath.pow(my_field,2)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.math_hypot","title":"<code>math_hypot(expr_x, expr_y)</code>  <code>staticmethod</code>","text":"<p>\u201cmath.hypot(...)\u201d expression. Returns the length of the hypotenuse given expr_x and expr_y.</p> <p>Parameters:</p> Name Type Description Default <code>expr_x</code> <code>str</code> <p>The expression or field for the first side of the triangle.</p> required <code>expr_y</code> <code>str</code> <p>The expression or field for the second side of the triangle.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>'math.hypot(expr_x, expr_y)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.math_hypot(\"my_field_x\", \"my_field_y\")\nprint(expr)\nmath.hypot(my_field_x, my_field_y)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.size","title":"<code>size(expr)</code>  <code>staticmethod</code>","text":"<p>\u201csize(...)\u201d expression. Returns the number of elements if expr is a list; otherwise returns 1.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The list expression or field.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'size(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.size(\"my_array\")\nprint(expr)\nsize(my_array)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.sort","title":"<code>sort(expr)</code>  <code>staticmethod</code>","text":"<p>\u201csort(...)\u201d expression. Sorts the elements of the list argument in ascending order.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The list expression or field to sort.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'sort(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.sort(\"my_array\")\nprint(expr)\nsort(my_array)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.reverse","title":"<code>reverse(expr)</code>  <code>staticmethod</code>","text":"<p>\u201creverse(...)\u201d expression. Reverses the elements of the list argument.</p> <p>Parameters:</p> Name Type Description Default <code>expr</code> <code>str</code> <p>The list expression or field to reverse.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'reverse(expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.reverse(\"my_array\")\nprint(expr)\nreverse(my_array)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.fixedwidth","title":"<code>fixedwidth(value, bucket_width)</code>  <code>staticmethod</code>","text":"<p>\u201cfixedwidth(...)\u201d bucket expression. Maps the value of the first argument into consecutive buckets whose width is the second argument.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The field or expression to bucket.</p> required <code>bucket_width</code> <code>Union[int, float]</code> <p>The width of each bucket.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'fixedwidth(value, bucket_width)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.fixedwidth(\"my_field\",10)\nprint(expr)\nfixedwidth(my_field,10)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.predefined","title":"<code>predefined(value, buckets)</code>  <code>staticmethod</code>","text":"<p>\u201cpredefined(...)\u201d bucket expression. Maps the value into the provided list of buckets.</p> <p>Each 'bucket' must be a string representing the range, e.g.: 'bucket(-inf,0)', 'bucket[0,10)', 'bucket[10,inf)', etc.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>str</code> <p>The field or expression to bucket.</p> required <code>buckets</code> <code>List[str]</code> <p>A list of bucket definitions.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'predefined(value, ( ))'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.predefined(\"my_field\", [\"bucket(-inf,0)\", \"bucket[0,10)\", \"bucket[10,inf)\"])\nprint(expr)\npredefined(my_field,bucket(-inf,0),bucket[0,10),bucket[10,inf))\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.interpolatedlookup","title":"<code>interpolatedlookup(array_attr, lookup_expr)</code>  <code>staticmethod</code>","text":"<p>\u201cinterpolatedlookup(...)\u201d expression. Counts elements in a sorted array that are less than an expression, with linear interpolation if the expression is between element values.</p> <p>Parameters:</p> Name Type Description Default <code>array_attr</code> <code>str</code> <p>The sorted array field name.</p> required <code>lookup_expr</code> <code>str</code> <p>The expression or value to lookup.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa expression string of the form 'interpolatedlookup(array, expr)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.interpolatedlookup(\"my_sorted_array\", \"4.2\")\nprint(expr)\ninterpolatedlookup(my_sorted_array, 4.2)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.summary","title":"<code>summary(summary_class='')</code>  <code>staticmethod</code>","text":"<p>\u201csummary(...)\u201d hit aggregator. Produces a summary of the requested summary class.</p> <p>If no summary class is specified, \u201csummary()\u201d is used.</p> <p>Parameters:</p> Name Type Description Default <code>summary_class</code> <code>str</code> <p>Name of the summary class. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>Expression</code> <p>A Vespa grouping expression string of the form 'summary(...)'.</p> Example <p><pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.summary()\nprint(expr)\nsummary()\n</code></pre> <pre><code>expr = G.summary(\"my_summary_class\")\nprint(expr)\nsummary(my_summary_class)\n</code></pre></p>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.as_","title":"<code>as_(expression, label)</code>  <code>staticmethod</code>","text":"<p>Appends an ' as(label)' part to a grouping block expression.</p> <p>Parameters:</p> Name Type Description Default <code>expression</code> <code>str</code> <p>The expression to be labeled.</p> required <code>label</code> <code>str</code> <p>The label to be used.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A Vespa grouping expression string of the form 'expression as(label)'</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.as_(G.each(G.output(G.count())), \"mylabel\")\nprint(expr)\neach(output(count())) as(mylabel)\n</code></pre>"},{"location":"api/vespa/querybuilder/grouping/grouping.html#vespa.querybuilder.grouping.grouping.Grouping.alias","title":"<code>alias(alias_name, expression)</code>  <code>staticmethod</code>","text":"<p>Defines an alias(...) grouping syntax. This lets you name an expression, so you can reference it later by $alias_name.</p> <p>Parameters:</p> Name Type Description Default <code>alias_name</code> <code>str</code> <p>The alias name.</p> required <code>expression</code> <code>str</code> <p>The expression to alias.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A Vespa grouping expression string of the form 'alias(alias_name, expression)'.</p> Example <pre><code>from vespa.querybuilder import Grouping as G\n\nexpr = G.alias(\"my_alias\", G.add(\"fieldA\", \"fieldB\"))\nprint(expr)\nalias(my_alias,add(fieldA, fieldB))\n</code></pre>"},{"location":"examples/index.html","title":"Examples","text":"<p>Here you can find a wide variety of examples that demonstrate how to use the Vespa Python API. These examples cover different use cases and functionalities, providing a practical understanding of how to interact with Vespa using Python.</p>"},{"location":"examples/index.html#vespa-cloud","title":"Vespa Cloud","text":"<p>To create a free Vespa Cloud account, visit Vespa Cloud.</p> <ul> <li>BGE-M3 - The Mother of all embedding models</li> <li>Billion-scale vector search with Cohere binary embeddings in Vespa</li> <li>Building cost-efficient retrieval-augmented personal AI assistants</li> <li>Chat with your pdfs with ColBERT, langchain, and Vespa</li> <li>ColPali Ranking Experiments on DocVQA</li> <li>Exploring the potential of OpenAI Matryoshka \ud83e\ude86 embeddings with Vespa</li> <li>Feeding to Vespa Cloud</li> <li>Multilingual Hybrid Search with Cohere binary embeddings and Vespa</li> <li>PDF-Retrieval using ColQWen2 (ColPali) with Vespa</li> <li>RAG Blueprint tutorial</li> <li>Scaling ColPALI (VLM) Retrieval</li> <li>Standalone ColBERT + Vespa for long-context ranking</li> <li>Standalone ColBERT with Vespa for end-to-end retrieval and ranking</li> <li>Turbocharge RAG with LangChain and Vespa Streaming Mode for Partitioned Data</li> <li>Using Cohere Binary Embeddings in Vespa</li> <li>Using Mixedbread.ai embedding model with support for binary vectors</li> <li>Vespa \ud83e\udd1d ColPali: Efficient Document Retrieval with Vision Language Models</li> <li>Video Search and Retrieval with Vespa and TwelveLabs</li> <li>Visual PDF RAG with Vespa - ColPali demo application</li> </ul>"},{"location":"examples/index.html#local-deployment-dockerpodman","title":"Local deployment (docker/podman)","text":"<ul> <li>Evaluating retrieval with Snowflake arctic embed</li> <li>Feeding performance</li> <li>LightGBM: Mapping model features to Vespa features</li> <li>LightGBM: Training the model with Vespa features</li> <li>Multi-vector indexing with HNSW</li> <li>Pyvespa examples</li> <li>Using Mixedbread.ai cross-encoder for reranking in Vespa.ai</li> </ul>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html","title":"Matryoshka embeddings in Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa ir_datasets openai pytrec_eval vespacli\n</pre> !pip3 install -U pyvespa ir_datasets openai pytrec_eval vespacli In\u00a0[1]: Copied! <pre>from openai import OpenAI\n\nopenai = OpenAI()\n\n\ndef embed(text, model=\"text-embedding-3-large\", dimensions=3072):\n    return (\n        openai.embeddings.create(input=[text], model=model, dimensions=dimensions)\n        .data[0]\n        .embedding\n    )\n</pre> from openai import OpenAI  openai = OpenAI()   def embed(text, model=\"text-embedding-3-large\", dimensions=3072):     return (         openai.embeddings.create(input=[text], model=model, dimensions=dimensions)         .data[0]         .embedding     ) <p>With these new embedding models, the API supports a <code>dimensions</code> parameter. Does this differ from just taking the first N dimensions?</p> In\u00a0[2]: Copied! <pre>test_input = \"This is just a test sentence.\"\n\nfull = embed(test_input)\nshort = embed(test_input, dimensions=8)\n\nprint(full[:8])\nprint(short)\n</pre> test_input = \"This is just a test sentence.\"  full = embed(test_input) short = embed(test_input, dimensions=8)  print(full[:8]) print(short) <pre>[0.0035371531266719103, 0.014166134409606457, -0.017565304413437843, 0.04296272248029709, 0.012746891938149929, -0.01731124334037304, -0.00855049304664135, 0.044189225882291794]\n[0.05076185241341591, 0.20329885184764862, -0.2520805299282074, 0.6165600419044495, 0.18293125927448273, -0.24843446910381317, -0.1227085217833519, 0.634161651134491]\n</pre> <p>Numerically, they are not the same. But looking more closely, they differ only by a scaling factor:</p> In\u00a0[12]: Copied! <pre>scale = short[0] / full[0]\nprint([x * scale for x in full[:8]])\nprint(short)\n</pre> scale = short[0] / full[0] print([x * scale for x in full[:8]]) print(short) <pre>[0.05076185241341591, 0.2032988673141365, -0.2520805173822377, 0.6165600695594861, 0.18293125124128834, -0.2484344748635628, -0.12270853156530777, 0.6341616780980419]\n[0.05076185241341591, 0.20329885184764862, -0.2520805299282074, 0.6165600419044495, 0.18293125927448273, -0.24843446910381317, -0.1227085217833519, 0.634161651134491]\n</pre> <p>It seems the shortened vector has been L2 normalized to have a magnitude of 1. By cosine similarity, they are equivalent:</p> In\u00a0[13]: Copied! <pre>from numpy.linalg import norm\nfrom numpy import dot\n\n\ndef cos_sim(e1, e2):\n    return dot(e1, e2) / (norm(e1) * norm(e2))\n\n\nprint(norm(short))\n\ncos_sim(short, full[:8])\n</pre> from numpy.linalg import norm from numpy import dot   def cos_sim(e1, e2):     return dot(e1, e2) / (norm(e1) * norm(e2))   print(norm(short))  cos_sim(short, full[:8]) <pre>0.9999999899058183\n</pre> Out[13]: <pre>0.9999999999999996</pre> <p>This is great, because it means that in a single API call we can get the full embeddings, and easily produce shortened embeddings just by slicing the list of numbers.</p> <p>Note that <code>text-embedding-3-large</code> and <code>text-embedding-3-small</code> do not produce compatible embeddings when sliced to the same size:</p> In\u00a0[14]: Copied! <pre>cos_sim(\n    embed(test_input, dimensions=1536),\n    embed(test_input, dimensions=1536, model=\"text-embedding-3-small\"),\n)\n</pre> cos_sim(     embed(test_input, dimensions=1536),     embed(test_input, dimensions=1536, model=\"text-embedding-3-small\"), ) Out[14]: <pre>-0.03217247156447633</pre> In\u00a0[15]: Copied! <pre>import ir_datasets\n\ndataset = ir_datasets.load(\"beir/trec-covid\")\nprint(\"Dataset has\", dataset.docs_count(), \"documents. Sample:\")\ndataset.docs_iter()[120]._asdict()\n</pre> import ir_datasets  dataset = ir_datasets.load(\"beir/trec-covid\") print(\"Dataset has\", dataset.docs_count(), \"documents. Sample:\") dataset.docs_iter()[120]._asdict() <pre>Dataset has 171332 documents. Sample:\n</pre> Out[15]: <pre>{'doc_id': 'z2u5frvq',\n 'text': 'The authors discuss humoral immune responses to HIV and approaches to designing vaccines that induce viral neutralizing and other potentially protective antibodies.',\n 'title': 'Antibody-Based HIV-1 Vaccines: Recent Developments and Future Directions: A summary report from a Global HIV Vaccine Enterprise Working Group',\n 'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2100141/',\n 'pubmed_id': '18052607'}</pre> In\u00a0[16]: Copied! <pre>print(next(dataset.queries_iter()))\nprint(next(dataset.qrels_iter()))\n</pre> print(next(dataset.queries_iter())) print(next(dataset.qrels_iter())) <pre>BeirCovidQuery(query_id='1', text='what is the origin of COVID-19', query='coronavirus origin', narrative=\"seeking range of information about the SARS-CoV-2 virus's origin, including its evolution, animal source, and first transmission into humans\")\nTrecQrel(query_id='1', doc_id='005b2j4b', relevance=2, iteration='0')\n</pre> <p>We'll use these later to evaluate the result quality.</p> In\u00a0[33]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"my_schema\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"pubmed_id\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"shortened\",\n                type=\"tensor&lt;float&gt;(x[256])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: angular\"],\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;float&gt;(x[3072])\",\n                indexing=[\"attribute\"],\n                attribute=[\"paged\", \"distance-metric: angular\"],\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"my_schema\",     mode=\"index\",     document=Document(         fields=[             Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"pubmed_id\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"shortened\",                 type=\"tensor(x[256])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: angular\"],             ),             Field(                 name=\"embedding\",                 type=\"tensor(x[3072])\",                 indexing=[\"attribute\"],                 attribute=[\"paged\", \"distance-metric: angular\"],             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])], ) <p>The two fields of type <code>tensor&lt;float&gt;(x[3072/256])</code> are not in the dataset - they are tensor fields to hold the embeddings from OpenAI.</p> <ul> <li><p><code>shortened</code>: This field holds the embedding shortened to 256 dimensions, requiring only 8.3% of the memory. <code>index</code> here means we will build an HNSW Approximate Nearest Neighbor index, by which we can find the closest vectors while exploring only a very small subset of the documents.</p> </li> <li><p><code>embedding</code>: This field contains the full size embedding. It is paged: accesses to this field may require disk access, unless it has been cached by the kernel.</p> </li> </ul> <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[34]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"matryoshka\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"matryoshka\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p>Vespa supports has a rich set of built-in rank-features, including many text-matching features such as:</p> <ul> <li>BM25,</li> <li>nativeRank and many more.</li> </ul> <p>Users can also define custom functions using ranking expressions.</p> <p>The following defines three runtime selectable Vespa ranking profiles:</p> <ul> <li><code>exact</code> uses the full-size embedding</li> <li><code>shortened</code> uses only 256 dimensions (exact, or using the approximate nearest neighbor HNSW index)</li> <li><code>rerank</code> uses the 256-dimension shortened embeddings (exact or ANN) in a first phase, and the full 3072-dimension embeddings in a second phase. By default the second phase is applied to the top 100 documents from the first phase.</li> </ul> In\u00a0[35]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\nexact = RankProfile(\n    name=\"exact\",\n    inputs=[(\"query(q3072)\", \"tensor&lt;float&gt;(x[3072])\")],\n    functions=[Function(name=\"cos_sim\", expression=\"closeness(field, embedding)\")],\n    first_phase=FirstPhaseRanking(expression=\"cos_sim\"),\n    match_features=[\"cos_sim\"],\n)\nmy_schema.add_rank_profile(exact)\n\n\nshortened = RankProfile(\n    name=\"shortened\",\n    inputs=[(\"query(q256)\", \"tensor&lt;float&gt;(x[256])\")],\n    functions=[Function(name=\"cos_sim_256\", expression=\"closeness(field, shortened)\")],\n    first_phase=FirstPhaseRanking(expression=\"cos_sim_256\"),\n    match_features=[\"cos_sim_256\"],\n)\nmy_schema.add_rank_profile(shortened)\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q3072)\", \"tensor&lt;float&gt;(x[3072])\"),\n        (\"query(q256)\", \"tensor&lt;float&gt;(x[256])\"),\n    ],\n    functions=[\n        Function(name=\"cos_sim_256\", expression=\"closeness(field, shortened)\"),\n        Function(\n            name=\"cos_sim_3072\",\n            expression=\"cosine_similarity(query(q3072), attribute(embedding), x)\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"cos_sim_256\"),\n    second_phase=SecondPhaseRanking(expression=\"cos_sim_3072\"),\n    match_features=[\"cos_sim_256\", \"cos_sim_3072\"],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  exact = RankProfile(     name=\"exact\",     inputs=[(\"query(q3072)\", \"tensor(x[3072])\")],     functions=[Function(name=\"cos_sim\", expression=\"closeness(field, embedding)\")],     first_phase=FirstPhaseRanking(expression=\"cos_sim\"),     match_features=[\"cos_sim\"], ) my_schema.add_rank_profile(exact)   shortened = RankProfile(     name=\"shortened\",     inputs=[(\"query(q256)\", \"tensor(x[256])\")],     functions=[Function(name=\"cos_sim_256\", expression=\"closeness(field, shortened)\")],     first_phase=FirstPhaseRanking(expression=\"cos_sim_256\"),     match_features=[\"cos_sim_256\"], ) my_schema.add_rank_profile(shortened)   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q3072)\", \"tensor(x[3072])\"),         (\"query(q256)\", \"tensor(x[256])\"),     ],     functions=[         Function(name=\"cos_sim_256\", expression=\"closeness(field, shortened)\"),         Function(             name=\"cos_sim_3072\",             expression=\"cosine_similarity(query(q3072), attribute(embedding), x)\",         ),     ],     first_phase=FirstPhaseRanking(expression=\"cos_sim_256\"),     second_phase=SecondPhaseRanking(expression=\"cos_sim_3072\"),     match_features=[\"cos_sim_256\", \"cos_sim_3072\"], ) my_schema.add_rank_profile(rerank) <p>For an example of a <code>hybrid</code> rank-profile which combines semantic search with traditional text retrieval such as BM25, see the previous blog post: Turbocharge RAG with LangChain and Vespa Streaming Mode for Sharded Data</p> In\u00a0[36]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[37]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 3 of dev-aws-us-east-1c for vespa-team.matryoshka. This may take a few minutes the first time.\nINFO    [15:51:53]  Deploying platform version 8.296.15 and application dev build 3 for dev-aws-us-east-1c of default ...\nINFO    [15:51:53]  Using CA signed certificate version 0\nINFO    [15:51:53]  Using 1 nodes in container cluster 'matryoshka_container'\nINFO    [15:51:57]  Session 282395 for tenant 'vespa-team' prepared and activated.\nINFO    [15:52:00]  ######## Details for all nodes ########\nINFO    [15:52:09]  h88969c.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [15:52:09]  --- platform vespa/cloud-tenant-rhel8:8.296.15 &lt;-- :\nINFO    [15:52:09]  --- logserver-container on port 4080 has not started \nINFO    [15:52:09]  --- metricsproxy-container on port 19092 has not started \nINFO    [15:52:09]  h88972f.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [15:52:09]  --- platform vespa/cloud-tenant-rhel8:8.296.15 &lt;-- :\nINFO    [15:52:09]  --- container-clustercontroller on port 19050 has not started \nINFO    [15:52:09]  --- metricsproxy-container on port 19092 has not started \nINFO    [15:52:09]  h90002a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [15:52:09]  --- platform vespa/cloud-tenant-rhel8:8.296.15 &lt;-- :\nINFO    [15:52:09]  --- storagenode on port 19102 has not started \nINFO    [15:52:09]  --- searchnode on port 19107 has not started \nINFO    [15:52:09]  --- distributor on port 19111 has not started \nINFO    [15:52:09]  --- metricsproxy-container on port 19092 has not started \nINFO    [15:52:09]  h90512a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [15:52:09]  --- platform vespa/cloud-tenant-rhel8:8.296.15 &lt;-- :\nINFO    [15:52:09]  --- container on port 4080 has not started \nINFO    [15:52:09]  --- metricsproxy-container on port 19092 has not started \nINFO    [15:53:11]  Found endpoints:\nINFO    [15:53:11]  - dev.aws-us-east-1c\nINFO    [15:53:11]   |-- https://e5ba4967.b2349765.z.vespa-app.cloud/ (cluster 'matryoshka_container')\nINFO    [15:53:12]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://e5ba4967.b2349765.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[44]: Copied! <pre>import concurrent.futures\n\n# only embed 100 docs while developing\nsample_docs = list(dataset.docs_iter())[:100]\n\n\ndef embed_doc(doc):\n    embedding = embed(\n        (doc.title + \" \" + doc.text)[:8192]\n    )  # we crop the ~25 documents which are longer than the context window\n    shortened = embedding[0:256]\n    return {\n        \"doc_id\": doc.doc_id,\n        \"text\": doc.text,\n        \"title\": doc.title,\n        \"url\": doc.url,\n        \"pubmed_id\": doc.pubmed_id,\n        \"shortened\": {\"type\": \"tensor&lt;float&gt;(x[256])\", \"values\": shortened},\n        \"embedding\": {\"type\": \"tensor&lt;float&gt;(x[3072])\", \"values\": embedding},\n    }\n\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    my_docs_to_feed = list(executor.map(embed_doc, sample_docs))\n</pre> import concurrent.futures  # only embed 100 docs while developing sample_docs = list(dataset.docs_iter())[:100]   def embed_doc(doc):     embedding = embed(         (doc.title + \" \" + doc.text)[:8192]     )  # we crop the ~25 documents which are longer than the context window     shortened = embedding[0:256]     return {         \"doc_id\": doc.doc_id,         \"text\": doc.text,         \"title\": doc.title,         \"url\": doc.url,         \"pubmed_id\": doc.pubmed_id,         \"shortened\": {\"type\": \"tensor(x[256])\", \"values\": shortened},         \"embedding\": {\"type\": \"tensor(x[3072])\", \"values\": embedding},     }   with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:     my_docs_to_feed = list(executor.map(embed_doc, sample_docs)) In\u00a0[45]: Copied! <pre>from typing import Iterable\n\n\ndef vespa_feed(user: str) -&gt; Iterable[dict]:\n    for doc in reversed(my_docs_to_feed):\n        yield {\"fields\": doc, \"id\": doc[\"doc_id\"], \"groupname\": user}\n</pre> from typing import Iterable   def vespa_feed(user: str) -&gt; Iterable[dict]:     for doc in reversed(my_docs_to_feed):         yield {\"fields\": doc, \"id\": doc[\"doc_id\"], \"groupname\": user} <p>Now, we can feed to the Vespa instance (<code>app</code>), using the <code>feed_iterable</code> API, using the generator function above as input with a custom <code>callback</code> function.</p> In\u00a0[46]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"\n        )\n\n\napp.feed_iterable(\n    schema=\"my_schema\",\n    iter=vespa_feed(\"\"),\n    callback=callback,\n    max_queue_size=2000,\n    max_workers=32,\n    max_connections=64,\n)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"         )   app.feed_iterable(     schema=\"my_schema\",     iter=vespa_feed(\"\"),     callback=callback,     max_queue_size=2000,     max_workers=32,     max_connections=64, ) In\u00a0[47]: Copied! <pre>queries = []\nfor q in dataset.queries_iter():\n    queries.append({\"text\": q.text, \"embedding\": embed(q.text), \"id\": q.query_id})\n</pre> queries = [] for q in dataset.queries_iter():     queries.append({\"text\": q.text, \"embedding\": embed(q.text), \"id\": q.query_id}) In\u00a0[73]: Copied! <pre>import json\n\n\ndef query_exact(q):\n    return session.query(\n        yql=\"select doc_id, title from my_schema where ({targetHits: 10, approximate:false}nearestNeighbor(embedding,q3072)) limit 10\",\n        ranking=\"exact\",\n        timeout=10,\n        body={\"presentation.timing\": \"true\", \"input.query(q3072)\": q[\"embedding\"]},\n    )\n\n\ndef query_256(q):\n    return session.query(\n        yql=\"select doc_id from my_schema where ({targetHits: 10, approximate:false}nearestNeighbor(shortened,q256)) limit 10\",\n        ranking=\"shortened\",\n        timeout=10,\n        body={\"presentation.timing\": \"true\", \"input.query(q256)\": q[\"embedding\"][:256]},\n    )\n\n\ndef query_256_ann(q):\n    return session.query(\n        yql=\"select doc_id from my_schema where ({targetHits: 100, approximate:true}nearestNeighbor(shortened,q256)) limit 10\",\n        ranking=\"shortened\",\n        timeout=10,\n        body={\"presentation.timing\": \"true\", \"input.query(q256)\": q[\"embedding\"][:256]},\n    )\n\n\ndef query_rerank(q):\n    return session.query(\n        yql=\"select doc_id from my_schema where ({targetHits: 100, approximate:true}nearestNeighbor(shortened,q256)) limit 10\",\n        ranking=\"rerank\",\n        timeout=10,\n        body={\n            \"presentation.timing\": \"true\",\n            \"input.query(q256)\": q[\"embedding\"][:256],\n            \"input.query(q3072)\": q[\"embedding\"],\n        },\n    )\n\n\nprint(\"Sample query:\", queries[0][\"text\"])\nwith app.syncio() as session:\n    print(json.dumps(query_rerank(queries[0]).hits[0], indent=2))\n</pre> import json   def query_exact(q):     return session.query(         yql=\"select doc_id, title from my_schema where ({targetHits: 10, approximate:false}nearestNeighbor(embedding,q3072)) limit 10\",         ranking=\"exact\",         timeout=10,         body={\"presentation.timing\": \"true\", \"input.query(q3072)\": q[\"embedding\"]},     )   def query_256(q):     return session.query(         yql=\"select doc_id from my_schema where ({targetHits: 10, approximate:false}nearestNeighbor(shortened,q256)) limit 10\",         ranking=\"shortened\",         timeout=10,         body={\"presentation.timing\": \"true\", \"input.query(q256)\": q[\"embedding\"][:256]},     )   def query_256_ann(q):     return session.query(         yql=\"select doc_id from my_schema where ({targetHits: 100, approximate:true}nearestNeighbor(shortened,q256)) limit 10\",         ranking=\"shortened\",         timeout=10,         body={\"presentation.timing\": \"true\", \"input.query(q256)\": q[\"embedding\"][:256]},     )   def query_rerank(q):     return session.query(         yql=\"select doc_id from my_schema where ({targetHits: 100, approximate:true}nearestNeighbor(shortened,q256)) limit 10\",         ranking=\"rerank\",         timeout=10,         body={             \"presentation.timing\": \"true\",             \"input.query(q256)\": q[\"embedding\"][:256],             \"input.query(q3072)\": q[\"embedding\"],         },     )   print(\"Sample query:\", queries[0][\"text\"]) with app.syncio() as session:     print(json.dumps(query_rerank(queries[0]).hits[0], indent=2)) <pre>Sample query: what is the origin of COVID-19\n</pre> <pre>{\n  \"id\": \"index:matryoshka_content/0/16c7e8749fb82d3b5e37bedb\",\n  \"relevance\": 0.6591723960884718,\n  \"source\": \"matryoshka_content\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"cos_sim_256\": 0.5481410972571522,\n      \"cos_sim_3072\": 0.6591723960884718\n    },\n    \"doc_id\": \"beguhous\"\n  }\n}\n</pre> <p>Here's the top result from the first query. Notice the <code>matchfeatures</code> that returns the match-features from the rank-profile.</p> <p>Now for each method of querying, we'll run all our queries and note the rank of each document in the response:</p> In\u00a0[72]: Copied! <pre>global qt\n\n\ndef run_queries(query_function):\n    print(\n        \"\\nrun\",\n        query_function.__name__,\n    )\n    results = {}\n    for q in queries:\n        response = query_function(q)\n        assert response.is_successful()\n        print(\".\", end=\"\")\n        results[q[\"id\"]] = {}\n        for pos, hit in enumerate(response.hits, start=1):\n            global qt\n            qt += float(response.get_json()[\"timing\"][\"querytime\"])\n            results[q[\"id\"]][hit[\"fields\"][\"doc_id\"]] = pos\n    return results\n\n\nquery_functions = (query_exact, query_256, query_256_ann, query_rerank)\nruns = {}\n\nwith app.syncio() as session:\n    for f in query_functions:\n        qt = 0\n        runs[f.__name__] = run_queries(f)\n        print(\" avg query time {:.4f} s\".format(qt / len(queries)))\n</pre> global qt   def run_queries(query_function):     print(         \"\\nrun\",         query_function.__name__,     )     results = {}     for q in queries:         response = query_function(q)         assert response.is_successful()         print(\".\", end=\"\")         results[q[\"id\"]] = {}         for pos, hit in enumerate(response.hits, start=1):             global qt             qt += float(response.get_json()[\"timing\"][\"querytime\"])             results[q[\"id\"]][hit[\"fields\"][\"doc_id\"]] = pos     return results   query_functions = (query_exact, query_256, query_256_ann, query_rerank) runs = {}  with app.syncio() as session:     for f in query_functions:         qt = 0         runs[f.__name__] = run_queries(f)         print(\" avg query time {:.4f} s\".format(qt / len(queries))) <pre>\nrun query_exact\n</pre> <pre>.................................................. avg query time 2.7918 s\n\nrun query_256\n.................................................. avg query time 0.3040 s\n\nrun query_256_ann\n.................................................. avg query time 0.0252 s\n\nrun query_rerank\n.................................................. avg query time 0.0310 s\n</pre> <p>The query time numbers here are NOT a proper benchmark but can illustrate some significant trends for this case:</p> <ul> <li>Doing exact NN with 3072 dimensions is too slow and expensive for many use cases</li> <li>Reducing dimensionality to 256 reduces latency by an order of magnitude</li> <li>Using an ANN index improves query time by another order of magnitude</li> <li>Re-ranking the top 100 results with the full embedding causes only a slight increase</li> </ul> <p>We could use more cores per search or sharding over multiple nodes to improve latency and handle larger content volumes.</p> In\u00a0[62]: Copied! <pre>qrels = {}\n\nfor q in dataset.queries_iter():\n    qrels[q.query_id] = {}\n\nfor qrel in dataset.qrels_iter():\n    qrels[qrel.query_id][qrel.doc_id] = qrel.relevance\n</pre> qrels = {}  for q in dataset.queries_iter():     qrels[q.query_id] = {}  for qrel in dataset.qrels_iter():     qrels[qrel.query_id][qrel.doc_id] = qrel.relevance <p>With that done, we can check the scores for the first query:</p> In\u00a0[70]: Copied! <pre>for docid in runs[\"query_256_ann\"][\"1\"]:\n    score = qrels[\"1\"].get(docid)\n    print(docid, score or \"-\")\n</pre> for docid in runs[\"query_256_ann\"][\"1\"]:     score = qrels[\"1\"].get(docid)     print(docid, score or \"-\") <pre>beguhous 2\nk9lcpjyo 2\npl48ev5o 2\njwxt4ygt 2\ndv9m19yk 1\nft4rbcxf 1\nh8ahn8fw 2\n6y1gwszn 2\n3xusxrij -\n2tyt8255 1\n</pre> <p>A lot of '2', that is, 'highly relevant' results: Looks promising! Now we can use trec_eval to evaluate all the data for each run. The quality measure we use here is <code>nDCG@10</code> - Normalized Discounted Cumulative Gain, computed for the first 10 results of each query. The evaluations are per-query so we compute and report the average per run.</p> In\u00a0[71]: Copied! <pre>import pytrec_eval\n\n\ndef evaluate(run):\n    evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\"})\n    evaluation = evaluator.evaluate(run)\n\n    sum = 0\n    for ev in evaluation:\n        sum += evaluation[ev][\"ndcg_cut_10\"]\n    return sum / len(evaluation)\n\n\nfor run in runs:\n    print(run, \"\\tndcg_cut_10: {:.4f}\".format(evaluate(runs[run])))\n</pre> import pytrec_eval   def evaluate(run):     evaluator = pytrec_eval.RelevanceEvaluator(qrels, {\"ndcg_cut.10\"})     evaluation = evaluator.evaluate(run)      sum = 0     for ev in evaluation:         sum += evaluation[ev][\"ndcg_cut_10\"]     return sum / len(evaluation)   for run in runs:     print(run, \"\\tndcg_cut_10: {:.4f}\".format(evaluate(runs[run]))) <pre>query_exact \tndcg_cut_10: 0.7870\nquery_256 \tndcg_cut_10: 0.7574\nquery_256_ann \tndcg_cut_10: 0.7552\nquery_rerank \tndcg_cut_10: 0.7886\n</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#exploring-the-potential-of-openai-matryoshka-embeddings-with-vespa","title":"Exploring the potential of OpenAI Matryoshka \ud83e\ude86 embeddings with Vespa\u00b6","text":"<p>This notebook demonstrates the effectiveness of using the recently released (as of January 2024) OpenAI <code>text-embedding-3</code> embeddings with Vespa.</p> <p>Specifically, we are interested in the Matryoshka Representation Learning technique used in training, which lets us \"shorten embeddings (i.e. remove some numbers from the end of the sequence) without the embedding losing its concept-representing properties\". This allow us to trade off a small amount of accuracy in exchange for much smaller embedding sizes, so we can store more documents and search them faster.</p> <p>Exploring the potential of OpenAI Matryoshka \ud83e\ude86 embeddings with Vespa and Matryoshka \ud83e\udd1d Binary vectors: Slash vector search costs with Vespa are good reads on this subject.</p> <p>By using phased ranking, we can re-rank the top K results with the full embeddings in a second step. This produces accuracy on par with using the full embeddings!</p> <p>We'll use a standard information retrieval benchmark to evaluate result quality with different embedding sizes and retrieval/ranking strategies.</p> <p></p> <p>Let's get started! First, install a few dependencies:</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#examining-the-openai-embeddings","title":"Examining the OpenAI embeddings\u00b6","text":""},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#getting-a-sample-dataset","title":"Getting a sample dataset\u00b6","text":"<p>Let's download a dataset so we have some real data to embed:</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#queries","title":"Queries\u00b6","text":"<p>This dataset also comes with a set of queries, and query/document relevance judgements:</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#get-openai-embeddings-for-documents-in-the-dataset","title":"Get OpenAI embeddings for documents in the dataset\u00b6","text":"<p>When producing the embeddings, we concatenate the title and text into a single string. We could also have created two separate embedding fields for text and title, combining the rank scores for these fields in a Vespa rank expression.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#feeding-the-dataset-and-embeddings-into-vespa","title":"Feeding the dataset and embeddings into Vespa\u00b6","text":"<p>Now that we have parsed the dataset and created an object with the fields that we want to add to Vespa, we must format the object into the format that PyVespa accepts. Notice the <code>fields</code>, <code>id</code> and <code>groupname</code> keys. The <code>groupname</code> is the key that is used to shard and co-locate the data and is only relevant when using Vespa with streaming mode.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#embedding-the-queries","title":"Embedding the queries\u00b6","text":"<p>We need to obtain embeddings for the queries from OpenAI. If only using the shortened embedding for the query, you should specify this in the OpenAI API call to reduce latency.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now we can query our data. We'll do it in a few different ways, using the rank profiles we defined in the schema:</p> <ul> <li>Exhaustive (exact) nearest neighbor search with the full embeddings (3072 dimensions)</li> <li>Exhaustive (exact) nearest neighbor search with the shortened 256 dimensions</li> <li>Approximate nearest neighbor search, using the 256 dimension ANN HNSW index</li> <li>Approximate nearest neighbor search, using the 256 dimension ANN HNSW index in the first phase, then reranking top 100 hits with the full embeddings</li> </ul> <p>The query request uses the Vespa Query API and the <code>Vespa.query()</code> function supports passing any of the Vespa query API parameters.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#evaluating-the-query-results","title":"Evaluating the query results\u00b6","text":"<p>We need to get the query relevance judgements into the format supported by pytrec_eval:</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#conclusions","title":"Conclusions\u00b6","text":"<p>What do the numbers mean? They are good, highly relevant results. This is no great surprise, as the OpenAI embedding models are reported to score high on the Massive Text Embedding Benchmark, of which our BEIR/TREC-COVID dataset is a part.</p> <p>More interesting to us, querying with the first 256 dimensions still gives quite good results, while requiring only 8.3% of the memory. We also note that although the HNSW index is an approximation, result quality is impacted very little, while producing the results an order of magnitude faster.</p> <p>When adding a second phase to re-rank the top 100 hits using the full embeddings, the results are as good as the exact search, while retaining the lower latency, giving us the best of both worlds.</p>"},{"location":"examples/Matryoshka_embeddings_in_Vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>For those interested in learning more about Vespa, join the Vespa community on Slack to exchange ideas, seek assistance, or stay in the loop on the latest Vespa developments.</p> <p>We can now delete the cloud instance:</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html","title":"Billion scale vector search with cohere embeddings cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa cohere==4.57 vespacli\n</pre> !pip3 install -U pyvespa cohere==4.57 vespacli In\u00a0[3]: Copied! <pre>import cohere\n\n# Make sure that the environment variable CO_API_KEY is set to your API key\nco = cohere.Client()\n</pre> import cohere  # Make sure that the environment variable CO_API_KEY is set to your API key co = cohere.Client() In\u00a0[4]: Copied! <pre>documents = [\n    \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",\n    \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",\n    \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\",\n]\n</pre> documents = [     \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",     \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",     \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",     \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\", ] In\u00a0[5]: Copied! <pre># Compute the  embeddings of our sample documents.\n# Set input_type to \"search_document\" and embedding_types to \"binary\" and \"int8\"\nembeddings = co.embed(\n    texts=documents,\n    model=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n    embedding_types=[\"binary\", \"int8\"],\n)\n</pre> # Compute the  embeddings of our sample documents. # Set input_type to \"search_document\" and embedding_types to \"binary\" and \"int8\" embeddings = co.embed(     texts=documents,     model=\"embed-english-v3.0\",     input_type=\"search_document\",     embedding_types=[\"binary\", \"int8\"], ) In\u00a0[6]: Copied! <pre>print(embeddings)\n</pre> print(embeddings) <pre>cohere.Embeddings {\n\tresponse_type: embeddings_by_type\n\tembeddings: cohere.EmbeddingsByType {\n\tfloat: None\n\tint8: [[-23, -22, -52, 18, -42, -48, 2, -8, 6, 44, 73, 9, 3, -44, -25, 15, 19, 3, 18, -19, 6, 17, 0, -62, -14, 46, -8, -14, 20, 22, 10, -40, 10, 48, -20, 40, -8, 8, 29, 0, -27, 11, -39, -28, -93, 33, -89, 4, 15, -41, -12, -2, 7, -23, -15, -21, 47, -9, 88, -107, -91, -50, 65, 27, 5, 5, 52, 27, -15, -4, 14, -7, 6, -1, -17, 13, 17, 74, 26, -9, -4, -1, 56, -15, -7, 6, -17, -25, -23, -38, -38, 78, -61, -27, -53, -20, -3, -8, -9, -18, 9, -24, 14, -13, -40, 90, 40, 24, -48, -7, -11, -116, 36, -56, -15, -1, -6, 31, 31, 8, 44, 80, 36, -35, -24, -13, -36, -64, 44, -11, -35, 46, -43, -68, -40, 12, 32, -8, -1, 58, -9, -4, 49, 3, 9, 44, 45, -33, -52, -25, -53, 27, -67, 22, 33, 29, -32, 36, 37, 83, -17, 19, 66, -17, 4, -57, -57, 20, 19, -20, -3, 18, 43, -16, -8, 29, -45, -39, -42, 121, 73, -49, -128, 127, -19, 41, -10, 55, 38, 13, -66, 1, -52, -35, 59, 6, -60, -35, 20, -11, -20, 58, -50, 27, -1, -27, 0, 33, 36, 39, -22, -6, 0, -43, -34, -4, -2, -27, -37, -19, -48, 30, -59, 33, -79, 27, -51, 38, -46, 7, 99, 0, 46, 21, -39, -13, -1, -87, 22, 65, 42, -47, -66, -109, 73, 77, 47, -79, -17, 28, 8, -2, -2, -36, -12, 35, -41, 25, -1, 13, -17, 57, 98, -31, -26, -23, -3, -8, -13, -33, 22, -13, 6, 63, -64, -12, 5, -11, 0, 27, 5, 50, 35, -7, 11, 64, 9, 30, 31, -14, 2, 53, 23, 54, 21, -19, -30, 90, -20, -16, -69, -5, -7, -79, 6, -2, 23, 8, 18, -11, -14, 8, 21, 16, -14, -58, -37, -8, -86, -34, -22, 7, -39, -14, 5, 27, -78, -2, -5, -39, 42, 1, 4, 22, 16, -7, -2, 48, -26, -68, -48, -37, -7, -26, -27, 44, 9, 4, -33, 47, -59, -19, 10, 44, 56, -123, -38, 111, -25, -10, 18, 29, 8, -41, -26, -51, 9, 20, 68, 46, -44, 45, -67, 0, 41, 35, 39, -28, 13, 21, 25, 71, -28, 32, -18, 59, 14, 7, 10, -40, 20, 72, 51, -26, -18, -25, -35, 39, -34, 23, 127, -24, -26, 77, -88, 104, 45, 37, 31, -36, 23, -34, -1, -50, 0, -35, -45, -8, 40, 1, -51, 71, -60, 4, -18, -26, 19, 20, 1, 30, 6, -20, -13, 3, 23, 88, -14, -12, -31, -36, 51, 15, -4, 13, 5, -42, 17, 29, 13, 23, -17, 8, 23, 25, -36, -60, 22, 57, 4, 2, 29, -36, -41, 34, 12, -34, 46, 10, -28, 31, 18, 11, 4, 3, 7, 19, -30, 25, -56, 7, 7, 0, 64, -35, -33, 19, -72, -35, -20, -79, -81, 2, -1, -54, -17, -6, -24, 97, 47, -46, 48, -12, -33, -20, 43, 7, -16, 45, -5, 27, -7, -8, 19, 43, -43, 15, -21, 35, -35, -18, -39, -21, 18, 4, 13, 12, 12, 57, 0, -11, 121, 15, 58, 29, -86, 11, -42, 17, 47, -18, -27, -29, -26, 55, -19, 20, -6, 34, 0, -9, 4, 7, 27, -17, -35, -4, -20, 11, 4, 36, 5, -7, 27, -40, 127, 23, -30, -111, 37, -15, -35, -22, 5, -17, -23, -36, -23, 45, -38, 16, 47, 5, -49, 52, -28, -20, -6, -51, -50, -53, 33, 4, 16, -63, -2, 13, -36, -37, -19, -9, -42, 46, -14, -22, 72, 93, 106, -27, -5, 13, -23, -47, 4, 25, -6, -30, 22, -45, -96, -34, 22, -44, 43, 40, -2, -9, -45, 15, -11, 23, 18, 0, -44, 11, 25, -30, -29, -6, -19, -20, 47, 35, 39, -24, -19, 25, 19, -11, -13, 2, -50, -4, -9, -22, 17, -2, -65, 37, 15, 30, 15, 107, -47, 28, 11, 18, -22, 53, -41, 58, 8, -14, -28, -8, -10, 11, -18, 20, -38, 4, 0, -18, -13, 26, -51, 20, -23, 23, 52, 5, -3, 25, 3, 27, 28, 60, 1, -13, -21, -14, 10, 7, 12, 21, 0, -5, -39, 7, 3, -2, 4, 42, -45, -12, 38, 0, -10, -7, -39, 6, -37, 24, 17, -37, 26, 13, -60, -22, 27, 36, 5, 54, -21, -19, 30, -79, 17, 19, -24, 17, 111, -54, 61, -56, 7, 86, 17, 60, 11, 26, -6, 59, 16, 21, 25, -17, 13, 15, 7, -13, -83, -2, -17, 39, 21, 60, 33, 40, -69, 36, 14, 19, -3, -2, -37, 14, -4, -40, -9, 3, 49, 16, 54, -6, 3, -11, -4, 4, -6, 25, -65, 47, -25, -29, -41, 31, 57, -35, 30, -7, -3, -27, -36, -23, -34, 39, -2, -25, 2, 58, 11, 16, -14, -55, -7, -7, -110, -14, -47, -85, 77, 71, -10, 6, 13, -72, -32, 69, 7, -27, 9, -41, -40, -28, 30, -12, 26, -58, 74, -1, -50, 37, -81, -41, 42, -49, -22, 25, 0, 86, -8, -4, -1, -17, 1, 58, 12, -34, -42, -24, -33, 23, 2, 23, 3, -44, -33, -19, 14, -70, 7, 25, -13, -90, -57, -29, -11, -46, -34, 6, 14, 79, 108, 26, 31, 3, -9, 27, 66, 2, 41, -17, -19, 62, 23, 48, -20, 6, -88, 74, -59, -53, 67, -77, -32, 1, -3, -43, 22, -45, -34, 20, 60, 58, -65, -48, 116, 76, 127, 24, -29, 59, 10, -20, -57, -19, -3, 35, 19, 3, 34, 6, 55, 27, 35, -4, -55, 32, 22, -4, -12, -34, -50, -16, 0, -22, 75, -48, -51, -26, -12, 1, -9, -17, -26, -4, -60, -128, -3, -19, -23, -17, -4, -5, -5, 37, -8, -21, -1, -16, 49, 6, -31, -21, -18, -13, 33, -11, -29, 16, -31, 41, -19, 0, 57, -4, -9, 16, 27, -27, 6, 104, -53, 39, -6, -8, 3, 0, 4, 39, -46, 33, 10, 26, -19, 53, 41, 31, 15, 12, 2, 44, -67, -18, -88, -29, 27, 3, 55, -8, -6, 38, 0, 13], [-18, -43, -29, 10, -43, -28, 0, -20, -10, 81, 107, 17, 35, -44, -27, 54, -4, 31, 17, -23, 19, -18, -41, -67, 31, -74, -39, 18, -4, 2, 13, 37, -7, 51, -7, 42, 9, 11, 43, 22, 12, 0, -32, -20, -39, 9, -21, -28, 27, -33, -11, 25, 11, -44, -24, -38, 109, -75, 73, -125, -89, -59, 103, 43, 20, -14, -24, 8, -3, 55, 22, -23, -4, 8, -25, -1, 28, 37, 28, 0, -27, 13, 40, -8, -43, -16, -39, -13, 9, 7, -11, 42, -32, 63, 2, -42, 2, 14, -34, -30, 17, -45, 21, -19, -41, 123, 32, 55, -63, -7, 11, -128, 28, 7, -29, -18, -17, 51, 7, 46, 25, 70, 61, -86, -7, -8, -27, -92, 88, 8, -20, 19, -42, -29, -19, 5, -10, 38, -8, 68, -45, -51, 46, 0, 5, 39, 35, -16, 2, -56, -16, 16, -26, 6, 21, -12, -28, 6, 53, 31, -35, -5, 20, 20, -1, 0, 46, 14, 33, 30, 29, -4, 56, 8, -21, -3, -3, -122, -24, 127, 71, 5, -128, 83, 30, -52, -14, -49, 29, 23, -21, 4, -45, -22, 16, 39, -64, 29, -2, -31, 18, 10, 27, 2, 4, 18, -13, 31, 91, 23, -37, -2, 2, -32, -69, 14, -7, 8, -38, 47, -45, 6, -52, -2, -24, 44, -50, 28, 18, 21, 98, -20, -25, 53, -2, 16, 68, 29, 14, -23, -4, -91, -40, 40, -30, 46, 17, 11, 37, 24, -18, -65, 13, -110, 39, 13, 15, 69, -78, 31, -39, 54, 43, -4, -21, 13, -36, -21, -62, 51, 56, -66, 8, 59, -80, 23, -13, 6, -2, 38, -17, 55, 11, -17, -19, -20, 23, -5, -13, -47, 31, -16, -21, 15, -26, -35, -39, 1, 28, -15, -52, 63, -3, 8, -9, 1, -20, 4, 0, -34, -19, 27, 17, -9, -11, -42, -10, 0, -66, -34, -7, -21, 17, -1, -11, 1, -7, 10, -5, 7, 127, 72, 37, 0, 49, -14, 28, -32, -11, 20, 31, 30, 0, -71, -50, 66, 9, 25, -28, 29, -43, -40, -27, -13, -1, -78, 23, 46, -33, -22, 1, -11, -22, -16, 36, -26, -24, 7, 5, 2, -29, 30, -87, -21, -5, 49, 0, -50, 23, -13, -11, 29, 24, 44, 3, 30, -44, -9, 13, 3, -10, -16, 16, -27, 54, -28, 6, 110, -99, -21, 127, 2, -1, 52, -86, 94, 23, 36, 22, -18, -14, 5, -59, 0, -26, -22, -103, 0, -18, 17, -50, 99, -72, 28, 48, 47, 9, -48, 51, 40, 45, -15, -34, -6, 14, 103, -1, 48, -21, 0, 41, -9, -6, 66, -11, 4, -33, -2, 52, 0, 16, -8, 58, 3, -33, -9, 50, 51, 20, 43, 64, 0, -53, 39, -41, -20, 98, 5, -49, 18, -39, 25, 5, 30, -9, 57, -31, 3, -41, 32, -2, 11, 33, -27, -47, 36, -76, -34, -4, -47, -51, -19, 31, -30, 14, -14, -30, 100, 42, -52, 47, -24, -77, -1, 45, 9, 20, 52, 4, 83, 44, 5, 45, 49, -15, -3, 81, 2, 22, -23, -39, -27, 20, 32, -14, 10, -21, 17, 13, 32, 77, -9, 45, 29, -51, -24, -4, 29, 22, -50, 7, 10, -25, -2, -20, 30, -35, 27, -12, -1, -9, -15, 27, -5, -29, -85, -52, -20, 16, 68, 48, -23, -6, -20, 92, 19, -63, -128, 30, -9, -51, -36, 54, 45, -24, -41, 10, 36, -32, -28, -2, 25, 3, 44, -61, 32, 33, -7, -31, -2, 20, 7, -31, -17, -2, 19, 63, 26, 61, -4, -18, 23, 3, -26, -11, 59, 45, -22, 14, 7, -11, -30, -21, 48, -18, -25, 2, -20, -25, -38, 15, -4, 5, 8, 18, -37, -42, -56, -41, -10, -67, -2, -54, -86, -4, 49, 1, -2, -21, -11, 59, 14, 10, -59, -62, -15, -15, -19, 3, 6, -19, -1, -46, 4, 51, -17, -32, 37, 1, 13, 19, 114, -11, 6, 21, -12, 1, 21, -22, 10, -31, 11, -51, -39, -4, 2, 78, 30, 28, -97, -8, -53, -12, 15, -19, 30, -15, -2, 43, 15, 53, 93, 0, 55, 23, 19, -23, -51, -3, 1, 2, -26, 14, -27, -15, 61, 26, -16, 9, 4, 12, 24, -16, -14, 43, -20, 35, 34, -18, -14, -33, -20, 4, -29, 20, -6, -37, -21, 13, 40, -36, 30, 12, -34, -3, -52, -5, 4, -58, -21, 57, -29, 11, -48, -15, 12, -4, 26, -22, -43, 4, 41, -58, 25, 10, 17, -33, 33, -60, 30, -50, 20, 34, 12, 20, 65, -57, -1, -16, 41, 26, 92, 20, 16, -128, -11, 2, -30, -41, -17, 35, 9, 67, 3, -21, -13, 17, 19, 8, 26, -37, 47, 10, 33, 34, 35, 14, -12, 55, -5, -65, -14, -84, 5, -30, 35, -5, -27, 22, 34, 16, 32, 0, 33, -12, 72, -68, 0, -50, -50, 20, 37, -43, 74, 0, -11, 15, -43, 23, -49, -29, -35, 61, 47, 85, -3, -9, -53, 41, -20, -14, 11, -59, 1, -1, -56, -12, 11, 19, 4, 38, -76, -23, 18, 20, -15, -25, -55, -69, 53, 54, -82, -60, 6, 17, -74, -40, -25, -40, -83, 0, 33, -73, -101, -50, -39, -26, 43, -67, 1, -51, -11, 24, -26, 23, -43, 7, 11, 73, 3, 59, 2, -7, 89, 54, 58, -37, 1, -84, 88, -65, -56, -11, -60, -58, 21, 25, -54, 16, -42, -58, -14, 24, 17, -41, 27, 48, 40, 79, 13, -38, 31, -8, 15, 4, 6, 3, -33, -59, 45, 55, -7, -12, 0, 10, -22, -17, 35, 4, -7, 4, -23, -34, -23, -3, 58, 50, -32, -72, -37, -56, 36, -46, 6, 3, 12, -39, 20, 13, 37, 15, 9, 0, -28, -21, -14, 4, 17, 57, 1, -24, -12, -1, -14, -11, -47, -13, 0, -36, -44, -4, -43, -48, -33, -4, 8, -19, 12, -23, 24, -10, 18, 19, 38, -5, -6, 54, -28, 41, -19, -28, -19, 17, -26, -41, 9, -15, 90, 33, 20, -6, 82, -60, -40, -84, -36, -3, -62, 6, -34, -10, -31, -31, 20], [5, 6, -45, -35, -3, -37, -11, -10, -18, 36, 26, 57, -17, -33, 25, -7, -3, 66, 44, 8, 1, 40, -34, -57, 2, -34, -7, 12, 9, 2, 20, -4, 19, 37, 2, 58, -16, 28, 50, -19, 20, -14, -59, -48, -98, 61, -61, 48, 7, -50, -4, -35, 11, -32, -38, -37, 60, -75, 47, -48, -56, -41, 69, 12, 3, -1, 85, 32, -35, -21, 23, 17, 12, -33, -6, -30, -2, -14, 39, 12, 34, 64, -5, -65, 24, -60, -14, -20, -58, -27, -48, -33, -87, 6, 11, -23, -33, -87, 3, 22, 52, -50, 73, -2, -33, 99, 4, 86, -9, 8, 18, -104, 40, -12, -64, -19, -3, -5, 11, -18, -4, 13, 77, -36, 32, 7, -56, -34, 65, -40, -24, 76, -62, -88, -58, 32, 4, 22, 23, 8, -2, -43, 39, 39, -6, -24, 8, 14, 13, 22, -42, 21, -36, 4, 26, 30, -25, 32, 0, 66, 12, 7, 16, 8, -16, -21, 9, 7, 29, -26, -4, 12, 55, 21, 16, 69, -2, -53, -50, 127, 98, -3, -128, 116, -8, -27, 11, 19, -4, 10, 16, 8, -38, -22, 66, 11, -37, -19, 30, -62, 29, 47, -34, -12, -57, -16, -14, 35, 47, 38, -16, -7, -6, -4, -18, -24, -23, -48, -25, -17, -7, 22, -27, 64, -23, 1, -29, 5, -32, 14, 18, 16, 23, 37, -1, 21, 46, -50, 19, 21, 49, -71, -17, -17, 34, 16, 33, -31, -4, 69, -57, 39, 3, -43, -22, 69, -50, 33, -32, 8, -7, 112, 84, 17, -23, -4, 1, 0, -9, -14, 26, -22, 17, 102, -47, -15, 26, -22, -32, 40, -22, 29, -7, -26, -21, -55, 11, -4, 16, -9, 39, 14, 8, 36, -13, -32, -88, 38, 22, -19, -69, 43, -15, -15, 8, 4, -29, 21, 23, -13, -55, 9, 23, -32, 21, -37, -23, -4, -55, -3, 28, -28, 19, -48, -1, -20, -59, 2, -30, 42, -9, 47, 24, 100, -12, 9, -9, 12, -41, -10, -49, -11, 16, -64, 21, 49, 33, 27, -46, 68, -75, -44, 3, 41, 62, -81, -31, 72, 13, -30, -28, 27, -22, 16, -12, -24, 8, 25, 16, 11, -64, 34, -13, -11, 8, 29, 16, -29, 16, 20, 38, 44, 22, 13, 12, 29, -23, -26, 25, -25, -8, 27, 41, -23, 10, -7, -45, 0, -63, 16, 127, -21, -8, 52, -59, 74, 55, 40, 18, 2, -12, -9, -42, -8, -11, -9, -71, 1, -2, 27, -50, 80, -62, 21, -4, 16, -25, 10, -8, -9, 0, -32, -8, -3, -11, 57, -5, 37, 0, -41, 52, 29, -20, 18, -18, -22, 46, 29, 36, 8, 21, -25, 42, 9, -30, 49, 22, 13, -3, 33, 35, 25, -75, -13, -33, -77, 95, -2, 1, -16, -49, 92, -27, 7, 13, 77, -13, -13, -42, 17, -57, 19, -30, -12, -45, 28, -45, -13, -8, 0, -16, 2, 47, -28, -9, 30, -38, 127, 39, -30, 15, -18, -16, 10, 14, -9, 41, 27, 18, 63, 14, 3, 45, 5, -24, 41, -36, 46, -32, -28, 4, -10, 18, 35, 0, -15, -15, -24, -29, 32, 43, 16, 23, -14, 7, -13, -54, 11, 69, 40, -2, -9, -26, 82, 0, -24, -27, 38, -94, 54, -31, -22, 20, -27, -13, -128, -39, -22, 47, 78, 36, 6, -4, -45, 33, 17, -37, -103, 55, -41, -42, -46, -17, -29, 8, 11, 25, 60, 10, -28, 54, -65, -62, -10, -40, 26, 30, 13, -24, -25, -17, -4, -15, -54, 19, 48, -47, -38, -3, 6, 1, 18, -6, -13, -1, 63, 111, -18, -10, -11, -6, -62, -19, 53, -25, 9, 75, -50, -42, -43, 2, 26, 5, 0, -25, -62, -21, -27, -25, -1, 1, 19, -47, -37, -16, 13, -23, -40, -3, 19, 23, 38, 43, -102, 71, 5, -13, 52, -18, -29, -68, 2, -48, 28, 54, 9, -12, -14, -37, 3, 50, 63, -109, 63, 8, 21, -28, 58, -30, -2, 22, -14, -37, 28, 9, -48, 14, 1, -94, 10, -8, 6, 45, -5, -39, 26, -43, 5, 50, 55, -5, 6, 9, 11, 4, 29, -4, -6, -16, -56, 6, 16, 0, 14, 8, 39, -35, 10, 38, 28, 43, -11, -128, -36, 22, 2, -32, 28, 30, -13, 1, -40, 5, 13, 24, -52, -16, 16, 15, 15, -41, 91, -32, -43, 28, -21, -32, -2, -57, 25, 70, -32, 37, -25, -19, 67, 2, 53, 10, -25, -9, 79, -20, -20, 30, -35, -31, -36, -26, 20, -71, 25, 18, -18, 8, -31, 12, 118, -27, 43, 24, 15, 5, 49, -66, 21, -33, -40, 14, 52, 35, 72, 36, -38, 24, 0, -20, -19, 5, -32, -60, 51, 29, 21, -1, 40, 82, 24, 88, 8, -45, -29, -83, -37, -39, 42, -10, -34, 36, -7, 19, -18, -10, 0, -39, 35, -98, -27, -35, -15, 28, 44, -7, 13, -31, -24, 25, -29, 13, 62, 8, 14, 64, 61, 107, -42, 51, -86, 70, 0, -18, 60, -76, -17, 11, -70, -11, 32, -15, 60, 14, -43, -14, 0, 7, 20, 7, -24, -1, -28, -42, 66, 29, -2, 1, -10, -60, 2, -1, -48, 8, 78, -25, -62, -57, 29, -41, 46, -36, 41, -78, -19, -10, -33, 6, -19, -18, -12, 44, 10, 22, -52, 1, 10, 37, 50, -24, -15, -50, 13, -22, -29, 44, -74, -50, 20, 44, -18, 50, -42, -53, 25, 35, 46, -30, -39, 76, 12, 127, 21, -9, 10, 59, -43, -11, 22, 45, -20, 8, 17, 10, -27, 14, -11, 43, -22, -108, 72, -1, -1, -37, -29, 6, 50, -15, -12, 76, -51, -91, -48, -45, -9, -14, -31, 28, 16, -47, -41, 8, 10, 20, -17, -19, -35, 13, -8, -5, 4, 80, 46, 20, 35, -44, 39, -22, -54, -11, -9, -38, -28, 9, 6, -4, 3, -24, -63, 43, 56, 2, 9, -12, 127, -65, 22, -30, -9, -41, -43, -23, 50, -43, 61, 24, 81, -35, 36, 53, 30, -23, 43, -38, 43, -40, 13, -18, 0, 0, 3, -52, -45, 8, 46, -16, 38], [-47, -19, -104, 29, -32, -72, 0, 5, -53, 69, 56, 17, 0, -38, -10, 10, -44, 69, 20, -17, -2, -45, -19, -128, 34, -4, -64, 21, -23, -9, 13, 21, 28, 55, -52, 39, -1, 24, 0, 30, 2, 5, 28, 3, -30, 19, -33, -47, 27, -35, -29, -28, 5, -41, -40, -60, 43, -21, 49, -92, -60, -22, 59, 65, 35, -10, 24, 35, -76, 31, 35, -58, -4, -13, -47, 4, 12, -7, 18, -14, -36, 47, -8, -35, -28, -15, -41, -18, -72, -38, -39, 36, -128, 20, 44, -26, -8, 14, 1, 17, -20, -23, 1, -38, -30, 85, 61, 81, -16, 4, 2, -39, 40, -77, -22, 26, 24, 48, 56, 41, 25, 99, -37, -16, 41, 50, 16, -61, -25, -18, -34, 48, 60, 20, -16, 0, 28, -17, 28, 12, 49, -46, 13, 47, -7, 10, 19, 15, 19, -26, 8, -24, -22, 12, -5, 7, -28, -4, 32, 21, 38, 16, 16, -1, -15, -32, -32, 12, 9, 47, 9, -5, -60, -39, -35, -14, -9, -10, -65, 127, 93, -48, -118, 63, 58, -71, 21, -58, -32, 37, -21, 33, 1, 0, -21, -27, -17, 12, -17, 0, -50, 64, -39, 52, 11, 24, -11, 33, 0, 0, -4, -37, 23, -52, -11, 31, -8, 1, -30, -1, -8, 6, -33, 43, 34, 27, -36, 7, 39, 19, 8, 30, 11, -12, -33, 33, 103, -15, 0, 8, 28, -66, -45, -18, 69, 64, 27, -22, 27, 60, -35, -49, 12, -86, -29, 50, 51, 71, -25, 27, -50, 72, 11, -1, -59, 63, 13, 29, -29, 8, 32, 41, -13, 60, -39, 58, -27, 0, -5, 48, 16, 65, 36, -11, 18, 59, 18, -7, -19, -41, 25, 6, 9, -9, 0, -30, 18, 73, 2, -31, -74, -32, -43, -44, 38, -18, 25, -4, 31, 31, -66, 35, 21, -39, 20, -36, -41, -6, -71, -11, -76, -57, 16, -51, -20, 28, -60, 34, 7, -1, 102, 58, 46, 3, 32, -37, -4, 32, -26, -4, -27, -56, -9, -47, -59, 62, 1, 18, -26, 18, -28, -30, 26, 46, 13, -18, -19, 53, -32, -27, -48, 13, -39, -1, 15, -15, -11, 29, 33, -5, 25, -21, -4, -16, 27, -46, 4, -40, 51, 21, 13, 0, 14, -16, 56, -28, -68, -22, 60, 52, 20, -29, 52, -27, -25, -28, -33, 34, -30, 39, 15, -16, -30, 19, -44, 4, 5, -29, 14, 34, -38, -62, -54, -72, 8, 2, -98, -33, 71, 36, -85, 71, -68, 51, -7, 3, 2, -9, 59, 62, 19, 5, -17, 4, 19, 9, -21, -24, 10, -20, 68, 3, -18, 59, -11, -2, -4, 20, 17, -7, 35, -12, 30, 20, -48, 72, 73, 30, 15, 91, 35, 19, -13, 0, -43, 12, 43, 8, -32, -2, -17, 35, -1, 51, 2, 100, -37, 44, -112, 33, -64, 36, 20, -50, -63, 31, -75, -69, 11, 1, -68, -1, 34, -19, 3, 39, -12, 38, 7, -54, -8, 0, 26, 21, 6, -8, -9, 2, -16, 18, -37, -19, 76, -18, 16, -31, 112, -42, 53, -5, -9, -13, 55, 19, -8, -39, 61, -11, -13, 3, 56, 33, 30, 4, -25, -46, -14, -32, 13, 9, 11, 4, -46, 38, -9, 39, -47, 27, -1, 59, -49, -80, 49, -32, -17, -44, 14, -40, 3, 17, 32, 17, -23, -39, 32, 11, -38, -87, 118, -81, -52, -59, -1, -24, 39, 11, 10, 24, 16, 18, 8, 29, 0, 27, 7, -21, 7, -38, 8, -31, -14, -15, 27, -83, -18, -21, -28, -15, -6, 0, 22, 16, 0, -15, -34, 21, 62, 35, 2, 39, -13, -64, 30, -1, 31, 9, -5, 24, -84, -6, -14, -38, 15, -76, -39, -27, -21, -73, -64, 9, -84, 10, -46, -92, -29, 79, -8, -45, -3, 38, 32, 20, -13, -35, -4, 0, -24, -12, -17, 4, 2, 4, -11, -11, 12, 14, -41, 49, 1, 6, -37, 49, 5, -6, -31, 48, 82, 54, 8, -1, 8, 69, -13, -42, 12, 24, 16, 15, 8, -61, -4, -6, 54, -34, -20, 14, 11, 8, 21, 0, 8, 43, -14, 29, 54, 30, -38, -46, 15, 22, 8, -22, 23, -32, -7, 22, 15, -6, -3, -12, 5, 47, -16, -8, 35, -45, -31, -40, -71, 22, -41, 22, 7, -37, 6, 24, -62, 14, 19, 40, -34, 53, -26, -44, 27, -23, 8, 42, 34, -4, 39, -33, 64, -13, 27, 59, 8, 87, 61, 43, 16, 43, -3, 4, 33, -3, 10, -39, -20, -8, -71, 11, 5, 44, -8, 40, -44, 6, -2, 0, -44, 65, -10, -4, -34, -32, 13, -32, -19, -16, 10, 0, 15, 12, -1, 8, 51, -5, -20, -7, 29, 47, -39, -1, -3, 37, 14, -8, 86, -15, -15, -94, -7, -25, 6, 27, -29, 12, 10, 44, 4, -36, 9, -17, 108, 35, -72, -9, -56, -55, 54, 53, -28, 32, 37, -6, 5, 5, -1, 26, 39, 14, -29, -24, 54, 13, 0, -128, 95, 0, -38, 40, -90, -7, -46, -23, 36, 11, 30, 26, 25, -49, -101, 44, 20, 59, 12, -40, -77, -35, 27, -10, 32, 18, 2, -107, -50, -60, -19, -128, 4, 76, 9, -71, -35, 7, 5, -50, -6, 9, -101, -68, -1, -11, 2, -95, 19, -10, 34, -22, 48, 11, 8, 78, 34, 8, -58, -14, -31, 29, -35, 3, -39, -42, 4, -12, -11, -41, 1, -33, 24, 30, 70, 48, -37, -9, 127, 61, 127, -5, -19, 30, 4, -9, -29, 10, -57, -7, 5, -16, -17, -26, 9, 47, 34, -3, 2, 17, 11, 32, 1, 20, -31, -22, 24, 8, 51, 3, -25, 44, 41, 32, -75, 14, -25, -32, -45, -28, 41, 64, 37, 12, 18, -34, -42, -59, -10, 40, 46, 4, -1, 0, -31, 20, -15, -56, 18, -19, 16, 51, -36, 44, -61, 15, 9, 1, -22, -14, -12, 4, -21, 13, 9, 31, 12, 10, 62, -45, 35, 23, 3, 0, -19, 0, 10, -10, -44, 35, -10, -89, -48, 41, 12, -32, -7, -27, -13, -23, -67, -34, -6, -16, -33, 16]]\n\tuint8: None\n\tbinary: [[-110, 121, 110, -50, 87, -59, 8, 35, 114, 30, -92, -112, -118, -16, 7, 96, 17, 19, 97, -9, -23, 25, -103, -35, -78, -45, 72, -123, -41, 67, 14, -31, -42, -126, 75, 111, 62, -64, 57, 64, -52, -66, -64, -12, 100, 99, 87, 61, -5, 5, 23, 34, -75, -66, -16, 91, 92, 121, 55, 117, 100, -112, -24, 84, 84, -65, 61, -31, -45, 7, 44, 8, -35, -125, 16, -50, -52, 11, -105, -32, 102, -62, -3, 86, -107, 21, 95, 15, 27, -79, -20, 114, 90, 125, 110, -97, -15, -98, 21, -102, -124, 112, -115, 26, -86, -55, 67, 7, 11, -127, 125, 103, -46, -55, 79, -31, 126, -32, 33, -128, -124, -80, 21, 27, -49, -9, 112, 103], [-110, -7, -24, 23, -33, 68, 24, 35, 22, -50, -32, 86, 74, -14, 71, 96, 81, -45, 105, -25, -73, 108, -99, 13, -76, 125, 73, -44, -34, -34, -105, 75, 86, -58, 85, -30, -92, -27, -39, 0, -91, -2, 30, -12, -116, 9, 81, 39, 76, 44, 87, 20, -107, 110, -75, 20, 44, 125, -75, 85, -28, -118, -24, 127, 78, -75, 108, -20, -48, 3, 12, 12, 71, -29, -98, -26, 68, 11, 0, -104, 96, 70, -3, 53, -98, -108, 127, -102, -17, -84, -88, 88, -54, -45, -11, -4, -4, 15, -67, 122, -108, 117, -115, 40, 98, -47, 102, -103, 3, -123, -85, 119, -48, -24, 95, -34, -26, -24, -31, -9, 99, 64, -128, -43, 74, -91, 80, -95], [64, -14, -4, 30, 118, 5, 8, 35, 51, 3, 72, -122, -70, -10, 2, -20, 17, 115, -67, -11, 115, 31, -103, -73, -78, 65, 64, -123, -41, 91, 14, -39, -41, -78, 73, -62, 60, -28, 89, 32, 33, -35, -62, 116, 102, -45, 83, 63, 73, 37, 23, 64, -43, -46, -106, 83, 109, 92, -87, -15, -60, -39, -23, 63, 84, 56, -6, -15, 20, 3, 76, 3, 104, -16, -79, 70, -123, 15, -125, -111, 109, -105, -99, 82, -19, -27, 95, -113, 94, -74, 57, 82, -102, -7, -95, -21, -3, -66, 73, 95, -124, 37, -115, -81, 107, -55, -25, 6, 19, -107, -120, 111, -110, -23, 79, -26, 106, -61, -96, -77, 9, 116, -115, -67, -63, -9, -43, 77], [-109, -7, -32, 19, 87, 116, 8, 35, 54, -102, -64, -106, -14, -10, 31, 78, -99, 59, -6, -45, 97, 96, -103, 37, 69, -35, 9, -59, 95, 25, 14, 73, 86, -9, -43, 110, -70, 96, 45, 32, -91, 62, -64, -12, 100, -55, 34, 62, 14, 5, 22, 67, -75, -17, -14, 81, 45, 125, -15, -11, -28, 75, -25, 20, 42, -78, -4, -67, -44, 11, 76, 3, 127, 40, 0, 103, 75, -62, -123, -111, 68, -13, -10, -5, -66, -89, 119, -70, -29, -95, -19, 82, 106, 127, -24, -11, -48, 15, -29, -102, -115, 107, -115, 55, -69, -61, 103, 11, 3, 25, -118, 63, -108, 11, 78, -28, 14, 124, 119, -61, 97, 84, 53, 69, 123, 89, -104, -127]]\n\tubinary: None\n}\n\tmeta: {'api_version': {'version': '1'}, 'billed_units': {'input_tokens': 106}}\n}\n</pre> <p>As we can see from the above, we got multiple vector representations for the same input strings.</p> In\u00a0[54]: Copied! <pre>print(\n    \"int8 dimensionality: {}, binary dimensionality {}\".format(\n        len(embeddings.embeddings.int8[0]), len(embeddings.embeddings.binary[0])\n    )\n)\n</pre> print(     \"int8 dimensionality: {}, binary dimensionality {}\".format(         len(embeddings.embeddings.int8[0]), len(embeddings.embeddings.binary[0])     ) ) <pre>int8 dimensionality: 1024, binary dimensionality 128\n</pre> In\u00a0[8]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"doc\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"binary_vector\",\n                type=\"tensor&lt;int8&gt;(x[128])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: hamming\"],\n            ),\n            Field(\n                name=\"int8_vector\",\n                type=\"tensor&lt;int8&gt;(x[1024])\",\n                indexing=[\"attribute\"],\n                attribute=[\"paged\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"doc\",     mode=\"index\",     document=Document(         fields=[             Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"binary_vector\",                 type=\"tensor(x[128])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: hamming\"],             ),             Field(                 name=\"int8_vector\",                 type=\"tensor(x[1024])\",                 indexing=[\"attribute\"],                 attribute=[\"paged\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])], ) <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[9]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"coherebillion\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"coherebillion\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p><code>unpack_bits</code>, unpacks the binary representation into a 1024-dimensional float vector doc.</p> <p>We define three tensor inputs that we intend to send with the query request.</p> In\u00a0[10]: Copied! <pre>from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q_binary)\", \"tensor&lt;int8&gt;(x[128])\"),\n        (\"query(q_full)\", \"tensor&lt;float&gt;(x[1024])\"),\n        (\"query(q_int8)\", \"tensor&lt;int8&gt;(x[1024])\"),\n    ],\n    functions=[\n        Function(  # this returns a tensor&lt;float&gt;(x[1024]) with values -1 or 1\n            name=\"unpack_binary_representation\",\n            expression=\"2*unpack_bits(attribute(binary_vector)) -1\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"sum(query(q_full)*unpack_binary_representation )\"  # phase 1 ranking using the float query and the unpacked float version of the binary_vector\n    ),\n    second_phase=SecondPhaseRanking(\n        expression=\"cosine_similarity(query(q_int8),attribute(int8_vector),x)\",  # phase 2 using the int8 vector representations\n        rerank_count=30,  # number of hits to rerank, upper bound on number of random IO operations\n    ),\n    match_features=[\n        \"distance(field, binary_vector)\",\n        \"closeness(field, binary_vector)\",\n        \"firstPhase\",\n    ],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q_binary)\", \"tensor(x[128])\"),         (\"query(q_full)\", \"tensor(x[1024])\"),         (\"query(q_int8)\", \"tensor(x[1024])\"),     ],     functions=[         Function(  # this returns a tensor(x[1024]) with values -1 or 1             name=\"unpack_binary_representation\",             expression=\"2*unpack_bits(attribute(binary_vector)) -1\",         )     ],     first_phase=FirstPhaseRanking(         expression=\"sum(query(q_full)*unpack_binary_representation )\"  # phase 1 ranking using the float query and the unpacked float version of the binary_vector     ),     second_phase=SecondPhaseRanking(         expression=\"cosine_similarity(query(q_int8),attribute(int8_vector),x)\",  # phase 2 using the int8 vector representations         rerank_count=30,  # number of hits to rerank, upper bound on number of random IO operations     ),     match_features=[         \"distance(field, binary_vector)\",         \"closeness(field, binary_vector)\",         \"firstPhase\",     ], ) my_schema.add_rank_profile(rerank) In\u00a0[39]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[41]: Copied! <pre>for i, doc in enumerate(documents):\n    response = app.feed_data_point(\n        schema=\"doc\",\n        data_id=str(i),\n        fields={\n            \"doc_id\": str(i),\n            \"text\": doc,\n            \"binary_vector\": embeddings.embeddings.binary[i],\n            \"int8_vector\": embeddings.embeddings.int8[i],\n        },\n    )\n    assert response.is_successful()\n</pre> for i, doc in enumerate(documents):     response = app.feed_data_point(         schema=\"doc\",         data_id=str(i),         fields={             \"doc_id\": str(i),             \"text\": doc,             \"binary_vector\": embeddings.embeddings.binary[i],             \"int8_vector\": embeddings.embeddings.int8[i],         },     )     assert response.is_successful() In\u00a0[30]: Copied! <pre>query = \"Who discovered x-ray?\"\n\n# Make sure to set input_type=\"search_query\" when getting the embeddings for the query.\n# We ask for 3 versions (float, binary, and int8) of the embeddings.\nquery_emb = co.embed(\n    [query],\n    model=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n    embedding_types=[\"float\", \"binary\", \"int8\"],\n)\n</pre> query = \"Who discovered x-ray?\"  # Make sure to set input_type=\"search_query\" when getting the embeddings for the query. # We ask for 3 versions (float, binary, and int8) of the embeddings. query_emb = co.embed(     [query],     model=\"embed-english-v3.0\",     input_type=\"search_query\",     embedding_types=[\"float\", \"binary\", \"int8\"], ) In\u00a0[\u00a0]: Copied! <pre>print(query_emb)\n</pre> print(query_emb) <p>Now, we use Vespa's nearestNeighbor query operator to expose up to 1000 hits to ranking using the configured distance-metric (hamming distance).</p> <p>This is the retrieve logic, or phase-0 search as it only uses the hamming distance. See phased ranking for more on phased ranking pipelines.</p> <p>The hits that are near in hamming space, are exposed to the flexibility of the Vespa ranking framework:</p> <ul> <li>the first-phase uses the unpacked version of the binary vector and computes the dot product against the float query version</li> <li>The second phase and final phase re-ranks the 30 best from the the previous phase, here using cosine similarity between the int8 embedding representations</li> </ul> In\u00a0[47]: Copied! <pre>response = app.query(\n    yql=\"select * from doc where {targetHits:1000}nearestNeighbor(binary_vector,q_binary)\",\n    ranking=\"rerank\",\n    body={\n        \"input.query(q_binary)\": query_emb.embeddings.binary[0],\n        \"input.query(q_full)\": query_emb.embeddings.float[0],\n        \"input.query(q_int8)\": query_emb.embeddings.int8[0],\n    },\n)\nassert response.is_successful()\n</pre> response = app.query(     yql=\"select * from doc where {targetHits:1000}nearestNeighbor(binary_vector,q_binary)\",     ranking=\"rerank\",     body={         \"input.query(q_binary)\": query_emb.embeddings.binary[0],         \"input.query(q_full)\": query_emb.embeddings.float[0],         \"input.query(q_int8)\": query_emb.embeddings.int8[0],     }, ) assert response.is_successful() In\u00a0[48]: Copied! <pre>response.hits\n</pre> response.hits Out[48]: <pre>[{'id': 'id:doc:doc::3',\n  'relevance': 0.45650564242263414,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0030303030303030303,\n    'distance(field,binary_vector)': 329.0,\n    'firstPhase': 4.905200004577637},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::3',\n   'doc_id': '3',\n   'text': 'Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity'}},\n {'id': 'id:doc:doc::1',\n  'relevance': 0.337421116422118,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.002544529262086514,\n    'distance(field,binary_vector)': 391.99999999999994,\n    'firstPhase': 3.7868080139160156},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::1',\n   'doc_id': '1',\n   'text': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.'}},\n {'id': 'id:doc:doc::2',\n  'relevance': 0.280400768492745,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0026595744680851063,\n    'distance(field,binary_vector)': 375.0,\n    'firstPhase': 3.854860305786133},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::2',\n   'doc_id': '2',\n   'text': 'Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.'}},\n {'id': 'id:doc:doc::0',\n  'relevance': 0.2570603626828106,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0024390243902439024,\n    'distance(field,binary_vector)': 409.0,\n    'firstPhase': 2.845644474029541},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::0',\n   'doc_id': '0',\n   'text': 'Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.'}}]</pre> <p>The <code>relevance</code> is the cosine similarity between the int8 vector representations calculated in the second-phase. Note also that we return the <code>hamming</code> distance and the firstPhase score which is the query, unpacked binary dot product.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#billion-scale-vector-search-with-cohere-binary-embeddings-in-vespa","title":"Billion-scale vector search with Cohere binary embeddings in Vespa\u00b6","text":"<p>Cohere just released a new embedding API with support for binary and <code>int8</code> vectors. Read the announcement in the blog post: Cohere int8 &amp; binary Embeddings - Scale Your Vector Database to Large Datasets.</p> <p>We are excited to announce that Cohere Embed is the first embedding model that natively supports int8 and binary embeddings.</p> <p>This is huge because:</p> <ul> <li>Binarization reduces the storage (disk/memory) footprint from 1024 floats (4096 bytes) per vector to 128 bytes.</li> <li>Faster distance calculations using hamming distance that Vespa natively supports for bits packed into <code>int8</code> tensor cells. More on hamming distance in Vespa.</li> <li>Multiple vector representations allow for coarse retrieval in hamming space and subsequent phases using higher-resolution representations.</li> <li>Drastically reduces the deployment due to tiered storage economics.</li> </ul> <p>Vespa supports <code>hamming</code> distance with and without HNSW indexing.</p> <p>For those wanting to learn more about binary vectors, we recommend our 2021 blog series on Billion-scale vector search with Vespa and Billion-scale vector search with Vespa - part two.</p> <p>This notebook demonstrates using the Cohere embeddings with a coarse-to-fine search and re-ranking pipeline that reduces costs, but offers the same retrieval (nDCG) accuracy.</p> <ul> <li>The packed binary vector representation is stored in memory, with an optional HNSW index using hamming distance.</li> <li>The <code>int8</code> vector representation is stored on disk using Vespa's paged option.</li> </ul> <p>At query time:</p> <ul> <li>Retrieve in hamming space (1000 candidates) as the coarse-level search using the compact binary representation.</li> <li>Re-rank by using a dot product between the float version of the query vector (1024 dims) against an unpacked float version of the binary embedding (also 1024 dims)</li> <li>A re-ranking phase using the 1024 dimensional int8 representations. This stage pages the vector data from the disk using Vespa's paged option (unless it is already cached).</li> </ul> <p></p> <p>Install the dependencies:</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#examining-the-cohere-embeddings","title":"Examining the Cohere embeddings\u00b6","text":"<p>Let us check out the Cohere embedding API and how we can obtain vector embeddings with different precisions for the same text input (without additional cost). See also Cohere embed API doc.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#some-sample-documents","title":"Some sample documents\u00b6","text":"<p>Define a few sample documents that we want to embed</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>Notice the <code>binary_vector</code> field that defines an indexed (dense) Vespa tensor with the dimension name <code>x[128]</code>.</p> <p>Indexing specifies <code>index</code> which means that Vespa will build HNSW graph for searching this vector field.</p> <p>Also, notice the configuration of the distance-metric.</p> <p>We also want to store the <code>int8_vector</code> on disk; we use <code>paged</code> to signalize this.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#feed-our-sample-documents-and-their-binary-embedding-representation","title":"Feed our sample documents and their binary embedding representation\u00b6","text":"<p>With few documents, we use the synchronous API. Read more in reads and writes.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> <li>Practical Nearest Neighbor Search Guide</li> </ul> <p>We now need to invoke the embed API again to embed the query text; we ask for all three representations:</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#conclusions","title":"Conclusions\u00b6","text":"<p>These new Cohere binary embeddings are a huge step forward for cost-efficient vector search at scale and integrate perfectly with Vespa features for building out vector search at scale.</p> <p>Storing the <code>int8</code> vector representation on disk using the paged attribute option enables phased retrieval and ranking close to the data. First, one can use the compact in-memory binary representation for the coarse-level search to efficiently find a limited number of candidates. Then, the candidates from the coarse search can be re-scored and re-ranked using a more advanced scoring function using a finer resolution.</p>"},{"location":"examples/billion-scale-vector-search-with-cohere-embeddings-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>We can now delete the cloud instance:</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html","title":"chat with your pdfs using colbert langchain and Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa langchain langchain-community langchain-openai pypdf==5.0.1 openai vespacli\n</pre> !pip3 install -U pyvespa langchain langchain-community langchain-openai pypdf==5.0.1 openai vespacli In\u00a0[2]: Copied! <pre>def sample_pdfs():\n    return [\n        {\n            \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n            \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",\n            \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",\n        },\n        {\n            \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n            \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",\n            \"authors\": \"Omar Khattab, Matei Zaharia\",\n        },\n        {\n            \"title\": \"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\",\n            \"url\": \"https://arxiv.org/pdf/2108.11480.pdf\",\n            \"authors\": \"Craig Macdonald, Nicola Tonellotto\",\n        },\n        {\n            \"title\": \"A Study on Token Pruning for ColBERT\",\n            \"url\": \"https://arxiv.org/pdf/2112.06540.pdf\",\n            \"authors\": \"Carlos Lassance, Maroua Maachou, Joohee Park, St\u00e9phane Clinchant\",\n        },\n        {\n            \"title\": \"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval\",\n            \"url\": \"https://arxiv.org/pdf/2106.11251.pdf\",\n            \"authors\": \"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis\",\n        },\n    ]\n</pre> def sample_pdfs():     return [         {             \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",             \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",             \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",         },         {             \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",             \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",             \"authors\": \"Omar Khattab, Matei Zaharia\",         },         {             \"title\": \"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\",             \"url\": \"https://arxiv.org/pdf/2108.11480.pdf\",             \"authors\": \"Craig Macdonald, Nicola Tonellotto\",         },         {             \"title\": \"A Study on Token Pruning for ColBERT\",             \"url\": \"https://arxiv.org/pdf/2112.06540.pdf\",             \"authors\": \"Carlos Lassance, Maroua Maachou, Joohee Park, St\u00e9phane Clinchant\",         },         {             \"title\": \"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval\",             \"url\": \"https://arxiv.org/pdf/2106.11251.pdf\",             \"authors\": \"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis\",         },     ] In\u00a0[3]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\npdf_schema = Schema(\n    name=\"pdf\",\n    mode=\"streaming\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(name=\"title\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"authors\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"metadata\",\n                type=\"map&lt;string,string&gt;\",\n                indexing=[\"summary\", \"index\"],\n            ),\n            Field(name=\"page\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(name=\"contexts\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;bfloat16&gt;(context{}, x[384])\",\n                indexing=[\n                    \"input contexts\",\n                    'for_each { (input title || \"\") . \" \" . ( _ || \"\") }',\n                    \"embed e5\",\n                    \"attribute\",\n                ],\n                attribute=[\"distance-metric: angular\"],\n                is_document_field=False,\n            ),\n            Field(\n                name=\"colbert\",\n                type=\"tensor&lt;int8&gt;(context{}, token{}, v[16])\",\n                indexing=[\"input contexts\", \"embed colbert context\", \"attribute\"],\n                is_document_field=False,\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"contexts\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  pdf_schema = Schema(     name=\"pdf\",     mode=\"streaming\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(name=\"title\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"authors\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"metadata\",                 type=\"map\",                 indexing=[\"summary\", \"index\"],             ),             Field(name=\"page\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(name=\"contexts\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"embedding\",                 type=\"tensor(context{}, x[384])\",                 indexing=[                     \"input contexts\",                     'for_each { (input title || \"\") . \" \" . ( _ || \"\") }',                     \"embed e5\",                     \"attribute\",                 ],                 attribute=[\"distance-metric: angular\"],                 is_document_field=False,             ),             Field(                 name=\"colbert\",                 type=\"tensor(context{}, token{}, v[16])\",                 indexing=[\"input contexts\", \"embed colbert context\", \"attribute\"],                 is_document_field=False,             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"contexts\"])], ) <p>The above defines our <code>pdf</code> schema using mode <code>streaming</code>. Most fields are straightforward, but take a note of:</p> <ul> <li><code>metadata</code> using <code>map&lt;string,string&gt;</code> - here we can store and match over page level metadata extracted by the PDF parser.</li> <li><code>contexts</code> using <code>array&lt;string&gt;</code>, these are the context-sized text parts that we use langchain document transformers for.</li> <li>The <code>embedding</code> field of type <code>tensor&lt;bfloat16&gt;(context{},x[384])</code> allows us to store and search the 384-dimensional embeddings per context in the same document</li> <li>The <code>colbert</code> field of type <code>tensor&lt;int8&gt;(context{}, token{}, v[16])</code> stores the ColBERT embeddings, retaining a (quantized) per-token representation of the text.</li> </ul> <p>The observant reader might have noticed the <code>e5</code> and <code>colbert</code> arguments to the <code>embed</code> expression in the above <code>embedding</code> field. The <code>e5</code> argument references a component of the type hugging-face-embedder, and <code>colbert</code> references the new cobert-embedder. We configure the application package and its name with the <code>pdf</code> schema and the <code>e5</code> and <code>colbert</code> embedder components.</p> In\u00a0[4]: Copied! <pre>from vespa.package import ApplicationPackage, Component, Parameter\n\nvespa_app_name = \"pdfs\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name,\n    schema=[pdf_schema],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    name=\"transformer-model\",\n                    args={\n                        \"url\": \"https://huggingface.co/intfloat/e5-small-v2/resolve/main/model.onnx\"\n                    },\n                ),\n                Parameter(\n                    name=\"tokenizer-model\",\n                    args={\n                        \"url\": \"https://huggingface.co/intfloat/e5-small-v2/raw/main/tokenizer.json\"\n                    },\n                ),\n                Parameter(\n                    name=\"prepend\",\n                    args={},\n                    children=[\n                        Parameter(name=\"query\", args={}, children=\"query: \"),\n                        Parameter(name=\"document\", args={}, children=\"passage: \"),\n                    ],\n                ),\n            ],\n        ),\n        Component(\n            id=\"colbert\",\n            type=\"colbert-embedder\",\n            parameters=[\n                Parameter(\n                    name=\"transformer-model\",\n                    args={\n                        \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/resolve/main/model.onnx\"\n                    },\n                ),\n                Parameter(\n                    name=\"tokenizer-model\",\n                    args={\n                        \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/raw/main/tokenizer.json\"\n                    },\n                ),\n            ],\n        ),\n    ],\n)\n</pre> from vespa.package import ApplicationPackage, Component, Parameter  vespa_app_name = \"pdfs\" vespa_application_package = ApplicationPackage(     name=vespa_app_name,     schema=[pdf_schema],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     name=\"transformer-model\",                     args={                         \"url\": \"https://huggingface.co/intfloat/e5-small-v2/resolve/main/model.onnx\"                     },                 ),                 Parameter(                     name=\"tokenizer-model\",                     args={                         \"url\": \"https://huggingface.co/intfloat/e5-small-v2/raw/main/tokenizer.json\"                     },                 ),                 Parameter(                     name=\"prepend\",                     args={},                     children=[                         Parameter(name=\"query\", args={}, children=\"query: \"),                         Parameter(name=\"document\", args={}, children=\"passage: \"),                     ],                 ),             ],         ),         Component(             id=\"colbert\",             type=\"colbert-embedder\",             parameters=[                 Parameter(                     name=\"transformer-model\",                     args={                         \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/resolve/main/model.onnx\"                     },                 ),                 Parameter(                     name=\"tokenizer-model\",                     args={                         \"url\": \"https://huggingface.co/colbert-ir/colbertv2.0/raw/main/tokenizer.json\"                     },                 ),             ],         ),     ], ) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p>Vespa supports phased ranking and has a rich set of built-in rank-features, including many text-matching features such as:</p> <ul> <li>BM25.</li> <li>nativeRank and many more.</li> </ul> <p>Users can also define custom functions using ranking expressions. The following defines a <code>colbert</code> Vespa ranking profile which uses the <code>e5</code> embedding in the first phase, and the <code>max_sim</code> function in the second phase. The <code>max_sim</code> function performs the late interaction for the ColBERT ranking, and is by default applied to the top 100 documents from the first phase.</p> In\u00a0[5]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolbert = RankProfile(\n    name=\"colbert\",\n    inputs=[\n        (\"query(q)\", \"tensor&lt;float&gt;(x[384])\"),\n        (\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"),\n    ],\n    functions=[\n        Function(name=\"cos_sim\", expression=\"closeness(field, embedding)\"),\n        Function(\n            name=\"max_sim_per_context\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(colbert)) , v\n                        ),\n                        max, token\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim\", expression=\"reduce(max_sim_per_context, max, context)\"\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"cos_sim\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\"),\n    match_features=[\"cos_sim\", \"max_sim\", \"max_sim_per_context\"],\n)\npdf_schema.add_rank_profile(colbert)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colbert = RankProfile(     name=\"colbert\",     inputs=[         (\"query(q)\", \"tensor(x[384])\"),         (\"query(qt)\", \"tensor(querytoken{}, v[128])\"),     ],     functions=[         Function(name=\"cos_sim\", expression=\"closeness(field, embedding)\"),         Function(             name=\"max_sim_per_context\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(colbert)) , v                         ),                         max, token                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim\", expression=\"reduce(max_sim_per_context, max, context)\"         ),     ],     first_phase=FirstPhaseRanking(expression=\"cos_sim\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\"),     match_features=[\"cos_sim\", \"max_sim\", \"max_sim_per_context\"], ) pdf_schema.add_rank_profile(colbert) <p>Using match-features, Vespa returns selected features along with the highest scoring documents. Here, we include <code>max_sim_per_context</code> which we can later use to select the top N scoring contexts for each page.</p> <p>For an example of a <code>hybrid</code> rank-profile which combines semantic search with traditional text retrieval such as BM25, see the previous blog post: Turbocharge RAG with LangChain and Vespa Streaming Mode for Sharded Data</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[12]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for vespa-team.pdfs. This may take a few minutes the first time.\nINFO    [19:04:30]  Deploying platform version 8.314.57 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [19:04:30]  Using CA signed certificate version 1\nINFO    [19:04:30]  Using 1 nodes in container cluster 'pdfs_container'\nINFO    [19:04:35]  Session 285265 for tenant 'vespa-team' prepared and activated.\nINFO    [19:04:39]  ######## Details for all nodes ########\nINFO    [19:04:44]  h88969d.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [19:04:44]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nINFO    [19:04:44]  --- container-clustercontroller on port 19050 has not started \nINFO    [19:04:44]  --- metricsproxy-container on port 19092 has not started \nINFO    [19:04:44]  h88978a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [19:04:44]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nINFO    [19:04:44]  --- logserver-container on port 4080 has not started \nINFO    [19:04:44]  --- metricsproxy-container on port 19092 has not started \nINFO    [19:04:44]  h90615b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [19:04:44]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nINFO    [19:04:44]  --- storagenode on port 19102 has not started \nINFO    [19:04:44]  --- searchnode on port 19107 has not started \nINFO    [19:04:44]  --- distributor on port 19111 has not started \nINFO    [19:04:44]  --- metricsproxy-container on port 19092 has not started \nINFO    [19:04:44]  h91135a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [19:04:44]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nINFO    [19:04:44]  --- container on port 4080 has not started \nINFO    [19:04:44]  --- metricsproxy-container on port 19092 has not started \nINFO    [19:05:52]  Waiting for convergence of 10 services across 4 nodes\nINFO    [19:05:52]  1/1 nodes upgrading platform\nINFO    [19:05:52]  1 application services still deploying\nDEBUG   [19:05:52]  h91135a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nDEBUG   [19:05:52]  --- platform vespa/cloud-tenant-rhel8:8.314.57 &lt;-- :\nDEBUG   [19:05:52]  --- container on port 4080 has not started \nDEBUG   [19:05:52]  --- metricsproxy-container on port 19092 has config generation 285265, wanted is 285265\nINFO    [19:06:21]  Found endpoints:\nINFO    [19:06:21]  - dev.aws-us-east-1c\nINFO    [19:06:21]   |-- https://bac3e5ad.c81e7b13.z.vespa-app.cloud/ (cluster 'pdfs_container')\nINFO    [19:06:22]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://bac3e5ad.c81e7b13.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[13]: Copied! <pre>from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1024,  # chars, not llm tokens\n    chunk_overlap=0,\n    length_function=len,\n    is_separator_regex=False,\n)\n</pre> from langchain_community.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(     chunk_size=1024,  # chars, not llm tokens     chunk_overlap=0,     length_function=len,     is_separator_regex=False, ) <p>The following iterates over the <code>sample_pdfs</code> and performs the following:</p> <ul> <li>Load the URL and extract the text into pages. A page is the retrievable unit we will use in Vespa</li> <li>For each page, use the text splitter to split the text into contexts. The contexts are represented as an <code>array&lt;string&gt;</code> in the Vespa schema</li> <li>Create the page level Vespa <code>fields</code>, note that we duplicate some content like the title and URL into the page level representation.</li> </ul> In\u00a0[14]: Copied! <pre>import hashlib\nimport unicodedata\n\n\ndef remove_control_characters(s):\n    return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")\n\n\nmy_docs_to_feed = []\nfor pdf in sample_pdfs():\n    url = pdf[\"url\"]\n    loader = PyPDFLoader(url)\n    pages = loader.load_and_split()\n    for index, page in enumerate(pages):\n        source = page.metadata[\"source\"]\n        chunks = text_splitter.transform_documents([page])\n        text_chunks = [chunk.page_content for chunk in chunks]\n        text_chunks = [remove_control_characters(chunk) for chunk in text_chunks]\n        page_number = index + 1\n        vespa_id = f\"{url}#{page_number}\"\n        hash_value = hashlib.sha1(vespa_id.encode()).hexdigest()\n        fields = {\n            \"title\": pdf[\"title\"],\n            \"url\": url,\n            \"page\": page_number,\n            \"id\": hash_value,\n            \"authors\": [a.strip() for a in pdf[\"authors\"].split(\",\")],\n            \"contexts\": text_chunks,\n            \"metadata\": page.metadata,\n        }\n        my_docs_to_feed.append(fields)\n</pre> import hashlib import unicodedata   def remove_control_characters(s):     return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")   my_docs_to_feed = [] for pdf in sample_pdfs():     url = pdf[\"url\"]     loader = PyPDFLoader(url)     pages = loader.load_and_split()     for index, page in enumerate(pages):         source = page.metadata[\"source\"]         chunks = text_splitter.transform_documents([page])         text_chunks = [chunk.page_content for chunk in chunks]         text_chunks = [remove_control_characters(chunk) for chunk in text_chunks]         page_number = index + 1         vespa_id = f\"{url}#{page_number}\"         hash_value = hashlib.sha1(vespa_id.encode()).hexdigest()         fields = {             \"title\": pdf[\"title\"],             \"url\": url,             \"page\": page_number,             \"id\": hash_value,             \"authors\": [a.strip() for a in pdf[\"authors\"].split(\",\")],             \"contexts\": text_chunks,             \"metadata\": page.metadata,         }         my_docs_to_feed.append(fields) <p>Now that we have parsed the input PDFs and created a list of pages that we want to add to Vespa, we must format the list into the format that PyVespa accepts. Notice the <code>fields</code>, <code>id</code> and <code>groupname</code> keys. The <code>groupname</code> is the key that is used to shard and co-locate the data and is only relevant when using Vespa with streaming mode.</p> In\u00a0[15]: Copied! <pre>from typing import Iterable\n\n\ndef vespa_feed(user: str) -&gt; Iterable[dict]:\n    for doc in my_docs_to_feed:\n        yield {\"fields\": doc, \"id\": doc[\"id\"], \"groupname\": user}\n</pre> from typing import Iterable   def vespa_feed(user: str) -&gt; Iterable[dict]:     for doc in my_docs_to_feed:         yield {\"fields\": doc, \"id\": doc[\"id\"], \"groupname\": user} In\u00a0[16]: Copied! <pre>my_docs_to_feed[0]\n</pre> my_docs_to_feed[0] Out[16]: <pre>{'title': 'ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction',\n 'url': 'https://arxiv.org/pdf/2112.01488.pdf',\n 'page': 1,\n 'id': 'a731a839198de04fa3d1a3cee6890d0d170ab025',\n 'authors': ['Keshav Santhanam',\n  'Omar Khattab',\n  'Jon Saad-Falcon',\n  'Christopher Potts',\n  'Matei Zaharia'],\n 'contexts': ['ColBERTv2:Effective and Ef\ufb01cient Retrieval via Lightweight Late InteractionKeshav Santhanam\u2217Stanford UniversityOmar Khattab\u2217Stanford UniversityJon Saad-FalconGeorgia Institute of TechnologyChristopher PottsStanford UniversityMatei ZahariaStanford UniversityAbstractNeural information retrieval (IR) has greatlyadvanced search and other knowledge-intensive language tasks. While many neuralIR methods encode queries and documentsinto single-vector representations, lateinteraction models produce multi-vector repre-sentations at the granularity of each token anddecompose relevance modeling into scalabletoken-level computations. This decompositionhas been shown to make late interaction moreeffective, but it in\ufb02ates the space footprint ofthese models by an order of magnitude. In thiswork, we introduce ColBERTv2, a retrieverthat couples an aggressive residual compres-sion mechanism with a denoised supervisionstrategy to simultaneously improve the quality',\n  'and space footprint of late interaction. Weevaluate ColBERTv2 across a wide rangeof benchmarks, establishing state-of-the-artquality within and outside the training domainwhile reducing the space footprint of lateinteraction models by 6\u201310 \u00d7.1 IntroductionNeural information retrieval (IR) has quickly domi-nated the search landscape over the past 2\u20133 years,dramatically advancing not only passage and doc-ument search (Nogueira and Cho, 2019) but alsomany knowledge-intensive NLP tasks like open-domain question answering (Guu et al., 2020),multi-hop claim veri\ufb01cation (Khattab et al., 2021a),and open-ended generation (Paranjape et al., 2022).Many neural IR methods follow a single-vectorsimilarity paradigm: a pretrained language modelis used to encode each query and each documentinto a single high-dimensional vector, and rele-vance is modeled as a simple dot product betweenboth vectors. An alternative is late interaction , in-troduced in ColBERT (Khattab and Zaharia, 2020),',\n  'where queries and documents are encoded at a \ufb01ner-granularity into multi-vector representations, and\u2217Equal contribution.relevance is estimated using rich yet scalable in-teractions between these two sets of vectors. Col-BERT produces an embedding for every token inthe query (and document) and models relevanceas the sum of maximum similarities between eachquery vector and all vectors in the document.By decomposing relevance modeling into token-level computations, late interaction aims to reducethe burden on the encoder: whereas single-vectormodels must capture complex query\u2013document re-lationships within one dot product, late interactionencodes meaning at the level of tokens and del-egates query\u2013document matching to the interac-tion mechanism. This added expressivity comesat a cost: existing late interaction systems imposean order-of-magnitude larger space footprint thansingle-vector models, as they must store billionsof small vectors for Web-scale collections. Con-',\n  'sidering this challenge, it might seem more fruit-ful to focus instead on addressing the fragility ofsingle-vector models (Menon et al., 2022) by in-troducing new supervision paradigms for negativemining (Xiong et al., 2020), pretraining (Gao andCallan, 2021), and distillation (Qu et al., 2021).Indeed, recent single-vector models with highly-tuned supervision strategies (Ren et al., 2021b; For-mal et al., 2021a) sometimes perform on-par oreven better than \u201cvanilla\u201d late interaction models,and it is not necessarily clear whether late inter-action architectures\u2014with their \ufb01xed token-levelinductive biases\u2014admit similarly large gains fromimproved supervision.In this work, we show that late interaction re-trievers naturally produce lightweight token rep-resentations that are amenable to ef\ufb01cient storageoff-the-shelf and that they can bene\ufb01t drasticallyfrom denoised supervision. We couple those inColBERTv2 ,1a new late-interaction retriever that'],\n 'metadata': {'source': 'https://arxiv.org/pdf/2112.01488.pdf', 'page': 0}}</pre> <p>Now, we can feed to the Vespa instance (<code>app</code>), using the <code>feed_iterable</code> API, using the generator function above as input with a custom <code>callback</code> function. Vespa also performs embedding inference during this step using the built-in Vespa embedding functionality.</p> In\u00a0[17]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"\n        )\n\n\napp.feed_iterable(\n    schema=\"pdf\", iter=vespa_feed(\"jo-bergum\"), namespace=\"personal\", callback=callback\n)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"         )   app.feed_iterable(     schema=\"pdf\", iter=vespa_feed(\"jo-bergum\"), namespace=\"personal\", callback=callback ) <p>Notice the <code>schema</code> and <code>namespace</code> arguments. PyVespa transforms the input operations to Vespa document v1 requests.</p> <p></p> In\u00a0[18]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select id,title,page,contexts from pdf where ({targetHits:10}nearestNeighbor(embedding,q))\",\n    groupname=\"jo-bergum\",\n    ranking=\"colbert\",\n    query=\"why is colbert effective?\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(q)\": 'embed(e5, \"why is colbert effective?\")',\n        \"input.query(qt)\": 'embed(colbert, \"why is colbert effective?\")',\n    },\n    timeout=\"2s\",\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select id,title,page,contexts from pdf where ({targetHits:10}nearestNeighbor(embedding,q))\",     groupname=\"jo-bergum\",     ranking=\"colbert\",     query=\"why is colbert effective?\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(q)\": 'embed(e5, \"why is colbert effective?\")',         \"input.query(qt)\": 'embed(colbert, \"why is colbert effective?\")',     },     timeout=\"2s\", ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:personal:pdf:g=jo-bergum:55ea3f735cb6748a2eddb9f76d3f0e7fff0c31a8\",\n  \"relevance\": 103.17699432373047,\n  \"source\": \"pdfs_content.pdf\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"cos_sim\": 0.6534222205340683,\n      \"max_sim\": 103.17699432373047,\n      \"max_sim_per_context\": {\n        \"0\": 74.16375732421875,\n        \"1\": 103.17699432373047\n      }\n    },\n    \"id\": \"55ea3f735cb6748a2eddb9f76d3f0e7fff0c31a8\",\n    \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n    \"page\": 18,\n    \"contexts\": [\n      \"at least once. While ColBERT encodes each document with BERTexactly once, existing BERT-based rankers would repeat similarcomputations on possibly hundreds of documents for each query.Se/t_ting Dimension( m) Bytes/Dim Space(GiBs) MRR@10Re-rank Cosine 128 4 286 34.9End-to-end L2 128 2 154 36.0Re-rank L2 128 2 143 34.8Re-rank Cosine 48 4 54 34.4Re-rank Cosine 24 2 27 33.9Table 4: Space Footprint vs MRR@10 (Dev) on MS MARCO.Table 4 reports the space footprint of ColBERT under variousse/t_tings as we reduce the embeddings dimension and/or the bytesper dimension. Interestingly, the most space-e\\ufb03cient se/t_ting, thatis, re-ranking with cosine similarity with 24-dimensional vectorsstored as 2-byte /f_loats, is only 1% worse in MRR@10 than the mostspace-consuming one, while the former requires only 27 GiBs torepresent the MS MARCO collection.5 CONCLUSIONSIn this paper, we introduced ColBERT, a novel ranking model thatemploys contextualized late interaction over deep LMs (in particular,\",\n      \"BERT) for e\\ufb03cient retrieval. By independently encoding queriesand documents into /f_ine-grained representations that interact viacheap and pruning-friendly computations, ColBERT can leveragethe expressiveness of deep LMs while greatly speeding up queryprocessing. In addition, doing so allows using ColBERT for end-to-end neural retrieval directly from a large document collection. Ourresults show that ColBERT is more than 170 \\u00d7faster and requires14,000\\u00d7fewer FLOPs/query than existing BERT-based models, allwhile only minimally impacting quality and while outperformingevery non-BERT baseline.Acknowledgments. OK was supported by the Eltoukhy FamilyGraduate Fellowship at the Stanford School of Engineering. /T_hisresearch was supported in part by a\\ufb03liate members and othersupporters of the Stanford DAWN project\\u2014Ant Financial, Facebook,Google, Infosys, NEC, and VMware\\u2014as well as Cisco, SAP, and the\"\n    ]\n  }\n}\n</pre> <p>Notice the <code>matchfeatures</code> that returns the configured match-features from the rank-profile, including all the context similarities.</p> In\u00a0[25]: Copied! <pre>from langchain_core.documents import Document\nfrom langchain_core.retrievers import BaseRetriever\nfrom typing import List\n\n\nclass VespaStreamingColBERTRetriever(BaseRetriever):\n    app: Vespa\n    user: str\n    pages: int = 5\n    chunks_per_page: int = 3\n    chunk_similarity_threshold: float = 0.8\n\n    def _get_relevant_documents(self, query: str) -&gt; List[Document]:\n        response: VespaQueryResponse = self.app.query(\n            yql=\"select id, url, title, page, authors, contexts from pdf where userQuery() or ({targetHits:20}nearestNeighbor(embedding,q))\",\n            groupname=self.user,\n            ranking=\"colbert\",\n            query=query,\n            hits=self.pages,\n            body={\n                \"presentation.format.tensors\": \"short-value\",\n                \"input.query(q)\": f'embed(e5, \"query: {query} \")',\n                \"input.query(qt)\": f'embed(colbert, \"{query}\")',\n            },\n            timeout=\"2s\",\n        )\n        if not response.is_successful():\n            raise ValueError(\n                f\"Query failed with status code {response.status_code}, url={response.url} response={response.json}\"\n            )\n        return self._parse_response(response)\n\n    def _parse_response(self, response: VespaQueryResponse) -&gt; List[Document]:\n        documents: List[Document] = []\n        for hit in response.hits:\n            fields = hit[\"fields\"]\n            chunks_with_scores = self._get_chunk_similarities(fields)\n            ## Best k chunks from each page\n            best_chunks_on_page = \" ### \".join(\n                [\n                    chunk\n                    for chunk, score in chunks_with_scores[0 : self.chunks_per_page]\n                    if score &gt; self.chunk_similarity_threshold\n                ]\n            )\n            documents.append(\n                Document(\n                    id=fields[\"id\"],\n                    page_content=best_chunks_on_page,\n                    title=fields[\"title\"],\n                    metadata={\n                        \"title\": fields[\"title\"],\n                        \"url\": fields[\"url\"],\n                        \"page\": fields[\"page\"],\n                        \"authors\": fields[\"authors\"],\n                        \"features\": fields[\"matchfeatures\"],\n                    },\n                )\n            )\n        return documents\n\n    def _get_chunk_similarities(self, hit_fields: dict) -&gt; List[tuple]:\n        match_features = hit_fields[\"matchfeatures\"]\n        similarities = match_features[\"max_sim_per_context\"]\n        chunk_scores = []\n        for i in range(0, len(similarities)):\n            chunk_scores.append(similarities.get(str(i), 0))\n        chunks = hit_fields[\"contexts\"]\n        chunks_with_scores = list(zip(chunks, chunk_scores))\n        return sorted(chunks_with_scores, key=lambda x: x[1], reverse=True)\n</pre> from langchain_core.documents import Document from langchain_core.retrievers import BaseRetriever from typing import List   class VespaStreamingColBERTRetriever(BaseRetriever):     app: Vespa     user: str     pages: int = 5     chunks_per_page: int = 3     chunk_similarity_threshold: float = 0.8      def _get_relevant_documents(self, query: str) -&gt; List[Document]:         response: VespaQueryResponse = self.app.query(             yql=\"select id, url, title, page, authors, contexts from pdf where userQuery() or ({targetHits:20}nearestNeighbor(embedding,q))\",             groupname=self.user,             ranking=\"colbert\",             query=query,             hits=self.pages,             body={                 \"presentation.format.tensors\": \"short-value\",                 \"input.query(q)\": f'embed(e5, \"query: {query} \")',                 \"input.query(qt)\": f'embed(colbert, \"{query}\")',             },             timeout=\"2s\",         )         if not response.is_successful():             raise ValueError(                 f\"Query failed with status code {response.status_code}, url={response.url} response={response.json}\"             )         return self._parse_response(response)      def _parse_response(self, response: VespaQueryResponse) -&gt; List[Document]:         documents: List[Document] = []         for hit in response.hits:             fields = hit[\"fields\"]             chunks_with_scores = self._get_chunk_similarities(fields)             ## Best k chunks from each page             best_chunks_on_page = \" ### \".join(                 [                     chunk                     for chunk, score in chunks_with_scores[0 : self.chunks_per_page]                     if score &gt; self.chunk_similarity_threshold                 ]             )             documents.append(                 Document(                     id=fields[\"id\"],                     page_content=best_chunks_on_page,                     title=fields[\"title\"],                     metadata={                         \"title\": fields[\"title\"],                         \"url\": fields[\"url\"],                         \"page\": fields[\"page\"],                         \"authors\": fields[\"authors\"],                         \"features\": fields[\"matchfeatures\"],                     },                 )             )         return documents      def _get_chunk_similarities(self, hit_fields: dict) -&gt; List[tuple]:         match_features = hit_fields[\"matchfeatures\"]         similarities = match_features[\"max_sim_per_context\"]         chunk_scores = []         for i in range(0, len(similarities)):             chunk_scores.append(similarities.get(str(i), 0))         chunks = hit_fields[\"contexts\"]         chunks_with_scores = list(zip(chunks, chunk_scores))         return sorted(chunks_with_scores, key=lambda x: x[1], reverse=True) <p>That's it! We can give our newborn retriever a spin for the user\u00a0<code>jo-bergum</code> by</p> In\u00a0[26]: Copied! <pre>vespa_hybrid_retriever = VespaStreamingColBERTRetriever(\n    app=app, user=\"jo-bergum\", pages=1, chunks_per_page=3\n)\n</pre> vespa_hybrid_retriever = VespaStreamingColBERTRetriever(     app=app, user=\"jo-bergum\", pages=1, chunks_per_page=3 ) In\u00a0[27]: Copied! <pre>vespa_hybrid_retriever.get_relevant_documents(\"what is the maxsim operator in colbert?\")\n</pre> vespa_hybrid_retriever.get_relevant_documents(\"what is the maxsim operator in colbert?\") Out[27]: <pre>[Document(page_content='ture that precisely does so. As illustrated, every query embeddinginteracts with all document embeddings via a MaxSim operator,which computes maximum similarity (e.g., cosine similarity), andthe scalar outputs of these operators are summed across queryterms. /T_his paradigm allows ColBERT to exploit deep LM-basedrepresentations while shi/f_ting the cost of encoding documents of-/f_line and amortizing the cost of encoding the query once acrossall ranked documents. Additionally, it enables ColBERT to lever-age vector-similarity search indexes (e.g., [ 1,15]) to retrieve thetop-kresults directly from a large document collection, substan-tially improving recall over models that only re-rank the output ofterm-based retrieval.As Figure 1 illustrates, ColBERT can serve queries in tens orfew hundreds of milliseconds. For instance, when used for re-ranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170 \u00d7speedup(and requires 14,000 \u00d7fewer FLOPs) relative to existing BERT-based ### models, while being more e\ufb00ective than every non-BERT baseline(\u00a74.2 &amp; 4.3). ColBERT\u2019s indexing\u2014the only time it needs to feeddocuments through BERT\u2014is also practical: it can index the MSMARCO collection of 9M passages in about 3 hours using a singleserver with four GPUs ( \u00a74.5), retaining its e\ufb00ectiveness with a spacefootprint of as li/t_tle as few tens of GiBs. Our extensive ablationstudy ( \u00a74.4) shows that late interaction, its implementation viaMaxSim operations, and crucial design choices within our BERT-based encoders are all essential to ColBERT\u2019s e\ufb00ectiveness.Our main contributions are as follows.(1)We propose late interaction (\u00a73.1) as a paradigm for e\ufb03cientand e\ufb00ective neural ranking.(2)We present ColBERT ( \u00a73.2 &amp; 3.3), a highly-e\ufb00ective modelthat employs novel BERT-based query and document en-coders within the late interaction paradigm.', metadata={'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'url': 'https://arxiv.org/pdf/2004.12832.pdf', 'page': 4, 'authors': ['Omar Khattab', 'Matei Zaharia'], 'features': {'cos_sim': 0.6664045997289173, 'max_sim': 124.19231414794922, 'max_sim_per_context': {'0': 124.19231414794922, '1': 92.21265411376953}}})]</pre> <p>Finally, we can connect our custom retriever with the complete flexibility and power of the [LangChain] LLM framework. The following uses LangChain Expression Language, or LCEL, a declarative way to compose chains.</p> <p>We have several steps composed into a chain:</p> <ul> <li>The prompt template and LLM model, in this case using OpenAI</li> <li>The retriever that provides the retrieved context for the question</li> <li>The formatting of the retrieved context</li> </ul> In\u00a0[28]: Copied! <pre>vespa_hybrid_retriever = VespaStreamingColBERTRetriever(\n    app=app, user=\"jo-bergum\", chunks_per_page=3\n)\n</pre> vespa_hybrid_retriever = VespaStreamingColBERTRetriever(     app=app, user=\"jo-bergum\", chunks_per_page=3 ) In\u00a0[30]: Copied! <pre>from langchain_openai import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\n\nprompt_template = \"\"\"\nAnswer the question based only on the following context.\nCite the page number and the url of the document you are citing.\n\n{context}\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(prompt_template)\nmodel = ChatOpenAI(model=\"gpt-4-0125-preview\")\n\n\ndef format_prompt_context(docs) -&gt; str:\n    context = []\n    for d in docs:\n        context.append(f\"{d.metadata['title']} by {d.metadata['authors']}\\n\")\n        context.append(f\"url: {d.metadata['url']}\\n\")\n        context.append(f\"page: {d.metadata['page']}\\n\")\n        context.append(f\"{d.page_content}\\n\\n\")\n    return \"\".join(context)\n\n\nchain = (\n    {\n        \"context\": vespa_hybrid_retriever | format_prompt_context,\n        \"question\": RunnablePassthrough(),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n</pre> from langchain_openai import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough  prompt_template = \"\"\" Answer the question based only on the following context. Cite the page number and the url of the document you are citing.  {context} Question: {question} \"\"\" prompt = ChatPromptTemplate.from_template(prompt_template) model = ChatOpenAI(model=\"gpt-4-0125-preview\")   def format_prompt_context(docs) -&gt; str:     context = []     for d in docs:         context.append(f\"{d.metadata['title']} by {d.metadata['authors']}\\n\")         context.append(f\"url: {d.metadata['url']}\\n\")         context.append(f\"page: {d.metadata['page']}\\n\")         context.append(f\"{d.page_content}\\n\\n\")     return \"\".join(context)   chain = (     {         \"context\": vespa_hybrid_retriever | format_prompt_context,         \"question\": RunnablePassthrough(),     }     | prompt     | model     | StrOutputParser() ) In\u00a0[31]: Copied! <pre>chain.invoke(\"what is colbert?\")\n</pre> chain.invoke(\"what is colbert?\") Out[31]: <pre>'ColBERT, introduced by Omar Khattab and Matei Zaharia, is a novel ranking model that employs contextualized late interaction over deep language models (LMs), specifically focusing on BERT (Bidirectional Encoder Representations from Transformers) for efficient and effective passage search. It achieves this by independently encoding queries and documents into fine-grained representations that interact via cheap and pruning-friendly computations. This approach allows ColBERT to leverage the expressiveness of deep LMs while significantly speeding up query processing compared to existing BERT-based models. ColBERT also enables end-to-end neural retrieval directly from a large document collection, offering more than 170 times faster performance and requiring 14,000 times fewer FLOPs (floating-point operations) per query than previous BERT-based models, with minimal impact on quality. It outperforms every non-BERT baseline in effectiveness (https://arxiv.org/pdf/2004.12832.pdf, page 18).\\n\\nColBERT differentiates itself with a mechanism that delays the query-document interaction, which allows for pre-computation of document representations for cheap neural re-ranking and supports practical end-to-end neural retrieval through pruning via vector-similarity search. This method preserves the effectiveness of state-of-the-art models that condition most of their computations on the joint query-document pair, making ColBERT a scalable solution for passage search challenges (https://arxiv.org/pdf/2004.12832.pdf, page 6).'</pre> In\u00a0[32]: Copied! <pre>chain.invoke(\"what is the colbert maxsim operator\")\n</pre> chain.invoke(\"what is the colbert maxsim operator\") Out[32]: <pre>'The ColBERT MaxSim operator is a mechanism for computing the maximum similarity between query embeddings and document embeddings. It operates by calculating the maximum similarity (e.g., cosine similarity) for each query embedding with all document embeddings, and then summing the scalar outputs of these operations across query terms. This paradigm enables the efficient and effective retrieval of documents by allowing for the interaction between deep language model-based representations of queries and documents to occur in a late stage of the processing pipeline, thereby shifting the cost of encoding documents offline and amortizing the cost of encoding the query across all ranked documents. Additionally, the MaxSim operator facilitates the use of vector-similarity search indexes to directly retrieve the top-k results from a large document collection, substantially improving recall over models that only re-rank the output of term-based retrieval. This operator is a key component of ColBERT\\'s approach to efficient and effective passage search.\\n\\nSource: \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\" by Omar Khattab and Matei Zaharia, page 4, https://arxiv.org/pdf/2004.12832.pdf'</pre> In\u00a0[33]: Copied! <pre>chain.invoke(\n    \"What is the difference between colbert and single vector representational models?\"\n)\n</pre> chain.invoke(     \"What is the difference between colbert and single vector representational models?\" ) Out[33]: <pre>'The main difference between ColBERT and single-vector representational models lies in their approach to handling document and query representations for information retrieval tasks. ColBERT utilizes a multi-vector representation for both queries and documents, whereas single-vector models encode each query and each document into a single, dense vector.\\n\\n1. **Multi-Vector vs. Single-Vector Representations**: ColBERT leverages a late interaction mechanism that allows for fine-grained matching between the multiple embeddings of query terms and document tokens. This approach enables capturing the nuanced semantics of the text by considering the contextualized representation of each term separately. On the other hand, single-vector models compress the entire content of a document or a query into a single dense vector, which might lead to a loss of detail and context specificity.\\n\\n2. **Efficiency and Effectiveness**: While single-vector models might be simpler and potentially faster in some scenarios due to their straightforward matching mechanism (e.g., cosine similarity between query and document vectors), this simplicity could come at the cost of effectiveness. ColBERT, with its detailed interaction between term-level vectors, can offer more accurate retrieval results because it preserves and utilizes the rich semantic relationships within and across the text of queries and documents. However, ColBERT\\'s detailed approach initially required more storage and computational resources compared to single-vector models. Nonetheless, advancements like ColBERTv2 have significantly improved the efficiency, achieving competitive storage requirements and reducing the computational cost while maintaining or even enhancing retrieval effectiveness.\\n\\n3. **Compression and Storage**: Initial versions of multi-vector models like ColBERT required significantly more storage space compared to single-vector models due to storing multiple vectors per document. However, with the introduction of techniques like residual compression in ColBERTv2, the storage requirements have been drastically reduced to levels competitive with single-vector models. Single-vector models, while naturally more storage-efficient, can also be compressed, but aggressive compression might exacerbate the loss in quality.\\n\\n4. **Search Quality and Compression**: Despite the potential for aggressive compression in single-vector models, such approaches often lead to a more pronounced loss in quality compared to late interaction methods like ColBERTv2. ColBERTv2, even when employing compression techniques to reduce its storage footprint, can achieve higher quality across systems, showcasing the robustness of its retrieval capabilities even when optimizing for space efficiency.\\n\\nIn summary, the difference between ColBERT and single-vector representational models is primarily in their approach to encoding and matching queries and documents, with ColBERT focusing on detailed, term-level interactions for improved accuracy, and single-vector models emphasizing simplicity and compactness, which might come at the cost of retrieval effectiveness.\\n\\nCitations:\\n- Santhanam et al., \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction,\" p. 14, 15, 17, https://arxiv.org/pdf/2112.01488.pdf'</pre> In\u00a0[34]: Copied! <pre>chain.invoke(\"Why does ColBERT work better for longer documents?\")\n</pre> chain.invoke(\"Why does ColBERT work better for longer documents?\") Out[34]: <pre>\"ColBERT is designed to efficiently handle the interaction between query and document representations through a mechanism called late interaction, which is particularly beneficial when dealing with longer documents. This is because ColBERT independently encodes queries and documents into fine-grained representations using BERT, and then employs a cheap yet powerful interaction step that models their fine-grained similarity. This approach allows for the pre-computation of document representations offline, significantly speeding up query processing by avoiding the need to feed each query-document pair through a massive neural network at query time.\\n\\nFor longer documents, the benefits of this approach are twofold:\\n\\n1. **Efficiency in Handling Long Documents**: Since ColBERT encodes document representations offline, it can efficiently manage longer documents without a proportional increase in computational cost at query time. This is unlike traditional BERT-based models that might require more computational resources to process longer documents due to their size and complexity.\\n\\n2. **Effectiveness in Capturing Fine-Grained Semantics**: The fine-grained representations and the late interaction mechanism enable ColBERT to effectively capture the nuances and detailed semantics of longer documents. This is crucial for maintaining high retrieval quality, as longer documents often contain more information and require a more nuanced understanding to match relevant queries accurately.\\n\\nThus, ColBERT's architecture, which leverages the strengths of BERT for deep language understanding while introducing efficiencies through late interaction, makes it particularly adept at handling longer documents. It achieves this by pre-computing and efficiently utilizing detailed semantic representations of documents, enabling both high-quality retrieval and significant speed-ups in query processing times compared to traditional BERT-based models.\\n\\nReference: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT by ['Omar Khattab', 'Matei Zaharia'] (https://arxiv.org/pdf/2004.12832.pdf), page 4.\"</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#chat-with-your-pdfs-with-colbert-langchain-and-vespa","title":"Chat with your pdfs with ColBERT, langchain, and Vespa\u00b6","text":"<p>This notebook illustrates using Vespa streaming mode to build cost-efficient RAG applications over naturally sharded data. It also demonstrates how you can now use ColBERT ranking natively in Vespa, which can now handle the ColBERT embedding process for you with no custom code!</p> <p>You can read more about Vespa vector streaming search in these blog posts:</p> <ul> <li>Announcing vector streaming search: AI assistants at scale without breaking the bank</li> <li>Yahoo Mail turns to Vespa to do RAG at scale</li> <li>Hands-On RAG guide for personal data with Vespa and LLamaIndex</li> <li>Turbocharge RAG with LangChain and Vespa Streaming Mode for Sharded Data</li> </ul>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#tldr-vespa-streaming-mode-for-partitioned-data","title":"TLDR; Vespa streaming mode for partitioned data\u00b6","text":"<p>Vespa's streaming search solution enables you to integrate a user ID (or any sharding key) into the Vespa document ID. This setup allows Vespa to efficiently group each user's data on a small set of nodes and the same disk chunk. Streaming mode enables low latency searches on a user's data without keeping data in memory.</p> <p>The key benefits of streaming mode:</p> <ul> <li>Eliminating compromises in precision introduced by approximate algorithms</li> <li>Achieve significantly higher write throughput, thanks to the absence of index builds required for supporting approximate search.</li> <li>Optimize efficiency by storing documents, including tensors and data, on disk, benefiting from the cost-effective economics of storage tiers.</li> <li>Storage cost is the primary cost driver of Vespa streaming mode; no data is in memory. Avoiding memory usage lowers deployment costs significantly.</li> </ul>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#connecting-langchain-retriever-with-vespa-for-context-retrieval-from-pdf-documents","title":"Connecting LangChain Retriever with Vespa for Context Retrieval from PDF Documents\u00b6","text":"<p>In this notebook, we seamlessly integrate a custom LangChain retriever with a Vespa app, leveraging Vespa's streaming mode to extract meaningful context from PDF documents.</p> <p>The workflow</p> <ul> <li>Define and deploy a Vespa application package using PyVespa.</li> <li>Utilize LangChain PDF Loaders to download and parse PDF files.</li> <li>Leverage LangChain Document Transformers to convert each PDF page into multiple model context-sized parts.</li> <li>Feed the transformer representation to the running Vespa instance</li> <li>Employ Vespa's built-in ColBERT embedder functionality (using an open-source embedding model) for embedding the contexts, resulting in a multi-vector representation per context</li> <li>Develop a custom Retriever to enable seamless retrieval for any unstructured text query.</li> </ul> <p></p> <p></p> <p>Let's get started! First, install dependencies:</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#sample-data","title":"Sample data\u00b6","text":"<p>We love ColBERT, so we'll use a few COlBERT related papers as examples of PDFs in this notebook.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#processing-pdfs-with-langchain","title":"Processing PDFs with LangChain\u00b6","text":"<p>LangChain has a rich set of document loaders that can be used to load and process various file formats. In this notebook, we use the PyPDFLoader.</p> <p>We also want to split the extracted text into contexts using a text splitter. Most text embedding models have limited input lengths (typically less than 512 language model tokens, so splitting the text into multiple contexts that each fits into the context limit of the embedding model is a common strategy.</p> <p>For embedding text data, models based on the Transformer architecture have become the de facto standard. A challenge with Transformer-based models is their input length limitation due to the quadratic self-attention computational complexity. For example, a popular open-source text embedding model like e5 has an absolute maximum input length of 512 wordpiece tokens. In addition to the technical limitation, trying to fit more tokens than used during fine-tuning of the model will impact the quality of the vector representation.</p> <p>One can view this text embedding encoding as a lossy compression technique, where variable-length texts are compressed into a fixed dimensional vector representation.</p> <p>Although this compressed representation is very useful, it can be imprecise especially as the size of the text increases. By adding the ColBERT embedding, we also retain token-level information which retains more of the original meaning of the text and allows the richer late interaction between the query and the document text.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now, we can also query our data. With streaming mode, we must pass the <code>groupname</code> parameter, or the request will fail with an error.</p> <p>The query request uses the Vespa Query API and the <code>Vespa.query()</code> function supports passing any of the Vespa query API parameters.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul> <p>Sample query request for <code>why is colbert effective?</code> for the user <code>jo-bergum</code>:</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#langchain-retriever","title":"LangChain Retriever\u00b6","text":"<p>We use the LangChain Retriever interface so that we can connect our Vespa app with the flexibility and power of the LangChain LLM framework.</p> <p>A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.</p> <p>The retriever interface fits perfectly with Vespa, as Vespa can support a wide range of features and ways to retrieve and rank content. The following implements a custom retriever <code>VespaStreamingColBERTRetriever</code> that takes the following arguments:</p> <ul> <li><code>app:Vespa</code> The Vespa application we retrieve from. This could be a Vespa Cloud instance or a local instance, for example running on a laptop.</li> <li><code>user:str</code> The user that that we want to retrieve for, this argument maps to the Vespa streaming mode groupname parameter</li> <li><code>pages:int</code> The target number of PDF pages we want to retrieve for a given query</li> <li><code>chunks_per_page</code> The is the target number of relevant text chunks that are associated with the page</li> <li><code>chunk_similarity_threshold</code> - The chunk similarity threshold, only chunks with a similarity above this threshold</li> </ul> <p>The core idea is to retrieve pages using max context similarity as the initial scoring function, then re-rank the top-K pages using the ColBERT embeddings. This re-ranking is handled by the second phase of the Vespa ranking expression defined above, and is transparent to the retriever code below.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#rag","title":"RAG\u00b6","text":""},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#interact-with-the-chain","title":"Interact with the chain\u00b6","text":"<p>Now, we can start asking questions using the <code>chain</code> define above.</p>"},{"location":"examples/chat_with_your_pdfs_using_colbert_langchain_and_Vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>Vespa\u2019s streaming mode is a game-changer, enabling the creation of highly cost-effective RAG applications for naturally partitioned data. Now it is also possible to use ColBERT for re-ranking, without having to integrate any custom embedder or re-ranking code.</p> <p>In this notebook, we delved into the hands-on application of LangChain, leveraging document loaders and transformers. Finally, we showcased a custom LangChain retriever that connected all the functionality of LangChain with Vespa.</p> <p>For those interested in learning more about Vespa, join the Vespa community on Slack to exchange ideas, seek assistance, or stay in the loop on the latest Vespa developments.</p> <p>We can now delete the cloud instance:</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html","title":"Cohere binary vectors in vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa cohere==4.57 vespacli\n</pre> !pip3 install -U pyvespa cohere==4.57 vespacli In\u00a0[2]: Copied! <pre>import cohere\n\n# Make sure that the environment variable CO_API_KEY is set to your API key\nco = cohere.Client()\n</pre> import cohere  # Make sure that the environment variable CO_API_KEY is set to your API key co = cohere.Client() In\u00a0[3]: Copied! <pre>documents = [\n    \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",\n    \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",\n    \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\",\n]\n</pre> documents = [     \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",     \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",     \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",     \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\", ] <p>Notice that we ask for <code>embedding_types=[\"binary]</code></p> In\u00a0[4]: Copied! <pre># Compute the binary embeddings ofdocuments.\n# Set input_type to \"search_document\" and embedding_types to \"binary\"\n\ncohere_response = co.embed(\n    documents,\n    model=\"embed-english-v3.0\",\n    input_type=\"search_document\",\n    embedding_types=[\"binary\"],\n)\n</pre> # Compute the binary embeddings ofdocuments. # Set input_type to \"search_document\" and embedding_types to \"binary\"  cohere_response = co.embed(     documents,     model=\"embed-english-v3.0\",     input_type=\"search_document\",     embedding_types=[\"binary\"], ) In\u00a0[5]: Copied! <pre>print(cohere_response.embeddings.binary)\n</pre> print(cohere_response.embeddings.binary) <pre>[[-110, 121, 110, -50, 87, -59, 8, 35, 114, 30, -92, -112, -118, -16, 7, 96, 17, 51, 97, -9, -23, 25, -103, -35, -78, -47, 64, -123, -41, 67, 14, -31, -42, -126, 75, 111, 62, -64, 57, 64, -52, -66, -64, -12, 100, 99, 87, 61, -5, 5, 23, 34, -75, -66, -16, 91, 92, 121, 55, 117, 100, -112, -24, 84, 84, -65, 61, -31, -45, 7, 44, 8, -35, -125, 16, -50, -52, 11, -105, -32, 102, -62, -3, 86, -107, 21, 95, 15, 27, -79, -20, 114, 90, 125, 110, -97, -15, -98, 21, -102, -124, 112, -115, 26, -86, -55, 67, 7, 11, -127, 125, 103, -46, -55, 79, -31, 126, -32, 33, -128, -124, -80, 21, 27, -49, -9, 112, 101], [-110, -7, -24, 23, -33, 68, 24, 35, 22, -50, -32, 86, 74, -14, 71, 96, 81, -45, 105, -25, -73, 108, -99, 13, -76, 125, 73, -44, -34, -34, -105, 75, 86, -58, 85, -30, -92, -27, -39, 0, -75, -2, 30, -12, -116, 9, 81, 39, 76, 44, 87, 20, -43, 110, -75, 20, 108, 125, -75, 85, -28, -118, -24, 127, 78, -75, 108, -20, -48, 3, 12, 12, 71, -29, -98, -26, 68, 11, 0, -104, 96, 70, -3, 53, -98, -108, 127, -102, -17, -84, -88, 88, -54, -45, -11, -4, -4, 15, -67, 122, -108, 117, -51, 40, 98, -47, 102, -103, 3, -123, -85, 119, -48, -24, 95, -34, -26, -24, -31, -9, 99, 64, -128, -43, 74, -91, 80, -95], [64, -14, -4, 30, 118, 5, 8, 35, 51, 3, 72, -122, -70, -10, 2, -20, 17, 115, -67, -9, 115, 31, -103, -73, -78, 65, 64, -123, -41, 91, 14, -39, -41, -78, 73, -62, 60, -28, 89, 32, 33, -35, -62, 116, 102, -45, 83, 63, 73, 37, 23, 64, -43, -46, -106, 83, 109, 92, -87, -15, -60, -39, -23, 63, 84, 56, -6, -15, 20, 3, 76, 3, 104, -16, -79, 70, -123, 15, -125, -111, 109, -105, -99, 82, -19, -27, 95, -113, 94, -74, 57, 82, -102, -7, -95, -21, -3, -66, 73, 95, -124, 37, -115, -81, 107, -55, -25, 6, 19, -107, -120, 111, -110, -23, 79, -26, 106, -61, -96, -77, 9, 116, -115, -67, -63, -9, -43, 77], [-109, -7, -32, 19, 87, 116, 8, 35, 54, -102, -64, -106, -14, -10, 31, 78, -99, 59, -6, -45, 97, 96, -103, 37, 69, -35, -119, -59, 95, 27, 14, 73, 86, -9, -43, 110, -70, 96, 45, 32, -91, 62, -64, -12, 100, -55, 34, 62, 14, 5, 22, 67, -75, -17, -14, 81, 45, 125, -15, -11, -28, 75, -25, 20, 42, -78, -4, -67, -44, 11, 76, 3, 127, 40, 0, 103, 75, -62, -123, -111, 64, -13, -10, -5, -66, -89, 119, -70, -29, -95, -19, 82, 106, 127, -24, -11, -48, 15, -29, -102, -115, 107, -115, 55, -69, -61, 103, 11, 3, 25, -118, 63, -108, 11, 78, -28, 14, 124, 119, -61, 97, 84, 53, 69, 123, 89, -104, -127]]\n</pre> <p>As we can see from the above, we got an array of binary embeddings, using signed <code>int8</code> precision in the numeric range [-128 to 127]. Each embedding vector has 128 dimensions:</p> In\u00a0[6]: Copied! <pre>len(cohere_response.embeddings.binary[0])\n</pre> len(cohere_response.embeddings.binary[0]) Out[6]: <pre>128</pre> In\u00a0[20]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"doc\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(\n                name=\"doc_id\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"word\"],\n                rank=\"filter\",\n            ),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"binary_vector\",\n                type=\"tensor&lt;int8&gt;(x[128])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: hamming\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"doc\",     mode=\"index\",     document=Document(         fields=[             Field(                 name=\"doc_id\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"word\"],                 rank=\"filter\",             ),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"binary_vector\",                 type=\"tensor(x[128])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: hamming\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])], ) <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[21]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"cohere\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"cohere\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p><code>unpack_bits</code> unpacks the binary representation into a 1024-dimensional float vector doc.</p> <p>We define two tensor inputs, one compact binary representation that is used for the nearestNeighbor search and one full version that is used in ranking.</p> In\u00a0[22]: Copied! <pre>from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q_binary)\", \"tensor&lt;int8&gt;(x[128])\"),\n        (\"query(q_full)\", \"tensor&lt;float&gt;(x[1024])\"),\n    ],\n    functions=[\n        Function(  # this returns a tensor&lt;float&gt;(x[1024]) with values -1 or 1\n            name=\"unpack_binary_representation\",\n            expression=\"2*unpack_bits(attribute(binary_vector)) -1\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"closeness(field, binary_vector)\"  # 1/(1 + hamming_distance). Calculated between the binary query and the binary_vector\n    ),\n    second_phase=SecondPhaseRanking(\n        expression=\"sum( query(q_full)* unpack_binary_representation )\",  # re-rank using the dot product between float query and the unpacked binary representation\n        rerank_count=100,\n    ),\n    match_features=[\n        \"distance(field, binary_vector)\",\n        \"closeness(field, binary_vector)\",\n    ],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q_binary)\", \"tensor(x[128])\"),         (\"query(q_full)\", \"tensor(x[1024])\"),     ],     functions=[         Function(  # this returns a tensor(x[1024]) with values -1 or 1             name=\"unpack_binary_representation\",             expression=\"2*unpack_bits(attribute(binary_vector)) -1\",         )     ],     first_phase=FirstPhaseRanking(         expression=\"closeness(field, binary_vector)\"  # 1/(1 + hamming_distance). Calculated between the binary query and the binary_vector     ),     second_phase=SecondPhaseRanking(         expression=\"sum( query(q_full)* unpack_binary_representation )\",  # re-rank using the dot product between float query and the unpacked binary representation         rerank_count=100,     ),     match_features=[         \"distance(field, binary_vector)\",         \"closeness(field, binary_vector)\",     ], ) my_schema.add_rank_profile(rerank) In\u00a0[26]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[28]: Copied! <pre>from vespa.io import VespaResponse\n\nwith app.syncio(connections=12) as sync:\n    for i, doc in enumerate(documents):\n        response: VespaResponse = sync.feed_data_point(\n            schema=\"doc\",\n            data_id=str(i),\n            fields={\n                \"doc_id\": str(i),\n                \"text\": doc,\n                \"binary_vector\": cohere_response.embeddings.binary[i],\n            },\n        )\n        assert response.is_successful()\n</pre> from vespa.io import VespaResponse  with app.syncio(connections=12) as sync:     for i, doc in enumerate(documents):         response: VespaResponse = sync.feed_data_point(             schema=\"doc\",             data_id=str(i),             fields={                 \"doc_id\": str(i),                 \"text\": doc,                 \"binary_vector\": cohere_response.embeddings.binary[i],             },         )         assert response.is_successful() <p>For some cases where we have lots of vector data, we can use the hex format for binary indexed tensors.</p> In\u00a0[30]: Copied! <pre>from binascii import hexlify\nimport numpy as np\n\n\ndef to_hex_str(binary_vector):\n    return str(hexlify(np.array(binary_vector, dtype=np.int8)), \"utf-8\")\n</pre> from binascii import hexlify import numpy as np   def to_hex_str(binary_vector):     return str(hexlify(np.array(binary_vector, dtype=np.int8)), \"utf-8\") <p>Feed using hex format</p> In\u00a0[32]: Copied! <pre>with app.syncio() as sync:\n    for i, doc in enumerate(documents):\n        response: VespaResponse = sync.feed_data_point(\n            schema=\"doc\",\n            data_id=str(i),\n            fields={\n                \"doc_id\": str(i),\n                \"text\": doc,\n                \"binary_vector\": {\n                    \"values\": to_hex_str(cohere_response.embeddings.binary[i])\n                },\n            },\n        )\n        assert response.is_successful()\n</pre> with app.syncio() as sync:     for i, doc in enumerate(documents):         response: VespaResponse = sync.feed_data_point(             schema=\"doc\",             data_id=str(i),             fields={                 \"doc_id\": str(i),                 \"text\": doc,                 \"binary_vector\": {                     \"values\": to_hex_str(cohere_response.embeddings.binary[i])                 },             },         )         assert response.is_successful() In\u00a0[33]: Copied! <pre>query = \"Who discovered x-ray?\"\n\n# Make sure to set input_type=\"search_query\" when getting the embeddings for the query.\n# We ask for both float and binary query embeddings\ncohere_query_response = co.embed(\n    [query],\n    model=\"embed-english-v3.0\",\n    input_type=\"search_query\",\n    embedding_types=[\"float\", \"binary\"],\n)\n</pre> query = \"Who discovered x-ray?\"  # Make sure to set input_type=\"search_query\" when getting the embeddings for the query. # We ask for both float and binary query embeddings cohere_query_response = co.embed(     [query],     model=\"embed-english-v3.0\",     input_type=\"search_query\",     embedding_types=[\"float\", \"binary\"], ) <p>Now, we use nearestNeighbor search to retrieve 100 hits using hamming distance, these hits are then exposed to vespa ranking framework, where we re-rank using the dot product between the float tensor and the unpacked binary vector (the unpack returns a 1024 float version).</p> In\u00a0[35]: Copied! <pre>response = app.query(\n    yql=\"select * from doc where {targetHits:100}nearestNeighbor(binary_vector,q_binary)\",\n    ranking=\"rerank\",\n    body={\n        \"input.query(q_binary)\": to_hex_str(cohere_query_response.embeddings.binary[0]),\n        \"input.query(q_full)\": cohere_query_response.embeddings.float[0],\n    },\n)\nassert response.is_successful()\n</pre> response = app.query(     yql=\"select * from doc where {targetHits:100}nearestNeighbor(binary_vector,q_binary)\",     ranking=\"rerank\",     body={         \"input.query(q_binary)\": to_hex_str(cohere_query_response.embeddings.binary[0]),         \"input.query(q_full)\": cohere_query_response.embeddings.float[0],     }, ) assert response.is_successful() In\u00a0[36]: Copied! <pre>response.hits\n</pre> response.hits Out[36]: <pre>[{'id': 'id:doc:doc::3',\n  'relevance': 8.697503089904785,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0029940119760479044,\n    'distance(field,binary_vector)': 333.0},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::3',\n   'doc_id': '3',\n   'text': 'Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity'}},\n {'id': 'id:doc:doc::1',\n  'relevance': 6.413589954376221,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.002551020408163265,\n    'distance(field,binary_vector)': 391.00000000000006},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::1',\n   'doc_id': '1',\n   'text': 'Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.'}},\n {'id': 'id:doc:doc::2',\n  'relevance': 6.379772663116455,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.002652519893899204,\n    'distance(field,binary_vector)': 376.0},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::2',\n   'doc_id': '2',\n   'text': 'Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.'}},\n {'id': 'id:doc:doc::0',\n  'relevance': 4.5963287353515625,\n  'source': 'cohere_content',\n  'fields': {'matchfeatures': {'closeness(field,binary_vector)': 0.0024271844660194173,\n    'distance(field,binary_vector)': 411.00000000000006},\n   'sddocname': 'doc',\n   'documentid': 'id:doc:doc::0',\n   'doc_id': '0',\n   'text': 'Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.'}}]</pre> <p>Notice the returned hits. The <code>relevance</code> is the score assigned by the second-phase dot product between the full query version and the unpacked binary vector. Also, we see the match features and the hamming distances. Notice that the re-ranking step has re-ordered doc 1 and doc 2.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#using-cohere-binary-embeddings-in-vespa","title":"Using Cohere Binary Embeddings in Vespa\u00b6","text":"<p>Cohere just released a new embedding API supporting binary and <code>int8</code> vectors. Read the announcement in the blog post: Cohere int8 &amp; binary Embeddings - Scale Your Vector Database to Large Datasets.</p> <p>We are excited to announce that Cohere Embed is the first embedding model that natively supports int8 and binary embeddings.</p> <p>This is significant because:</p> <ul> <li>Binarization reduces the storage footprint from 1024 floats (4096 bytes) per vector to 128 int8 (128 bytes).</li> <li>32x less data to store</li> <li>Faster distance calculations using hamming distance, which Vespa natively supports for bits packed into int8 precision. More on hamming distance in Vespa.</li> </ul> <p>Vespa supports <code>hamming</code> distance with and without hnsw indexing.</p> <p>For those wanting to learn more about binary vectors, we recommend our 2021 blog series on Billion-scale vector search with Vespa and Billion-scale vector search with Vespa - part two.</p> <p></p> <p>This notebook demonstrates how to use the Cohere binary vectors with Vespa, including a re-ranking phase that uses the float query vector version for improved accuracy. From the Cohere blog announcement:</p> <p>To improve the search quality, the float query embedding can be compared with the binary document embeddings using dot-product. So we first retrieve 10*top_k results with the binary query embedding, and then rescore the binary document embeddings with the float query embedding. This pushes the search quality from 90% to 95%.</p> <p></p> <p>Install the dependencies:</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#examining-the-cohere-embeddings","title":"Examining the Cohere embeddings\u00b6","text":"<p>Let us check out the Cohere embedding API and how we can obtain binarized embeddings. See also the Cohere embed API doc.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#some-sample-documents","title":"Some sample documents\u00b6","text":"<p>Define a few sample documents that we want to embed</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>Notice the <code>binary_vector</code> field that defines an indexed (dense) Vespa tensor with the dimension name <code>x[128]</code>. Indexing specifies <code>index</code> which means that Vespa will use HNSW indexing for this field. Also notice the configuration of distance-metric where we specify <code>hamming</code>.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#feed-our-sample-documents-and-their-binary-embedding-representation","title":"Feed our sample documents and their binary embedding representation\u00b6","text":"<p>With few documents, we use the synchronous API. Read more in reads and writes.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> <li>Practical Nearest Neighbor Search Guide</li> </ul>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#conclusions","title":"Conclusions\u00b6","text":"<p>These new Cohere binary embeddings are a huge step forward for cost-efficient vector search at scale and integrates perfectly with the rich feature set in Vespa.</p>"},{"location":"examples/cohere-binary-vectors-in-vespa-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>We can now delete the cloud instance:</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html","title":"colbert standalone Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa colbert-ai numpy torch transformers&lt;=4.49.0\n</pre> !pip3 install -U pyvespa colbert-ai numpy torch transformers&lt;=4.49.0 <p>Load a checkpoint with colbert and obtain document and query embeddings</p> In\u00a0[\u00a0]: Copied! <pre>from colbert.modeling.checkpoint import Checkpoint\nfrom colbert.infra import ColBERTConfig\n\nckpt = Checkpoint(\n    \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\")\n)\n</pre> from colbert.modeling.checkpoint import Checkpoint from colbert.infra import ColBERTConfig  ckpt = Checkpoint(     \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\") ) In\u00a0[139]: Copied! <pre>passage = [\n    \"Alan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\"\n]\n</pre> passage = [     \"Alan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\" ] In\u00a0[\u00a0]: Copied! <pre>vectors = ckpt.docFromText(passage)[0]\n</pre> vectors = ckpt.docFromText(passage)[0] In\u00a0[129]: Copied! <pre>vectors.shape\n</pre> vectors.shape Out[129]: <pre>torch.Size([27, 128])</pre> <p>In this case, we got 27 token-level embeddings, each using 128 float dimensions. This includes CLS token and special tokens used to differentiate the query from the document encoding.</p> In\u00a0[130]: Copied! <pre>query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0]\nquery_vectors.shape\n</pre> query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0] query_vectors.shape Out[130]: <pre>torch.Size([32, 128])</pre> <p>Routines for binarization and output in Vespa tensor format that can be used in queries and in JSON feed.</p> In\u00a0[118]: Copied! <pre>import numpy as np\nimport torch\nfrom binascii import hexlify\nfrom typing import Dict, List\n\n\ndef binarize_token_vectors_hex(vectors: torch.Tensor) -&gt; Dict[str, str]:\n    binarized_token_vectors = np.packbits(np.where(vectors &gt; 0, 1, 0), axis=1).astype(\n        np.int8\n    )\n    vespa_token_feed = dict()\n    for index in range(0, len(binarized_token_vectors)):\n        vespa_token_feed[index] = str(\n            hexlify(binarized_token_vectors[index].tobytes()), \"utf-8\"\n        )\n    return vespa_token_feed\n\n\ndef float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:\n    vespa_token_feed = dict()\n    for index in range(0, len(vectors)):\n        vespa_token_feed[index] = vectors[index].tolist()\n    return vespa_token_feed\n</pre> import numpy as np import torch from binascii import hexlify from typing import Dict, List   def binarize_token_vectors_hex(vectors: torch.Tensor) -&gt; Dict[str, str]:     binarized_token_vectors = np.packbits(np.where(vectors &gt; 0, 1, 0), axis=1).astype(         np.int8     )     vespa_token_feed = dict()     for index in range(0, len(binarized_token_vectors)):         vespa_token_feed[index] = str(             hexlify(binarized_token_vectors[index].tobytes()), \"utf-8\"         )     return vespa_token_feed   def float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:     vespa_token_feed = dict()     for index in range(0, len(vectors)):         vespa_token_feed[index] = vectors[index].tolist()     return vespa_token_feed In\u00a0[\u00a0]: Copied! <pre>import json\n\nprint(json.dumps(binarize_token_vectors_hex(vectors)))\nprint(json.dumps(float_query_token_vectors(query_vectors)))\n</pre> import json  print(json.dumps(binarize_token_vectors_hex(vectors))) print(json.dumps(float_query_token_vectors(query_vectors))) In\u00a0[151]: Copied! <pre>from vespa.package import Schema, Document, Field\n\ncolbert_schema = Schema(\n    name=\"doc\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(name=\"passage\", type=\"string\", indexing=[\"index\", \"summary\"]),\n            Field(\n                name=\"colbert\",\n                type=\"tensor&lt;int8&gt;(token{}, v[16])\",\n                indexing=[\"attribute\", \"summary\", \"index\"],\n                attribute=[\"distance-metric:hamming\"],\n            ),\n        ]\n    ),\n)\n</pre> from vespa.package import Schema, Document, Field  colbert_schema = Schema(     name=\"doc\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(name=\"passage\", type=\"string\", indexing=[\"index\", \"summary\"]),             Field(                 name=\"colbert\",                 type=\"tensor(token{}, v[16])\",                 indexing=[\"attribute\", \"summary\", \"index\"],                 attribute=[\"distance-metric:hamming\"],             ),         ]     ), ) In\u00a0[152]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"colbert\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colbert_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"colbert\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colbert_schema] ) <p>We need to define all the query input tensors. We are going to input up to 32 query tensors in binary form these are used for retrieval</p> In\u00a0[92]: Copied! <pre>query_binary_input_tensors = []\nfor index in range(0, 32):\n    query_binary_input_tensors.append(\n        (\"query(binary_vector_{})\".format(index), \"tensor&lt;int8&gt;(v[16])\")\n    )\n</pre> query_binary_input_tensors = [] for index in range(0, 32):     query_binary_input_tensors.append(         (\"query(binary_vector_{})\".format(index), \"tensor(v[16])\")     ) <p>Note that we just use max sim in the first phase ranking over all the hits that are retrieved by the query</p> In\u00a0[153]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking\n\ncolbert = RankProfile(\n    name=\"default\",\n    inputs=[\n        (\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"),\n        *query_binary_input_tensors,\n    ],\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(colbert)) , v\n                        ),\n                        max, token\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n)\ncolbert_schema.add_rank_profile(colbert)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking  colbert = RankProfile(     name=\"default\",     inputs=[         (\"query(qt)\", \"tensor(querytoken{}, v[128])\"),         *query_binary_input_tensors,     ],     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(colbert)) , v                         ),                         max, token                     ),                     querytoken                 )             \"\"\",         )     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"), ) colbert_schema.add_rank_profile(colbert) <p>Install the Vespa CLI.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install vespacli\n</pre> !pip3 install vespacli <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial. Make note of the tenant name, it is used in the next steps.</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"TENANT_NAME\"] = \"vespa-team\"  # Replace with your tenant name\n\nvespa_cli_command = (\n    f'vespa config set application {os.environ[\"TENANT_NAME\"]}.{vespa_app_name}'\n)\n\n!vespa config set target cloud\n!{vespa_cli_command}\n!vespa auth cert -N\n</pre> import os  os.environ[\"TENANT_NAME\"] = \"vespa-team\"  # Replace with your tenant name  vespa_cli_command = (     f'vespa config set application {os.environ[\"TENANT_NAME\"]}.{vespa_app_name}' )  !vespa config set target cloud !{vespa_cli_command} !vespa auth cert -N <p>Validate that we have the expected data-plane credential files:</p> In\u00a0[52]: Copied! <pre>from os.path import exists\nfrom pathlib import Path\n\ncert_path = (\n    Path.home()\n    / \".vespa\"\n    / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-public-cert.pem\"\n)\nkey_path = (\n    Path.home()\n    / \".vespa\"\n    / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-private-key.pem\"\n)\n\nif not exists(cert_path) or not exists(key_path):\n    print(\n        \"ERROR: set the correct paths to security credentials. Correct paths above and rerun until you do not see this error\"\n    )\n</pre> from os.path import exists from pathlib import Path  cert_path = (     Path.home()     / \".vespa\"     / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-public-cert.pem\" ) key_path = (     Path.home()     / \".vespa\"     / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-private-key.pem\" )  if not exists(cert_path) or not exists(key_path):     print(         \"ERROR: set the correct paths to security credentials. Correct paths above and rerun until you do not see this error\"     ) <p>Note that the subsequent Vespa Cloud deploy call below will add <code>data-plane-public-cert.pem</code> to the application before deploying it to Vespa Cloud, so that you have access to both the private key and the public certificate. At the same time, Vespa Cloud only knows the public certificate.</p> In\u00a0[\u00a0]: Copied! <pre>!vespa auth api-key\n\nfrom pathlib import Path\n\napi_key_path = Path.home() / \".vespa\" / f\"{os.environ['TENANT_NAME']}.api-key.pem\"\n</pre> !vespa auth api-key  from pathlib import Path  api_key_path = Path.home() / \".vespa\" / f\"{os.environ['TENANT_NAME']}.api-key.pem\" In\u00a0[154]: Copied! <pre>from vespa.deployment import VespaCloud\n\n\ndef read_secret():\n    \"\"\"Read the API key from the environment variable. This is\n    only used for CI/CD purposes.\"\"\"\n    t = os.getenv(\"VESPA_TEAM_API_KEY\")\n    if t:\n        return t.replace(r\"\\n\", \"\\n\")\n    else:\n        return t\n\n\nvespa_cloud = VespaCloud(\n    tenant=os.environ[\"TENANT_NAME\"],\n    application=vespa_app_name,\n    key_content=read_secret() if read_secret() else None,\n    key_location=api_key_path,\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud   def read_secret():     \"\"\"Read the API key from the environment variable. This is     only used for CI/CD purposes.\"\"\"     t = os.getenv(\"VESPA_TEAM_API_KEY\")     if t:         return t.replace(r\"\\n\", \"\\n\")     else:         return t   vespa_cloud = VespaCloud(     tenant=os.environ[\"TENANT_NAME\"],     application=vespa_app_name,     key_content=read_secret() if read_secret() else None,     key_location=api_key_path,     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[156]: Copied! <pre>from vespa.io import VespaResponse\n\nvespa_feed_format = {\n    \"id\": \"1\",\n    \"passage\": passage[0],\n    \"colbert\": binarize_token_vectors_hex(vectors),\n}\nwith app.syncio() as sync:\n    response: VespaResponse = sync.feed_data_point(\n        data_id=1, fields=vespa_feed_format, schema=\"doc\"\n    )\n</pre> from vespa.io import VespaResponse  vespa_feed_format = {     \"id\": \"1\",     \"passage\": passage[0],     \"colbert\": binarize_token_vectors_hex(vectors), } with app.syncio() as sync:     response: VespaResponse = sync.feed_data_point(         data_id=1, fields=vespa_feed_format, schema=\"doc\"     ) In\u00a0[\u00a0]: Copied! <pre>query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0]\nbinary_query_input_tensors = binarize_token_vectors_hex(query_vectors)\n</pre> query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0] binary_query_input_tensors = binarize_token_vectors_hex(query_vectors) In\u00a0[158]: Copied! <pre>binary_query_vectors = dict()\nnn_operators = list()\nfor index in range(0, 32):\n    name = \"input.query(binary_vector_{})\".format(index)\n    nn_argument = \"binary_vector_{}\".format(index)\n    value = binary_query_input_tensors[index]\n    binary_query_vectors[name] = value\n    nn_operators.append(\"({targetHits:100}nearestNeighbor(colbert, %s))\" % nn_argument)\n</pre> binary_query_vectors = dict() nn_operators = list() for index in range(0, 32):     name = \"input.query(binary_vector_{})\".format(index)     nn_argument = \"binary_vector_{}\".format(index)     value = binary_query_input_tensors[index]     binary_query_vectors[name] = value     nn_operators.append(\"({targetHits:100}nearestNeighbor(colbert, %s))\" % nn_argument) In\u00a0[159]: Copied! <pre>nn_operators = \" OR \".join(nn_operators)\n</pre> nn_operators = \" OR \".join(nn_operators) Out[159]: <pre>'({targetHits:100}nearestNeighbor(colbert, binary_vector_0)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_1)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_2)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_3)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_4)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_5)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_6)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_7)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_8)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_9)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_10)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_11)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_12)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_13)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_14)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_15)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_16)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_17)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_18)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_19)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_20)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_21)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_22)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_23)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_24)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_25)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_26)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_27)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_28)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_29)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_30)) OR ({targetHits:100}nearestNeighbor(colbert, binary_vector_31))'</pre> In\u00a0[161]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select * from doc where {}\".format(nn_operators),\n    ranking=\"default\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(qt)\": float_query_token_vectors(query_vectors),\n        **binary_query_vectors,\n    },\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select * from doc where {}\".format(nn_operators),     ranking=\"default\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(qt)\": float_query_token_vectors(query_vectors),         **binary_query_vectors,     }, ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:doc:doc::1\",\n  \"relevance\": 100.57648777961731,\n  \"source\": \"colbert_content\",\n  \"fields\": {\n    \"sddocname\": \"doc\",\n    \"documentid\": \"id:doc:doc::1\",\n    \"id\": \"1\",\n    \"passage\": \"Alan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"colbert\": {\n      \"0\": [\n        3,\n        120,\n        69,\n        0,\n        37,\n        -60,\n        -58,\n        -95,\n        -120,\n        32,\n        -127,\n        67,\n        -36,\n        68,\n        -106,\n        -12\n      ],\n      \"1\": [\n        -106,\n        40,\n        -119,\n        -128,\n        96,\n        -60,\n        -58,\n        33,\n        48,\n        96,\n        -127,\n        67,\n        -100,\n        96,\n        -106,\n        -12\n      ],\n      \"2\": [\n        -28,\n        -84,\n        73,\n        -18,\n        113,\n        -60,\n        -51,\n        40,\n        -96,\n        121,\n        4,\n        24,\n        -99,\n        68,\n        -47,\n        -60\n      ],\n      \"3\": [\n        -13,\n        40,\n        75,\n        -124,\n        65,\n        64,\n        -32,\n        -53,\n        12,\n        64,\n        125,\n        4,\n        24,\n        -64,\n        -69,\n        101\n      ],\n      \"4\": [\n        33,\n        -54,\n        113,\n        24,\n        77,\n        -36,\n        -44,\n        3,\n        -32,\n        -72,\n        40,\n        41,\n        -38,\n        102,\n        53,\n        -35\n      ],\n      \"5\": [\n        3,\n        -22,\n        73,\n        -95,\n        73,\n        -51,\n        85,\n        -128,\n        -121,\n        25,\n        17,\n        68,\n        90,\n        64,\n        -113,\n        -28\n      ],\n      \"6\": [\n        -109,\n        -72,\n        -114,\n        0,\n        97,\n        -58,\n        -57,\n        -95,\n        40,\n        -96,\n        -112,\n        67,\n        -97,\n        -85,\n        -42,\n        -12\n      ],\n      \"7\": [\n        -112,\n        56,\n        -114,\n        0,\n        97,\n        -58,\n        -57,\n        -83,\n        40,\n        -96,\n        -127,\n        67,\n        -97,\n        43,\n        -42,\n        -12\n      ],\n      \"8\": [\n        22,\n        -71,\n        65,\n        96,\n        0,\n        -60,\n        108,\n        37,\n        16,\n        106,\n        -55,\n        115,\n        -117,\n        -56,\n        -28,\n        -12\n      ],\n      \"9\": [\n        -106,\n        -72,\n        94,\n        30,\n        32,\n        -60,\n        -60,\n        -19,\n        24,\n        -56,\n        -47,\n        -63,\n        -40,\n        -53,\n        -103,\n        -11\n      ],\n      \"10\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"11\": [\n        -126,\n        121,\n        3,\n        -103,\n        32,\n        70,\n        103,\n        -23,\n        88,\n        -55,\n        -61,\n        71,\n        -101,\n        -106,\n        -8,\n        -68\n      ],\n      \"12\": [\n        18,\n        24,\n        -106,\n        30,\n        36,\n        -42,\n        -60,\n        104,\n        57,\n        -120,\n        -128,\n        -61,\n        -67,\n        -53,\n        -100,\n        -11\n      ],\n      \"13\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"14\": [\n        22,\n        49,\n        -38,\n        17,\n        36,\n        -42,\n        -25,\n        65,\n        25,\n        -56,\n        -45,\n        -59,\n        -102,\n        -2,\n        -65,\n        125\n      ],\n      \"15\": [\n        -105,\n        25,\n        -50,\n        16,\n        0,\n        -42,\n        -28,\n        45,\n        48,\n        -56,\n        -112,\n        -55,\n        -3,\n        -87,\n        -112,\n        -11\n      ],\n      \"16\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"17\": [\n        55,\n        43,\n        -62,\n        33,\n        -91,\n        68,\n        99,\n        32,\n        72,\n        10,\n        -41,\n        70,\n        -117,\n        -78,\n        -73,\n        -11\n      ],\n      \"18\": [\n        3,\n        53,\n        -117,\n        20,\n        36,\n        -42,\n        79,\n        33,\n        9,\n        -120,\n        -41,\n        69,\n        -36,\n        -69,\n        -111,\n        117\n      ],\n      \"19\": [\n        23,\n        16,\n        -42,\n        20,\n        44,\n        -42,\n        -26,\n        33,\n        57,\n        -120,\n        -112,\n        -63,\n        -3,\n        -24,\n        -108,\n        -11\n      ],\n      \"20\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"21\": [\n        -110,\n        53,\n        -106,\n        28,\n        32,\n        -42,\n        -58,\n        77,\n        61,\n        -56,\n        -42,\n        -15,\n        -68,\n        -5,\n        -110,\n        -11\n      ],\n      \"22\": [\n        -109,\n        56,\n        -114,\n        0,\n        96,\n        -42,\n        -58,\n        -83,\n        40,\n        -96,\n        -128,\n        -61,\n        -99,\n        -21,\n        -44,\n        -12\n      ],\n      \"23\": [\n        18,\n        57,\n        -50,\n        30,\n        36,\n        86,\n        -60,\n        69,\n        9,\n        -120,\n        -48,\n        -63,\n        -75,\n        -22,\n        -98,\n        -11\n      ],\n      \"24\": [\n        30,\n        -71,\n        -106,\n        26,\n        32,\n        -42,\n        -50,\n        104,\n        56,\n        64,\n        -48,\n        -61,\n        -4,\n        -8,\n        -104,\n        -12\n      ],\n      \"25\": [\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0,\n        0\n      ],\n      \"26\": [\n        7,\n        56,\n        70,\n        0,\n        36,\n        -58,\n        -42,\n        33,\n        -104,\n        34,\n        -127,\n        67,\n        -99,\n        96,\n        -105,\n        -12\n      ]\n    }\n  }\n}\n</pre> <p>Another example where we brute-force \"true\" search without a retrieval step using nearestNeighbor or other filters.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select * from doc where true\",\n    ranking=\"default\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(qt)\": float_query_token_vectors(query_vectors),\n    },\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select * from doc where true\",     ranking=\"default\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(qt)\": float_query_token_vectors(query_vectors),     }, ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/colbert_standalone_Vespa-cloud.html#standalone-colbert-with-vespa-for-end-to-end-retrieval-and-ranking","title":"Standalone ColBERT with Vespa for end-to-end retrieval and ranking\u00b6","text":"<p>This notebook illustrates using ColBERT package to produce token vectors, instead of using the native Vespa colbert embedder.</p> <p>This guide illustrates how to feed and query using a single passage representation</p> <ul> <li>Compress token vectors using binarization compatible with Vespa unpackbits used in ranking. This implements the binarization of token-level vectors using <code>numpy</code>.</li> <li>Use Vespa hex feed format for binary vectors doc.</li> <li>Query examples.</li> </ul> <p>As a bonus, this also demonstrates how to use ColBERT end-to-end with Vespa for both retrieval and ranking. The retrieval step searches the binary token-level representations using hamming distance. This uses 32 nearestNeighbor operators in the same query, each finding 100 nearest hits in hamming space. Then the results are re-ranked using the full-blown MaxSim calculation.</p> <p>See Announcing the Vespa ColBERT embedder for details on ColBERT and the binary quantization used to compress ColBERT's token-level vectors.</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>We use HNSW with hamming distance for retrieval</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud. It is also possible to deploy the app using docker; see the Hybrid Search - Quickstart guide for an example of deploying it to a local docker container.</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#configure-vespa-cloud-date-plane-security","title":"Configure Vespa Cloud date-plane security\u00b6","text":"<p>Create Vespa Cloud data-plane mTLS cert/key-pair. The mutual certificate pair is used to talk to your Vespa cloud endpoints. See Vespa Cloud Security Guide for details.</p> <p>We save the paths to the credentials for later data-plane access without using pyvespa APIs.</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#configure-vespa-cloud-control-plane-security","title":"Configure Vespa Cloud control-plane security\u00b6","text":"<p>Authenticate to generate a tenant level control plane API key for deploying the applications to Vespa Cloud, and save the path to it.</p> <p>The generated tenant api key must be added in the Vespa Console before attempting to deploy the application.</p> <pre><code>To use this key in Vespa Cloud click 'Add custom key' at\nhttps://console.vespa-cloud.com/tenant/TENANT_NAME/account/keys\nand paste the entire public key including the BEGIN and END lines.\n</code></pre>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>Now that we have data-plane and control-plane credentials ready, we can deploy our application to Vespa Cloud!</p> <p><code>PyVespa</code> supports deploying apps to the development zone.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/colbert_standalone_Vespa-cloud.html#querying","title":"Querying\u00b6","text":"<p>Now we create all the query token vectors in binary form and use 32 nearestNeighbor query operators that are combined with OR. These hits are then exposed to ranking where the final MaxSim is performed using the unpacked binary representations.</p>"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html","title":"colbert standalone long context Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa colbert-ai numpy torch vespacli transformers&lt;=4.49.0\n</pre> !pip3 install -U pyvespa colbert-ai numpy torch vespacli transformers&lt;=4.49.0 <p>Load a checkpoint with ColBERT and obtain document and query embeddings</p> In\u00a0[\u00a0]: Copied! <pre>from colbert.modeling.checkpoint import Checkpoint\nfrom colbert.infra import ColBERTConfig\n\nckpt = Checkpoint(\n    \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\")\n)\n</pre> from colbert.modeling.checkpoint import Checkpoint from colbert.infra import ColBERTConfig  ckpt = Checkpoint(     \"colbert-ir/colbertv2.0\", colbert_config=ColBERTConfig(root=\"experiments\") ) <p>A few sample documents:</p> In\u00a0[50]: Copied! <pre>document_passages = [\n    \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"Born in Maida Vale, London, Turing was raised in southern England. He graduated from King's College, Cambridge, with a degree in mathematics.\",\n    \"After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer.\",\n    \"Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations.\",\n]\n</pre> document_passages = [     \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",     \"Born in Maida Vale, London, Turing was raised in southern England. He graduated from King's College, Cambridge, with a degree in mathematics.\",     \"After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer.\",     \"Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations.\", ] In\u00a0[\u00a0]: Copied! <pre>document_token_vectors = ckpt.docFromText(document_passages)\n</pre> document_token_vectors = ckpt.docFromText(document_passages) <p>See the shape of the ColBERT document embeddings:</p> In\u00a0[52]: Copied! <pre>document_token_vectors.shape\n</pre> document_token_vectors.shape Out[52]: <pre>torch.Size([4, 35, 128])</pre> In\u00a0[53]: Copied! <pre>query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0]\nquery_vectors.shape\n</pre> query_vectors = ckpt.queryFromText([\"Who was Alan Turing?\"])[0] query_vectors.shape Out[53]: <pre>torch.Size([32, 128])</pre> <p>The query is always padded to 32 so in the above we have 32 query token vectors.</p> <p>Routines for binarization and output in Vespa tensor format that can be used in queries and JSON feed.</p> In\u00a0[67]: Copied! <pre>import numpy as np\nimport torch\nfrom binascii import hexlify\nfrom typing import List, Dict\n\n\ndef binarize_token_vectors_hex(vectors: torch.Tensor) -&gt; Dict[str, str]:\n    # Notice axix=2 to pack the bits in the last dimension, which is the token level vectors\n    binarized_token_vectors = np.packbits(np.where(vectors &gt; 0, 1, 0), axis=2).astype(\n        np.int8\n    )\n    vespa_tensor = list()\n    for chunk_index in range(0, len(binarized_token_vectors)):\n        token_vectors = binarized_token_vectors[chunk_index]\n        for token_index in range(0, len(token_vectors)):\n            values = str(hexlify(token_vectors[token_index].tobytes()), \"utf-8\")\n            if (\n                values == \"00000000000000000000000000000000\"\n            ):  # skip empty vectors due to padding with batch of passages\n                continue\n            vespa_tensor_cell = {\n                \"address\": {\"context\": chunk_index, \"token\": token_index},\n                \"values\": values,\n            }\n            vespa_tensor.append(vespa_tensor_cell)\n\n    return vespa_tensor\n\n\ndef float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:\n    vespa_token_feed = dict()\n    for index in range(0, len(vectors)):\n        vespa_token_feed[index] = vectors[index].tolist()\n    return vespa_token_feed\n</pre> import numpy as np import torch from binascii import hexlify from typing import List, Dict   def binarize_token_vectors_hex(vectors: torch.Tensor) -&gt; Dict[str, str]:     # Notice axix=2 to pack the bits in the last dimension, which is the token level vectors     binarized_token_vectors = np.packbits(np.where(vectors &gt; 0, 1, 0), axis=2).astype(         np.int8     )     vespa_tensor = list()     for chunk_index in range(0, len(binarized_token_vectors)):         token_vectors = binarized_token_vectors[chunk_index]         for token_index in range(0, len(token_vectors)):             values = str(hexlify(token_vectors[token_index].tobytes()), \"utf-8\")             if (                 values == \"00000000000000000000000000000000\"             ):  # skip empty vectors due to padding with batch of passages                 continue             vespa_tensor_cell = {                 \"address\": {\"context\": chunk_index, \"token\": token_index},                 \"values\": values,             }             vespa_tensor.append(vespa_tensor_cell)      return vespa_tensor   def float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:     vespa_token_feed = dict()     for index in range(0, len(vectors)):         vespa_token_feed[index] = vectors[index].tolist()     return vespa_token_feed In\u00a0[\u00a0]: Copied! <pre>import json\n\nprint(json.dumps(binarize_token_vectors_hex(document_token_vectors)))\nprint(json.dumps(float_query_token_vectors(query_vectors)))\n</pre> import json  print(json.dumps(binarize_token_vectors_hex(document_token_vectors))) print(json.dumps(float_query_token_vectors(query_vectors))) In\u00a0[60]: Copied! <pre>from vespa.package import Schema, Document, Field\n\ncolbert_schema = Schema(\n    name=\"doc\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"passages\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"colbert\",\n                type=\"tensor&lt;int8&gt;(context{}, token{}, v[16])\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n        ]\n    ),\n)\n</pre> from vespa.package import Schema, Document, Field  colbert_schema = Schema(     name=\"doc\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"passages\",                 type=\"array\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"colbert\",                 type=\"tensor(context{}, token{}, v[16])\",                 indexing=[\"attribute\", \"summary\"],             ),         ]     ), ) In\u00a0[61]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"colbertlong\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colbert_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"colbertlong\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colbert_schema] ) <p>Note that we use max sim in the first phase ranking over all the hits that are retrieved by the query logic. Also note that asymmetric MaxSim where we use <code>unpack_bits</code> to obtain a 128-d float vector representation from the binary vector representation.</p> In\u00a0[62]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking\n\ncolbert_profile = RankProfile(\n    name=\"default\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    functions=[\n        Function(\n            name=\"max_sim_per_context\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(colbert)) , v\n                        ),\n                        max, token\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim\", expression=\"reduce(max_sim_per_context, max, context)\"\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n    match_features=[\"max_sim_per_context\"],\n)\ncolbert_schema.add_rank_profile(colbert_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking  colbert_profile = RankProfile(     name=\"default\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     functions=[         Function(             name=\"max_sim_per_context\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(colbert)) , v                         ),                         max, token                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim\", expression=\"reduce(max_sim_per_context, max, context)\"         ),     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"),     match_features=[\"max_sim_per_context\"], ) colbert_schema.add_rank_profile(colbert_profile) In\u00a0[63]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>Use Vespa tensor <code>blocks</code> format for mixed tensors (two mapped dimensions with one dense) doc.</p> In\u00a0[65]: Copied! <pre>from vespa.io import VespaResponse\n\nvespa_feed_format = {\n    \"id\": \"1\",\n    \"passages\": document_passages,\n    \"colbert\": {\"blocks\": binarize_token_vectors_hex(document_token_vectors)},\n}\n# synchrounous feed (this is blocking and slow, but few docs..)\nwith app.syncio() as sync:\n    response: VespaResponse = sync.feed_data_point(\n        data_id=1, fields=vespa_feed_format, schema=\"doc\"\n    )\n</pre> from vespa.io import VespaResponse  vespa_feed_format = {     \"id\": \"1\",     \"passages\": document_passages,     \"colbert\": {\"blocks\": binarize_token_vectors_hex(document_token_vectors)}, } # synchrounous feed (this is blocking and slow, but few docs..) with app.syncio() as sync:     response: VespaResponse = sync.feed_data_point(         data_id=1, fields=vespa_feed_format, schema=\"doc\"     ) <p>This example uses brute-force \"true\" search without a retrieval step using nearestNeighbor or keywords.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select * from doc where true\",\n    ranking=\"default\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(qt)\": float_query_token_vectors(query_vectors),\n    },\n)\nassert response.is_successful()\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select * from doc where true\",     ranking=\"default\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(qt)\": float_query_token_vectors(query_vectors),     }, ) assert response.is_successful() <p>You should see output similar to this:</p> <pre>{\n  \"id\": \"id:doc:doc::1\",\n  \"relevance\": 100.0651626586914,\n  \"source\": \"colbertlong_content\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"max_sim_per_context\": {\n        \"0\": 100.0651626586914,\n        \"1\": 62.7861328125,\n        \"2\": 67.44772338867188,\n        \"3\": 60.133323669433594\n      }\n    },\n    \"sddocname\": \"doc\",\n    \"documentid\": \"id:doc:doc::1\",\n    \"id\": \"1\",\n    \"passages\": [\n      \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n      \"Born in Maida Vale, London, Turing was raised in southern England. He graduated from King's College, Cambridge, with a degree in mathematics.\",\n      \"After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer.\",\n      \"Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations.\"\n    ],\n    \"colbert\": [\n      {\n        \"address\": {\n          \"context\": \"0\",\n          \"token\": \"0\"\n        },\n        \"values\": [\n          1,\n          120,\n          69,\n          0,\n          33,\n          -60,\n          -58,\n          -95,\n          -120,\n          32,\n          -127,\n          67,\n          -51,\n          68,\n          -106,\n          -12\n        ]\n      },\n      {\n        \"address\": {\n          \"context\": \"0\",\n          \"token\": \"1\"\n        },\n        \"values\": [\n          -122,\n          60,\n          9,\n          -128,\n          97,\n          -60,\n          -58,\n          -95,\n          -80,\n          112,\n          -127,\n          67,\n          -99,\n          68,\n          -106,\n          -28\n        ]\n      },\n      \"...\"\n    ],\n\n  }\n}\n</pre> <p>As can be seen from the matchfeatures, the first context (index 0) scored the highest and this is the score that is used to score the entire document.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html#standalone-colbert-vespa-for-long-context-ranking","title":"Standalone ColBERT + Vespa for long-context ranking\u00b6","text":"<p>This is a guide on how to use the ColBERT package to produce token-level vectors. This as an alternative for using the native Vespa colbert embedder.</p> <p>This guide illustrates how to feed multiple passages per Vespa document (long-context)</p> <ul> <li>Compress token vectors using binarization compatible with Vespa <code>unpack_bits</code></li> <li>Use Vespa hex feed format for binary vectors with mixed vespa tensors</li> <li>How to query Vespa with the ColBERT query tensor representation</li> </ul> <p>Read more about Vespa Long-Context ColBERT.</p> <p></p>"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/colbert_standalone_long_context_Vespa-cloud.html#querying-vespa-with-colbert-tensors","title":"Querying Vespa with ColBERT tensors\u00b6","text":""},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html","title":"colpali benchmark vqa vlm Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install transformers==4.51.3 accelerate pyvespa vespacli requests numpy scipy ir_measures pillow datasets\n</pre> !pip3 install transformers==4.51.3 accelerate pyvespa vespacli requests numpy scipy ir_measures pillow datasets In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom transformers import ColPaliForRetrieval, ColPaliProcessor\n</pre> import torch from torch.utils.data import DataLoader from tqdm import tqdm from PIL import Image from transformers import ColPaliForRetrieval, ColPaliProcessor <p>Choose the right device to run the model on.</p> In\u00a0[\u00a0]: Copied! <pre># Load model (bfloat16 support is limited; fallback to float32 if needed)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif torch.backends.mps.is_available():\n    device = \"mps\"  # For Apple Silicon devices\ndtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n</pre> # Load model (bfloat16 support is limited; fallback to float32 if needed) device = \"cuda\" if torch.cuda.is_available() else \"cpu\" if torch.backends.mps.is_available():     device = \"mps\"  # For Apple Silicon devices dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32 <p>Load the base model and the adapter.</p> In\u00a0[\u00a0]: Copied! <pre>model_name = \"vidore/colpali-v1.2-hf\"\nmodel = ColPaliForRetrieval.from_pretrained(\n    model_name,\n    torch_dtype=dtype,\n    device_map=device,  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\n).eval()\n\nprocessor = ColPaliProcessor.from_pretrained(model_name)\n</pre> model_name = \"vidore/colpali-v1.2-hf\" model = ColPaliForRetrieval.from_pretrained(     model_name,     torch_dtype=dtype,     device_map=device,  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon ).eval()  processor = ColPaliProcessor.from_pretrained(model_name) In\u00a0[5]: Copied! <pre>from datasets import load_dataset\n\nds = load_dataset(\"vidore/docvqa_test_subsampled\", split=\"test\")\n</pre> from datasets import load_dataset  ds = load_dataset(\"vidore/docvqa_test_subsampled\", split=\"test\") <p>Now we use the ColPali model to generate embeddings for the images in the dataset. We use a dataloader to process each image and store the embeddings in a list.</p> <p>Batch size 4 requires a GPU with 16GB of memory and fits into a T4 GPU. If you have a smaller GPU, you can reduce the batch size to 2.</p> In\u00a0[\u00a0]: Copied! <pre>dataloader = DataLoader(\n    ds[\"image\"],\n    batch_size=4,\n    shuffle=False,\n    collate_fn=lambda x: processor(images=x, return_tensors=\"pt\"),\n)\nembeddings = []\nfor batch_doc in tqdm(dataloader):\n    with torch.no_grad():\n        batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n        embeddings_doc = model(**batch_doc).embeddings\n        embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n</pre> dataloader = DataLoader(     ds[\"image\"],     batch_size=4,     shuffle=False,     collate_fn=lambda x: processor(images=x, return_tensors=\"pt\"), ) embeddings = [] for batch_doc in tqdm(dataloader):     with torch.no_grad():         batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}         embeddings_doc = model(**batch_doc).embeddings         embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\")))) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 125/125 [29:29&lt;00:00, 14.16s/it]\n</pre> <p>Generate embeddings for the queries in the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>dummy_image = Image.new(\"RGB\", (448, 448), (255, 255, 255))\ndataloader = DataLoader(\n    ds[\"query\"],\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"),\n)\nquery_embeddings = []\nfor batch_query in tqdm(dataloader):\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query).embeddings\n        query_embeddings.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> dummy_image = Image.new(\"RGB\", (448, 448), (255, 255, 255)) dataloader = DataLoader(     ds[\"query\"],     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"), ) query_embeddings = [] for batch_query in tqdm(dataloader):     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query).embeddings         query_embeddings.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [01:45&lt;00:00,  4.72it/s]\n</pre> <p>Now we have all the embeddings. We'll define two helper functions to perform binarization (BQ) and also packing float values to shorter hex representation in JSON. Both save bandwidth and improve feed performance.</p> In\u00a0[8]: Copied! <pre>import struct\nimport numpy as np\n\n\ndef binarize_tensor(tensor: torch.Tensor) -&gt; str:\n    \"\"\"\n    Binarize a floating-point 1-d tensor by thresholding at zero\n    and packing the bits into bytes. Returns the hex str representation of the bytes.\n    \"\"\"\n    if not tensor.is_floating_point():\n        raise ValueError(\"Input tensor must be of floating-point type.\")\n    return (\n        np.packbits(np.where(tensor &gt; 0, 1, 0), axis=0).astype(np.int8).tobytes().hex()\n    )\n</pre> import struct import numpy as np   def binarize_tensor(tensor: torch.Tensor) -&gt; str:     \"\"\"     Binarize a floating-point 1-d tensor by thresholding at zero     and packing the bits into bytes. Returns the hex str representation of the bytes.     \"\"\"     if not tensor.is_floating_point():         raise ValueError(\"Input tensor must be of floating-point type.\")     return (         np.packbits(np.where(tensor &gt; 0, 1, 0), axis=0).astype(np.int8).tobytes().hex()     ) In\u00a0[9]: Copied! <pre>def tensor_to_hex_bfloat16(tensor: torch.Tensor) -&gt; str:\n    if not tensor.is_floating_point():\n        raise ValueError(\"Input tensor must be of float32 type.\")\n\n    def float_to_bfloat16_hex(f: float) -&gt; str:\n        packed_float = struct.pack(\"=f\", f)\n        bfloat16_bits = struct.unpack(\"=H\", packed_float[2:])[0]\n        return format(bfloat16_bits, \"04X\")\n\n    hex_list = [float_to_bfloat16_hex(float(val)) for val in tensor.flatten()]\n    return \"\".join(hex_list)\n</pre> def tensor_to_hex_bfloat16(tensor: torch.Tensor) -&gt; str:     if not tensor.is_floating_point():         raise ValueError(\"Input tensor must be of float32 type.\")      def float_to_bfloat16_hex(f: float) -&gt; str:         packed_float = struct.pack(\"=f\", f)         bfloat16_bits = struct.unpack(\"=H\", packed_float[2:])[0]         return format(bfloat16_bits, \"04X\")      hex_list = [float_to_bfloat16_hex(float(val)) for val in tensor.flatten()]     return \"\".join(hex_list) In\u00a0[\u00a0]: Copied! <pre>from scipy.cluster.hierarchy import fcluster, linkage\nfrom typing import Dict, List\n\n\ndef pool_embeddings(embeddings: torch.Tensor, pool_factor=3) -&gt; torch.Tensor:\n    \"\"\"\n    pool embeddings using hierarchical clustering to reduce the number of patch embeddings.\n    Adapted from https://github.com/illuin-tech/vidore-benchmark/blob/e3b4f456d50271c69bce3d2c23131f5245d0c270/src/vidore_benchmark/compression/token_pooling.py#L32\n    Inspired by https://www.answer.ai/posts/colbert-pooling.html\n    \"\"\"\n\n    pooled_embeddings = []\n    token_length = embeddings.size(0)\n\n    if token_length == 1:\n        raise ValueError(\"The input tensor must have more than one token.\")\n    embeddings.to(device)\n\n    similarities = torch.mm(embeddings, embeddings.t())\n    if similarities.dtype == torch.bfloat16:\n        similarities = similarities.to(torch.float16)\n    similarities = 1 - similarities.cpu().numpy()\n\n    Z = linkage(similarities, metric=\"euclidean\", method=\"ward\")  # noqa: N806\n    max_clusters = max(token_length // pool_factor, 1)\n    cluster_labels = fcluster(Z, t=max_clusters, criterion=\"maxclust\")\n\n    cluster_id_to_indices: Dict[int, torch.Tensor] = {}\n\n    with torch.no_grad():\n        for cluster_id in range(1, max_clusters + 1):\n            cluster_indices = torch.where(torch.tensor(cluster_labels == cluster_id))[0]\n            cluster_id_to_indices[cluster_id] = cluster_indices\n\n            if cluster_indices.numel() &gt; 0:\n                pooled_embedding = embeddings[cluster_indices].mean(dim=0)\n                pooled_embedding = torch.nn.functional.normalize(\n                    pooled_embedding, p=2, dim=-1\n                )\n                pooled_embeddings.append(pooled_embedding)\n\n        pooled_embeddings = torch.stack(pooled_embeddings, dim=0)\n\n    return pooled_embeddings\n</pre> from scipy.cluster.hierarchy import fcluster, linkage from typing import Dict, List   def pool_embeddings(embeddings: torch.Tensor, pool_factor=3) -&gt; torch.Tensor:     \"\"\"     pool embeddings using hierarchical clustering to reduce the number of patch embeddings.     Adapted from https://github.com/illuin-tech/vidore-benchmark/blob/e3b4f456d50271c69bce3d2c23131f5245d0c270/src/vidore_benchmark/compression/token_pooling.py#L32     Inspired by https://www.answer.ai/posts/colbert-pooling.html     \"\"\"      pooled_embeddings = []     token_length = embeddings.size(0)      if token_length == 1:         raise ValueError(\"The input tensor must have more than one token.\")     embeddings.to(device)      similarities = torch.mm(embeddings, embeddings.t())     if similarities.dtype == torch.bfloat16:         similarities = similarities.to(torch.float16)     similarities = 1 - similarities.cpu().numpy()      Z = linkage(similarities, metric=\"euclidean\", method=\"ward\")  # noqa: N806     max_clusters = max(token_length // pool_factor, 1)     cluster_labels = fcluster(Z, t=max_clusters, criterion=\"maxclust\")      cluster_id_to_indices: Dict[int, torch.Tensor] = {}      with torch.no_grad():         for cluster_id in range(1, max_clusters + 1):             cluster_indices = torch.where(torch.tensor(cluster_labels == cluster_id))[0]             cluster_id_to_indices[cluster_id] = cluster_indices              if cluster_indices.numel() &gt; 0:                 pooled_embedding = embeddings[cluster_indices].mean(dim=0)                 pooled_embedding = torch.nn.functional.normalize(                     pooled_embedding, p=2, dim=-1                 )                 pooled_embeddings.append(pooled_embedding)          pooled_embeddings = torch.stack(pooled_embeddings, dim=0)      return pooled_embeddings <p>Create the Vespa feed format. We use hex formats for mixed tensors doc.</p> In\u00a0[12]: Copied! <pre>vespa_docs = []\n\nfor row, embedding in zip(ds, embeddings):\n    embedding_full = dict()\n    embedding_binary = dict()\n    # You can experiment with pooling if you want to reduce the number of embeddings\n    # pooled_embedding = pool_embeddings(embedding, pool_factor=2) # reduce the number of embeddings by a factor of 2\n    for j, emb in enumerate(embedding):\n        embedding_full[j] = tensor_to_hex_bfloat16(emb)\n        embedding_binary[j] = binarize_tensor(emb)\n    vespa_doc = {\n        \"id\": row[\"docId\"],\n        \"embedding\": embedding_full,\n        \"binary_embedding\": embedding_binary,\n    }\n    vespa_docs.append(vespa_doc)\n</pre> vespa_docs = []  for row, embedding in zip(ds, embeddings):     embedding_full = dict()     embedding_binary = dict()     # You can experiment with pooling if you want to reduce the number of embeddings     # pooled_embedding = pool_embeddings(embedding, pool_factor=2) # reduce the number of embeddings by a factor of 2     for j, emb in enumerate(embedding):         embedding_full[j] = tensor_to_hex_bfloat16(emb)         embedding_binary[j] = binarize_tensor(emb)     vespa_doc = {         \"id\": row[\"docId\"],         \"embedding\": embedding_full,         \"binary_embedding\": embedding_binary,     }     vespa_docs.append(vespa_doc) In\u00a0[14]: Copied! <pre>from vespa.package import Schema, Document, Field\n\ncolpali_schema = Schema(\n    name=\"pdf_page\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;bfloat16&gt;(patch{}, v[128])\",\n                indexing=[\"attribute\"],\n            ),\n            Field(\n                name=\"binary_embedding\",\n                type=\"tensor&lt;int8&gt;(patch{}, v[16])\",\n                indexing=[\"attribute\"],\n            ),\n        ]\n    ),\n)\n</pre> from vespa.package import Schema, Document, Field  colpali_schema = Schema(     name=\"pdf_page\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),             Field(                 name=\"embedding\",                 type=\"tensor(patch{}, v[128])\",                 indexing=[\"attribute\"],             ),             Field(                 name=\"binary_embedding\",                 type=\"tensor(patch{}, v[16])\",                 indexing=[\"attribute\"],             ),         ]     ), ) In\u00a0[15]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"visionragtest\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colpali_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"visionragtest\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colpali_schema] ) <p>Now we define how we want to rank the pages. We have 4 ranking models that we want to evaluate. These are all MaxSim variants but with various precision trade-offs.</p> <ol> <li>float-float A regular MaxSim implementation that uses the float representation of both query and page embeddings.</li> <li>float-binary Use the binarized representation of the page embeddings and where we unpack it into float representation. The query representation is still float.</li> <li>binary-binary Use the binarized representation of the doc embeddings and the query embeddings and replaces the dot product with inverted hamming distance.</li> <li>phased This uses the binary-binary in a first-phase, and then re-ranks using the float-binary representation. Only top 20 pages are re-ranked (This can be overriden in the query request as well).</li> </ol> In\u00a0[17]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolpali_profile = RankProfile(\n    name=\"float-float\",\n    # We define both the float and binary query inputs here; the rest of the profiles inherit these inputs\n    inputs=[\n        (\"query(qtb)\", \"tensor&lt;int8&gt;(querytoken{}, v[16])\"),\n        (\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"),\n    ],\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * cell_cast(attribute(embedding), float), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n)\n\ncolpali_binary_profile = RankProfile(\n    name=\"float-binary\",\n    inherits=\"float-float\",\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(binary_embedding)), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n)\n\ncolpali_hamming_profile = RankProfile(\n    name=\"binary-binary\",\n    inherits=\"float-float\",\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        1/(1+ sum(\n                            hamming(query(qtb), attribute(binary_embedding)),v\n                        )),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim\"),\n)\n\ncolpali__phased_hamming_profile = RankProfile(\n    name=\"phased\",\n    inherits=\"float-float\",\n    functions=[\n        Function(\n            name=\"max_sim_hamming\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        1/(1+ sum(\n                            hamming(query(qtb), attribute(binary_embedding)),v\n                        )),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(binary_embedding)), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim_hamming\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=20),\n)\n\n\ncolpali_schema.add_rank_profile(colpali_profile)\ncolpali_schema.add_rank_profile(colpali_binary_profile)\ncolpali_schema.add_rank_profile(colpali_hamming_profile)\ncolpali_schema.add_rank_profile(colpali__phased_hamming_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colpali_profile = RankProfile(     name=\"float-float\",     # We define both the float and binary query inputs here; the rest of the profiles inherit these inputs     inputs=[         (\"query(qtb)\", \"tensor(querytoken{}, v[16])\"),         (\"query(qt)\", \"tensor(querytoken{}, v[128])\"),     ],     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * cell_cast(attribute(embedding), float), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         )     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"), )  colpali_binary_profile = RankProfile(     name=\"float-binary\",     inherits=\"float-float\",     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(binary_embedding)), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         )     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"), )  colpali_hamming_profile = RankProfile(     name=\"binary-binary\",     inherits=\"float-float\",     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         1/(1+ sum(                             hamming(query(qtb), attribute(binary_embedding)),v                         )),                         max, patch                     ),                     querytoken                 )             \"\"\",         )     ],     first_phase=FirstPhaseRanking(expression=\"max_sim\"), )  colpali__phased_hamming_profile = RankProfile(     name=\"phased\",     inherits=\"float-float\",     functions=[         Function(             name=\"max_sim_hamming\",             expression=\"\"\"                 sum(                     reduce(                         1/(1+ sum(                             hamming(query(qtb), attribute(binary_embedding)),v                         )),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(binary_embedding)), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),     ],     first_phase=FirstPhaseRanking(expression=\"max_sim_hamming\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=20), )   colpali_schema.add_rank_profile(colpali_profile) colpali_schema.add_rank_profile(colpali_binary_profile) colpali_schema.add_rank_profile(colpali_hamming_profile) colpali_schema.add_rank_profile(colpali__phased_hamming_profile) <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial. Make note of the tenant name, it is used in the next steps.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>This example uses the asynchronous feed method and feeds one document at a time.</p> In\u00a0[23]: Copied! <pre>from vespa.io import VespaResponse\n\nasync with app.asyncio(connections=1, timeout=180) as session:\n    for doc in tqdm(vespa_docs):\n        response: VespaResponse = await session.feed_data_point(\n            data_id=doc[\"id\"], fields=doc, schema=\"pdf_page\"\n        )\n        if not response.is_successful():\n            print(response.json())\n</pre> from vespa.io import VespaResponse  async with app.asyncio(connections=1, timeout=180) as session:     for doc in tqdm(vespa_docs):         response: VespaResponse = await session.feed_data_point(             data_id=doc[\"id\"], fields=doc, schema=\"pdf_page\"         )         if not response.is_successful():             print(response.json()) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 500/500 [01:13&lt;00:00,  6.77it/s]\n</pre> <p>We use ir_measures to evaluate the effectiveness of the retrieval model.</p> In\u00a0[24]: Copied! <pre>from ir_measures import calc_aggregate, nDCG, ScoredDoc, Qrel\n</pre> from ir_measures import calc_aggregate, nDCG, ScoredDoc, Qrel <p>A simple routine for querying Vespa. Note that we send both vector representations in the query independently of the ranking method used, this for simplicity. Not all the ranking models we evaluate need both representations.</p> In\u00a0[32]: Copied! <pre>from vespa.io import VespaQueryResponse\nfrom vespa.application import VespaAsync\n\n\nasync def get_vespa_response(\n    embedding: torch.Tensor,\n    qid: str,\n    session: VespaAsync,\n    depth=20,\n    profile=\"float-float\",\n) -&gt; List[ScoredDoc]:\n    # The query tensor api does not support hex formats yet\n    float_embedding = {index: vector.tolist() for index, vector in enumerate(embedding)}\n    binary_embedding = {\n        index: np.packbits(np.where(vector &gt; 0, 1, 0), axis=0).astype(np.int8).tolist()\n        for index, vector in enumerate(embedding)\n    }\n    response: VespaQueryResponse = await session.query(\n        yql=\"select id from pdf_page where true\",  # brute force search, rank all pages\n        ranking=profile,\n        hits=5,\n        timeout=10,\n        body={\n            \"input.query(qt)\": float_embedding,\n            \"input.query(qtb)\": binary_embedding,\n            \"ranking.rerankCount\": depth,\n        },\n    )\n    assert response.is_successful()\n    scored_docs = []\n    for hit in response.hits:\n        doc_id = hit[\"fields\"][\"id\"]\n        score = hit[\"relevance\"]\n        scored_docs.append(ScoredDoc(qid, doc_id, score))\n    return scored_docs\n</pre> from vespa.io import VespaQueryResponse from vespa.application import VespaAsync   async def get_vespa_response(     embedding: torch.Tensor,     qid: str,     session: VespaAsync,     depth=20,     profile=\"float-float\", ) -&gt; List[ScoredDoc]:     # The query tensor api does not support hex formats yet     float_embedding = {index: vector.tolist() for index, vector in enumerate(embedding)}     binary_embedding = {         index: np.packbits(np.where(vector &gt; 0, 1, 0), axis=0).astype(np.int8).tolist()         for index, vector in enumerate(embedding)     }     response: VespaQueryResponse = await session.query(         yql=\"select id from pdf_page where true\",  # brute force search, rank all pages         ranking=profile,         hits=5,         timeout=10,         body={             \"input.query(qt)\": float_embedding,             \"input.query(qtb)\": binary_embedding,             \"ranking.rerankCount\": depth,         },     )     assert response.is_successful()     scored_docs = []     for hit in response.hits:         doc_id = hit[\"fields\"][\"id\"]         score = hit[\"relevance\"]         scored_docs.append(ScoredDoc(qid, doc_id, score))     return scored_docs <p>Run a test query first..</p> In\u00a0[28]: Copied! <pre>async with app.asyncio() as session:\n    for profile in [\"float-float\", \"float-binary\", \"binary-binary\", \"phased\"]:\n        print(\n            await get_vespa_response(\n                query_embeddings[0], profile, session, profile=profile\n            )\n        )\n</pre> async with app.asyncio() as session:     for profile in [\"float-float\", \"float-binary\", \"binary-binary\", \"phased\"]:         print(             await get_vespa_response(                 query_embeddings[0], profile, session, profile=profile             )         ) <pre>[ScoredDoc(query_id='float-float', doc_id='4720', score=16.292504370212555), ScoredDoc(query_id='float-float', doc_id='4858', score=13.315170526504517), ScoredDoc(query_id='float-float', doc_id='14686', score=12.212152108550072), ScoredDoc(query_id='float-float', doc_id='4846', score=12.002869427204132), ScoredDoc(query_id='float-float', doc_id='864', score=11.308563649654388)]\n[ScoredDoc(query_id='float-binary', doc_id='4720', score=82.99432492256165), ScoredDoc(query_id='float-binary', doc_id='4858', score=71.45464742183685), ScoredDoc(query_id='float-binary', doc_id='14686', score=68.46699643135071), ScoredDoc(query_id='float-binary', doc_id='4846', score=64.85357594490051), ScoredDoc(query_id='float-binary', doc_id='2161', score=63.85516130924225)]\n[ScoredDoc(query_id='binary-binary', doc_id='4720', score=0.771387243643403), ScoredDoc(query_id='binary-binary', doc_id='4858', score=0.7132036704570055), ScoredDoc(query_id='binary-binary', doc_id='14686', score=0.6979007869958878), ScoredDoc(query_id='binary-binary', doc_id='6087', score=0.6534321829676628), ScoredDoc(query_id='binary-binary', doc_id='2161', score=0.6525899451225996)]\n[ScoredDoc(query_id='phased', doc_id='4720', score=82.99432492256165), ScoredDoc(query_id='phased', doc_id='4858', score=71.45464742183685), ScoredDoc(query_id='phased', doc_id='14686', score=68.46699643135071), ScoredDoc(query_id='phased', doc_id='4846', score=64.85357594490051), ScoredDoc(query_id='phased', doc_id='2161', score=63.85516130924225)]\n</pre> <p>Now, run through all of the test queries for each of the ranking models.</p> In\u00a0[29]: Copied! <pre>qrels = []\nprofiles = [\"float-float\", \"float-binary\", \"binary-binary\", \"phased\"]\nresults = {profile: [] for profile in profiles}\nasync with app.asyncio(connections=3) as session:\n    for row, embedding in zip(tqdm(ds), query_embeddings):\n        qrels.append(Qrel(row[\"questionId\"], str(row[\"docId\"]), 1))\n        for profile in profiles:\n            scored_docs = await get_vespa_response(\n                embedding, row[\"questionId\"], session, profile=profile\n            )\n            results[profile].extend(scored_docs)\n</pre> qrels = [] profiles = [\"float-float\", \"float-binary\", \"binary-binary\", \"phased\"] results = {profile: [] for profile in profiles} async with app.asyncio(connections=3) as session:     for row, embedding in zip(tqdm(ds), query_embeddings):         qrels.append(Qrel(row[\"questionId\"], str(row[\"docId\"]), 1))         for profile in profiles:             scored_docs = await get_vespa_response(                 embedding, row[\"questionId\"], session, profile=profile             )             results[profile].extend(scored_docs) <pre>500it [11:32,  1.39s/it]\n</pre> <p>Calculate the effectiveness of the 4 different models</p> In\u00a0[30]: Copied! <pre>for profile in profiles:\n    score = calc_aggregate([nDCG @ 5], qrels, results[profile])[nDCG @ 5]\n    print(f\"nDCG@5 for {profile}: {100*score:.2f}\")\n</pre> for profile in profiles:     score = calc_aggregate([nDCG @ 5], qrels, results[profile])[nDCG @ 5]     print(f\"nDCG@5 for {profile}: {100*score:.2f}\") <pre>nDCG@5 for float-float: 52.37\nnDCG@5 for float-binary: 51.64\nnDCG@5 for binary-binary: 49.48\nnDCG@5 for phased: 51.70\n</pre> <p>This is encouraging as the binary-binary representation is 4x faster than the float-float representation and saves 32x space. We can also largely retain the effectiveness of the float-binary representation by using the phased approach, where we re-rank the top 20 pages from the hamming (binary-binary) version using the float-binary representation. Now we can explore the ranking depth and see how the phased approach performs with different ranking depths.</p> In\u00a0[35]: Copied! <pre>results = {\n    profile: []\n    for profile in [\n        \"phased-rerank-count=5\",\n        \"phased-rerank-count=10\",\n        \"phased-rerank-count=20\",\n        \"phased-rerank-count=40\",\n    ]\n}\nasync with app.asyncio(connections=3) as session:\n    for row, embedding in zip(tqdm(ds), query_embeddings):\n        qrels.append(Qrel(row[\"questionId\"], str(row[\"docId\"]), 1))\n        for count in [5, 10, 20, 40]:\n            scored_docs = await get_vespa_response(\n                embedding, row[\"questionId\"], session, profile=\"phased\", depth=count\n            )\n            results[\"phased-rerank-count=\" + str(count)].extend(scored_docs)\n</pre> results = {     profile: []     for profile in [         \"phased-rerank-count=5\",         \"phased-rerank-count=10\",         \"phased-rerank-count=20\",         \"phased-rerank-count=40\",     ] } async with app.asyncio(connections=3) as session:     for row, embedding in zip(tqdm(ds), query_embeddings):         qrels.append(Qrel(row[\"questionId\"], str(row[\"docId\"]), 1))         for count in [5, 10, 20, 40]:             scored_docs = await get_vespa_response(                 embedding, row[\"questionId\"], session, profile=\"phased\", depth=count             )             results[\"phased-rerank-count=\" + str(count)].extend(scored_docs) <pre>500it [08:18,  1.00it/s]\n</pre> In\u00a0[36]: Copied! <pre>for profile in results.keys():\n    score = calc_aggregate([nDCG @ 5], qrels, results[profile])[nDCG @ 5]\n    print(f\"nDCG@5 for {profile}: {100*score:.2f}\")\n</pre> for profile in results.keys():     score = calc_aggregate([nDCG @ 5], qrels, results[profile])[nDCG @ 5]     print(f\"nDCG@5 for {profile}: {100*score:.2f}\") <pre>nDCG@5 for phased-rerank-count=5: 50.77\nnDCG@5 for phased-rerank-count=10: 51.58\nnDCG@5 for phased-rerank-count=20: 51.70\nnDCG@5 for phased-rerank-count=40: 51.64\n</pre>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#colpali-ranking-experiments-on-docvqa","title":"ColPali Ranking Experiments on DocVQA\u00b6","text":"<p>This notebook demonstrates how to reproduce the ColPali results on DocVQA with Vespa. The dataset consists of PDF documents with questions and answers.</p> <p>We demonstrate how we can binarize the patch embeddings and replace the float MaxSim scoring with a <code>hamming</code> based MaxSim without much loss in ranking accuracy but with a significant speedup (close to 4x) and reducing the memory (and storage) requirements by 32x.</p> <p>In this notebook, we represent one PDF page as one vespa document. See other notebooks for more information about using ColPali with Vespa:</p> <ul> <li>Scaling ColPALI (VLM) Retrieval</li> <li>Vespa \ud83e\udd1d ColPali: Efficient Document Retrieval with Vision Language Models</li> </ul> <p></p> <p>Install dependencies:</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#load-the-model","title":"Load the model\u00b6","text":"<p>Load the model, also choose the correct device and model weights.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#the-vidore-benchmark","title":"The ViDoRe Benchmark\u00b6","text":"<p>We load the DocVQA test set, a subset of the ViDoRe dataset It has 500 pages and a question per page. The task is retrieve the page across the 500 indexed pages.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#patch-vector-pooling","title":"Patch Vector pooling\u00b6","text":"<p>This reduces the number of patch embeddings by a factor of 3, meaning that we go from 1030 patch vectors to 343 patch vectors. This reduces both the memory and the number of dotproducts we need to calculate. This function is not in use in this notebook, but it is included for reference.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#configure-vespa","title":"Configure Vespa\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type. This is a simple schema, which is all we need to evaluate the effectiveness of the model.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p><code>PyVespa</code> supports deploying apps to the development zone.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#run-queries-and-evaluate-effectiveness","title":"Run queries and evaluate effectiveness\u00b6","text":""},{"location":"examples/colpali-benchmark-vqa-vlm_Vespa-cloud.html#conclusion","title":"Conclusion\u00b6","text":"<p>The binary representation of the patch embeddings reduces the storage by 32x, and using hamming distance instead of dotproduct saves us about 4x in computation compared to the float-float model or the float-binary model (which only saves storage). Using a re-ranking step with only depth 10, we can improve the effectiveness of the binary-binary model to almost match the float-float MaxSim model. The additional re-ranking step only requires that we pass also the float query embedding version without any additional storage overhead.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html","title":"Colpali document retrieval vision language models cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y\n</pre> !sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y <p>Install python packages</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install transformers==4.51.3 accelerate vidore_benchmark==4.0.0 pdf2image google-generativeai pypdf==5.0.1 pyvespa vespacli requests\n</pre> !pip3 install transformers==4.51.3 accelerate vidore_benchmark==4.0.0 pdf2image google-generativeai pypdf==5.0.1 pyvespa vespacli requests In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom io import BytesIO\n\nfrom transformers import ColPaliForRetrieval, ColPaliProcessor\nfrom vidore_benchmark.utils.image_utils import scale_image, get_base64_image\n</pre> import torch from torch.utils.data import DataLoader from tqdm import tqdm from PIL import Image from io import BytesIO  from transformers import ColPaliForRetrieval, ColPaliProcessor from vidore_benchmark.utils.image_utils import scale_image, get_base64_image <p>Choose the right device to run the model.</p> In\u00a0[\u00a0]: Copied! <pre># Load model (bfloat16 support is limited; fallback to float32 if needed)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif torch.backends.mps.is_available():\n    device = \"mps\"  # For Apple Silicon devices\ndtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n</pre> # Load model (bfloat16 support is limited; fallback to float32 if needed) device = \"cuda\" if torch.cuda.is_available() else \"cpu\" if torch.backends.mps.is_available():     device = \"mps\"  # For Apple Silicon devices dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32 In\u00a0[\u00a0]: Copied! <pre>model_name = \"vidore/colpali-v1.2-hf\"\nmodel = ColPaliForRetrieval.from_pretrained(\n    model_name,\n    torch_dtype=dtype,\n    device_map=device,  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\n).eval()\n\nprocessor = ColPaliProcessor.from_pretrained(model_name)\n</pre> model_name = \"vidore/colpali-v1.2-hf\" model = ColPaliForRetrieval.from_pretrained(     model_name,     torch_dtype=dtype,     device_map=device,  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon ).eval()  processor = ColPaliProcessor.from_pretrained(model_name) In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader\n\n\ndef download_pdf(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return BytesIO(response.content)\n    else:\n        raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")\n\n\ndef get_pdf_images(pdf_url):\n    # Download the PDF\n    pdf_file = download_pdf(pdf_url)\n    # Save the PDF temporarily to disk (pdf2image requires a file path)\n    with open(\"temp.pdf\", \"wb\") as f:\n        f.write(pdf_file.read())\n    reader = PdfReader(\"temp.pdf\")\n    page_texts = []\n    for page_number in range(len(reader.pages)):\n        page = reader.pages[page_number]\n        text = page.extract_text()\n        page_texts.append(text)\n    images = convert_from_path(\"temp.pdf\")\n    assert len(images) == len(page_texts)\n    return (images, page_texts)\n</pre> import requests from pdf2image import convert_from_path from pypdf import PdfReader   def download_pdf(url):     response = requests.get(url)     if response.status_code == 200:         return BytesIO(response.content)     else:         raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")   def get_pdf_images(pdf_url):     # Download the PDF     pdf_file = download_pdf(pdf_url)     # Save the PDF temporarily to disk (pdf2image requires a file path)     with open(\"temp.pdf\", \"wb\") as f:         f.write(pdf_file.read())     reader = PdfReader(\"temp.pdf\")     page_texts = []     for page_number in range(len(reader.pages)):         page = reader.pages[page_number]         text = page.extract_text()         page_texts.append(text)     images = convert_from_path(\"temp.pdf\")     assert len(images) == len(page_texts)     return (images, page_texts) <p>We define a few sample PDFs to work with.</p> In\u00a0[\u00a0]: Copied! <pre>sample_pdfs = [\n    {\n        \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n        \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",\n        \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",\n    },\n    {\n        \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n        \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",\n        \"authors\": \"Omar Khattab, Matei Zaharia\",\n    },\n]\n</pre> sample_pdfs = [     {         \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",         \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",         \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",     },     {         \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",         \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",         \"authors\": \"Omar Khattab, Matei Zaharia\",     }, ] <p>Now we can convert the PDFs to images and also extract the text content.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_images, page_texts = get_pdf_images(pdf[\"url\"])\n    pdf[\"images\"] = page_images\n    pdf[\"texts\"] = page_texts\n</pre> for pdf in sample_pdfs:     page_images, page_texts = get_pdf_images(pdf[\"url\"])     pdf[\"images\"] = page_images     pdf[\"texts\"] = page_texts <p>Let us look at the extracted image of the first PDF page. This is the input to ColPali.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display\n\ndisplay(scale_image(sample_pdfs[0][\"images\"][0], 720))\n</pre> from IPython.display import display  display(scale_image(sample_pdfs[0][\"images\"][0], 720)) <p>Now we use the ColPali model to generate embeddings for the images.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_embeddings = []\n    dataloader = DataLoader(\n        pdf[\"images\"],\n        batch_size=2,\n        shuffle=False,\n        collate_fn=lambda x: processor(images=x, return_tensors=\"pt\"),\n    )\n    for batch_doc in tqdm(dataloader):\n        with torch.no_grad():\n            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n            embeddings_doc = model(**batch_doc).embeddings\n            page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n    pdf[\"embeddings\"] = page_embeddings\n</pre> for pdf in sample_pdfs:     page_embeddings = []     dataloader = DataLoader(         pdf[\"images\"],         batch_size=2,         shuffle=False,         collate_fn=lambda x: processor(images=x, return_tensors=\"pt\"),     )     for batch_doc in tqdm(dataloader):         with torch.no_grad():             batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}             embeddings_doc = model(**batch_doc).embeddings             page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))     pdf[\"embeddings\"] = page_embeddings <p>Now we are done with the document side embeddings, we now convert the custom dict to Vespa JSON feed format.</p> <p>We use binarization of the vector embeddings to reduce their size. Read more about binarization of multi-vector representations in the colbert blog post. This maps 128 dimensional floats to 128 bits, or 16 bytes per vector. Reducing the size by 32x.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\nfrom typing import Dict, List\nfrom binascii import hexlify\n\n\ndef binarize_token_vectors_hex(vectors: List[torch.Tensor]) -&gt; Dict[str, str]:\n    vespa_tensor = list()\n    for page_id in range(0, len(vectors)):\n        page_vector = vectors[page_id]\n        binarized_token_vectors = np.packbits(\n            np.where(page_vector &gt; 0, 1, 0), axis=1\n        ).astype(np.int8)\n        for patch_index in range(0, len(page_vector)):\n            values = str(\n                hexlify(binarized_token_vectors[patch_index].tobytes()), \"utf-8\"\n            )\n            if (\n                values == \"00000000000000000000000000000000\"\n            ):  # skip empty vectors due to padding of batch\n                continue\n            vespa_tensor_cell = {\n                \"address\": {\"page\": page_id, \"patch\": patch_index},\n                \"values\": values,\n            }\n            vespa_tensor.append(vespa_tensor_cell)\n\n    return vespa_tensor\n</pre> import numpy as np from typing import Dict, List from binascii import hexlify   def binarize_token_vectors_hex(vectors: List[torch.Tensor]) -&gt; Dict[str, str]:     vespa_tensor = list()     for page_id in range(0, len(vectors)):         page_vector = vectors[page_id]         binarized_token_vectors = np.packbits(             np.where(page_vector &gt; 0, 1, 0), axis=1         ).astype(np.int8)         for patch_index in range(0, len(page_vector)):             values = str(                 hexlify(binarized_token_vectors[patch_index].tobytes()), \"utf-8\"             )             if (                 values == \"00000000000000000000000000000000\"             ):  # skip empty vectors due to padding of batch                 continue             vespa_tensor_cell = {                 \"address\": {\"page\": page_id, \"patch\": patch_index},                 \"values\": values,             }             vespa_tensor.append(vespa_tensor_cell)      return vespa_tensor <p>Iterate over the sample and create the Vespa JSON feed format, including the base64 encoded page images.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_feed = []\nfor idx, pdf in enumerate(sample_pdfs):\n    images_base_64 = []\n    for image in pdf[\"images\"]:\n        images_base_64.append(get_base64_image(image, add_url_prefix=False))\n    pdf[\"images_base_64\"] = images_base_64\n    doc = {\n        \"fields\": {\n            \"url\": pdf[\"url\"],\n            \"title\": pdf[\"title\"],\n            \"images\": pdf[\"images_base_64\"],\n            \"texts\": pdf[\"texts\"],  # Array of text per page\n            \"colbert\": {  # Colbert embeddings per page\n                \"blocks\": binarize_token_vectors_hex(pdf[\"embeddings\"])\n            },\n        }\n    }\n    vespa_feed.append(doc)\n</pre> vespa_feed = [] for idx, pdf in enumerate(sample_pdfs):     images_base_64 = []     for image in pdf[\"images\"]:         images_base_64.append(get_base64_image(image, add_url_prefix=False))     pdf[\"images_base_64\"] = images_base_64     doc = {         \"fields\": {             \"url\": pdf[\"url\"],             \"title\": pdf[\"title\"],             \"images\": pdf[\"images_base_64\"],             \"texts\": pdf[\"texts\"],  # Array of text per page             \"colbert\": {  # Colbert embeddings per page                 \"blocks\": binarize_token_vectors_hex(pdf[\"embeddings\"])             },         }     }     vespa_feed.append(doc) In\u00a0[\u00a0]: Copied! <pre>vespa_feed[0][\"fields\"][\"colbert\"][\"blocks\"][0:5]\n</pre> vespa_feed[0][\"fields\"][\"colbert\"][\"blocks\"][0:5] <p>Above is the feed format for mixed tensors with more than one mapped dimension, see details. We have the <code>page</code> and <code>patch</code> dimensions and for each combination with have a binary representation of the 128 dimensional embeddings, packed into 16 bytes.</p> <p>For each page image, we have 1030 patches, each with a 128 dimensional embedding.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\ncolbert_schema = Schema(\n    name=\"doc\",\n    document=Document(\n        fields=[\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"texts\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"images\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\"],\n            ),\n            Field(\n                name=\"colbert\",\n                type=\"tensor&lt;int8&gt;(page{}, patch{}, v[16])\",\n                indexing=[\"attribute\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"texts\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  colbert_schema = Schema(     name=\"doc\",     document=Document(         fields=[             Field(name=\"url\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"texts\",                 type=\"array\",                 indexing=[\"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"images\",                 type=\"array\",                 indexing=[\"summary\"],             ),             Field(                 name=\"colbert\",                 type=\"tensor(page{}, patch{}, v[16])\",                 indexing=[\"attribute\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"texts\"])], ) <p>Notice the <code>colbert</code> field is a tensor field with the type <code>tensor(page{}, patch{}, v[128])</code>. This is the field that will store the embeddings generated by ColPali. This is an example of a mixed tensor where we combine two mapped (sparse) dimensions with one dense.</p> <p>Read more in Tensor guide. We also enable BM25 for the <code>title</code> and <code>texts</code>\u00a0fields.</p> <p>Create the Vespa application package:</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"visionrag\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colbert_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"visionrag\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colbert_schema] ) <p>Now we define how we want to rank the pages. We use BM25 for the text and late interaction with Max Sim for the image embeddings. This means that we retrieve using the text representations to find relevant PDF documents, then we use the ColPALI embeddings to rerank the pages within the document using the max of the page scores.</p> <p>We also return all the page level scores using <code>match-features</code>,  so that we can render multiple scoring pages in the search result.</p> <p>As LLMs gets longer context windows, we can input more than a single page per PDF.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolbert_profile = RankProfile(\n    name=\"default\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    functions=[\n        Function(\n            name=\"max_sim_per_page\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(colbert)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(name=\"max_sim\", expression=\"reduce(max_sim_per_page, max, page)\"),\n        Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(texts)\"),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"bm25_score\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),\n    match_features=[\"max_sim_per_page\", \"bm25_score\"],\n)\ncolbert_schema.add_rank_profile(colbert_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colbert_profile = RankProfile(     name=\"default\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     functions=[         Function(             name=\"max_sim_per_page\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(colbert)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(name=\"max_sim\", expression=\"reduce(max_sim_per_page, max, page)\"),         Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(texts)\"),     ],     first_phase=FirstPhaseRanking(expression=\"bm25_score\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),     match_features=[\"max_sim_per_page\", \"bm25_score\"], ) colbert_schema.add_rank_profile(colbert_profile) <p>Validate that certificates are ok and deploy the application to Vespa Cloud.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>This example uses the synchronous feed method and feeds one document at a time. For larger datasets, consider using the asynchronous feed method.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\nwith app.syncio() as sync:\n    for operation in vespa_feed:\n        fields = operation[\"fields\"]\n        response: VespaResponse = sync.feed_data_point(\n            data_id=fields[\"url\"], fields=fields, schema=\"doc\"\n        )\n        if not response.is_successful():\n            print(response.json())\n</pre> from vespa.io import VespaResponse  with app.syncio() as sync:     for operation in vespa_feed:         fields = operation[\"fields\"]         response: VespaResponse = sync.feed_data_point(             data_id=fields[\"url\"], fields=fields, schema=\"doc\"         )         if not response.is_successful():             print(response.json()) <p>Our demo query:</p> <p>Composition of the Lotte Benchmark</p> In\u00a0[\u00a0]: Copied! <pre>queries = [\"Composition of the LoTTE benchmark\"]\n</pre> queries = [\"Composition of the LoTTE benchmark\"] <p>Obtain the query embeddings using the ColPali model</p> In\u00a0[\u00a0]: Copied! <pre>dataloader = DataLoader(\n    queries,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"),\n)\nqs = []\nfor batch_query in dataloader:\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query).embeddings\n        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> dataloader = DataLoader(     queries,     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"), ) qs = [] for batch_query in dataloader:     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query).embeddings         qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <p>A simple routine to format the ColPali multi-vector emebeddings to a format that can be used in Vespa. See querying with tensors for more details.</p> In\u00a0[\u00a0]: Copied! <pre>def float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:\n    vespa_token_dict = dict()\n    for index in range(0, len(vectors)):\n        vespa_token_dict[index] = vectors[index].tolist()\n    return vespa_token_dict\n\n\ndataloader = DataLoader(\n    queries,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"),\n)\nqs = []\nfor batch_query in dataloader:\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query).embeddings\n        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> def float_query_token_vectors(vectors: torch.Tensor) -&gt; Dict[str, List[float]]:     vespa_token_dict = dict()     for index in range(0, len(vectors)):         vespa_token_dict[index] = vectors[index].tolist()     return vespa_token_dict   dataloader = DataLoader(     queries,     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"), ) qs = [] for batch_query in dataloader:     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query).embeddings         qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <p>We create a simple routine to display the results.</p> <p>Notice that each hit is a PDF document. Within a PDF document we have multiple pages and we have the MaxSim score for each page.</p> <p>The PDF documents are ranked by the maximum page score. But, we have access to all the page level scores and below we display the top 2-pages for each PDF document. We convert the base64 encoded image to a PIL image for rendering. We could also render the extracted text, but we skip that for now.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, HTML\nimport base64\n\n\ndef display_query_results(query, response):\n    \"\"\"\n    Displays the query result, including the two best matching pages per matched pdf.\n    \"\"\"\n    html_content = f\"&lt;h3&gt;Query text: {query}&lt;/h3&gt;\"\n\n    for i, hit in enumerate(response.hits[:2]):  # Adjust to show more hits if needed\n        title = hit[\"fields\"][\"title\"]\n        url = hit[\"fields\"][\"url\"]\n        match_scores = hit[\"fields\"][\"matchfeatures\"][\"max_sim_per_page\"]\n        images = hit[\"fields\"][\"images\"]\n\n        html_content += f\"&lt;h3&gt;PDF Result {i + 1}&lt;/h3&gt;\"\n        html_content += f'&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; &lt;a href=\"{url}\"&gt;{title}&lt;/a&gt;&lt;/p&gt;'\n\n        # Find the two best matching pages\n        sorted_pages = sorted(match_scores.items(), key=lambda x: x[1], reverse=True)\n        best_pages = sorted_pages[:2]\n\n        for page, score in best_pages:\n            page = int(page)\n            image_data = base64.b64decode(images[page])\n            image = Image.open(BytesIO(image_data))\n            scaled_image = scale_image(image, 648)\n\n            buffered = BytesIO()\n            scaled_image.save(buffered, format=\"PNG\")\n            img_str = base64.b64encode(buffered.getvalue()).decode()\n\n            html_content += f\"&lt;p&gt;&lt;strong&gt;Best Matching Page {page+1} for PDF document:&lt;/strong&gt; with MaxSim score {score:.2f}&lt;/p&gt;\"\n            html_content += (\n                f'&lt;img src=\"data:image/png;base64,{img_str}\" style=\"max-width:100%;\"&gt;'\n            )\n\n    display(HTML(html_content))\n</pre> from IPython.display import display, HTML import base64   def display_query_results(query, response):     \"\"\"     Displays the query result, including the two best matching pages per matched pdf.     \"\"\"     html_content = f\"Query text: {query}\"      for i, hit in enumerate(response.hits[:2]):  # Adjust to show more hits if needed         title = hit[\"fields\"][\"title\"]         url = hit[\"fields\"][\"url\"]         match_scores = hit[\"fields\"][\"matchfeatures\"][\"max_sim_per_page\"]         images = hit[\"fields\"][\"images\"]          html_content += f\"PDF Result {i + 1}\"         html_content += f'<p>Title: {title}</p>'          # Find the two best matching pages         sorted_pages = sorted(match_scores.items(), key=lambda x: x[1], reverse=True)         best_pages = sorted_pages[:2]          for page, score in best_pages:             page = int(page)             image_data = base64.b64decode(images[page])             image = Image.open(BytesIO(image_data))             scaled_image = scale_image(image, 648)              buffered = BytesIO()             scaled_image.save(buffered, format=\"PNG\")             img_str = base64.b64encode(buffered.getvalue()).decode()              html_content += f\"<p>Best Matching Page {page+1} for PDF document: with MaxSim score {score:.2f}</p>\"             html_content += (                 f''             )      display(HTML(html_content)) <p>Query Vespa with a text query and display the results.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nfor idx, query in enumerate(queries):\n    response: VespaQueryResponse = app.query(\n        yql=\"select title,url,images from doc where userInput(@userQuery)\",\n        ranking=\"default\",\n        userQuery=query,\n        timeout=2,\n        hits=3,\n        body={\n            \"presentation.format.tensors\": \"short-value\",\n            \"input.query(qt)\": float_query_token_vectors(qs[idx]),\n        },\n    )\n    assert response.is_successful()\n    display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  for idx, query in enumerate(queries):     response: VespaQueryResponse = app.query(         yql=\"select title,url,images from doc where userInput(@userQuery)\",         ranking=\"default\",         userQuery=query,         timeout=2,         hits=3,         body={             \"presentation.format.tensors\": \"short-value\",             \"input.query(qt)\": float_query_token_vectors(qs[idx]),         },     )     assert response.is_successful()     display_query_results(query, response) In\u00a0[\u00a0]: Copied! <pre>import google.generativeai as genai\n\ngenai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n</pre> import google.generativeai as genai  genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"]) <p>Just extract the best page image from the first hit to demonstrate how to use the image with Gemini Flash to answer the question.</p> In\u00a0[\u00a0]: Copied! <pre>best_hit = response.hits[0]\npdf_url = best_hit[\"fields\"][\"url\"]\npdf_title = best_hit[\"fields\"][\"title\"]\nmatch_scores = best_hit[\"fields\"][\"matchfeatures\"][\"max_sim_per_page\"]\nimages = best_hit[\"fields\"][\"images\"]\nsorted_pages = sorted(match_scores.items(), key=lambda x: x[1], reverse=True)\nbest_page, score = sorted_pages[0]\nbest_page = int(best_page)\nimage_data = base64.b64decode(images[best_page])\nimage = Image.open(BytesIO(image_data))\nscaled_image = scale_image(image, 720)\ndisplay(scaled_image)\n</pre> best_hit = response.hits[0] pdf_url = best_hit[\"fields\"][\"url\"] pdf_title = best_hit[\"fields\"][\"title\"] match_scores = best_hit[\"fields\"][\"matchfeatures\"][\"max_sim_per_page\"] images = best_hit[\"fields\"][\"images\"] sorted_pages = sorted(match_scores.items(), key=lambda x: x[1], reverse=True) best_page, score = sorted_pages[0] best_page = int(best_page) image_data = base64.b64decode(images[best_page]) image = Image.open(BytesIO(image_data)) scaled_image = scale_image(image, 720) display(scaled_image) <p>Initialize the Gemini Flash model and answer the question.</p> In\u00a0[\u00a0]: Copied! <pre>model = genai.GenerativeModel(model_name=\"gemini-flash-lite-latest\")\nresponse = model.generate_content([queries[0], image])\n</pre> model = genai.GenerativeModel(model_name=\"gemini-flash-lite-latest\") response = model.generate_content([queries[0], image]) <p>Some formatting of the response from Gemini Flash.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Markdown, display\n\nmarkdown_text = response.candidates[0].content.parts[0].text\ndisplay(Markdown(markdown_text))\n</pre> from IPython.display import Markdown, display  markdown_text = response.candidates[0].content.parts[0].text display(Markdown(markdown_text)) In\u00a0[\u00a0]: Copied! <pre>if os.getenv(\"CI\", \"false\") == \"true\":\n    vespa_cloud.delete()\n</pre> if os.getenv(\"CI\", \"false\") == \"true\":     vespa_cloud.delete()"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#vespa-colpali-efficient-document-retrieval-with-vision-language-models","title":"Vespa \ud83e\udd1d ColPali: Efficient Document Retrieval with Vision Language Models\u00b6","text":"<p>For a simpler example of using ColPali, where we use one Vespa document = One PDF page, see simplified-retrieval-with-colpali.</p> <p>This notebook demonstrates how to represent ColPali in Vespa. ColPali is a powerful visual language model that can generate embeddings for images and text. In this notebook, we will use ColPali to generate embeddings for images of PDF pages and store them in Vespa. We will also store the base64 encoded image of the PDF page and some meta data like title and url. We will then demonstrate how to retrieve the pdf pages using the embeddings generated by ColPali.</p> <p>ColPali: Efficient Document Retrieval with Vision Language Models Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, C\u00e9line Hudelot, Pierre Colombo</p> <p>ColPali is a combination of ColBERT and PaliGemma:</p> <p>ColPali is enabled by the latest advances in Vision Language Models, notably the PaliGemma model from the Google Z\u00fcrich team, and leverages multi-vector retrieval through late interaction mechanisms as proposed in ColBERT by Omar Khattab.</p> <p>Quote from ColPali: Efficient Document Retrieval with Vision Language Models \ud83d\udc40</p> <p></p> <p>The ColPali model achieves remarkable retrieval performance on the ViDoRe (Visual Document Retrieval) Benchmark. Beating complex pipelines with a single model.</p> <p></p> <p>The TLDR of this notebook:</p> <ul> <li>Generate an image per PDF page using pdf2image and also extract the text using pypdf.</li> <li>For each page image, use ColPali to obtain the visual multi-vector embeddings</li> </ul> <p>Then we store colbert embeddings in Vespa and use the long-context variant where we represent the colbert embeddings per document with the tensor <code>tensor(page{}, patch{}, v[128])</code>. This enables us to use the PDF as the document (retrievable unit), storing the page embeddings in the same document.</p> <p>The upside of this is that we do not need to duplicate document level meta data like title, url, etc. But, the downside is that we cannot retrieve using the ColPali embeddings directly, but need to use the extracted text for retrieval. The ColPali embeddings are only used for reranking the results.</p> <p>For a simpler example where we use one vespa document = One PDF page, see simplified-retrieval-with-colpali.</p> <p>Consider following the ColQWen2 notebook instead as it use a better model with improved performance (Both accuracy and speed).</p> <p>We also store the base64 encoded image, and page meta data like title and url so that we can display it in the result page, but also use it for RAG with powerful LLMs with vision capabilities.</p> <p>At query time, we retrieve using BM25 over all the text from all pages, then use the ColPali embeddings to rerank the results using the max page score.</p> <p>Let us get started.</p> <p></p> <p>Install dependencies:</p> <p>Note that the python pdf2image package requires poppler-utils, see other installation options here.</p> <p>Install dependencies:</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#load-the-model","title":"Load the model\u00b6","text":"<p>This requires that the HF_TOKEN environment variable is set as the underlaying PaliGemma model is hosted on Hugging Face and has a restricive licence that requires authentication.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#working-with-pdfs","title":"Working with pdfs\u00b6","text":"<p>We need to convert a PDF to an array of images. One image per page. We will use pdf2image for this. Secondary, we also extract the text content of the pdf using pypdf.</p> <p>NOTE: This step requires that you have <code>poppler</code> installed on your system. Read more in pdf2image docs.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#configure-vespa","title":"Configure Vespa\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#deploy-to-vespa-cloud","title":"Deploy to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p><code>PyVespa</code> supports deploying apps to the development zone.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Ok, so now we have indexed the PDF pages in Vespa. Let us now obtain ColPali embeddings for a text query and use it to match against the indexed PDF pages.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#rag-with-llms-with-vision-capabilities","title":"RAG with LLMs with vision capabilities.\u00b6","text":"<p>Now we can use the top k documents to answer the question using a LLM with vision capabilities.</p> <p>This then becomes an end-to-end pipeline using vision capable language models, where we use ColPali visual embeddings for retrieval and Gemini Flash to read the retrieved PDF pages and answer the question with that context.</p> <p>We will use the Gemini Flash model for reading and answering.</p> <p>In the following, we input the best matching PDF page image and the question.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#summary","title":"Summary\u00b6","text":"<p>In this notebook, we have demonstrated how to represent ColPali in Vespa. We have used ColPali to generate embeddings for images of pdf pages and stored them in Vespa. We have also stored the base64 encoded image of the pdf page and some meta data like title and url. We have then demonstrated how to retrieve the pdf pages using the embeddings generated by ColPali. We have also demonstrated how to use the top k documents to answer a question using a LLM with vision capabilities.</p>"},{"location":"examples/colpali-document-retrieval-vision-language-models-cloud.html#cleanup","title":"Cleanup\u00b6","text":"<p>When this notebook is running in CI, we want to delete the application.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html","title":"Cross encoders for global reranking","text":"In\u00a0[1]: Copied! <pre>import requests\nfrom pathlib import Path\n\nurl = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model_quantized.onnx\"\nlocal_model_path = \"model/model_quantized.onnx\"\n\nr = requests.get(url)\n# Create path if it doesn't exist\nPath(local_model_path).parent.mkdir(parents=True, exist_ok=True)\nwith open(local_model_path, \"wb\") as f:\n    f.write(r.content)\n    print(f\"Downloaded model to {local_model_path}\")\n</pre> import requests from pathlib import Path  url = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model_quantized.onnx\" local_model_path = \"model/model_quantized.onnx\"  r = requests.get(url) # Create path if it doesn't exist Path(local_model_path).parent.mkdir(parents=True, exist_ok=True) with open(local_model_path, \"wb\") as f:     f.write(r.content)     print(f\"Downloaded model to {local_model_path}\") <pre>Downloaded model to model/model_quantized.onnx\n</pre> <p>We can see that the input pairs (query, document) are prefixed with a special <code>[CLS]</code> token, and separated by a <code>[SEP]</code> token.</p> <p>In Vespa, we want to tokenize the document body at indexing time, and the query at query time, and then combine them in the same way as the cross-encoder does, during ranking.</p> <p>Let us see how we can achieve this in Vespa.</p> In\u00a0[2]: Copied! <pre>from vespa.package import (\n    Component,\n    Document,\n    Field,\n    FieldSet,\n    Function,\n    GlobalPhaseRanking,\n    OnnxModel,\n    Parameter,\n    RankProfile,\n    Schema,\n)\n\nschema = Schema(\n    name=\"doc\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                index=\"enable-bm25\",\n            ),\n            # Let\u00b4s add a synthetic field (see https://docs.vespa.ai/en/schemas.html#field)\n            # to define how the tokens are derived from the text field\n            Field(\n                name=\"body_tokens\",\n                type=\"tensor&lt;float&gt;(d0[512])\",\n                # The tokenizer will be defined in the next cell\n                indexing=[\"input text\", \"embed tokenizer\", \"attribute\", \"summary\"],\n                is_document_field=False,  # Indicates a synthetic field\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n    models=[\n        OnnxModel(\n            model_name=\"crossencoder\",\n            model_file_path=f\"{local_model_path}\",\n            inputs={\n                \"input_ids\": \"input_ids\",\n                \"attention_mask\": \"attention_mask\",\n            },\n            outputs={\"logits\": \"logits\"},\n        )\n    ],\n    rank_profiles=[\n        RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),\n        RankProfile(\n            name=\"reranking\",\n            inherits=\"default\",\n            # We truncate the query to 64 tokens, meaning we have 512-64=448 tokens left for the document.\n            inputs=[(\"query(q)\", \"tensor&lt;float&gt;(d0[64])\")],\n            # See https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/blob/main/tokenizer_config.json\n            functions=[\n                Function(\n                    name=\"input_ids\",\n                    # See https://docs.vespa.ai/en/cross-encoders.html#roberta-based-model and https://docs.vespa.ai/en/reference/rank-features.html\n                    expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",\n                ),\n                Function(\n                    name=\"attention_mask\",\n                    expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",\n                ),\n            ],\n            first_phase=\"bm25(text)\",\n            global_phase=GlobalPhaseRanking(\n                rerank_count=10,\n                # We use the sigmoid function to force the output to be between 0 and 1, converting logits to probabilities.\n                expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\",\n            ),\n            summary_features=[\n                \"query(q)\",\n                \"input_ids\",\n                \"attention_mask\",\n                \"onnx(crossencoder).logits\",\n            ],\n        ),\n    ],\n)\n</pre> from vespa.package import (     Component,     Document,     Field,     FieldSet,     Function,     GlobalPhaseRanking,     OnnxModel,     Parameter,     RankProfile,     Schema, )  schema = Schema(     name=\"doc\",     mode=\"index\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 index=\"enable-bm25\",             ),             # Let\u00b4s add a synthetic field (see https://docs.vespa.ai/en/schemas.html#field)             # to define how the tokens are derived from the text field             Field(                 name=\"body_tokens\",                 type=\"tensor(d0[512])\",                 # The tokenizer will be defined in the next cell                 indexing=[\"input text\", \"embed tokenizer\", \"attribute\", \"summary\"],                 is_document_field=False,  # Indicates a synthetic field             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],     models=[         OnnxModel(             model_name=\"crossencoder\",             model_file_path=f\"{local_model_path}\",             inputs={                 \"input_ids\": \"input_ids\",                 \"attention_mask\": \"attention_mask\",             },             outputs={\"logits\": \"logits\"},         )     ],     rank_profiles=[         RankProfile(name=\"bm25\", first_phase=\"bm25(text)\"),         RankProfile(             name=\"reranking\",             inherits=\"default\",             # We truncate the query to 64 tokens, meaning we have 512-64=448 tokens left for the document.             inputs=[(\"query(q)\", \"tensor(d0[64])\")],             # See https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/blob/main/tokenizer_config.json             functions=[                 Function(                     name=\"input_ids\",                     # See https://docs.vespa.ai/en/cross-encoders.html#roberta-based-model and https://docs.vespa.ai/en/reference/rank-features.html                     expression=\"customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\",                 ),                 Function(                     name=\"attention_mask\",                     expression=\"tokenAttentionMask(512, query(q), attribute(body_tokens))\",                 ),             ],             first_phase=\"bm25(text)\",             global_phase=GlobalPhaseRanking(                 rerank_count=10,                 # We use the sigmoid function to force the output to be between 0 and 1, converting logits to probabilities.                 expression=\"sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\",             ),             summary_features=[                 \"query(q)\",                 \"input_ids\",                 \"attention_mask\",                 \"onnx(crossencoder).logits\",             ],         ),     ], ) In\u00a0[3]: Copied! <pre>from vespa.package import ApplicationPackage\n\napp_package = ApplicationPackage(\n    name=\"reranking\",\n    schema=[schema],\n    components=[\n        Component(\n            # See https://docs.vespa.ai/en/reference/embedding-reference.html#huggingface-tokenizer-embedder\n            id=\"tokenizer\",\n            type=\"hugging-face-tokenizer\",\n            parameters=[\n                Parameter(\n                    \"model\",\n                    {\n                        \"url\": \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"\n                    },\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import ApplicationPackage  app_package = ApplicationPackage(     name=\"reranking\",     schema=[schema],     components=[         Component(             # See https://docs.vespa.ai/en/reference/embedding-reference.html#huggingface-tokenizer-embedder             id=\"tokenizer\",             type=\"hugging-face-tokenizer\",             parameters=[                 Parameter(                     \"model\",                     {                         \"url\": \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/raw/main/tokenizer.json\"                     },                 ),             ],         )     ], ) <p>It is useful to inspect the schema-file (see https://docs.vespa.ai/en/reference/schema-reference.html) before deploying the application.</p> In\u00a0[4]: Copied! <pre>print(schema.schema_to_text)\n</pre> print(schema.schema_to_text) <pre>schema doc {\n    document doc {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field text type string {\n            indexing: index | summary\n            index: enable-bm25\n        }\n    }\n    field body_tokens type tensor&lt;float&gt;(d0[512]) {\n        indexing: input text | embed tokenizer | attribute | summary\n    }\n    fieldset default {\n        fields: text\n    }\n    onnx-model crossencoder {\n        file: files/crossencoder.onnx\n        input input_ids: input_ids\n        input attention_mask: attention_mask\n        output logits: logits\n    }\n    rank-profile bm25 {\n        first-phase {\n            expression {\n                bm25(text)\n            }\n        }\n    }\n    rank-profile reranking inherits default {\n        inputs {\n            query(q) tensor&lt;float&gt;(d0[64])             \n        \n        }\n        function input_ids() {\n            expression {\n                customTokenInputIds(1, 2, 512, query(q), attribute(body_tokens))\n            }\n        }\n        function attention_mask() {\n            expression {\n                tokenAttentionMask(512, query(q), attribute(body_tokens))\n            }\n        }\n        first-phase {\n            expression {\n                bm25(text)\n            }\n        }\n        global-phase {\n            rerank-count: 10\n            expression {\n                sigmoid(onnx(crossencoder).logits{d0:0,d1:0})\n            }\n        }\n        summary-features {\n            query(q)\n            input_ids\n            attention_mask\n            onnx(crossencoder).logits\n        }\n    }\n}\n</pre> <p>It looks fine. Now, let's just save the application package first, so that we also have more insight into the other files that are part of the application package.</p> In\u00a0[5]: Copied! <pre># Optionally, we can also write the application package to disk before deploying it.\napp_package.to_files(\"crossencoder-demo\")\n</pre> # Optionally, we can also write the application package to disk before deploying it. app_package.to_files(\"crossencoder-demo\") In\u00a0[6]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker(port=8080)\n\napp = vespa_docker.deploy(application_package=app_package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker(port=8080)  app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/60 seconds...\nUsing plain http against endpoint http://localhost:8089/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8089/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8089/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8089/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[7]: Copied! <pre>from docker.models.containers import Container\n\n\ndef download_and_analyze_model(url: str, container: Container) -&gt; None:\n    \"\"\"\n    Downloads an ONNX model from a specified URL and analyzes it within a Docker container.\n\n    Parameters:\n    url (str): The URL from where the ONNX model should be downloaded.\n    container (Container): The Docker container in which the command will be executed.\n\n    Raises:\n    Exception: Raises an exception if the command execution fails or if there are issues in streaming the output.\n\n    Note:\n    This function assumes that 'curl' and 'vespa-analyze-onnx-model' are available in the container environment.\n    \"\"\"\n\n    # Define the path inside the container where the model will be stored.\n    model_path = \"/opt/vespa/var/model.onnx\"\n\n    # Construct the command to download and analyze the model inside the container.\n    command = f\"bash -c 'curl -Lo {model_path} {url} &amp;&amp; vespa-analyze-onnx-model {model_path}'\"\n\n    # Command to delete the model after analysis.\n    delete_command = f\"rm {model_path}\"\n\n    # Execute the command in the container and handle potential errors.\n    try:\n        exit_code, output = container.exec_run(command, stream=True)\n        # Print the output from the command.\n        for line in output:\n            print(line.decode(), end=\"\")\n        # Remove the model after analysis.\n        container.exec_run(delete_command)\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        raise\n\n\nurl = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\"\n# Example usage:\n# download_and_analyze_model(url, vespa_docker.container)\n</pre> from docker.models.containers import Container   def download_and_analyze_model(url: str, container: Container) -&gt; None:     \"\"\"     Downloads an ONNX model from a specified URL and analyzes it within a Docker container.      Parameters:     url (str): The URL from where the ONNX model should be downloaded.     container (Container): The Docker container in which the command will be executed.      Raises:     Exception: Raises an exception if the command execution fails or if there are issues in streaming the output.      Note:     This function assumes that 'curl' and 'vespa-analyze-onnx-model' are available in the container environment.     \"\"\"      # Define the path inside the container where the model will be stored.     model_path = \"/opt/vespa/var/model.onnx\"      # Construct the command to download and analyze the model inside the container.     command = f\"bash -c 'curl -Lo {model_path} {url} &amp;&amp; vespa-analyze-onnx-model {model_path}'\"      # Command to delete the model after analysis.     delete_command = f\"rm {model_path}\"      # Execute the command in the container and handle potential errors.     try:         exit_code, output = container.exec_run(command, stream=True)         # Print the output from the command.         for line in output:             print(line.decode(), end=\"\")         # Remove the model after analysis.         container.exec_run(delete_command)      except Exception as e:         print(f\"An error occurred: {e}\")         raise   url = \"https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1/resolve/main/onnx/model.onnx\" # Example usage: # download_and_analyze_model(url, vespa_docker.container) <pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1126  100  1126    0     0   5715      0 --:--:-- --:--:-- --:--:--  5686\n100  271M  100  271M    0     0  15.8M      0  0:00:17  0:00:17 --:--:-- 16.3M\nunspecified option[0](optimize model), fallback: true\nvm_size: 166648 kB, vm_rss: 46700 kB, malloc_peak: 0 kb, malloc_curr: 1100 (before loading model)\nvm_size: 517176 kB, vm_rss: 405592 kB, malloc_peak: 0 kb, malloc_curr: 351628 (after loading model)\nmodel meta-data:\n  input[0]: 'input_ids' long[batch_size][sequence_length]\n  input[1]: 'attention_mask' long[batch_size][sequence_length]\n  output[0]: 'logits' float[batch_size][1]\nunspecified option[1](symbolic size 'batch_size'), fallback: 1\nunspecified option[2](symbolic size 'sequence_length'), fallback: 1\n1717140328.769314\tlocalhost\t1305/26134\t-\t.eval.onnx_wrapper\twarning\tinput 'input_ids' with element type 'long' is bound to vespa value with cell type 'double'; adding explicit conversion step (this conversion might be lossy)\n1717140328.769336\tlocalhost\t1305/26134\t-\t.eval.onnx_wrapper\twarning\tinput 'attention_mask' with element type 'long' is bound to vespa value with cell type 'double'; adding explicit conversion step (this conversion might be lossy)\ntest setup:\n  input[0]: tensor(d0[1],d1[1]) -&gt; long[1][1]\n  input[1]: tensor(d0[1],d1[1]) -&gt; long[1][1]\n  output[0]: float[1][1] -&gt; tensor&lt;float&gt;(d0[1],d1[1])\nunspecified option[3](max concurrent evaluations), fallback: 1\nvm_size: 517176 kB, vm_rss: 405592 kB, malloc_peak: 0 kb, malloc_curr: 351628 (no evaluations yet)\nvm_size: 517176 kB, vm_rss: 405856 kB, malloc_peak: 0 kb, malloc_curr: 351628 (concurrent evaluations: 1)\nestimated model evaluation time: 3.77819 ms\n</pre> <p>By doing this with the different size models and their quantized versions, we get this table.</p> Model Model File Inference Time (ms) Size N docs in 200ms mixedbread-ai/mxbai-rerank-xsmall-v1 model_quantized.onnx 2.4 87MB 82 mixedbread-ai/mxbai-rerank-xsmall-v1 model.onnx 3.8 284MB 52 mixedbread-ai/mxbai-rerank-base-v1 model_quantized.onnx 5.4 244MB 37 mixedbread-ai/mxbai-rerank-base-v1 model.onnx 10.3 739MB 19 mixedbread-ai/mxbai-rerank-large-v1 model_quantized.onnx 16.0 643MB 12 mixedbread-ai/mxbai-rerank-large-v1 model.onnx 35.6 1.74GB 5 <p>With a time budget of 200ms for reranking, we can add a column indicating the number of documents we are able to rerank within the budget time.</p> In\u00a0[8]: Copied! <pre># Feed a few sample documents to the application\nsample_docs = [\n    {\"id\": i, \"fields\": {\"text\": text}}\n    for i, text in enumerate(\n        [\n            \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",\n            \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",\n            \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",\n            \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",\n            \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",\n            \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",\n        ]\n    )\n]\n</pre> # Feed a few sample documents to the application sample_docs = [     {\"id\": i, \"fields\": {\"text\": text}}     for i, text in enumerate(         [             \"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was immediately successful, winning the Pulitzer Prize, and has become a classic of modern American literature.\",             \"The novel 'Moby-Dick' was written by Herman Melville and first published in 1851. It is considered a masterpiece of American literature and deals with complex themes of obsession, revenge, and the conflict between good and evil.\",             \"Harper Lee, an American novelist widely known for her novel 'To Kill a Mockingbird', was born in 1926 in Monroeville, Alabama. She received the Pulitzer Prize for Fiction in 1961.\",             \"Jane Austen was an English novelist known primarily for her six major novels, which interpret, critique and comment upon the British landed gentry at the end of the 18th century.\",             \"The 'Harry Potter' series, which consists of seven fantasy novels written by British author J.K. Rowling, is among the most popular and critically acclaimed books of the modern era.\",             \"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, was published in 1925. The story is set in the Jazz Age and follows the life of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.\",         ]     ) ] In\u00a0[9]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\napp.feed_iterable(sample_docs, schema=\"doc\", callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   app.feed_iterable(sample_docs, schema=\"doc\", callback=callback) In\u00a0[10]: Copied! <pre>from pprint import pprint\n\nwith app.syncio(connections=1) as sync_app:\n    query = sync_app.query(\n        body={\n            \"yql\": \"select * from sources * where userQuery();\",\n            \"query\": \"who wrote to kill a mockingbird?\",\n            \"input.query(q)\": \"embed(tokenizer, @query)\",\n            \"ranking.profile\": \"reranking\",\n            \"ranking.listFeatures\": \"true\",\n            \"presentation.timing\": \"true\",\n        }\n    )\n    for hit in query.hits:\n        pprint(hit[\"fields\"][\"text\"])\n        pprint(hit[\"relevance\"])\n</pre> from pprint import pprint  with app.syncio(connections=1) as sync_app:     query = sync_app.query(         body={             \"yql\": \"select * from sources * where userQuery();\",             \"query\": \"who wrote to kill a mockingbird?\",             \"input.query(q)\": \"embed(tokenizer, @query)\",             \"ranking.profile\": \"reranking\",             \"ranking.listFeatures\": \"true\",             \"presentation.timing\": \"true\",         }     )     for hit in query.hits:         pprint(hit[\"fields\"][\"text\"])         pprint(hit[\"relevance\"]) <pre>(\"'To Kill a Mockingbird' is a novel by Harper Lee published in 1960. It was \"\n 'immediately successful, winning the Pulitzer Prize, and has become a classic '\n 'of modern American literature.')\n0.9634037778787636\n(\"Harper Lee, an American novelist widely known for her novel 'To Kill a \"\n \"Mockingbird', was born in 1926 in Monroeville, Alabama. She received the \"\n 'Pulitzer Prize for Fiction in 1961.')\n0.8672221280618897\n(\"'The Great Gatsby', a novel written by American author F. Scott Fitzgerald, \"\n 'was published in 1925. The story is set in the Jazz Age and follows the life '\n 'of millionaire Jay Gatsby and his pursuit of Daisy Buchanan.')\n0.09325768904619067\n(\"The novel 'Moby-Dick' was written by Herman Melville and first published in \"\n '1851. It is considered a masterpiece of American literature and deals with '\n 'complex themes of obsession, revenge, and the conflict between good and '\n 'evil.')\n0.010269765303083865\n</pre> <p>It will of course be necessary to evaluate the performance of the cross-encoder in your specific use-case, but this notebook should give you a good starting point.</p> In\u00a0[11]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/cross-encoders-for-global-reranking.html#using-mixedbreadai-cross-encoder-for-reranking-in-vespaai","title":"Using Mixedbread.ai cross-encoder for reranking in Vespa.ai\u00b6","text":"<p>First, let us recap what cross-encoders are and where they might fit in a Vespa application.</p> <p>In contrast to bi-encoders, it is important to know that cross-encoders do NOT produce an embedding. Instead, a cross-encoder acts on pairs of input sequences and produces a single scalar score between 0 and 1, indicating the similarity or relevance between the two sentences.</p> <p>The cross-encoder model is a transformer-based model with a classification head on top of the Transformer CLS token (classification token).</p> <p>The model has been fine-tuned using the MS Marco passage training set and is a binary classifier which classifies if a query,document pair is relevant or not.</p> <p>The quote is from this blog post from 2021 that explains cross-encoders more in-depth. Note that the reference to the MS Marco dataset is for the model used in the blog post, and not the model we will use in this notebook.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#properties-of-cross-encoders-and-where-they-fit-in-vespa","title":"Properties of cross-encoders and where they fit in Vespa\u00b6","text":"<p>Cross-encoders are great at comparing a query and a document, but the time complexity increases linearly with the number of documents a query is compared to.</p> <p>This is why cross-encoders are often part of solutions at the top of leaderboards for ranking performance, such as MS MARCO Passage Ranking leaderboard.</p> <p>However, this leaderboard does not evaluate a solution's latency, and for production systems, doing cross-encoder inference for all documents in a corpus become prohibitively expensive.</p> <p>With Vespa's phased ranking capabilities, doing cross-encoder inference for a subset of documents at a later stage in the ranking pipeline can be a good trade-off between ranking performance and latency. For the remainder of this notebook, we will look at using a cross-encoder in global-phase reranking, introduced in this blog post.</p> <p></p> <p>In this notebook, we will show how to use the Mixedbread.ai cross-encoder for global-phase reranking in Vespa.</p> <p>The inference can also be run on GPU in Vespa Cloud, to accelerate inference even further.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#exploring-the-mixedbreadai-cross-encoder","title":"Exploring the Mixedbread.ai cross-encoder\u00b6","text":"<p>mixedbread.ai has done an amazing job of releasing both (binary) embedding-models and rerankers on huggingface \ud83e\udd17 the last weeks.</p> <p>Check out our previous notebook on using binary embeddings from mixedbread.ai in Vespa Cloud here</p> <p>For this demo, we will use mixedbread-ai/mxbai-rerank-xsmall-v1, but you can experiment with the larger models, depending on how you want to balance speed, accuracy, and cost (if you want to use GPU).</p> <p>This model is really powerful despite its small size, and provides a good trade-off between speed and accuracy.</p> <p>Table of accuracy on a BEIR (11 datasets):</p> Model Accuracy Lexical Search 66.4 bge-reranker-base 66.9 bge-reranker-large 70.6 cohere-embed-v3 70.9 mxbai-rerank-xsmall-v1 70.0 mxbai-rerank-base-v1 72.3 mxbai-rerank-large-v1 74.9 <p>(Table from mixedbread.ai's introductory blog post.)</p> <p>As we can see, the <code>mxbai-rerank-xsmall-v1</code> model is almost on par with much larger models while being much faster and cheaper to run.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#downloading-the-model","title":"Downloading the model\u00b6","text":"<p>We will use the quantized version of <code>mxbai-rerank-xsmall-v1</code> for this demo, as it is faster and cheaper to run, but feel free to change to the model of your choice.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#inspecting-the-model","title":"Inspecting the model\u00b6","text":"<p>It is useful to inspect the expected inputs and outputs, along with their shapes, before integrating the model into Vespa.</p> <p>This can either be done by, for instance, by using the <code>sentence_transformers</code> and <code>onnxruntime</code> libraries.</p> <p>One-off tasks like this are well suited for a Colab notebook. One example of how to do this in Colab can be found here:</p> <p></p>"},{"location":"examples/cross-encoders-for-global-reranking.html#what-does-a-crossencoder-do","title":"What does a crossencoder do?\u00b6","text":"<p>Below, we have tried to visualize what is going on in a cross-encoder, which helps us understand how we can use it in Vespa.</p> <p></p>"},{"location":"examples/cross-encoders-for-global-reranking.html#defining-our-vespa-application","title":"Defining our Vespa application\u00b6","text":""},{"location":"examples/cross-encoders-for-global-reranking.html#next-steps","title":"Next steps\u00b6","text":"<p>Try to use hybrid search for the first phase, and rerank with a cross-encoder.</p>"},{"location":"examples/cross-encoders-for-global-reranking.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/evaluating-with-snowflake-arctic-embed.html","title":"Evaluating with snowflake arctic embed","text":"In\u00a0[1]: Copied! <pre>from vespa.package import (\n    HNSW,\n    ApplicationPackage,\n    Component,\n    Field,\n    Parameter,\n    Function,\n)\n\napp_name = \"snowflake\"\n\napp_package = ApplicationPackage(\n    name=app_name,\n    components=[\n        Component(\n            id=\"snow\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"url\": \"https://huggingface.co/Snowflake/snowflake-arctic-embed-s/resolve/main/onnx/model_int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\n                        \"url\": \"https://huggingface.co/Snowflake/snowflake-arctic-embed-s/raw/main/tokenizer.json\"\n                    },\n                ),\n                Parameter(\n                    \"normalize\",\n                    {},\n                    \"true\",\n                ),\n                Parameter(\n                    \"pooling-strategy\",\n                    {},\n                    \"cls\",\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     HNSW,     ApplicationPackage,     Component,     Field,     Parameter,     Function, )  app_name = \"snowflake\"  app_package = ApplicationPackage(     name=app_name,     components=[         Component(             id=\"snow\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"url\": \"https://huggingface.co/Snowflake/snowflake-arctic-embed-s/resolve/main/onnx/model_int8.onnx\"                     },                 ),                 Parameter(                     \"tokenizer-model\",                     {                         \"url\": \"https://huggingface.co/Snowflake/snowflake-arctic-embed-s/raw/main/tokenizer.json\"                     },                 ),                 Parameter(                     \"normalize\",                     {},                     \"true\",                 ),                 Parameter(                     \"pooling-strategy\",                     {},                     \"cls\",                 ),             ],         )     ], ) In\u00a0[2]: Copied! <pre>app_package.schema.add_fields(\n    Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),\n    Field(\n        name=\"doc\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"doc_embeddings\",\n        type=\"tensor&lt;float&gt;(x[384])\",\n        indexing=[\"input doc\", \"embed\", \"index\", \"attribute\"],\n        ann=HNSW(distance_metric=\"prenormalized-angular\"),\n        is_document_field=False,\n    ),\n)\n</pre> app_package.schema.add_fields(     Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),     Field(         name=\"doc\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ),     Field(         name=\"doc_embeddings\",         type=\"tensor(x[384])\",         indexing=[\"input doc\", \"embed\", \"index\", \"attribute\"],         ann=HNSW(distance_metric=\"prenormalized-angular\"),         is_document_field=False,     ), ) In\u00a0[3]: Copied! <pre>from vespa.package import (\n    DocumentSummary,\n    FieldSet,\n    FirstPhaseRanking,\n    RankProfile,\n    SecondPhaseRanking,\n    Summary,\n)\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"semantic\",\n        inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n        inherits=\"default\",\n        first_phase=\"closeness(field, doc_embeddings)\",\n        match_features=[\"closeness(field, doc_embeddings)\"],\n    )\n)\n\napp_package.schema.add_rank_profile(RankProfile(name=\"bm25\", first_phase=\"bm25(doc)\"))\n</pre> from vespa.package import (     DocumentSummary,     FieldSet,     FirstPhaseRanking,     RankProfile,     SecondPhaseRanking,     Summary, )  app_package.schema.add_rank_profile(     RankProfile(         name=\"semantic\",         inputs=[(\"query(q)\", \"tensor(x[384])\")],         inherits=\"default\",         first_phase=\"closeness(field, doc_embeddings)\",         match_features=[\"closeness(field, doc_embeddings)\"],     ) )  app_package.schema.add_rank_profile(RankProfile(name=\"bm25\", first_phase=\"bm25(doc)\")) In\u00a0[4]: Copied! <pre>app_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"hybrid\",\n        inherits=\"semantic\",\n        # Guard against no keword match -&gt; bm25 = 0 -&gt; log10(0) = undefined\n        functions=[\n            Function(\n                name=\"log_guard\", expression=\"if (bm25(doc) &gt; 0, log10(bm25(doc)), 0)\"\n            )\n        ],\n        first_phase=FirstPhaseRanking(expression=\"closeness(field, doc_embeddings)\"),\n        # Notice that we use log10 here, as the bm25 values with the natural logarithm tends to dominate the closeness values for these documents.\n        second_phase=SecondPhaseRanking(expression=\"firstPhase + log_guard\"),\n        match_features=[\n            \"firstPhase\",\n            \"bm25(doc)\",\n        ],\n    )\n)\n</pre> app_package.schema.add_rank_profile(     RankProfile(         name=\"hybrid\",         inherits=\"semantic\",         # Guard against no keword match -&gt; bm25 = 0 -&gt; log10(0) = undefined         functions=[             Function(                 name=\"log_guard\", expression=\"if (bm25(doc) &gt; 0, log10(bm25(doc)), 0)\"             )         ],         first_phase=FirstPhaseRanking(expression=\"closeness(field, doc_embeddings)\"),         # Notice that we use log10 here, as the bm25 values with the natural logarithm tends to dominate the closeness values for these documents.         second_phase=SecondPhaseRanking(expression=\"firstPhase + log_guard\"),         match_features=[             \"firstPhase\",             \"bm25(doc)\",         ],     ) ) In\u00a0[5]: Copied! <pre>app_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"doc\"]))\n</pre> app_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"doc\"])) In\u00a0[6]: Copied! <pre>app_package.schema.add_document_summary(\n    DocumentSummary(\n        name=\"minimal\",\n        summary_fields=[Summary(\"id\", \"int\"), Summary(\"doc\", \"string\")],\n    )\n)\n</pre> app_package.schema.add_document_summary(     DocumentSummary(         name=\"minimal\",         summary_fields=[Summary(\"id\", \"int\"), Summary(\"doc\", \"string\")],     ) ) <p>Create some sample documents that will help us see where the different ranking strategies have their strengths and weaknesses.</p> <p>These sample documents were created with a little help from ChatGPT.</p> <p>Looking through the documents, we can see that a ranking of the documents in the order they are presented seem quite reasonable.</p> In\u00a0[7]: Copied! <pre># Query that the user is searching for\nquery = \"How does Vespa handle real-time indexing and search?\"\n\ndocuments = [\n    \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n    \"Instant data availability and maintaining query performance while simultaneously indexing are key features of the Vespa search engine.\",\n    \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n    \"While not as robust as Vespa, our vector database strives to meet your search needs, despite certain, shall we say, 'flexible' features.\",\n    \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\",\n    \"Modern search platforms emphasize quick data retrieval from continuously updated indexes.\",\n    \"Discover the history and cultural impact of the classic Italian Vespa scooter brand.\",\n    \"Tips for maintaining your Vespa to ensure optimal performance and longevity of your scooter.\",\n    \"Review of different scooter brands including Vespa, highlighting how they handle features like speed, cost, and aesthetics, and how consumers search for the best options.\",\n    \"Vespa scooter safety regulations and best practices for urban commuting.\",\n]\n</pre> # Query that the user is searching for query = \"How does Vespa handle real-time indexing and search?\"  documents = [     \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",     \"Instant data availability and maintaining query performance while simultaneously indexing are key features of the Vespa search engine.\",     \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",     \"While not as robust as Vespa, our vector database strives to meet your search needs, despite certain, shall we say, 'flexible' features.\",     \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\",     \"Modern search platforms emphasize quick data retrieval from continuously updated indexes.\",     \"Discover the history and cultural impact of the classic Italian Vespa scooter brand.\",     \"Tips for maintaining your Vespa to ensure optimal performance and longevity of your scooter.\",     \"Review of different scooter brands including Vespa, highlighting how they handle features like speed, cost, and aesthetics, and how consumers search for the best options.\",     \"Vespa scooter safety regulations and best practices for urban commuting.\", ] In\u00a0[8]: Copied! <pre>app_package.to_files(\"snowflake\")\n</pre> app_package.to_files(\"snowflake\") In\u00a0[9]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(app_package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(app_package) <pre>Waiting for configuration server, 0/60 seconds...\nWaiting for configuration server, 5/60 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 25/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[10]: Copied! <pre>feed_docs = [\n    {\n        \"id\": str(i),\n        \"fields\": {\n            \"doc\": doc,\n        },\n    }\n    for i, doc in enumerate(documents)\n]\n</pre> feed_docs = [     {         \"id\": str(i),         \"fields\": {             \"doc\": doc,         },     }     for i, doc in enumerate(documents) ] In\u00a0[11]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[12]: Copied! <pre>app.feed_iterable(feed_docs, schema=app_package.schema.name, callback=callback)\n</pre> app.feed_iterable(feed_docs, schema=app_package.schema.name, callback=callback) In\u00a0[13]: Copied! <pre>import math\nfrom typing import List\n\n\ndef ndcg_at_k(rank_order: List[int], ideal_order: List[int], k: int) -&gt; float:\n    \"\"\"\n    Calculate the normalized Discounted Cumulative Gain (nDCG) at position k.\n\n    Parameters:\n        rank_order (List[int]): The list of document indices as ranked by the search system.\n        ideal_order (List[int]): The list of document indices in the ideal order.\n        k (int): The position up to which to calculate nDCG.\n\n    Returns:\n        float: The nDCG value at position k.\n    \"\"\"\n    dcg = 0.0\n    idcg = 0.0\n\n    # Calculate DCG based on the ranked order up to k\n    for i in range(min(k, len(rank_order))):\n        rank_index = rank_order[i]\n        # Find the rank index in the ideal order to assign relevance\n        if rank_index in ideal_order:\n            relevance = len(ideal_order) - ideal_order.index(rank_index)\n        else:\n            relevance = 0\n        dcg += relevance / math.log2(i + 2)\n\n    # Calculate IDCG based on the ideal order up to k\n    for i in range(min(k, len(ideal_order))):\n        relevance = len(ideal_order) - i\n        idcg += relevance / math.log2(i + 2)\n\n    # Handle the case where IDCG is zero to avoid division by zero\n    if idcg == 0:\n        return 0.0\n    return dcg / idcg\n\n\n# Example usage\nrank_order = [5, 6, 1]  # Example ranked order indices\nideal_result_order = [0, 1, 2, 4, 5, 3, 6, 7, 8, 9]  # Example ideal order indices\n\n# Calculate nDCG@3\nresult = ndcg_at_k(rank_order, ideal_result_order, 3)\nprint(f\"nDCG@3: {result:.4f}\")\n\nassert ndcg_at_k([0, 1, 2], ideal_result_order, 3) == 1.0\n</pre> import math from typing import List   def ndcg_at_k(rank_order: List[int], ideal_order: List[int], k: int) -&gt; float:     \"\"\"     Calculate the normalized Discounted Cumulative Gain (nDCG) at position k.      Parameters:         rank_order (List[int]): The list of document indices as ranked by the search system.         ideal_order (List[int]): The list of document indices in the ideal order.         k (int): The position up to which to calculate nDCG.      Returns:         float: The nDCG value at position k.     \"\"\"     dcg = 0.0     idcg = 0.0      # Calculate DCG based on the ranked order up to k     for i in range(min(k, len(rank_order))):         rank_index = rank_order[i]         # Find the rank index in the ideal order to assign relevance         if rank_index in ideal_order:             relevance = len(ideal_order) - ideal_order.index(rank_index)         else:             relevance = 0         dcg += relevance / math.log2(i + 2)      # Calculate IDCG based on the ideal order up to k     for i in range(min(k, len(ideal_order))):         relevance = len(ideal_order) - i         idcg += relevance / math.log2(i + 2)      # Handle the case where IDCG is zero to avoid division by zero     if idcg == 0:         return 0.0     return dcg / idcg   # Example usage rank_order = [5, 6, 1]  # Example ranked order indices ideal_result_order = [0, 1, 2, 4, 5, 3, 6, 7, 8, 9]  # Example ideal order indices  # Calculate nDCG@3 result = ndcg_at_k(rank_order, ideal_result_order, 3) print(f\"nDCG@3: {result:.4f}\")  assert ndcg_at_k([0, 1, 2], ideal_result_order, 3) == 1.0 <pre>nDCG@3: 0.6618\n</pre> In\u00a0[14]: Copied! <pre># Define the different rank profiles to evaluate\n\nrank_profiles = {\n    \"unranked\": {\n        \"yql\": f\"select * from {app_name} where true\",\n        \"ranking.profile\": \"unranked\",\n    },\n    \"bm25\": {\n        \"yql\": f\"select * from {app_name} where userQuery()\",\n        \"ranking.profile\": \"bm25\",\n    },\n    \"semantic\": {\n        \"yql\": f\"select * from {app_name} where {{targetHits:5}}nearestNeighbor(doc_embeddings,q)\",\n        \"ranking.profile\": \"semantic\",\n        \"input.query(q)\": f\"embed({query})\",\n    },\n    \"hybrid\": {\n        \"yql\": f\"select * from {app_name} where userQuery() or ({{targetHits:5}}nearestNeighbor(doc_embeddings,q))\",\n        \"ranking.profile\": \"hybrid\",\n        \"input.query(q)\": f\"embed({query})\",\n    },\n    \"hybrid_filtered\": {\n        # Here, we will add an heuristic, filtering out documents that contain the word \"scooter\"\n        \"yql\": f'select * from {app_name} where !(doc contains \"scooter\") and userQuery() or ({{targetHits:5}}nearestNeighbor(doc_embeddings,q))',\n        \"ranking.profile\": \"hybrid\",\n        \"input.query(q)\": f\"embed({query})\",\n    },\n}\n\n# Define some common params that will be used for all queries\n\ncommon_params = {\n    \"query\": query,\n    \"hits\": 3,\n}\n</pre> # Define the different rank profiles to evaluate  rank_profiles = {     \"unranked\": {         \"yql\": f\"select * from {app_name} where true\",         \"ranking.profile\": \"unranked\",     },     \"bm25\": {         \"yql\": f\"select * from {app_name} where userQuery()\",         \"ranking.profile\": \"bm25\",     },     \"semantic\": {         \"yql\": f\"select * from {app_name} where {{targetHits:5}}nearestNeighbor(doc_embeddings,q)\",         \"ranking.profile\": \"semantic\",         \"input.query(q)\": f\"embed({query})\",     },     \"hybrid\": {         \"yql\": f\"select * from {app_name} where userQuery() or ({{targetHits:5}}nearestNeighbor(doc_embeddings,q))\",         \"ranking.profile\": \"hybrid\",         \"input.query(q)\": f\"embed({query})\",     },     \"hybrid_filtered\": {         # Here, we will add an heuristic, filtering out documents that contain the word \"scooter\"         \"yql\": f'select * from {app_name} where !(doc contains \"scooter\") and userQuery() or ({{targetHits:5}}nearestNeighbor(doc_embeddings,q))',         \"ranking.profile\": \"hybrid\",         \"input.query(q)\": f\"embed({query})\",     }, }  # Define some common params that will be used for all queries  common_params = {     \"query\": query,     \"hits\": 3, } In\u00a0[15]: Copied! <pre>from typing import List, Tuple\n\nfrom vespa.application import Vespa\nfrom vespa.io import VespaQueryResponse\n\n\ndef evaluate_rank_profile(\n    app: Vespa, rank_profile: str, params: dict, k: int\n) -&gt; Tuple[float, List[str]]:\n    \"\"\"\n    Run a query against a Vespa application using a specific rank profile and parameters.\n    Evaluate the nDCG@3 of the search results based on the ideal order.\n\n    Parameters:\n        app (Vespa): The Vespa application to query.\n        rank_profile (str): The name of the rank profile to use.\n        params (dict): The common parameters to use in addition to the rank profile specific parameters.\n        k (int): The position up to which to calculate nDCG.\n\n    Returns:\n        List[str]: The search results\n    \"\"\"\n    body_params = {\n        **rank_profiles[rank_profile],\n        **params,\n    }\n    response: VespaQueryResponse = app.query(body_params)\n    rankings = [int(hit[\"id\"][-1]) for hit in response.hits]\n    docs = [hit[\"fields\"][\"doc\"] for hit in response.hits]\n    ndcg = ndcg_at_k(rankings, ideal_order=ideal_result_order, k=3)\n    return ndcg, docs\n</pre> from typing import List, Tuple  from vespa.application import Vespa from vespa.io import VespaQueryResponse   def evaluate_rank_profile(     app: Vespa, rank_profile: str, params: dict, k: int ) -&gt; Tuple[float, List[str]]:     \"\"\"     Run a query against a Vespa application using a specific rank profile and parameters.     Evaluate the nDCG@3 of the search results based on the ideal order.      Parameters:         app (Vespa): The Vespa application to query.         rank_profile (str): The name of the rank profile to use.         params (dict): The common parameters to use in addition to the rank profile specific parameters.         k (int): The position up to which to calculate nDCG.      Returns:         List[str]: The search results     \"\"\"     body_params = {         **rank_profiles[rank_profile],         **params,     }     response: VespaQueryResponse = app.query(body_params)     rankings = [int(hit[\"id\"][-1]) for hit in response.hits]     docs = [hit[\"fields\"][\"doc\"] for hit in response.hits]     ndcg = ndcg_at_k(rankings, ideal_order=ideal_result_order, k=3)     return ndcg, docs In\u00a0[16]: Copied! <pre>import json\n\nrank_results = {}\n\nfor rank_profile, params in rank_profiles.items():\n    ndcg, docs = evaluate_rank_profile(\n        app, rank_profile=rank_profile, params=common_params, k=3\n    )\n    rank_results[rank_profile] = ndcg\n    print(f\"Rank profile: {rank_profile}, nDCG@3: {ndcg:.2f}\")\n    print(json.dumps(docs, indent=2))\n</pre> import json  rank_results = {}  for rank_profile, params in rank_profiles.items():     ndcg, docs = evaluate_rank_profile(         app, rank_profile=rank_profile, params=common_params, k=3     )     rank_results[rank_profile] = ndcg     print(f\"Rank profile: {rank_profile}, nDCG@3: {ndcg:.2f}\")     print(json.dumps(docs, indent=2)) <pre>Rank profile: unranked, nDCG@3: 0.68\n[\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n  \"Tips for maintaining your Vespa to ensure optimal performance and longevity of your scooter.\",\n  \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\"\n]\nRank profile: bm25, nDCG@3: 0.78\n[\n  \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n  \"Review of different scooter brands including Vespa, highlighting how they handle features like speed, cost, and aesthetics, and how consumers search for the best options.\",\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\"\n]\nRank profile: semantic, nDCG@3: 0.94\n[\n  \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n  \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\"\n]\nRank profile: hybrid, nDCG@3: 0.82\n[\n  \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n  \"Review of different scooter brands including Vespa, highlighting how they handle features like speed, cost, and aesthetics, and how consumers search for the best options.\"\n]\nRank profile: hybrid_filtered, nDCG@3: 0.94\n[\n  \"Vespa excels in real-time data indexing and its ability to search large datasets quickly.\",\n  \"With our search solution, real-time updates are seamlessly integrated into the search index, enhancing responsiveness.\",\n  \"Search engines like ours utilize complex algorithms to handle immediate data querying and indexing.\"\n]\n</pre> <p>Uncomment the cell below to install dependencies needed to generate the plot.</p> In\u00a0[17]: Copied! <pre>#!pip3 install pandas plotly\n</pre> #!pip3 install pandas plotly In\u00a0[20]: Copied! <pre>import pandas as pd\nimport plotly.express as px\n\n\ndef plot_rank_profiles(rank_profiles):\n    # Convert dictionary to DataFrame for easier manipulation\n    data = pd.DataFrame(list(rank_profiles.items()), columns=[\"Rank Profile\", \"nDCG@3\"])\n\n    colors = {\n        \"unranked\": \"#e74c3c\",  # Red\n        \"bm25\": \"#2ecc71\",  # Green\n        \"semantic\": \"#9b59b6\",  # Purple\n        \"hybrid\": \"#3498db\",  # Blue\n        \"hybrid_filtered\": \"#2980b9\",  # Darker Blue\n    }\n\n    # Map the colors to the dataframe\n    data[\"Color\"] = data[\"Rank Profile\"].map(colors)\n\n    # Create a bar chart using Plotly\n    fig = px.bar(\n        data,\n        x=\"Rank Profile\",\n        y=\"nDCG@3\",\n        title=\"Rank Profile Performance - snowflake-arctic-embed-s\",\n        labels={\"nDCG@3\": \"nDCG@3 Score\"},\n        text=\"nDCG@3\",\n        template=\"simple_white\",\n        color=\"Color\",\n        color_discrete_map=\"identity\",\n    )\n\n    # Set bar width and update traces for individual colors\n    fig.update_traces(\n        marker_line_color=\"black\", marker_line_width=1.5, width=0.4\n    )  # Less wide bars\n\n    # Enhance chart design adhering to Tufte's principles\n    fig.update_traces(texttemplate=\"%{text:.2f}\", textposition=\"outside\")\n    fig.update_layout(\n        plot_bgcolor=\"white\",\n        xaxis=dict(\n            title=\"Rank Profile\",\n            showline=True,\n            linewidth=2,\n            linecolor=\"black\",\n            mirror=True,\n        ),\n        yaxis=dict(\n            title=\"nDCG@3 Score\",\n            range=[0, 1.1],\n            showline=True,\n            linewidth=2,\n            linecolor=\"black\",\n            mirror=True,\n        ),\n        title_font=dict(size=24),\n        font=dict(family=\"Arial, sans-serif\", size=18, color=\"black\"),\n        margin=dict(l=40, r=40, t=40, b=40),\n        width=800,  # Set the width of the plot\n    )\n\n    # Show the plot\n    fig.show()\n\n\nplot_rank_profiles(rank_profiles=rank_results)\n</pre> import pandas as pd import plotly.express as px   def plot_rank_profiles(rank_profiles):     # Convert dictionary to DataFrame for easier manipulation     data = pd.DataFrame(list(rank_profiles.items()), columns=[\"Rank Profile\", \"nDCG@3\"])      colors = {         \"unranked\": \"#e74c3c\",  # Red         \"bm25\": \"#2ecc71\",  # Green         \"semantic\": \"#9b59b6\",  # Purple         \"hybrid\": \"#3498db\",  # Blue         \"hybrid_filtered\": \"#2980b9\",  # Darker Blue     }      # Map the colors to the dataframe     data[\"Color\"] = data[\"Rank Profile\"].map(colors)      # Create a bar chart using Plotly     fig = px.bar(         data,         x=\"Rank Profile\",         y=\"nDCG@3\",         title=\"Rank Profile Performance - snowflake-arctic-embed-s\",         labels={\"nDCG@3\": \"nDCG@3 Score\"},         text=\"nDCG@3\",         template=\"simple_white\",         color=\"Color\",         color_discrete_map=\"identity\",     )      # Set bar width and update traces for individual colors     fig.update_traces(         marker_line_color=\"black\", marker_line_width=1.5, width=0.4     )  # Less wide bars      # Enhance chart design adhering to Tufte's principles     fig.update_traces(texttemplate=\"%{text:.2f}\", textposition=\"outside\")     fig.update_layout(         plot_bgcolor=\"white\",         xaxis=dict(             title=\"Rank Profile\",             showline=True,             linewidth=2,             linecolor=\"black\",             mirror=True,         ),         yaxis=dict(             title=\"nDCG@3 Score\",             range=[0, 1.1],             showline=True,             linewidth=2,             linecolor=\"black\",             mirror=True,         ),         title_font=dict(size=24),         font=dict(family=\"Arial, sans-serif\", size=18, color=\"black\"),         margin=dict(l=40, r=40, t=40, b=40),         width=800,  # Set the width of the plot     )      # Show the plot     fig.show()   plot_rank_profiles(rank_profiles=rank_results) <p>For this particular synthetic small dataset, we can see that using the <code>snowflake-arctic-embed</code>-model improved the results significantly compared to keyword search only. Still, our experience with real-world data is that hybrid search is often the way to go.</p> <p>We also provided a little taste of how one can evaluate different ranking profiles if you have a ground truth dataset available, (or can create a synthetic one).</p> In\u00a0[19]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#evaluating-retrieval-with-snowflake-arctic-embed","title":"Evaluating retrieval with Snowflake arctic embed\u00b6","text":"<p>This notebook will demonstrate how different rank profiles in Vespa can be set up and evaluated. For the rank profiles that use semantic search, we will use the small version of Snowflake's arctic embed model series for generating embeddings.</p> <p>Feel free to experiment with different sizes based on your need and compute/latency constraints.</p> <p>The snowflake-arctic-embedding models achieve state-of-the-art performance on the MTEB/BEIR leaderboard for each of their size variants.</p> <p>We will set up and compare the following rank profiles:</p> <ul> <li>unranked: No ranking at all, for a dummy baseline.</li> <li>bm25: The classic BM25 ranker.</li> <li>semantic: Using <code>closeness(query_embedding, document_embedding)</code> only.</li> <li>hybrid: Combining BM25 and semantic search - <code>closeness(query_embedding, document_embedding) + log10( bm25(doc) )</code>.</li> <li>hybrid_filter: Same as the previous, but with a filter to exclude hits based on some heuristics.</li> </ul>"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#dumping-the-application-package-to-files","title":"Dumping the application package to files\u00b6","text":"<p>This is a good practice to inspect and understand the structure of the application package and schema files, generated by pyvespa.</p>"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#choosing-a-metric","title":"Choosing a metric\u00b6","text":"<p>Check out this wikipedia-article for an overview of evaluation measures in information retrieval.</p> <p>In our case, we have a query and ranked documents as the ground truth.</p> <p>When evaluating a ranking against our ground truth ranking, the Normalized Discounted Cumulative Gain (NDCG) metric is a good choice.</p> <p>The NDCG is a measure of ranking quality. It is calculated as the sum of the discounted gain of the relevant documents(DCG), divided by the ideal DCG. The ideal DCG is the DCG of the perfect ranking, where the documents are ordered by relevance.</p> <p>If you are already familiar with NDCG, feel free to skip this part. There is also an implementation in scikit-learn that you can use.</p> <p>The formula for NDCG is:</p> <p>$$ NDCG = \\frac{DCG}{IDCG} $$</p> <p>where:</p> <p>$$ DCG = \\sum_{i=1}^{n} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)} $$</p> <p>Let us create a function to calculate the NDCG for a given ranking.</p>"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#next-steps","title":"Next steps\u00b6","text":"<p>Check out global reranking strategies, and try to introduce a global_phase reranking strategy.</p>"},{"location":"examples/evaluating-with-snowflake-arctic-embed.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/feed_performance.html","title":"Feed performance","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install Vespa CLI. The <code>vespacli</code> python package is just a thin wrapper, allowing for installation through pypi.</p> <p>Do NOT install if you already have the Vespa CLI installed.</p> In\u00a0[\u00a0]: Copied! <pre>#!pip3 install vespacli\n</pre> #!pip3 install vespacli <p>Install pyvespa, other dependencies, and start the Docker Daemon.</p> In\u00a0[1]: Copied! <pre>#!pip3 install pyvespa datasets plotly&gt;=5.20\n#!docker info\n</pre> #!pip3 install pyvespa datasets plotly&gt;=5.20 #!docker info In\u00a0[2]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    FieldSet,\n)\n\npackage = ApplicationPackage(\n    name=\"pyvespafeed\",\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(name=\"text\", type=\"string\", indexing=[\"summary\"]),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     FieldSet, )  package = ApplicationPackage(     name=\"pyvespafeed\",     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(name=\"text\", type=\"string\", indexing=[\"summary\"]),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],         )     ], ) <p>Note that the <code>ApplicationPackage</code> name cannot have <code>-</code> or <code>_</code>.</p> In\u00a0[3]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=package)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 25/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p><code>app</code> now holds a reference to a Vespa instance.</p> In\u00a0[4]: Copied! <pre>from datasets import load_dataset\n\ndataset = load_dataset(\n    \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",\n    \"simple\",\n    split=\"train\",\n    streaming=True,\n)\n</pre> from datasets import load_dataset  dataset = load_dataset(     \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",     \"simple\",     split=\"train\",     streaming=True, ) In\u00a0[5]: Copied! <pre>def get_dataset(n_docs: int = 1000):\n    return (\n        dataset.map(lambda x: {\"id\": x[\"_id\"] + \"-iter\", \"fields\": {\"text\": x[\"text\"]}})\n        .select_columns([\"id\", \"fields\"])\n        .take(n_docs)\n    )\n</pre> def get_dataset(n_docs: int = 1000):     return (         dataset.map(lambda x: {\"id\": x[\"_id\"] + \"-iter\", \"fields\": {\"text\": x[\"text\"]}})         .select_columns([\"id\", \"fields\"])         .take(n_docs)     ) In\u00a0[6]: Copied! <pre>from dataclasses import dataclass\nfrom typing import Callable, Optional, Iterable, Dict\n\n\n@dataclass\nclass FeedParams:\n    name: str\n    num_docs: int\n    max_connections: int\n    function_name: str\n    max_workers: Optional[int] = None\n    max_queue_size: Optional[int] = None\n    num_concurrent_requests: Optional[int] = None\n\n\n@dataclass\nclass FeedResult(FeedParams):\n    feed_time: Optional[float] = None\n</pre> from dataclasses import dataclass from typing import Callable, Optional, Iterable, Dict   @dataclass class FeedParams:     name: str     num_docs: int     max_connections: int     function_name: str     max_workers: Optional[int] = None     max_queue_size: Optional[int] = None     num_concurrent_requests: Optional[int] = None   @dataclass class FeedResult(FeedParams):     feed_time: Optional[float] = None In\u00a0[7]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[8]: Copied! <pre>import time\nimport asyncio\n\n\ndef feed_sync(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:\n    start_time = time.time()\n    with app.syncio(connections=params.max_connections):\n        for doc in data:\n            app.feed_data_point(\n                data_id=doc[\"id\"],\n                fields=doc[\"fields\"],\n                schema=\"doc\",\n                namespace=\"pyvespa-feed\",\n                callback=callback,\n            )\n    end_time = time.time()\n    return FeedResult(\n        **params.__dict__,\n        feed_time=end_time - start_time,\n    )\n\n\nasync def feed_async(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:\n    start_time = time.time()\n    tasks = []\n    # We use a semaphore to limit the number of concurrent requests, this is useful to avoid\n    # running into memory issues when feeding a large number of documents\n    semaphore = asyncio.Semaphore(params.num_concurrent_requests)\n\n    async with app.asyncio(\n        connections=params.max_connections, timeout=params.num_docs // 10\n    ) as async_app:\n        for doc in data:\n            async with semaphore:\n                task = asyncio.create_task(\n                    async_app.feed_data_point(\n                        data_id=doc[\"id\"],\n                        fields=doc[\"fields\"],\n                        schema=\"doc\",\n                        namespace=\"pyvespa-feed\",\n                        timeout=10,\n                    )\n                )\n                tasks.append(task)\n\n        await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)\n\n    end_time = time.time()\n    return FeedResult(\n        **params.__dict__,\n        feed_time=end_time - start_time,\n    )\n\n\ndef feed_iterable(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:\n    start = time.time()\n    app.feed_iterable(\n        data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"feed\",\n        max_queue_size=params.max_queue_size,\n        max_workers=params.max_workers,\n        max_connections=params.max_connections,\n        callback=callback,\n    )\n    end = time.time()\n    sync_feed_time = end - start\n    return FeedResult(\n        **params.__dict__,\n        feed_time=sync_feed_time,\n    )\n</pre> import time import asyncio   def feed_sync(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:     start_time = time.time()     with app.syncio(connections=params.max_connections):         for doc in data:             app.feed_data_point(                 data_id=doc[\"id\"],                 fields=doc[\"fields\"],                 schema=\"doc\",                 namespace=\"pyvespa-feed\",                 callback=callback,             )     end_time = time.time()     return FeedResult(         **params.__dict__,         feed_time=end_time - start_time,     )   async def feed_async(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:     start_time = time.time()     tasks = []     # We use a semaphore to limit the number of concurrent requests, this is useful to avoid     # running into memory issues when feeding a large number of documents     semaphore = asyncio.Semaphore(params.num_concurrent_requests)      async with app.asyncio(         connections=params.max_connections, timeout=params.num_docs // 10     ) as async_app:         for doc in data:             async with semaphore:                 task = asyncio.create_task(                     async_app.feed_data_point(                         data_id=doc[\"id\"],                         fields=doc[\"fields\"],                         schema=\"doc\",                         namespace=\"pyvespa-feed\",                         timeout=10,                     )                 )                 tasks.append(task)          await asyncio.wait(tasks, return_when=asyncio.ALL_COMPLETED)      end_time = time.time()     return FeedResult(         **params.__dict__,         feed_time=end_time - start_time,     )   def feed_iterable(params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:     start = time.time()     app.feed_iterable(         data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"feed\",         max_queue_size=params.max_queue_size,         max_workers=params.max_workers,         max_connections=params.max_connections,         callback=callback,     )     end = time.time()     sync_feed_time = end - start     return FeedResult(         **params.__dict__,         feed_time=sync_feed_time,     ) In\u00a0[9]: Copied! <pre>from itertools import product\n\n# We will only run for 1000 documents here as notebook is run as part of CI.\n# But you will see some plots when run with 100k documents as well.\n\nnum_docs = [1000]\n\nparams_by_function = {\n    \"feed_sync\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [16, 64],\n    },\n    \"feed_async\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [16, 64],\n        \"num_concurrent_requests\": [1000, 10_000],\n    },\n    \"feed_iterable\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [64, 128],\n        \"max_workers\": [16, 64],\n        \"max_queue_size\": [1000, 10000],\n    },\n}\n\nfeed_params = []\n# Create one FeedParams instance of each permutation\nfor func, parameters in params_by_function.items():\n    print(f\"Function: {func}\")\n    keys, values = zip(*parameters.items())\n    for combination in product(*values):\n        settings = dict(zip(keys, combination))\n        print(settings)\n        feed_params.append(\n            FeedParams(\n                name=f\"{settings['num_docs']}_{settings['max_connections']}_{settings.get('max_workers', 0)}_{func}\",\n                function_name=func,\n                **settings,\n            )\n        )\n    print(\"\\n\")  # Just to add space between different functions\n</pre> from itertools import product  # We will only run for 1000 documents here as notebook is run as part of CI. # But you will see some plots when run with 100k documents as well.  num_docs = [1000]  params_by_function = {     \"feed_sync\": {         \"num_docs\": num_docs,         \"max_connections\": [16, 64],     },     \"feed_async\": {         \"num_docs\": num_docs,         \"max_connections\": [16, 64],         \"num_concurrent_requests\": [1000, 10_000],     },     \"feed_iterable\": {         \"num_docs\": num_docs,         \"max_connections\": [64, 128],         \"max_workers\": [16, 64],         \"max_queue_size\": [1000, 10000],     }, }  feed_params = [] # Create one FeedParams instance of each permutation for func, parameters in params_by_function.items():     print(f\"Function: {func}\")     keys, values = zip(*parameters.items())     for combination in product(*values):         settings = dict(zip(keys, combination))         print(settings)         feed_params.append(             FeedParams(                 name=f\"{settings['num_docs']}_{settings['max_connections']}_{settings.get('max_workers', 0)}_{func}\",                 function_name=func,                 **settings,             )         )     print(\"\\n\")  # Just to add space between different functions <pre>Function: feed_sync\n{'num_docs': 1000, 'max_connections': 16}\n{'num_docs': 1000, 'max_connections': 64}\n\n\nFunction: feed_async\n{'num_docs': 1000, 'max_connections': 16, 'num_concurrent_requests': 1000}\n{'num_docs': 1000, 'max_connections': 16, 'num_concurrent_requests': 10000}\n{'num_docs': 1000, 'max_connections': 64, 'num_concurrent_requests': 1000}\n{'num_docs': 1000, 'max_connections': 64, 'num_concurrent_requests': 10000}\n\n\nFunction: feed_iterable\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 16, 'max_queue_size': 1000}\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 16, 'max_queue_size': 10000}\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 1000}\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 10000}\n{'num_docs': 1000, 'max_connections': 128, 'max_workers': 16, 'max_queue_size': 1000}\n{'num_docs': 1000, 'max_connections': 128, 'max_workers': 16, 'max_queue_size': 10000}\n{'num_docs': 1000, 'max_connections': 128, 'max_workers': 64, 'max_queue_size': 1000}\n{'num_docs': 1000, 'max_connections': 128, 'max_workers': 64, 'max_queue_size': 10000}\n\n\n</pre> In\u00a0[10]: Copied! <pre>print(f\"Total number of feed_params: {len(feed_params)}\")\n</pre> print(f\"Total number of feed_params: {len(feed_params)}\") <pre>Total number of feed_params: 14\n</pre> <p>Now, we will need a way to retrieve the callable function from the function name.</p> In\u00a0[11]: Copied! <pre># Get reference to function from string name\ndef get_func_from_str(func_name: str) -&gt; Callable:\n    return globals()[func_name]\n</pre> # Get reference to function from string name def get_func_from_str(func_name: str) -&gt; Callable:     return globals()[func_name] In\u00a0[12]: Copied! <pre>from typing import Iterable, Dict\n\n\ndef delete_data(data: Iterable[Dict]):\n    app.feed_iterable(\n        iter=data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"delete\",\n        callback=callback,\n    )\n</pre> from typing import Iterable, Dict   def delete_data(data: Iterable[Dict]):     app.feed_iterable(         iter=data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"delete\",         callback=callback,     ) <p>The line below is used to make the code run in Jupyter, as it is already running an event loop</p> In\u00a0[13]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[14]: Copied! <pre>results = []\nfor params in feed_params:\n    print(\"-\" * 50)\n    print(\"Starting feed with params:\")\n    print(params)\n    data = get_dataset(params.num_docs)\n    if \"async\" not in params.function_name:\n        feed_result = get_func_from_str(params.function_name)(params=params, data=data)\n    else:\n        feed_result = asyncio.run(\n            get_func_from_str(params.function_name)(params=params, data=data)\n        )\n    print(feed_result.feed_time)\n    results.append(feed_result)\n    print(\"Deleting data\")\n    delete_data(data)\n</pre> results = [] for params in feed_params:     print(\"-\" * 50)     print(\"Starting feed with params:\")     print(params)     data = get_dataset(params.num_docs)     if \"async\" not in params.function_name:         feed_result = get_func_from_str(params.function_name)(params=params, data=data)     else:         feed_result = asyncio.run(             get_func_from_str(params.function_name)(params=params, data=data)         )     print(feed_result.feed_time)     results.append(feed_result)     print(\"Deleting data\")     delete_data(data) <pre>--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_16_0_feed_sync', num_docs=1000, max_connections=16, function_name='feed_sync', max_workers=None, max_queue_size=None, num_concurrent_requests=None)\n15.175757884979248\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_0_feed_sync', num_docs=1000, max_connections=64, function_name='feed_sync', max_workers=None, max_queue_size=None, num_concurrent_requests=None)\n12.517201900482178\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_16_0_feed_async', num_docs=1000, max_connections=16, function_name='feed_async', max_workers=None, max_queue_size=None, num_concurrent_requests=1000)\n4.953256130218506\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_16_0_feed_async', num_docs=1000, max_connections=16, function_name='feed_async', max_workers=None, max_queue_size=None, num_concurrent_requests=10000)\n4.914812088012695\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_0_feed_async', num_docs=1000, max_connections=64, function_name='feed_async', max_workers=None, max_queue_size=None, num_concurrent_requests=1000)\n4.711783170700073\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_0_feed_async', num_docs=1000, max_connections=64, function_name='feed_async', max_workers=None, max_queue_size=None, num_concurrent_requests=10000)\n4.942464113235474\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_16_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=16, max_queue_size=1000, num_concurrent_requests=None)\n5.707854270935059\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_16_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=16, max_queue_size=10000, num_concurrent_requests=None)\n5.798462867736816\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_64_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=1000, num_concurrent_requests=None)\n5.706255674362183\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_64_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=10000, num_concurrent_requests=None)\n5.976051330566406\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_128_16_feed_iterable', num_docs=1000, max_connections=128, function_name='feed_iterable', max_workers=16, max_queue_size=1000, num_concurrent_requests=None)\n5.959493160247803\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_128_16_feed_iterable', num_docs=1000, max_connections=128, function_name='feed_iterable', max_workers=16, max_queue_size=10000, num_concurrent_requests=None)\n5.757789134979248\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_128_64_feed_iterable', num_docs=1000, max_connections=128, function_name='feed_iterable', max_workers=64, max_queue_size=1000, num_concurrent_requests=None)\n5.612061023712158\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_128_64_feed_iterable', num_docs=1000, max_connections=128, function_name='feed_iterable', max_workers=64, max_queue_size=10000, num_concurrent_requests=None)\n5.622947692871094\nDeleting data\n</pre> In\u00a0[15]: Copied! <pre># Create a pandas DataFrame with the results\nimport pandas as pd\n\ndf = pd.DataFrame([result.__dict__ for result in results])\ndf[\"requests_per_second\"] = df[\"num_docs\"] / df[\"feed_time\"]\ndf\n</pre> # Create a pandas DataFrame with the results import pandas as pd  df = pd.DataFrame([result.__dict__ for result in results]) df[\"requests_per_second\"] = df[\"num_docs\"] / df[\"feed_time\"] df Out[15]: name num_docs max_connections function_name max_workers max_queue_size num_concurrent_requests feed_time requests_per_second 0 1000_16_0_feed_sync 1000 16 feed_sync NaN NaN NaN 15.175758 65.894567 1 1000_64_0_feed_sync 1000 64 feed_sync NaN NaN NaN 12.517202 79.890059 2 1000_16_0_feed_async 1000 16 feed_async NaN NaN 1000.0 4.953256 201.887400 3 1000_16_0_feed_async 1000 16 feed_async NaN NaN 10000.0 4.914812 203.466579 4 1000_64_0_feed_async 1000 64 feed_async NaN NaN 1000.0 4.711783 212.233875 5 1000_64_0_feed_async 1000 64 feed_async NaN NaN 10000.0 4.942464 202.328227 6 1000_64_16_feed_iterable 1000 64 feed_iterable 16.0 1000.0 NaN 5.707854 175.197185 7 1000_64_16_feed_iterable 1000 64 feed_iterable 16.0 10000.0 NaN 5.798463 172.459499 8 1000_64_64_feed_iterable 1000 64 feed_iterable 64.0 1000.0 NaN 5.706256 175.246266 9 1000_64_64_feed_iterable 1000 64 feed_iterable 64.0 10000.0 NaN 5.976051 167.334573 10 1000_128_16_feed_iterable 1000 128 feed_iterable 16.0 1000.0 NaN 5.959493 167.799505 11 1000_128_16_feed_iterable 1000 128 feed_iterable 16.0 10000.0 NaN 5.757789 173.677774 12 1000_128_64_feed_iterable 1000 128 feed_iterable 64.0 1000.0 NaN 5.612061 178.187656 13 1000_128_64_feed_iterable 1000 128 feed_iterable 64.0 10000.0 NaN 5.622948 177.842664 In\u00a0[16]: Copied! <pre>import plotly.express as px\n\n\ndef plot_performance(df: pd.DataFrame):\n    # Create a scatter plot with logarithmic scale for both axes using Plotly Express\n    fig = px.scatter(\n        df,\n        x=\"num_docs\",\n        y=\"requests_per_second\",\n        color=\"function_name\",  # Defines color based on different functions\n        log_x=True,  # Set x-axis to logarithmic scale\n        log_y=False,  # If you also want the y-axis in logarithmic scale, set this to True\n        title=\"Performance: Requests per Second vs. Number of Documents\",\n        labels={  # Customizing axis labels\n            \"num_docs\": \"Number of Documents\",\n            \"requests_per_second\": \"Requests per Second\",\n            \"max_workers\": \"max_workers\",\n            \"max_queue_size\": \"max_queue_size\",\n            \"max_connections\": \"max_connections\",\n            \"num_concurrent_requests\": \"num_concurrent_requests\",\n        },\n        template=\"plotly_white\",  # This sets the style to a white background, adhering to Tufte's minimalist principles\n        hover_data=[\n            \"max_workers\",\n            \"max_queue_size\",\n            \"max_connections\",\n            \"num_concurrent_requests\",\n        ],  # Additional information to show on hover\n    )\n\n    # Update layout for better readability, similar to 'talk' context in Seaborn\n    fig.update_layout(\n        font=dict(\n            size=16,  # Adjusting font size for better visibility, similar to 'talk' context\n        ),\n        legend_title_text=\"Function Details\",  # Custom legend title\n        legend=dict(\n            title_font_size=16,\n            x=800,  # Adjusting legend position similar to bbox_to_anchor in Matplotlib\n            xanchor=\"auto\",\n            y=1,\n            yanchor=\"auto\",\n        ),\n        width=800,  # Adjusting width of the plot\n    )\n    fig.update_xaxes(\n        tickvals=[1000, 10000, 100000],  # Set specific tick values\n        ticktext=[\"1k\", \"10k\", \"100k\"],  # Set corresponding tick labels\n    )\n\n    fig.update_traces(\n        marker=dict(size=12, opacity=0.7)\n    )  # Adjust marker size and opacity\n    # Show plot\n    fig.show()\n    # Save plot as HTML file\n    fig.write_html(\"performance.html\")\n\n\nplot_performance(df)\n</pre> import plotly.express as px   def plot_performance(df: pd.DataFrame):     # Create a scatter plot with logarithmic scale for both axes using Plotly Express     fig = px.scatter(         df,         x=\"num_docs\",         y=\"requests_per_second\",         color=\"function_name\",  # Defines color based on different functions         log_x=True,  # Set x-axis to logarithmic scale         log_y=False,  # If you also want the y-axis in logarithmic scale, set this to True         title=\"Performance: Requests per Second vs. Number of Documents\",         labels={  # Customizing axis labels             \"num_docs\": \"Number of Documents\",             \"requests_per_second\": \"Requests per Second\",             \"max_workers\": \"max_workers\",             \"max_queue_size\": \"max_queue_size\",             \"max_connections\": \"max_connections\",             \"num_concurrent_requests\": \"num_concurrent_requests\",         },         template=\"plotly_white\",  # This sets the style to a white background, adhering to Tufte's minimalist principles         hover_data=[             \"max_workers\",             \"max_queue_size\",             \"max_connections\",             \"num_concurrent_requests\",         ],  # Additional information to show on hover     )      # Update layout for better readability, similar to 'talk' context in Seaborn     fig.update_layout(         font=dict(             size=16,  # Adjusting font size for better visibility, similar to 'talk' context         ),         legend_title_text=\"Function Details\",  # Custom legend title         legend=dict(             title_font_size=16,             x=800,  # Adjusting legend position similar to bbox_to_anchor in Matplotlib             xanchor=\"auto\",             y=1,             yanchor=\"auto\",         ),         width=800,  # Adjusting width of the plot     )     fig.update_xaxes(         tickvals=[1000, 10000, 100000],  # Set specific tick values         ticktext=[\"1k\", \"10k\", \"100k\"],  # Set corresponding tick labels     )      fig.update_traces(         marker=dict(size=12, opacity=0.7)     )  # Adjust marker size and opacity     # Show plot     fig.show()     # Save plot as HTML file     fig.write_html(\"performance.html\")   plot_performance(df) <p>Here is the corresponding plot when run with 1k, 10k, and 100k documents:</p> <p></p> <p>Interesting. Let's try to summarize the insights we got from this experiment:</p> <ul> <li>The <code>feed_sync</code> method is the slowest, and does not benefit much from increasing <code>max_connections</code>. As there is no concurrency, and each request is blocking, this will be bound by the network latency, which in many cases will be a lot higher than when running against a local VespaDocker instance. For example, if you have 100ms latency against your Vespa instance, you can only feed 10 documents per second using the <code>VespaSync</code> method.</li> <li>The <code>feed_async</code> method is the fastest, and benefits slightly from increasing <code>max_connections</code> regardless of the number of documents. This method is non-blocking, and will likely be even more beneficial when running against a remote Vespa instance, such as Vespa Cloud, when network latency is higher.</li> <li>The <code>feed_iterable</code> performance is somewhere in between the other two methods, and benefits a lot from increasing <code>num_workers</code> when the number of documents increases.</li> </ul> <p>We have not looked at multiprocessing, but there is definitively room to utilize more cores to improve further upon these results. But there is one alternative that it is interesting to compare against, the Vespa CLI.</p> In\u00a0[17]: Copied! <pre>for n in num_docs:\n    print(f\"Getting dataset with {n} docs...\")\n    # First, let's load the dataset in non-streaming mode this time, as we want to save it to a jsonl file\n    dataset_cli = load_dataset(\n        \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",\n        \"simple\",\n        split=f\"train[:{n}]\",  # Notice the slicing here, see https://huggingface.co/docs/datasets/loading#slice-splits\n        streaming=False,\n    )\n    # Map to the format expected by the CLI.\n    # Note that this differs a little bit from the format expected by the Python API.\n    dataset_cli = dataset_cli.map(\n        lambda x: {\n            \"put\": f\"id:pyvespa-feed:doc::{x['_id']}-json\",\n            \"fields\": {\"text\": x[\"text\"]},\n        }\n    ).select_columns([\"put\", \"fields\"])\n    # Save to a jsonl file\n    assert len(dataset_cli) == n\n    dataset_cli.to_json(f\"vespa_feed-{n}.json\", orient=\"records\", lines=True)\n</pre> for n in num_docs:     print(f\"Getting dataset with {n} docs...\")     # First, let's load the dataset in non-streaming mode this time, as we want to save it to a jsonl file     dataset_cli = load_dataset(         \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",         \"simple\",         split=f\"train[:{n}]\",  # Notice the slicing here, see https://huggingface.co/docs/datasets/loading#slice-splits         streaming=False,     )     # Map to the format expected by the CLI.     # Note that this differs a little bit from the format expected by the Python API.     dataset_cli = dataset_cli.map(         lambda x: {             \"put\": f\"id:pyvespa-feed:doc::{x['_id']}-json\",             \"fields\": {\"text\": x[\"text\"]},         }     ).select_columns([\"put\", \"fields\"])     # Save to a jsonl file     assert len(dataset_cli) == n     dataset_cli.to_json(f\"vespa_feed-{n}.json\", orient=\"records\", lines=True) <pre>Getting dataset with 1000 docs...\n</pre> <pre>Creating json from Arrow format:   0%|          | 0/1 [00:00&lt;?, ?ba/s]</pre> <p>Let's look at the first line of one of the saved files to verify the format.</p> In\u00a0[18]: Copied! <pre>from pprint import pprint\nimport json\n\nwith open(\"vespa_feed-1000.json\", \"r\") as f:\n    sample = f.readline()\n    pprint(json.loads(sample))\n</pre> from pprint import pprint import json  with open(\"vespa_feed-1000.json\", \"r\") as f:     sample = f.readline()     pprint(json.loads(sample)) <pre>{'fields': {'text': 'April (Apr.) is the fourth month of the year in the '\n                    'Julian and Gregorian calendars, and comes between March '\n                    'and May. It is one of the four months to have 30 days.'},\n 'put': 'id:pyvespa-feed:doc::20231101.simple_1_0-json'}\n</pre> <p>Ok, now we are ready to feed the data using Vespa CLI. We also want to capture the output of feed statistics for each file.</p> In\u00a0[19]: Copied! <pre>cli_results = {}\nfor n in num_docs:\n    print(f\"Feeding {n} docs...\")\n    output_list = !vespa feed vespa_feed-{n}.json\n    results = json.loads(\"\".join(output_list))\n    pprint(results)\n    cli_results[n] = results\n</pre> cli_results = {} for n in num_docs:     print(f\"Feeding {n} docs...\")     output_list = !vespa feed vespa_feed-{n}.json     results = json.loads(\"\".join(output_list))     pprint(results)     cli_results[n] = results <pre>Feeding 1000 docs...\n{'feeder.error.count': 0,\n 'feeder.inflight.count': 0,\n 'feeder.ok.count': 1000,\n 'feeder.ok.rate': 1000.0,\n 'feeder.operation.count': 1000,\n 'feeder.seconds': 0.826,\n 'http.exception.count': 0,\n 'http.request.MBps': 1.423,\n 'http.request.bytes': 1422960,\n 'http.request.count': 4817,\n 'http.response.MBps': 0.712,\n 'http.response.bytes': 712421,\n 'http.response.code.counts': {'200': 1000, '429': 3817},\n 'http.response.count': 4817,\n 'http.response.error.count': 3817,\n 'http.response.latency.millis.avg': 107,\n 'http.response.latency.millis.max': 342,\n 'http.response.latency.millis.min': 0}\n</pre> In\u00a0[20]: Copied! <pre>cli_results\n</pre> cli_results Out[20]: <pre>{1000: {'feeder.operation.count': 1000,\n  'feeder.seconds': 0.826,\n  'feeder.ok.count': 1000,\n  'feeder.ok.rate': 1000.0,\n  'feeder.error.count': 0,\n  'feeder.inflight.count': 0,\n  'http.request.count': 4817,\n  'http.request.bytes': 1422960,\n  'http.request.MBps': 1.423,\n  'http.exception.count': 0,\n  'http.response.count': 4817,\n  'http.response.bytes': 712421,\n  'http.response.MBps': 0.712,\n  'http.response.error.count': 3817,\n  'http.response.latency.millis.min': 0,\n  'http.response.latency.millis.avg': 107,\n  'http.response.latency.millis.max': 342,\n  'http.response.code.counts': {'200': 1000, '429': 3817}}}</pre> In\u00a0[21]: Copied! <pre># Let's add the CLI results to the DataFrame\ndf_cli = pd.DataFrame(\n    [\n        {\n            \"name\": f\"{n}_cli\",\n            \"num_docs\": n,\n            \"max_connections\": \"unknown\",\n            \"function_name\": \"cli\",\n            \"max_workers\": \"unknown\",\n            \"max_queue_size\": \"n/a\",\n            \"feed_time\": result[\"feeder.seconds\"],\n        }\n        for n, result in cli_results.items()\n    ]\n)\ndf_cli[\"requests_per_second\"] = df_cli[\"num_docs\"] / df_cli[\"feed_time\"]\ndf_cli\n</pre> # Let's add the CLI results to the DataFrame df_cli = pd.DataFrame(     [         {             \"name\": f\"{n}_cli\",             \"num_docs\": n,             \"max_connections\": \"unknown\",             \"function_name\": \"cli\",             \"max_workers\": \"unknown\",             \"max_queue_size\": \"n/a\",             \"feed_time\": result[\"feeder.seconds\"],         }         for n, result in cli_results.items()     ] ) df_cli[\"requests_per_second\"] = df_cli[\"num_docs\"] / df_cli[\"feed_time\"] df_cli Out[21]: name num_docs max_connections function_name max_workers max_queue_size feed_time requests_per_second 0 1000_cli 1000 unknown cli unknown n/a 0.826 1210.653753 In\u00a0[22]: Copied! <pre>df_total = pd.concat([df, df_cli])\n\nplot_performance(df_total)\n</pre> df_total = pd.concat([df, df_cli])  plot_performance(df_total) <p>And here is the corresponding plot when run with 1k, 10k, and 100k documents:</p> <p></p> <p>As you can tell, the CLI is orders of magnitude faster.</p> <ul> <li>Prefer to use the CLI if you care about performance. \ud83d\ude80</li> <li>If you are using Python, prefer the async method, as it is the fastest way to feed data using <code>pyvespa</code>.</li> </ul> In\u00a0[23]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/feed_performance.html#feeding-performance","title":"Feeding performance\u00b6","text":"<p>This explorative notebook intends to shine some light on the different modes of feeding documents to Vespa. We will look at these 4 different methods:</p> <ol> <li>Using <code>VespaSync</code>.</li> <li>Using <code>VespaAsync</code>.</li> <li>Using <code>feed_iterable()</code></li> <li>Using Vespa CLI</li> </ol>"},{"location":"examples/feed_performance.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files.</p> <p>For this demo, we will just use a dummy application package without any indexing, just to ease the load for the server, as it is the clients we want to compare in this experiment.</p>"},{"location":"examples/feed_performance.html#deploy-the-vespa-application","title":"Deploy the Vespa application\u00b6","text":"<p>Deploy <code>package</code> on the local machine using Docker, without leaving the notebook, by creating an instance of VespaDocker. <code>VespaDocker</code> connects to the local Docker daemon socket and starts the Vespa docker image.</p> <p>If this step fails, please check that the Docker daemon is running, and that the Docker daemon socket can be used by clients (Configurable under advanced settings in Docker Desktop).</p>"},{"location":"examples/feed_performance.html#preparing-the-data","title":"Preparing the data\u00b6","text":"<p>In this example we use HF Datasets library to stream the \"Cohere/wikipedia-2023-11-embed-multilingual-v3\" dataset and index in our newly deployed Vespa instance.</p> <p>The dataset contains wikipedia-pages, and their corresponding embeddings.</p> <p>For this exploration we will just use the <code>id</code> and <code>text</code>-fields</p> <p>The following uses the stream option of datasets to stream the data without downloading all the contents locally.</p> <p>The <code>map</code> functionality allows us to convert the dataset fields into the expected feed format for <code>pyvespa</code> which expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p>"},{"location":"examples/feed_performance.html#utility-function-to-stream-different-number-of-documents","title":"Utility function to stream different number of documents\u00b6","text":""},{"location":"examples/feed_performance.html#a-dataclass-to-store-the-parameters-and-results-of-the-different-feeding-methods","title":"A dataclass to store the parameters and results of the different feeding methods\u00b6","text":""},{"location":"examples/feed_performance.html#a-common-callback-function-to-notify-if-something-goes-wrong","title":"A common callback function to notify if something goes wrong\u00b6","text":""},{"location":"examples/feed_performance.html#defining-our-feeding-functions","title":"Defining our feeding functions\u00b6","text":""},{"location":"examples/feed_performance.html#defining-our-hyperparameters","title":"Defining our hyperparameters\u00b6","text":""},{"location":"examples/feed_performance.html#function-to-clean-up-after-each-feed","title":"Function to clean up after each feed\u00b6","text":"<p>For a fair comparison, we will delete the data before feeding it again.</p>"},{"location":"examples/feed_performance.html#main-experiment-loop","title":"Main experiment loop\u00b6","text":""},{"location":"examples/feed_performance.html#feeding-with-vespa-cli","title":"Feeding with Vespa CLI\u00b6","text":"<p>Vespa CLI is a command-line interface for interacting with Vespa.</p> <p>Among many useful features are a <code>vespa feed</code> command that is the recommended way of feeding large datasets into Vespa. This is optimized for high feeding performance, and it will be interesting to get a feel for how performant feeding to a local Vespa instance is using the CLI.</p> <p>Note that comparing feeding with the CLI is not entirely fair, as the CLI relies on prepared data files, while the pyvespa methods are streaming the data before feeding it.</p>"},{"location":"examples/feed_performance.html#prepare-the-data-for-vespa-cli","title":"Prepare the data for Vespa CLI\u00b6","text":"<p>Vespa CLI can feed data from either many .json files or a single .jsonl file with many documents. The json format needs to be in the following format:</p> <pre>{\n  \"put\": \"id:namespace:document-type::document-id\",\n  \"fields\": {\n    \"field1\": \"value1\",\n    \"field2\": \"value2\"\n  }\n}\n</pre> <p>Where, <code>put</code> is the document operation in this case. Other allowed operations are <code>get</code>, <code>update</code> and <code>remove</code>.</p> <p>For reference, see https://docs.vespa.ai/en/vespa-cli#cheat-sheet</p>"},{"location":"examples/feed_performance.html#getting-the-datasets-as-jsonl-files","title":"Getting the datasets as .jsonl files\u00b6","text":"<p>Now, let`s save the dataset to 3 different jsonl files of 1k, 10k, and 100k documents.</p>"},{"location":"examples/feed_performance.html#conclusion","title":"Conclusion\u00b6","text":""},{"location":"examples/feed_performance.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/feed_performance.html#next-steps","title":"Next steps\u00b6","text":"<p>Check out some of the other examples in the documentation.</p>"},{"location":"examples/feed_performance_cloud.html","title":"Feed performance cloud","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install Vespa CLI. The <code>vespacli</code> python package is just a thin wrapper, allowing for installation through pypi.</p> <p>Do NOT install if you already have the Vespa CLI installed.</p> <p>Install pyvespa, and other dependencies.</p> In\u00a0[1]: Copied! <pre>!pip3 install vespacli pyvespa datasets plotly&gt;=5.20\n</pre> !pip3 install vespacli pyvespa datasets plotly&gt;=5.20 <pre>zsh:1: 5.20 not found\n</pre> In\u00a0[2]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    FieldSet,\n    HNSW,\n)\n\n# Define the application name (can NOT contain `_` or `-`)\n\napplication = \"feedperformancecloud\"\n\n\npackage = ApplicationPackage(\n    name=application,\n    schema=[\n        Schema(\n            name=\"doc\",\n            document=Document(\n                fields=[\n                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n                    Field(name=\"text\", type=\"string\", indexing=[\"index\", \"summary\"]),\n                    Field(\n                        name=\"embedding\",\n                        type=\"tensor&lt;float&gt;(x[1024])\",\n                        # Note that we are NOT embedding with a vespa model here, but that is also possible.\n                        indexing=[\"summary\", \"attribute\", \"index\"],\n                        ann=HNSW(distance_metric=\"angular\"),\n                    ),\n                ]\n            ),\n            fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     FieldSet,     HNSW, )  # Define the application name (can NOT contain `_` or `-`)  application = \"feedperformancecloud\"   package = ApplicationPackage(     name=application,     schema=[         Schema(             name=\"doc\",             document=Document(                 fields=[                     Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),                     Field(name=\"text\", type=\"string\", indexing=[\"index\", \"summary\"]),                     Field(                         name=\"embedding\",                         type=\"tensor(x[1024])\",                         # Note that we are NOT embedding with a vespa model here, but that is also possible.                         indexing=[\"summary\", \"attribute\", \"index\"],                         ann=HNSW(distance_metric=\"angular\"),                     ),                 ]             ),             fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],         )     ], ) <p>Note that the <code>ApplicationPackage</code> name cannot have <code>-</code> or <code>_</code>.</p> <p>Follow the instructions from the output above and add the control-plane key in the console at <code>https://console.vespa-cloud.com/tenant/TENANT_NAME/account/keys</code> (replace TENANT_NAME with your tenant name).</p> In\u00a0[3]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly   vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=package, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.feedperformancecloud\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p><code>app</code> now holds a reference to a VespaCloud instance.</p> In\u00a0[4]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 9 of dev-aws-us-east-1c for vespa-team.feedperformancecloud. This may take a few minutes the first time.\nINFO    [07:22:29]  Deploying platform version 8.392.14 and application dev build 7 for dev-aws-us-east-1c of default ...\nINFO    [07:22:30]  Using CA signed certificate version 1\nINFO    [07:22:30]  Using 1 nodes in container cluster 'feedperformancecloud_container'\nWARNING [07:22:33]  Auto-overriding validation which would be disallowed in production: certificate-removal: Data plane certificate(s) from cluster 'feedperformancecloud_container' is removed (removed certificates: [CN=cloud.vespa.example]) This can cause client connection issues.. To allow this add &lt;allow until='yyyy-mm-dd'&gt;certificate-removal&lt;/allow&gt; to validation-overrides.xml, see https://docs.vespa.ai/en/reference/validation-overrides.html\nINFO    [07:22:34]  Session 304192 for tenant 'vespa-team' prepared and activated.\nINFO    [07:22:35]  ######## Details for all nodes ########\nINFO    [07:22:35]  h95731a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:22:35]  --- platform vespa/cloud-tenant-rhel8:8.392.14\nINFO    [07:22:35]  --- container on port 4080 has not started \nINFO    [07:22:35]  --- metricsproxy-container on port 19092 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  h95729b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:22:35]  --- platform vespa/cloud-tenant-rhel8:8.392.14\nINFO    [07:22:35]  --- storagenode on port 19102 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- searchnode on port 19107 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- distributor on port 19111 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- metricsproxy-container on port 19092 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  h93272g.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:22:35]  --- platform vespa/cloud-tenant-rhel8:8.392.14\nINFO    [07:22:35]  --- logserver-container on port 4080 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- metricsproxy-container on port 19092 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  h93272h.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [07:22:35]  --- platform vespa/cloud-tenant-rhel8:8.392.14\nINFO    [07:22:35]  --- container-clustercontroller on port 19050 has config generation 304192, wanted is 304192\nINFO    [07:22:35]  --- metricsproxy-container on port 19092 has config generation 304192, wanted is 304192\nINFO    [07:22:42]  Found endpoints:\nINFO    [07:22:42]  - dev.aws-us-east-1c\nINFO    [07:22:42]   |-- https://b48e8812.bc737822.z.vespa-app.cloud/ (cluster 'feedperformancecloud_container')\nINFO    [07:22:44]  Deployment of new application complete!\nFound mtls endpoint for feedperformancecloud_container\nURL: https://b48e8812.bc737822.z.vespa-app.cloud/\nConnecting to https://b48e8812.bc737822.z.vespa-app.cloud/\nUsing mtls_key_cert Authentication against endpoint https://b48e8812.bc737822.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Note that if you already have a Vespa Cloud instance running, the recommended way to initialize a <code>Vespa</code> instance is directly, by passing the <code>endpoint</code> and <code>tenant</code> parameters to the <code>Vespa</code> constructor, along with either:</p> <ol> <li>Key/cert for dataplane authentication (generated as part of deployment, copied into the application package, in <code>/security/clients.pem</code>, and <code>~/.vespa/mytenant.myapplication/data-plane-public-cert.pem</code> and <code>~/.vespa/mytenant.myapplication/data-plane-private-key.pem</code>).</li> </ol> <pre>from vespa.application import Vespa\n\napp: Vespa = Vespa(\n    url=\"https://my-endpoint.z.vespa-app.cloud\",\n    tenant=\"my-tenant\",\n    key_file=\"path/to/private-key.pem\",\n    cert_file=\"path/to/certificate.pem\",\n)\n</pre> <ol> <li>Using a token (must be generated in Vespa Cloud Console and defined in the application package, see https://cloud.vespa.ai/en/security/guide.</li> </ol> <pre>from vespa.application import Vespa\nimport os\n\napp: Vespa = Vespa(\n    url=\"https://my-endpoint.z.vespa-app.cloud\",\n    tenant=\"my-tenant\",\n    vespa_cloud_secret_token=os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\"),\n)\n</pre> In\u00a0[5]: Copied! <pre>app.get_application_status()\n</pre> app.get_application_status() <pre>Using mtls_key_cert Authentication against endpoint https://b48e8812.bc737822.z.vespa-app.cloud//ApplicationStatus\n</pre> Out[5]: <pre>&lt;Response [200]&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n</pre> from datasets import load_dataset In\u00a0[7]: Copied! <pre>def get_dataset(n_docs: int = 1000):\n    dataset = load_dataset(\n        \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",\n        \"simple\",\n        split=f\"train[:{n_docs}]\",\n    )\n    dataset = dataset.map(\n        lambda x: {\n            \"id\": x[\"_id\"] + \"-iter\",\n            \"fields\": {\"text\": x[\"text\"], \"embedding\": x[\"emb\"]},\n        }\n    ).select_columns([\"id\", \"fields\"])\n    return dataset\n</pre> def get_dataset(n_docs: int = 1000):     dataset = load_dataset(         \"Cohere/wikipedia-2023-11-embed-multilingual-v3\",         \"simple\",         split=f\"train[:{n_docs}]\",     )     dataset = dataset.map(         lambda x: {             \"id\": x[\"_id\"] + \"-iter\",             \"fields\": {\"text\": x[\"text\"], \"embedding\": x[\"emb\"]},         }     ).select_columns([\"id\", \"fields\"])     return dataset In\u00a0[8]: Copied! <pre>from dataclasses import dataclass\nfrom typing import Callable, Optional, Iterable, Dict\n\n\n@dataclass\nclass FeedParams:\n    name: str\n    num_docs: int\n    max_connections: int\n    function_name: str\n    max_workers: Optional[int] = None\n    max_queue_size: Optional[int] = None\n\n\n@dataclass\nclass FeedResult(FeedParams):\n    feed_time: Optional[float] = None\n</pre> from dataclasses import dataclass from typing import Callable, Optional, Iterable, Dict   @dataclass class FeedParams:     name: str     num_docs: int     max_connections: int     function_name: str     max_workers: Optional[int] = None     max_queue_size: Optional[int] = None   @dataclass class FeedResult(FeedParams):     feed_time: Optional[float] = None In\u00a0[9]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[10]: Copied! <pre>import time\nimport asyncio\nfrom vespa.application import Vespa\n</pre> import time import asyncio from vespa.application import Vespa In\u00a0[11]: Copied! <pre>def feed_iterable(app: Vespa, params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:\n    start = time.time()\n    app.feed_iterable(\n        data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"feed\",\n        max_queue_size=params.max_queue_size,\n        max_workers=params.max_workers,\n        max_connections=params.max_connections,\n        callback=callback,\n    )\n    end = time.time()\n    sync_feed_time = end - start\n    return FeedResult(\n        **params.__dict__,\n        feed_time=sync_feed_time,\n    )\n\n\ndef feed_async_iterable(\n    app: Vespa, params: FeedParams, data: Iterable[Dict]\n) -&gt; FeedResult:\n    start = time.time()\n    app.feed_async_iterable(\n        data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"feed\",\n        max_queue_size=params.max_queue_size,\n        max_workers=params.max_workers,\n        max_connections=params.max_connections,\n        callback=callback,\n    )\n    end = time.time()\n    sync_feed_time = end - start\n    return FeedResult(\n        **params.__dict__,\n        feed_time=sync_feed_time,\n    )\n</pre> def feed_iterable(app: Vespa, params: FeedParams, data: Iterable[Dict]) -&gt; FeedResult:     start = time.time()     app.feed_iterable(         data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"feed\",         max_queue_size=params.max_queue_size,         max_workers=params.max_workers,         max_connections=params.max_connections,         callback=callback,     )     end = time.time()     sync_feed_time = end - start     return FeedResult(         **params.__dict__,         feed_time=sync_feed_time,     )   def feed_async_iterable(     app: Vespa, params: FeedParams, data: Iterable[Dict] ) -&gt; FeedResult:     start = time.time()     app.feed_async_iterable(         data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"feed\",         max_queue_size=params.max_queue_size,         max_workers=params.max_workers,         max_connections=params.max_connections,         callback=callback,     )     end = time.time()     sync_feed_time = end - start     return FeedResult(         **params.__dict__,         feed_time=sync_feed_time,     ) In\u00a0[12]: Copied! <pre>from itertools import product\n\n# We will only run for up to 10 000 documents here as notebook is run as part of CI.\n\nnum_docs = [\n    1000,\n    5_000,\n    10_000,\n]\nparams_by_function = {\n    \"feed_async_iterable\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [1],\n        \"max_workers\": [64],\n        \"max_queue_size\": [2500],\n    },\n    \"feed_iterable\": {\n        \"num_docs\": num_docs,\n        \"max_connections\": [64],\n        \"max_workers\": [64],\n        \"max_queue_size\": [2500],\n    },\n}\n\nfeed_params = []\n# Create one FeedParams instance of each permutation\nfor func, parameters in params_by_function.items():\n    print(f\"Function: {func}\")\n    keys, values = zip(*parameters.items())\n    for combination in product(*values):\n        settings = dict(zip(keys, combination))\n        print(settings)\n        feed_params.append(\n            FeedParams(\n                name=f\"{settings['num_docs']}_{settings['max_connections']}_{settings.get('max_workers', 0)}_{func}\",\n                function_name=func,\n                **settings,\n            )\n        )\n    print(\"\\n\")  # Just to add space between different functions\n</pre> from itertools import product  # We will only run for up to 10 000 documents here as notebook is run as part of CI.  num_docs = [     1000,     5_000,     10_000, ] params_by_function = {     \"feed_async_iterable\": {         \"num_docs\": num_docs,         \"max_connections\": [1],         \"max_workers\": [64],         \"max_queue_size\": [2500],     },     \"feed_iterable\": {         \"num_docs\": num_docs,         \"max_connections\": [64],         \"max_workers\": [64],         \"max_queue_size\": [2500],     }, }  feed_params = [] # Create one FeedParams instance of each permutation for func, parameters in params_by_function.items():     print(f\"Function: {func}\")     keys, values = zip(*parameters.items())     for combination in product(*values):         settings = dict(zip(keys, combination))         print(settings)         feed_params.append(             FeedParams(                 name=f\"{settings['num_docs']}_{settings['max_connections']}_{settings.get('max_workers', 0)}_{func}\",                 function_name=func,                 **settings,             )         )     print(\"\\n\")  # Just to add space between different functions <pre>Function: feed_async_iterable\n{'num_docs': 1000, 'max_connections': 1, 'max_workers': 64, 'max_queue_size': 2500}\n{'num_docs': 5000, 'max_connections': 1, 'max_workers': 64, 'max_queue_size': 2500}\n{'num_docs': 10000, 'max_connections': 1, 'max_workers': 64, 'max_queue_size': 2500}\n\n\nFunction: feed_iterable\n{'num_docs': 1000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 2500}\n{'num_docs': 5000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 2500}\n{'num_docs': 10000, 'max_connections': 64, 'max_workers': 64, 'max_queue_size': 2500}\n\n\n</pre> In\u00a0[13]: Copied! <pre>print(f\"Total number of feed_params: {len(feed_params)}\")\n</pre> print(f\"Total number of feed_params: {len(feed_params)}\") <pre>Total number of feed_params: 6\n</pre> <p>Now, we will need a way to retrieve the callable function from the function name.</p> In\u00a0[14]: Copied! <pre># Get reference to function from string name\ndef get_func_from_str(func_name: str) -&gt; Callable:\n    return globals()[func_name]\n</pre> # Get reference to function from string name def get_func_from_str(func_name: str) -&gt; Callable:     return globals()[func_name] In\u00a0[15]: Copied! <pre>from typing import Iterable, Dict\nfrom vespa.application import Vespa\n\n\ndef delete_data(app: Vespa, data: Iterable[Dict]):\n    app.feed_iterable(\n        iter=data,\n        schema=\"doc\",\n        namespace=\"pyvespa-feed\",\n        operation_type=\"delete\",\n        callback=callback,\n        max_workers=16,\n        max_connections=16,\n    )\n</pre> from typing import Iterable, Dict from vespa.application import Vespa   def delete_data(app: Vespa, data: Iterable[Dict]):     app.feed_iterable(         iter=data,         schema=\"doc\",         namespace=\"pyvespa-feed\",         operation_type=\"delete\",         callback=callback,         max_workers=16,         max_connections=16,     ) <p>The line below is used to make the code run in Jupyter, as it is already running an event loop</p> In\u00a0[16]: Copied! <pre>import nest_asyncio\n\nnest_asyncio.apply()\n</pre> import nest_asyncio  nest_asyncio.apply() In\u00a0[17]: Copied! <pre>results = []\nfor params in feed_params:\n    print(\"-\" * 50)\n    print(\"Starting feed with params:\")\n    print(params)\n    data = get_dataset(params.num_docs)\n    if \"xxx\" not in params.function_name:\n        if \"feed_sync\" in params.function_name:\n            print(\"Skipping feed_sync\")\n            continue\n        feed_result = get_func_from_str(params.function_name)(\n            app=app, params=params, data=data\n        )\n    else:\n        feed_result = asyncio.run(\n            get_func_from_str(params.function_name)(app=app, params=params, data=data)\n        )\n    print(feed_result.feed_time)\n    results.append(feed_result)\n    print(\"Deleting data\")\n    time.sleep(3)\n    delete_data(app, data)\n</pre> results = [] for params in feed_params:     print(\"-\" * 50)     print(\"Starting feed with params:\")     print(params)     data = get_dataset(params.num_docs)     if \"xxx\" not in params.function_name:         if \"feed_sync\" in params.function_name:             print(\"Skipping feed_sync\")             continue         feed_result = get_func_from_str(params.function_name)(             app=app, params=params, data=data         )     else:         feed_result = asyncio.run(             get_func_from_str(params.function_name)(app=app, params=params, data=data)         )     print(feed_result.feed_time)     results.append(feed_result)     print(\"Deleting data\")     time.sleep(3)     delete_data(app, data) <pre>--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_1_64_feed_async_iterable', num_docs=1000, max_connections=1, function_name='feed_async_iterable', max_workers=64, max_queue_size=2500)\n</pre> <pre>Using mtls_key_cert Authentication against endpoint https://b48e8812.bc737822.z.vespa-app.cloud//ApplicationStatus\n7.062151908874512\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='5000_1_64_feed_async_iterable', num_docs=5000, max_connections=1, function_name='feed_async_iterable', max_workers=64, max_queue_size=2500)\n20.979923963546753\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='10000_1_64_feed_async_iterable', num_docs=10000, max_connections=1, function_name='feed_async_iterable', max_workers=64, max_queue_size=2500)\n41.321199893951416\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='1000_64_64_feed_iterable', num_docs=1000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=2500)\n16.278107166290283\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='5000_64_64_feed_iterable', num_docs=5000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=2500)\n78.27990508079529\nDeleting data\n--------------------------------------------------\nStarting feed with params:\nFeedParams(name='10000_64_64_feed_iterable', num_docs=10000, max_connections=64, function_name='feed_iterable', max_workers=64, max_queue_size=2500)\n156.38266611099243\nDeleting data\n</pre> In\u00a0[18]: Copied! <pre># Create a pandas DataFrame with the results\nimport pandas as pd\n\ndf = pd.DataFrame([result.__dict__ for result in results])\ndf[\"requests_per_second\"] = df[\"num_docs\"] / df[\"feed_time\"]\ndf\n</pre> # Create a pandas DataFrame with the results import pandas as pd  df = pd.DataFrame([result.__dict__ for result in results]) df[\"requests_per_second\"] = df[\"num_docs\"] / df[\"feed_time\"] df Out[18]: name num_docs max_connections function_name max_workers max_queue_size feed_time requests_per_second 0 1000_1_64_feed_async_iterable 1000 1 feed_async_iterable 64 2500 7.062152 141.599899 1 5000_1_64_feed_async_iterable 5000 1 feed_async_iterable 64 2500 20.979924 238.323075 2 10000_1_64_feed_async_iterable 10000 1 feed_async_iterable 64 2500 41.321200 242.006525 3 1000_64_64_feed_iterable 1000 64 feed_iterable 64 2500 16.278107 61.432204 4 5000_64_64_feed_iterable 5000 64 feed_iterable 64 2500 78.279905 63.873353 5 10000_64_64_feed_iterable 10000 64 feed_iterable 64 2500 156.382666 63.945706 In\u00a0[19]: Copied! <pre>import plotly.express as px\n\n\ndef plot_performance(df: pd.DataFrame):\n    # Create a scatter plot with logarithmic scale for both axes using Plotly Express\n    fig = px.scatter(\n        df,\n        x=\"num_docs\",\n        y=\"requests_per_second\",\n        color=\"function_name\",  # Defines color based on different functions\n        log_x=True,  # Set x-axis to logarithmic scale\n        log_y=False,  # If you also want the y-axis in logarithmic scale, set this to True\n        title=\"Performance: Requests per Second vs. Number of Documents\",\n        labels={  # Customizing axis labels\n            \"num_docs\": \"Number of Documents\",\n            \"requests_per_second\": \"Requests per Second\",\n            \"max_workers\": \"max_workers\",\n            \"max_queue_size\": \"max_queue_size\",\n        },\n        template=\"plotly_white\",  # This sets the style to a white background, adhering to Tufte's minimalist principles\n        hover_data=[\n            \"max_workers\",\n            \"max_queue_size\",\n            \"max_connections\",\n        ],  # Additional information to show on hover\n    )\n\n    # Update layout for better readability, similar to 'talk' context in Seaborn\n    fig.update_layout(\n        font=dict(\n            size=16,  # Adjusting font size for better visibility, similar to 'talk' context\n        ),\n        legend_title_text=\"Function Details\",  # Custom legend title\n        legend=dict(\n            title_font_size=16,\n            x=800,  # Adjusting legend position similar to bbox_to_anchor in Matplotlib\n            xanchor=\"auto\",\n            y=1,\n            yanchor=\"auto\",\n        ),\n        width=800,  # Adjusting width of the plot\n    )\n    fig.update_xaxes(\n        tickvals=[1000, 5000, 10000],  # Set specific tick values\n        ticktext=[\"1k\", \"5k\", \"10k\"],  # Set corresponding tick labels\n    )\n\n    fig.update_traces(\n        marker=dict(size=12, opacity=0.7)\n    )  # Adjust marker size and opacity\n    # Show plot\n    fig.show()\n    # Save plot as HTML file\n    fig.write_html(\"performance.html\")\n\n\nplot_performance(df)\n</pre> import plotly.express as px   def plot_performance(df: pd.DataFrame):     # Create a scatter plot with logarithmic scale for both axes using Plotly Express     fig = px.scatter(         df,         x=\"num_docs\",         y=\"requests_per_second\",         color=\"function_name\",  # Defines color based on different functions         log_x=True,  # Set x-axis to logarithmic scale         log_y=False,  # If you also want the y-axis in logarithmic scale, set this to True         title=\"Performance: Requests per Second vs. Number of Documents\",         labels={  # Customizing axis labels             \"num_docs\": \"Number of Documents\",             \"requests_per_second\": \"Requests per Second\",             \"max_workers\": \"max_workers\",             \"max_queue_size\": \"max_queue_size\",         },         template=\"plotly_white\",  # This sets the style to a white background, adhering to Tufte's minimalist principles         hover_data=[             \"max_workers\",             \"max_queue_size\",             \"max_connections\",         ],  # Additional information to show on hover     )      # Update layout for better readability, similar to 'talk' context in Seaborn     fig.update_layout(         font=dict(             size=16,  # Adjusting font size for better visibility, similar to 'talk' context         ),         legend_title_text=\"Function Details\",  # Custom legend title         legend=dict(             title_font_size=16,             x=800,  # Adjusting legend position similar to bbox_to_anchor in Matplotlib             xanchor=\"auto\",             y=1,             yanchor=\"auto\",         ),         width=800,  # Adjusting width of the plot     )     fig.update_xaxes(         tickvals=[1000, 5000, 10000],  # Set specific tick values         ticktext=[\"1k\", \"5k\", \"10k\"],  # Set corresponding tick labels     )      fig.update_traces(         marker=dict(size=12, opacity=0.7)     )  # Adjust marker size and opacity     # Show plot     fig.show()     # Save plot as HTML file     fig.write_html(\"performance.html\")   plot_performance(df) <p>Interesting. Let's try to summarize the insights we got from this experiment:</p> <ul> <li>The <code>feed_async_iterable</code> method is approximately 3x faster than the <code>feed_iterable</code> method for this specific setup.</li> <li>Note that this will vary depending on the network latency between the client and the Vespa instance.</li> <li>If you are feeding from a cloud instance with less latency to the Vespa instance, the difference between the methods will be less, and the <code>feed_iterable</code> method might even be faster.</li> </ul> <ul> <li>Still prefer to use the Vespa CLI if you really care about performance. \ud83d\ude80</li> <li>If you want to use pyvespa, prefer the <code>feed_async_iterable</code>- method, if you are I/O-bound.</li> </ul> In\u00a0[26]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-team.feedperformancecloud in dev.aws-us-east-1c\nDeleted instance vespa-team.feedperformancecloud.default\n</pre>"},{"location":"examples/feed_performance_cloud.html#feeding-to-vespa-cloud","title":"Feeding to Vespa Cloud\u00b6","text":"<p>Our previous notebook, we demonstrated one way of benchmarking feed performance to a local Vespa instance running in Docker. In this notebook, we will look at the same methods but how feeding to Vespa Cloud affects the performance of the different methods.</p> <p>The key difference between feeding to a local Vespa instance and a Vespa Cloud instance is the network latency. Additionally, we will introduce embedding in Vespa at feed time, which is a realistic scenario for many use cases.</p> <p>We will look at these 3 different methods:</p> <ol> <li>Using <code>feed_iterable()</code> - which uses threading to parallelize the feed operation. Best for CPU-bound operations.</li> <li>Using <code>feed_async_iterable()</code> - which uses asyncio to parallelize the feed operation. Also uses <code>httpx</code> with HTTP/2-support. Performs best for IO-bound operations.</li> <li>Using Vespa CLI.</li> </ol>"},{"location":"examples/feed_performance_cloud.html#create-an-application-package","title":"Create an application package\u00b6","text":"<p>The application package has all the Vespa configuration files.</p> <p>For this demo, we will use a simple application package</p>"},{"location":"examples/feed_performance_cloud.html#deploy-the-vespa-application","title":"Deploy the Vespa application\u00b6","text":"<p>Deploy <code>package</code> on the local machine using Docker, without leaving the notebook, by creating an instance of VespaDocker. <code>VespaDocker</code> connects to the local Docker daemon socket and starts the Vespa docker image.</p> <p>If this step fails, please check that the Docker daemon is running, and that the Docker daemon socket can be used by clients (Configurable under advanced settings in Docker Desktop).</p>"},{"location":"examples/feed_performance_cloud.html#preparing-the-data","title":"Preparing the data\u00b6","text":"<p>In this example we use HF Datasets library to stream the \"Cohere/wikipedia-2023-11-embed-multilingual-v3\" dataset and index in our newly deployed Vespa instance.</p> <p>The dataset contains Wikipedia-pages, and their corresponding embeddings.</p> <p>For this exploration, we will use the <code>id</code>, <code>text</code> and <code>embedding</code>-fields</p> <p>The following uses the stream option of datasets to stream the data without downloading all the contents locally.</p> <p>The <code>map</code> functionality allows us to convert the dataset fields into the expected feed format for <code>pyvespa</code> which expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p>"},{"location":"examples/feed_performance_cloud.html#utility-function-to-create-a-dataset-with-different-number-of-documents","title":"Utility function to create a dataset with different number of documents\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#a-dataclass-to-store-the-parameters-and-results-of-the-different-feeding-methods","title":"A dataclass to store the parameters and results of the different feeding methods\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#a-common-callback-function-to-notify-if-something-goes-wrong","title":"A common callback function to notify if something goes wrong\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#defining-our-feeding-functions","title":"Defining our feeding functions\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#defining-our-hyperparameters","title":"Defining our hyperparameters\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#function-to-clean-up-after-each-feed","title":"Function to clean up after each feed\u00b6","text":"<p>For a fair comparison, we will delete the data before feeding it again.</p>"},{"location":"examples/feed_performance_cloud.html#main-experiment-loop","title":"Main experiment loop\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#plotting-the-results","title":"Plotting the results\u00b6","text":"<p>Let's plot the results to see how the different methods compare.</p>"},{"location":"examples/feed_performance_cloud.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/feed_performance_cloud.html#next-steps","title":"Next steps\u00b6","text":"<p>Check out some of the other examples in the documentation.</p>"},{"location":"examples/lightgbm-with-categorical-mapping.html","title":"Lightgbm with categorical mapping","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install and load required packages.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install numpy pandas pyvespa lightgbm\n</pre> !pip3 install numpy pandas pyvespa lightgbm In\u00a0[3]: Copied! <pre>import json\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n</pre> import json import lightgbm as lgb import numpy as np import pandas as pd <p>Simulate data that will be used to train the LightGBM model. Note that Vespa does not automatically recognize the feature names <code>feature_1</code>, <code>feature_2</code> and <code>feature_3</code>. When creating the application package we need to map those variables to something that the Vespa application recognizes, such as a document attribute or query value.</p> In\u00a0[4]: Copied! <pre># Create random training set\nfeatures = pd.DataFrame(\n    {\n        \"feature_1\": np.random.random(100),\n        \"feature_2\": np.random.random(100),\n        \"feature_3\": pd.Series(\n            np.random.choice([\"a\", \"b\", \"c\"], size=100), dtype=\"category\"\n        ),\n    }\n)\nfeatures.head()\n</pre> # Create random training set features = pd.DataFrame(     {         \"feature_1\": np.random.random(100),         \"feature_2\": np.random.random(100),         \"feature_3\": pd.Series(             np.random.choice([\"a\", \"b\", \"c\"], size=100), dtype=\"category\"         ),     } ) features.head() Out[4]: feature_1 feature_2 feature_3 0 0.856415 0.550705 a 1 0.615107 0.509030 a 2 0.089759 0.667729 c 3 0.161664 0.361693 b 4 0.841505 0.967227 b <p>Create a target variable that depends on <code>feature_1</code>, <code>feature_2</code> and <code>feature_3</code>:</p> In\u00a0[5]: Copied! <pre>numeric_features = pd.get_dummies(features)\ntargets = (\n    (\n        numeric_features[\"feature_1\"]\n        + numeric_features[\"feature_2\"]\n        - 0.5 * numeric_features[\"feature_3_a\"]\n        + 0.5 * numeric_features[\"feature_3_c\"]\n    )\n    &gt; 1.0\n) * 1.0\ntargets\n</pre> numeric_features = pd.get_dummies(features) targets = (     (         numeric_features[\"feature_1\"]         + numeric_features[\"feature_2\"]         - 0.5 * numeric_features[\"feature_3_a\"]         + 0.5 * numeric_features[\"feature_3_c\"]     )     &gt; 1.0 ) * 1.0 targets Out[5]: <pre>0     0.0\n1     0.0\n2     1.0\n3     0.0\n4     1.0\n     ... \n95    1.0\n96    1.0\n97    0.0\n98    1.0\n99    1.0\nLength: 100, dtype: float64</pre> <p>Train the LightGBM model on the simulated data,</p> In\u00a0[6]: Copied! <pre>training_set = lgb.Dataset(features, targets)\n\n# Train the model\nparams = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"num_leaves\": 3,\n}\nmodel = lgb.train(params, training_set, num_boost_round=5)\n</pre> training_set = lgb.Dataset(features, targets)  # Train the model params = {     \"objective\": \"binary\",     \"metric\": \"binary_logloss\",     \"num_leaves\": 3, } model = lgb.train(params, training_set, num_boost_round=5) <pre>[LightGBM] [Info] Number of positive: 48, number of negative: 52\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000404 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 74\n[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.480000 -&gt; initscore=-0.080043\n[LightGBM] [Info] Start training from score -0.080043\n</pre> <p>Create the application package and map the LightGBM feature names to the related Vespa names.</p> <p>In this example we are going to assume that <code>feature_1</code> represents the document field <code>numeric</code> and map <code>feature_1</code> to <code>attribute(numeric)</code> through the use of a Vespa <code>Function</code> in the corresponding <code>RankProfile</code>. <code>feature_2</code> maps to a <code>value</code> that will be sent along with the query, and this is represented in Vespa by mapping <code>query(value)</code> to <code>feature_2</code>. Lastly, the categorical feature is mapped from <code>attribute(categorical)</code> to <code>feature_3</code>.</p> In\u00a0[7]: Copied! <pre>from vespa.package import ApplicationPackage, Field, RankProfile, Function\n\napp_package = ApplicationPackage(name=\"lightgbm\")\napp_package.schema.add_fields(\n    Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"numeric\", type=\"double\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"categorical\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n)\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"classify\",\n        functions=[\n            Function(name=\"feature_1\", expression=\"attribute(numeric)\"),\n            Function(name=\"feature_2\", expression=\"query(value)\"),\n            Function(name=\"feature_3\", expression=\"attribute(categorical)\"),\n        ],\n        first_phase=\"lightgbm('lightgbm_model.json')\",\n    )\n)\n</pre> from vespa.package import ApplicationPackage, Field, RankProfile, Function  app_package = ApplicationPackage(name=\"lightgbm\") app_package.schema.add_fields(     Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),     Field(name=\"numeric\", type=\"double\", indexing=[\"summary\", \"attribute\"]),     Field(name=\"categorical\", type=\"string\", indexing=[\"summary\", \"attribute\"]), ) app_package.schema.add_rank_profile(     RankProfile(         name=\"classify\",         functions=[             Function(name=\"feature_1\", expression=\"attribute(numeric)\"),             Function(name=\"feature_2\", expression=\"query(value)\"),             Function(name=\"feature_3\", expression=\"attribute(categorical)\"),         ],         first_phase=\"lightgbm('lightgbm_model.json')\",     ) ) <p>We can check how the Vespa search defition file will look like. Note that <code>feature_1</code>, <code>feature_2</code> and <code>feature_3</code> required by the LightGBM model are now defined on the schema definition:</p> In\u00a0[8]: Copied! <pre>print(app_package.schema.schema_to_text)\n</pre> print(app_package.schema.schema_to_text) <pre>schema lightgbm {\n    document lightgbm {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field numeric type double {\n            indexing: summary | attribute\n        }\n        field categorical type string {\n            indexing: summary | attribute\n        }\n    }\n    rank-profile classify {\n        function feature_1() {\n            expression {\n                attribute(numeric)\n            }\n        }\n        function feature_2() {\n            expression {\n                query(value)\n            }\n        }\n        function feature_3() {\n            expression {\n                attribute(categorical)\n            }\n        }\n        first-phase {\n            expression {\n                lightgbm('lightgbm_model.json')\n            }\n        }\n    }\n}\n</pre> <p>We can export the application package files to disk:</p> In\u00a0[9]: Copied! <pre>from pathlib import Path\n\nPath(\"lightgbm\").mkdir(parents=True, exist_ok=True)\napp_package.to_files(\"lightgbm\")\n</pre> from pathlib import Path  Path(\"lightgbm\").mkdir(parents=True, exist_ok=True) app_package.to_files(\"lightgbm\") <p>Note that we don't have any models under the <code>models</code> folder. We need to export the lightGBM model that we trained earlier to <code>models/lightgbm.json</code>.</p> In\u00a0[13]: Copied! <pre>!tree lightgbm\n</pre> !tree lightgbm <pre>lightgbm\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm_model.json\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm.sd\n\u251c\u2500\u2500 search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 query-profiles\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 default.xml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 types\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 root.xml\n\u2514\u2500\u2500 services.xml\n\n7 directories, 5 files\n</pre> In\u00a0[12]: Copied! <pre>with open(\"lightgbm/models/lightgbm_model.json\", \"w\") as f:\n    json.dump(model.dump_model(), f, indent=2)\n</pre> with open(\"lightgbm/models/lightgbm_model.json\", \"w\") as f:     json.dump(model.dump_model(), f, indent=2) <p>Now we can see that the model is where Vespa expects it to be:</p> In\u00a0[14]: Copied! <pre>!tree lightgbm\n</pre> !tree lightgbm <pre>lightgbm\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm_model.json\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm.sd\n\u251c\u2500\u2500 search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 query-profiles\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 default.xml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 types\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 root.xml\n\u2514\u2500\u2500 services.xml\n\n7 directories, 5 files\n</pre> <p>Deploy the application package from disk with Docker:</p> In\u00a0[15]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy_from_disk(\n    application_name=\"lightgbm\", application_root=\"lightgbm\"\n)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy_from_disk(     application_name=\"lightgbm\", application_root=\"lightgbm\" ) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 25/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Feed the simulated data. To feed data in batch we need to create a list of dictionaries containing id and fields keys:</p> In\u00a0[16]: Copied! <pre>feed_batch = [\n    {\n        \"id\": idx,\n        \"fields\": {\n            \"id\": idx,\n            \"numeric\": row[\"feature_1\"],\n            \"categorical\": row[\"feature_3\"],\n        },\n    }\n    for idx, row in features.iterrows()\n]\n</pre> feed_batch = [     {         \"id\": idx,         \"fields\": {             \"id\": idx,             \"numeric\": row[\"feature_1\"],             \"categorical\": row[\"feature_3\"],         },     }     for idx, row in features.iterrows() ] In\u00a0[17]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Document {id} was not fed to Vespa due to error: {response.get_json()}\")\n\n\napp.feed_iterable(feed_batch, callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Document {id} was not fed to Vespa due to error: {response.get_json()}\")   app.feed_iterable(feed_batch, callback=callback) <p>Predict with the trained LightGBM model so that we can later compare with the predictions returned by Vespa.</p> In\u00a0[18]: Copied! <pre>features[\"model_prediction\"] = model.predict(features)\n</pre> features[\"model_prediction\"] = model.predict(features) In\u00a0[19]: Copied! <pre>features\n</pre> features Out[19]: feature_1 feature_2 feature_3 model_prediction 0 0.856415 0.550705 a 0.402572 1 0.615107 0.509030 a 0.356262 2 0.089759 0.667729 c 0.641578 3 0.161664 0.361693 b 0.388184 4 0.841505 0.967227 b 0.632525 ... ... ... ... ... 95 0.087768 0.451850 c 0.641578 96 0.839063 0.644387 b 0.632525 97 0.725573 0.327668 a 0.376350 98 0.937481 0.199995 b 0.376350 99 0.918530 0.734004 a 0.402572 <p>100 rows \u00d7 4 columns</p> <p>Create a <code>compute_vespa_relevance</code> function that takes a document <code>id</code> and a query <code>value</code> and return the LightGBM model deployed.</p> In\u00a0[20]: Copied! <pre>def compute_vespa_relevance(id_value: int):\n    hits = app.query(\n        body={\n            \"yql\": \"select * from sources * where id = {}\".format(str(id_value)),\n            \"ranking\": \"classify\",\n            \"ranking.features.query(value)\": features.loc[id_value, \"feature_2\"],\n            \"hits\": 1,\n        }\n    ).hits\n    return hits[0][\"relevance\"]\n\n\ncompute_vespa_relevance(id_value=0)\n</pre> def compute_vespa_relevance(id_value: int):     hits = app.query(         body={             \"yql\": \"select * from sources * where id = {}\".format(str(id_value)),             \"ranking\": \"classify\",             \"ranking.features.query(value)\": features.loc[id_value, \"feature_2\"],             \"hits\": 1,         }     ).hits     return hits[0][\"relevance\"]   compute_vespa_relevance(id_value=0) Out[20]: <pre>0.4025720849980601</pre> <p>Loop through the <code>features</code> to compute a vespa prediction for all the data points, so that we can compare it to the predictions made by the model outside Vespa.</p> In\u00a0[21]: Copied! <pre>vespa_relevance = []\nfor idx, row in features.iterrows():\n    vespa_relevance.append(compute_vespa_relevance(id_value=idx))\nfeatures[\"vespa_relevance\"] = vespa_relevance\n</pre> vespa_relevance = [] for idx, row in features.iterrows():     vespa_relevance.append(compute_vespa_relevance(id_value=idx)) features[\"vespa_relevance\"] = vespa_relevance In\u00a0[22]: Copied! <pre>features\n</pre> features Out[22]: feature_1 feature_2 feature_3 model_prediction vespa_relevance 0 0.856415 0.550705 a 0.402572 0.402572 1 0.615107 0.509030 a 0.356262 0.356262 2 0.089759 0.667729 c 0.641578 0.641578 3 0.161664 0.361693 b 0.388184 0.388184 4 0.841505 0.967227 b 0.632525 0.632525 ... ... ... ... ... ... 95 0.087768 0.451850 c 0.641578 0.641578 96 0.839063 0.644387 b 0.632525 0.632525 97 0.725573 0.327668 a 0.376350 0.376350 98 0.937481 0.199995 b 0.376350 0.376350 99 0.918530 0.734004 a 0.402572 0.402572 <p>100 rows \u00d7 5 columns</p> <p>Predictions from the model should be equal to predictions from Vespa, showing the model was correctly deployed to Vespa.</p> In\u00a0[23]: Copied! <pre>assert features[\"model_prediction\"].tolist() == features[\"vespa_relevance\"].tolist()\n</pre> assert features[\"model_prediction\"].tolist() == features[\"vespa_relevance\"].tolist() In\u00a0[24]: Copied! <pre>!rm -fr lightgbm\nvespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> !rm -fr lightgbm vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/lightgbm-with-categorical-mapping.html#lightgbm-mapping-model-features-to-vespa-features","title":"LightGBM: Mapping model features to Vespa features\u00b6","text":"<p>The main goal of this tutorial is to show how to deploy a LightGBM model with feature names that do not match Vespa feature names.</p> <p>The following tasks will be accomplished throughout the tutorial:</p> <ol> <li>Train a LightGBM classification model with generic feature names that will not be available in the Vespa application.</li> <li>Create an application package and include a mapping from Vespa feature names to LightGBM model feature names.</li> <li>Create Vespa application package files and export then to an application folder.</li> <li>Export the trained LightGBM model to the Vespa application folder.</li> <li>Deploy the Vespa application using the application folder.</li> <li>Feed data to the Vespa application.</li> <li>Assert that the LightGBM predictions from the deployed model are correct.</li> </ol>"},{"location":"examples/lightgbm-with-categorical-mapping.html#setup","title":"Setup\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#create-data","title":"Create data\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#fit-lightgbm-model","title":"Fit lightgbm model\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#vespa-application-package","title":"Vespa application package\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#export-the-model","title":"Export the model\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#deploy-the-application","title":"Deploy the application\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#feed-the-data","title":"Feed the data\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#model-predictions","title":"Model predictions\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#query","title":"Query\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#compare-model-and-vespa-predictions","title":"Compare model and Vespa predictions\u00b6","text":""},{"location":"examples/lightgbm-with-categorical-mapping.html#clean-environment","title":"Clean environment\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html","title":"Lightgbm with categorical","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Install and load required packages.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install numpy pandas pyvespa lightgbm\n</pre> !pip3 install numpy pandas pyvespa lightgbm In\u00a0[3]: Copied! <pre>import json\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n</pre> import json import lightgbm as lgb import numpy as np import pandas as pd <p>Generate a toy dataset to follow along. Note that we set the column names in a format that Vespa understands. <code>query(value)</code> means that the user will send a parameter named <code>value</code> along with the query. <code>attribute(field)</code> means that <code>field</code> is a document attribute defined in a schema. In the example below we have a query parameter named <code>value</code> and two document's attributes, <code>numeric</code> and <code>categorical</code>. If we want <code>lightgbm</code> to handle categorical variables we should use <code>dtype=\"category\"</code> when creating the dataframe, as shown below.</p> In\u00a0[4]: Copied! <pre># Create random training set\nfeatures = pd.DataFrame(\n    {\n        \"query(value)\": np.random.random(100),\n        \"attribute(numeric)\": np.random.random(100),\n        \"attribute(categorical)\": pd.Series(\n            np.random.choice([\"a\", \"b\", \"c\"], size=100), dtype=\"category\"\n        ),\n    }\n)\nfeatures.head()\n</pre> # Create random training set features = pd.DataFrame(     {         \"query(value)\": np.random.random(100),         \"attribute(numeric)\": np.random.random(100),         \"attribute(categorical)\": pd.Series(             np.random.choice([\"a\", \"b\", \"c\"], size=100), dtype=\"category\"         ),     } ) features.head() Out[4]: query(value) attribute(numeric) attribute(categorical) 0 0.437748 0.442222 c 1 0.957135 0.323047 b 2 0.514168 0.426117 a 3 0.713511 0.886630 b 4 0.626918 0.663179 c <p>We generate the target variable as a function of the three features defined above:</p> In\u00a0[5]: Copied! <pre>numeric_features = pd.get_dummies(features)\ntargets = (\n    (\n        numeric_features[\"query(value)\"]\n        + numeric_features[\"attribute(numeric)\"]\n        - 0.5 * numeric_features[\"attribute(categorical)_a\"]\n        + 0.5 * numeric_features[\"attribute(categorical)_c\"]\n    )\n    &gt; 1.0\n) * 1.0\ntargets\n</pre> numeric_features = pd.get_dummies(features) targets = (     (         numeric_features[\"query(value)\"]         + numeric_features[\"attribute(numeric)\"]         - 0.5 * numeric_features[\"attribute(categorical)_a\"]         + 0.5 * numeric_features[\"attribute(categorical)_c\"]     )     &gt; 1.0 ) * 1.0 targets Out[5]: <pre>0     1.0\n1     1.0\n2     0.0\n3     1.0\n4     1.0\n     ... \n95    0.0\n96    1.0\n97    0.0\n98    0.0\n99    1.0\nLength: 100, dtype: float64</pre> <p>Train an LightGBM model with a binary loss function:</p> In\u00a0[6]: Copied! <pre>training_set = lgb.Dataset(features, targets)\n\n# Train the model\nparams = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"num_leaves\": 3,\n}\nmodel = lgb.train(params, training_set, num_boost_round=5)\n</pre> training_set = lgb.Dataset(features, targets)  # Train the model params = {     \"objective\": \"binary\",     \"metric\": \"binary_logloss\",     \"num_leaves\": 3, } model = lgb.train(params, training_set, num_boost_round=5) <pre>[LightGBM] [Info] Number of positive: 48, number of negative: 52\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000484 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 74\n[LightGBM] [Info] Number of data points in the train set: 100, number of used features: 3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.480000 -&gt; initscore=-0.080043\n[LightGBM] [Info] Start training from score -0.080043\n</pre> <p>Create a Vespa application package. The model expects two document attributes, <code>numeric</code> and <code>categorical</code>. We can use the model in the first-phase ranking by using the <code>lightgbm</code> rank feature.</p> In\u00a0[7]: Copied! <pre>from vespa.package import ApplicationPackage, Field, RankProfile\n\napp_package = ApplicationPackage(name=\"lightgbm\")\napp_package.schema.add_fields(\n    Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"numeric\", type=\"double\", indexing=[\"summary\", \"attribute\"]),\n    Field(name=\"categorical\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n)\napp_package.schema.add_rank_profile(\n    RankProfile(name=\"classify\", first_phase=\"lightgbm('lightgbm_model.json')\")\n)\n</pre> from vespa.package import ApplicationPackage, Field, RankProfile  app_package = ApplicationPackage(name=\"lightgbm\") app_package.schema.add_fields(     Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),     Field(name=\"numeric\", type=\"double\", indexing=[\"summary\", \"attribute\"]),     Field(name=\"categorical\", type=\"string\", indexing=[\"summary\", \"attribute\"]), ) app_package.schema.add_rank_profile(     RankProfile(name=\"classify\", first_phase=\"lightgbm('lightgbm_model.json')\") ) <p>We can check how the Vespa search defition file will look like:</p> In\u00a0[8]: Copied! <pre>print(app_package.schema.schema_to_text)\n</pre> print(app_package.schema.schema_to_text) <pre>schema lightgbm {\n    document lightgbm {\n        field id type string {\n            indexing: summary | attribute\n        }\n        field numeric type double {\n            indexing: summary | attribute\n        }\n        field categorical type string {\n            indexing: summary | attribute\n        }\n    }\n    rank-profile classify {\n        first-phase {\n            expression {\n                lightgbm('lightgbm_model.json')\n            }\n        }\n    }\n}\n</pre> <p>We can export the application package files to disk:</p> In\u00a0[9]: Copied! <pre>from pathlib import Path\n\nPath(\"lightgbm\").mkdir(parents=True, exist_ok=True)\napp_package.to_files(\"lightgbm\")\n</pre> from pathlib import Path  Path(\"lightgbm\").mkdir(parents=True, exist_ok=True) app_package.to_files(\"lightgbm\") <p>Note that we don't have any models under the <code>models</code> folder. We need to export the lightGBM model that we trained earlier to <code>models/lightgbm.json</code>.</p> In\u00a0[10]: Copied! <pre>!tree lightgbm\n</pre> !tree lightgbm <pre>lightgbm\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 models\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm.sd\n\u251c\u2500\u2500 search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 query-profiles\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 default.xml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 types\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 root.xml\n\u2514\u2500\u2500 services.xml\n\n7 directories, 4 files\n</pre> In\u00a0[11]: Copied! <pre>with open(\"lightgbm/models/lightgbm_model.json\", \"w\") as f:\n    json.dump(model.dump_model(), f, indent=2)\n</pre> with open(\"lightgbm/models/lightgbm_model.json\", \"w\") as f:     json.dump(model.dump_model(), f, indent=2) <p>Now we can see that the model is where Vespa expects it to be:</p> In\u00a0[12]: Copied! <pre>!tree lightgbm\n</pre> !tree lightgbm <pre>lightgbm\n\u251c\u2500\u2500 files\n\u251c\u2500\u2500 models\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm_model.json\n\u251c\u2500\u2500 schemas\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 lightgbm.sd\n\u251c\u2500\u2500 search\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 query-profiles\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 default.xml\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 types\n\u2502\u00a0\u00a0         \u2514\u2500\u2500 root.xml\n\u2514\u2500\u2500 services.xml\n\n7 directories, 5 files\n</pre> <p>Deploy the application package from disk with Docker:</p> In\u00a0[13]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy_from_disk(\n    application_name=\"lightgbm\", application_root=\"lightgbm\"\n)\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy_from_disk(     application_name=\"lightgbm\", application_root=\"lightgbm\" ) <pre>Waiting for configuration server, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Feed the simulated data. To feed data in batch we need to create a list of dictionaries containing <code>id</code> and <code>fields</code> keys:</p> In\u00a0[14]: Copied! <pre>feed_batch = [\n    {\n        \"id\": idx,\n        \"fields\": {\n            \"id\": idx,\n            \"numeric\": row[\"attribute(numeric)\"],\n            \"categorical\": row[\"attribute(categorical)\"],\n        },\n    }\n    for idx, row in features.iterrows()\n]\n</pre> feed_batch = [     {         \"id\": idx,         \"fields\": {             \"id\": idx,             \"numeric\": row[\"attribute(numeric)\"],             \"categorical\": row[\"attribute(categorical)\"],         },     }     for idx, row in features.iterrows() ] <p>Feed the batch of data:</p> In\u00a0[15]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Document {id} was not fed to Vespa due to error: {response.get_json()}\")\n\n\napp.feed_iterable(feed_batch, callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Document {id} was not fed to Vespa due to error: {response.get_json()}\")   app.feed_iterable(feed_batch, callback=callback) <p>Predict with the trained LightGBM model so that we can later compare with the predictions returned by Vespa.</p> In\u00a0[16]: Copied! <pre>features[\"model_prediction\"] = model.predict(features)\n</pre> features[\"model_prediction\"] = model.predict(features) In\u00a0[17]: Copied! <pre>features\n</pre> features Out[17]: query(value) attribute(numeric) attribute(categorical) model_prediction 0 0.437748 0.442222 c 0.645663 1 0.957135 0.323047 b 0.645663 2 0.514168 0.426117 a 0.354024 3 0.713511 0.886630 b 0.645663 4 0.626918 0.663179 c 0.645663 ... ... ... ... ... 95 0.208583 0.103319 c 0.352136 96 0.882902 0.224213 c 0.645663 97 0.604831 0.675583 a 0.354024 98 0.278674 0.008019 b 0.352136 99 0.417318 0.616241 b 0.645663 <p>100 rows \u00d7 4 columns</p> <p>Create a <code>compute_vespa_relevance</code> function that takes a document <code>id</code> and a query <code>value</code> and return the LightGBM model deployed.</p> In\u00a0[18]: Copied! <pre>def compute_vespa_relevance(id_value: int):\n    hits = app.query(\n        body={\n            \"yql\": \"select * from sources * where id = {}\".format(str(id_value)),\n            \"ranking\": \"classify\",\n            \"ranking.features.query(value)\": features.loc[id_value, \"query(value)\"],\n            \"hits\": 1,\n        }\n    ).hits\n    return hits[0][\"relevance\"]\n\n\ncompute_vespa_relevance(id_value=0)\n</pre> def compute_vespa_relevance(id_value: int):     hits = app.query(         body={             \"yql\": \"select * from sources * where id = {}\".format(str(id_value)),             \"ranking\": \"classify\",             \"ranking.features.query(value)\": features.loc[id_value, \"query(value)\"],             \"hits\": 1,         }     ).hits     return hits[0][\"relevance\"]   compute_vespa_relevance(id_value=0) Out[18]: <pre>0.645662636917761</pre> <p>Loop through the <code>features</code> to compute a vespa prediction for all the data points, so that we can compare it to the predictions made by the model outside Vespa.</p> In\u00a0[19]: Copied! <pre>vespa_relevance = []\nfor idx, row in features.iterrows():\n    vespa_relevance.append(compute_vespa_relevance(id_value=idx))\nfeatures[\"vespa_relevance\"] = vespa_relevance\n</pre> vespa_relevance = [] for idx, row in features.iterrows():     vespa_relevance.append(compute_vespa_relevance(id_value=idx)) features[\"vespa_relevance\"] = vespa_relevance In\u00a0[20]: Copied! <pre>features\n</pre> features Out[20]: query(value) attribute(numeric) attribute(categorical) model_prediction vespa_relevance 0 0.437748 0.442222 c 0.645663 0.645663 1 0.957135 0.323047 b 0.645663 0.645663 2 0.514168 0.426117 a 0.354024 0.354024 3 0.713511 0.886630 b 0.645663 0.645663 4 0.626918 0.663179 c 0.645663 0.645663 ... ... ... ... ... ... 95 0.208583 0.103319 c 0.352136 0.352136 96 0.882902 0.224213 c 0.645663 0.645663 97 0.604831 0.675583 a 0.354024 0.354024 98 0.278674 0.008019 b 0.352136 0.352136 99 0.417318 0.616241 b 0.645663 0.645663 <p>100 rows \u00d7 5 columns</p> <p>Predictions from the model should be equal to predictions from Vespa, showing the model was correctly deployed to Vespa.</p> In\u00a0[21]: Copied! <pre>assert features[\"model_prediction\"].tolist() == features[\"vespa_relevance\"].tolist()\n</pre> assert features[\"model_prediction\"].tolist() == features[\"vespa_relevance\"].tolist() In\u00a0[22]: Copied! <pre>!rm -fr lightgbm\nvespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> !rm -fr lightgbm vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/lightgbm-with-categorical.html#lightgbm-training-the-model-with-vespa-features","title":"LightGBM: Training the model with Vespa features\u00b6","text":"<p>The main goal of this tutorial is to deploy and use a LightGBM model in a Vespa application. The following tasks will be accomplished throughout the tutorial:</p> <ol> <li>Train a LightGBM classification model with variable names supported by Vespa.</li> <li>Create Vespa application package files and export then to an application folder.</li> <li>Export the trained LightGBM model to the Vespa application folder.</li> <li>Deploy the Vespa application using the application folder.</li> <li>Feed data to the Vespa application.</li> <li>Assert that the LightGBM predictions from the deployed model are correct.</li> </ol>"},{"location":"examples/lightgbm-with-categorical.html#setup","title":"Setup\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#create-data","title":"Create data\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#fit-lightgbm-model","title":"Fit lightgbm model\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#vespa-application-package","title":"Vespa application package\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#export-the-model","title":"Export the model\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#deploy-the-application","title":"Deploy the application\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#feed-the-data","title":"Feed the data\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#model-predictions","title":"Model predictions\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#query","title":"Query\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#compare-model-and-vespa-predictions","title":"Compare model and Vespa predictions\u00b6","text":""},{"location":"examples/lightgbm-with-categorical.html#clean-environment","title":"Clean environment\u00b6","text":""},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html","title":"Mixedbread binary embeddings with sentence transformers cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa sentence-transformers vespacli\n</pre> !pip3 install -U pyvespa sentence-transformers vespacli In\u00a0[1]: Copied! <pre>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\n    \"mixedbread-ai/mxbai-embed-large-v1\",\n    prompts={\n        \"retrieval\": \"Represent this sentence for searching relevant passages: \",\n    },\n    default_prompt_name=\"retrieval\",\n)\n</pre> from sentence_transformers import SentenceTransformer  model = SentenceTransformer(     \"mixedbread-ai/mxbai-embed-large-v1\",     prompts={         \"retrieval\": \"Represent this sentence for searching relevant passages: \",     },     default_prompt_name=\"retrieval\", ) <pre>Default prompt name is set to 'retrieval'. This prompt will be applied to all `encode()` calls, except if `encode()` is called with `prompt` or `prompt_name` parameters.\n</pre> In\u00a0[4]: Copied! <pre>documents = [\n    \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",\n    \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",\n    \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",\n    \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\",\n]\n</pre> documents = [     \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\",     \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\",     \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\",     \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\", ] <p>Run embedding inference, notice how we specify <code>precision=\"binary\"</code>.</p> In\u00a0[5]: Copied! <pre>binary_embeddings = model.encode(documents, precision=\"binary\")\n</pre> binary_embeddings = model.encode(documents, precision=\"binary\") In\u00a0[8]: Copied! <pre>print(\n    \"Binary embedding shape {} with type {}\".format(\n        binary_embeddings.shape, binary_embeddings.dtype\n    )\n)\n</pre> print(     \"Binary embedding shape {} with type {}\".format(         binary_embeddings.shape, binary_embeddings.dtype     ) ) <pre>Binary embedding shape (4, 128) with type int8\n</pre> In\u00a0[9]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"doc\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(\n                name=\"doc_id\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"word\"],\n                rank=\"filter\",\n            ),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"binary_vector\",\n                type=\"tensor&lt;int8&gt;(x[128])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: hamming\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"doc\",     mode=\"index\",     document=Document(         fields=[             Field(                 name=\"doc_id\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"word\"],                 rank=\"filter\",             ),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"binary_vector\",                 type=\"tensor(x[128])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: hamming\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])], ) <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[15]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"mixedbreadai\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"mixedbreadai\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p><code>unpack_bits</code> unpacks the binary representation into a 1024-dimensional float vector doc.</p> <p>We define two tensor inputs, one compact binary representation that is used for the nearestNeighbor search and one full version that is used in ranking.</p> In\u00a0[16]: Copied! <pre>from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q_binary)\", \"tensor&lt;int8&gt;(x[128])\"),\n        (\"query(q_full)\", \"tensor&lt;float&gt;(x[1024])\"),\n    ],\n    functions=[\n        Function(  # this returns a tensor&lt;float&gt;(x[1024]) with values -1 or 1\n            name=\"unpack_binary_representation\",\n            expression=\"2*unpack_bits(attribute(binary_vector)) -1\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"closeness(field, binary_vector)\"  # 1/(1 + hamming_distance). Calculated between the binary query and the binary_vector\n    ),\n    second_phase=SecondPhaseRanking(\n        expression=\"sum( query(q_full)* unpack_binary_representation )\",  # re-rank using the dot product between float query and the unpacked binary representation\n        rerank_count=100,\n    ),\n    match_features=[\"distance(field, binary_vector)\"],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q_binary)\", \"tensor(x[128])\"),         (\"query(q_full)\", \"tensor(x[1024])\"),     ],     functions=[         Function(  # this returns a tensor(x[1024]) with values -1 or 1             name=\"unpack_binary_representation\",             expression=\"2*unpack_bits(attribute(binary_vector)) -1\",         )     ],     first_phase=FirstPhaseRanking(         expression=\"closeness(field, binary_vector)\"  # 1/(1 + hamming_distance). Calculated between the binary query and the binary_vector     ),     second_phase=SecondPhaseRanking(         expression=\"sum( query(q_full)* unpack_binary_representation )\",  # re-rank using the dot product between float query and the unpacked binary representation         rerank_count=100,     ),     match_features=[\"distance(field, binary_vector)\"], ) my_schema.add_rank_profile(rerank) In\u00a0[22]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[23]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for samples.mixedbreadai. This may take a few minutes the first time.\nINFO    [22:14:39]  Deploying platform version 8.322.22 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [22:14:39]  Using CA signed certificate version 0\nINFO    [22:14:46]  Using 1 nodes in container cluster 'mixedbreadai_container'\nINFO    [22:15:18]  Session 2205 for tenant 'samples' prepared and activated.\nINFO    [22:15:21]  ######## Details for all nodes ########\nINFO    [22:15:35]  h90193a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:15:35]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nINFO    [22:15:35]  --- logserver-container on port 4080 has not started \nINFO    [22:15:35]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:15:35]  h90971b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:15:35]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nINFO    [22:15:35]  --- container-clustercontroller on port 19050 has not started \nINFO    [22:15:35]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:15:35]  h91168a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:15:35]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nINFO    [22:15:35]  --- storagenode on port 19102 has not started \nINFO    [22:15:35]  --- searchnode on port 19107 has not started \nINFO    [22:15:35]  --- distributor on port 19111 has not started \nINFO    [22:15:35]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:15:35]  h91567a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:15:35]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nINFO    [22:15:35]  --- container on port 4080 has not started \nINFO    [22:15:35]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:16:41]  Waiting for convergence of 10 services across 4 nodes\nINFO    [22:16:41]  1/1 nodes upgrading platform\nINFO    [22:16:41]  2 application services still deploying\nDEBUG   [22:16:41]  h91567a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nDEBUG   [22:16:41]  --- platform vespa/cloud-tenant-rhel8:8.322.22 &lt;-- :\nDEBUG   [22:16:41]  --- container on port 4080 has not started \nDEBUG   [22:16:41]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:17:11]  Found endpoints:\nINFO    [22:17:11]  - dev.aws-us-east-1c\nINFO    [22:17:11]   |-- https://cf949f23.b8a7f611.z.vespa-app.cloud/ (cluster 'mixedbreadai_container')\nINFO    [22:17:12]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://cf949f23.b8a7f611.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[24]: Copied! <pre>from vespa.io import VespaResponse\n\nfor i, doc in enumerate(documents):\n    response: VespaResponse = app.feed_data_point(\n        schema=\"doc\",\n        data_id=str(i),\n        fields={\n            \"doc_id\": str(i),\n            \"text\": doc,\n            \"binary_vector\": binary_embeddings[i].tolist(),\n        },\n    )\n    assert response.is_successful()\n</pre> from vespa.io import VespaResponse  for i, doc in enumerate(documents):     response: VespaResponse = app.feed_data_point(         schema=\"doc\",         data_id=str(i),         fields={             \"doc_id\": str(i),             \"text\": doc,             \"binary_vector\": binary_embeddings[i].tolist(),         },     )     assert response.is_successful() In\u00a0[54]: Copied! <pre>query = \"Who was Isac Newton?\"\n# This returns the float version\nquery_embedding_float = model.encode([query])\n</pre> query = \"Who was Isac Newton?\" # This returns the float version query_embedding_float = model.encode([query]) In\u00a0[\u00a0]: Copied! <pre>from sentence_transformers.quantization import quantize_embeddings\n\nquery_embedding_binary = quantize_embeddings(query_embedding_float, precision=\"binary\")\n</pre> from sentence_transformers.quantization import quantize_embeddings  query_embedding_binary = quantize_embeddings(query_embedding_float, precision=\"binary\") <p>Now, we use nearestNeighbor search to retrieve 100 hits (<code>targetHits</code>) using the configured distance-metric (hamming distance). The retrieved hits are exposed to the \u2039espa ranking framework, where we re-rank using the dot product between the float tensor and the unpacked binary vector.</p> In\u00a0[55]: Copied! <pre>response = app.query(\n    yql=\"select * from doc where {targetHits:100}nearestNeighbor(binary_vector,q_binary)\",\n    ranking=\"rerank\",\n    body={\n        \"input.query(q_binary)\": query_embedding_binary[0].tolist(),\n        \"input.query(q_full)\": query_embedding_float[0].tolist(),\n    },\n)\nassert response.is_successful()\n</pre> response = app.query(     yql=\"select * from doc where {targetHits:100}nearestNeighbor(binary_vector,q_binary)\",     ranking=\"rerank\",     body={         \"input.query(q_binary)\": query_embedding_binary[0].tolist(),         \"input.query(q_full)\": query_embedding_float[0].tolist(),     }, ) assert response.is_successful() In\u00a0[56]: Copied! <pre>import json\n\nprint(json.dumps(response.hits, indent=2))\n</pre> import json  print(json.dumps(response.hits, indent=2)) <pre>[\n  {\n    \"id\": \"id:doc:doc::2\",\n    \"relevance\": 177.8957977294922,\n    \"source\": \"mixedbreadai_content\",\n    \"fields\": {\n      \"matchfeatures\": {\n        \"closeness(field,binary_vector)\": 0.003484320557491289,\n        \"distance(field,binary_vector)\": 286.0\n      },\n      \"sddocname\": \"doc\",\n      \"documentid\": \"id:doc:doc::2\",\n      \"doc_id\": \"2\",\n      \"text\": \"Isaac Newton was an English polymath active as a mathematician, physicist, astronomer, alchemist, theologian, and author who was described in his time as a natural philosopher.\"\n    }\n  },\n  {\n    \"id\": \"id:doc:doc::1\",\n    \"relevance\": 144.52731323242188,\n    \"source\": \"mixedbreadai_content\",\n    \"fields\": {\n      \"matchfeatures\": {\n        \"closeness(field,binary_vector)\": 0.002890173410404624,\n        \"distance(field,binary_vector)\": 345.0\n      },\n      \"sddocname\": \"doc\",\n      \"documentid\": \"id:doc:doc::1\",\n      \"doc_id\": \"1\",\n      \"text\": \"Albert Einstein was a German-born theoretical physicist who is widely held to be one of the greatest and most influential scientists of all time.\"\n    }\n  },\n  {\n    \"id\": \"id:doc:doc::0\",\n    \"relevance\": 138.78799438476562,\n    \"source\": \"mixedbreadai_content\",\n    \"fields\": {\n      \"matchfeatures\": {\n        \"closeness(field,binary_vector)\": 0.00273224043715847,\n        \"distance(field,binary_vector)\": 365.0\n      },\n      \"sddocname\": \"doc\",\n      \"documentid\": \"id:doc:doc::0\",\n      \"doc_id\": \"0\",\n      \"text\": \"Alan Turing  was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.\"\n    }\n  },\n  {\n    \"id\": \"id:doc:doc::3\",\n    \"relevance\": 115.2405776977539,\n    \"source\": \"mixedbreadai_content\",\n    \"fields\": {\n      \"matchfeatures\": {\n        \"closeness(field,binary_vector)\": 0.002652519893899204,\n        \"distance(field,binary_vector)\": 376.0\n      },\n      \"sddocname\": \"doc\",\n      \"documentid\": \"id:doc:doc::3\",\n      \"doc_id\": \"3\",\n      \"text\": \"Marie Curie was a Polish and naturalised-French physicist and chemist who conducted pioneering research on radioactivity\"\n    }\n  }\n]\n</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#using-mixedbreadai-embedding-model-with-support-for-binary-vectors","title":"Using Mixedbread.ai embedding model with support for binary vectors\u00b6","text":"<p>Check out the amazing blog post: Binary and Scalar Embedding Quantization for Significantly Faster &amp; Cheaper Retrieval</p> <p>Binarization is significant because:</p> <ul> <li>Binarization reduces the storage footprint from 1024 floats (4096 bytes) per vector to 128 int8 (128 bytes).</li> <li>32x less data to store</li> <li>Faster distance calculations using hamming distance, which Vespa natively supports for bits packed into int8 precision. More on hamming distance in Vespa.</li> </ul> <p>Vespa supports <code>hamming</code> distance with and without hnsw indexing.</p> <p>For those wanting to learn more about binary vectors, we recommend our 2021 blog series on Billion-scale vector search with Vespa and Billion-scale vector search with Vespa - part two.</p> <p>This notebook demonstrates how to use the Mixedbread mixedbread-ai/mxbai-embed-large-v1 model with support for binary vectors with Vespa. The notebook example also includes a re-ranking phase that uses the float query vector version for improved accuracy. The re-ranking step makes the model perform at 96.45% of the full float version, with a 32x decrease in storage footprint.</p> <p></p> <p></p> <p>Install the dependencies:</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#examining-the-embeddings-using-sentence-transformers","title":"Examining the embeddings using sentence-transformers\u00b6","text":"<p>Read the blog post for <code>sentence-transformer</code> usage.</p> <p>sentence-transformer API. Model card: mixedbread-ai/mxbai-embed-large-v1.</p> <p>Load the model using the sentence-transformers library:</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#some-sample-documents","title":"Some sample documents\u00b6","text":"<p>Define a few sample documents that we want to embed</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>Notice the <code>binary_vector</code> field that defines an indexed (dense) Vespa tensor with the dimension name <code>x[128]</code>.</p> <p>The indexing statement includes <code>index</code> which means that Vespa will use HNSW indexing for this field.</p> <p>Also notice the configuration of distance-metric where we specify <code>hamming</code>.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#feed-our-sample-documents-and-their-binary-embedding-representation","title":"Feed our sample documents and their binary embedding representation\u00b6","text":"<p>With few documents, we use the synchronous API. Read more in reads and writes.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> <li>Practical Nearest Neighbor Search Guide</li> </ul> <p>In this case, we use quantization.quantize_embeddings after first obtaining the float version, this to avoid running the model inference twice.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#summary","title":"Summary\u00b6","text":"<p>Binary embeddings is an exciting development, as it reduces storage (32) and speed up vector searches as the hamming distance is much more efficient than distance metrics like angular or euclidean.</p>"},{"location":"examples/mixedbread-binary-embeddings-with-sentence-transformers-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>We can now delete the cloud instance:</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html","title":"Mother of all embedding models cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa FlagEmbedding vespacli\n</pre> !pip3 install -U pyvespa FlagEmbedding vespacli In\u00a0[\u00a0]: Copied! <pre>from FlagEmbedding import BGEM3FlagModel\n\nmodel = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False)\n</pre> from FlagEmbedding import BGEM3FlagModel  model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False) In\u00a0[3]: Copied! <pre>passage = [\n    \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n]\n</pre> passage = [     \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\" ] In\u00a0[\u00a0]: Copied! <pre>passage_embeddings = model.encode(\n    passage, return_dense=True, return_sparse=True, return_colbert_vecs=True\n)\n</pre> passage_embeddings = model.encode(     passage, return_dense=True, return_sparse=True, return_colbert_vecs=True ) In\u00a0[5]: Copied! <pre>passage_embeddings.keys()\n</pre> passage_embeddings.keys() Out[5]: <pre>dict_keys(['dense_vecs', 'lexical_weights', 'colbert_vecs'])</pre> In\u00a0[6]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nm_schema = Schema(\n    name=\"m\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"lexical_rep\",\n                type=\"tensor&lt;bfloat16&gt;(t{})\",\n                indexing=[\"summary\", \"attribute\"],\n            ),\n            Field(\n                name=\"dense_rep\",\n                type=\"tensor&lt;bfloat16&gt;(x[1024])\",\n                indexing=[\"summary\", \"attribute\"],\n                attribute=[\"distance-metric: angular\"],\n            ),\n            Field(\n                name=\"colbert_rep\",\n                type=\"tensor&lt;bfloat16&gt;(t{}, x[1024])\",\n                indexing=[\"summary\", \"attribute\"],\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  m_schema = Schema(     name=\"m\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"lexical_rep\",                 type=\"tensor(t{})\",                 indexing=[\"summary\", \"attribute\"],             ),             Field(                 name=\"dense_rep\",                 type=\"tensor(x[1024])\",                 indexing=[\"summary\", \"attribute\"],                 attribute=[\"distance-metric: angular\"],             ),             Field(                 name=\"colbert_rep\",                 type=\"tensor(t{}, x[1024])\",                 indexing=[\"summary\", \"attribute\"],             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])], ) <p>The above defines our <code>m</code> schema with the original text and the three different representations</p> In\u00a0[7]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"m\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[m_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"m\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[m_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p>We define three functions that implement the three different scoring functions for the different representations</p> <ul> <li>dense (dense cosine similarity)</li> <li>sparse (sparse dot product)</li> <li>max_sim (The colbert max sim operation)</li> </ul> <p>Then, we combine these three scoring functions using a linear combination with weights, as suggested by the authors here.</p> In\u00a0[8]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking\n\n\nsemantic = RankProfile(\n    name=\"m3hybrid\",\n    inputs=[\n        (\"query(q_dense)\", \"tensor&lt;bfloat16&gt;(x[1024])\"),\n        (\"query(q_lexical)\", \"tensor&lt;bfloat16&gt;(t{})\"),\n        (\"query(q_colbert)\", \"tensor&lt;bfloat16&gt;(qt{}, x[1024])\"),\n        (\"query(q_len_colbert)\", \"float\"),\n    ],\n    functions=[\n        Function(\n            name=\"dense\",\n            expression=\"cosine_similarity(query(q_dense), attribute(dense_rep),x)\",\n        ),\n        Function(\n            name=\"lexical\", expression=\"sum(query(q_lexical) * attribute(lexical_rep))\"\n        ),\n        Function(\n            name=\"max_sim\",\n            expression=\"sum(reduce(sum(query(q_colbert) * attribute(colbert_rep) , x),max, t),qt)/query(q_len_colbert)\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"0.4*dense + 0.2*lexical +  0.4*max_sim\", rank_score_drop_limit=0.0\n    ),\n    match_features=[\"dense\", \"lexical\", \"max_sim\", \"bm25(text)\"],\n)\nm_schema.add_rank_profile(semantic)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking   semantic = RankProfile(     name=\"m3hybrid\",     inputs=[         (\"query(q_dense)\", \"tensor(x[1024])\"),         (\"query(q_lexical)\", \"tensor(t{})\"),         (\"query(q_colbert)\", \"tensor(qt{}, x[1024])\"),         (\"query(q_len_colbert)\", \"float\"),     ],     functions=[         Function(             name=\"dense\",             expression=\"cosine_similarity(query(q_dense), attribute(dense_rep),x)\",         ),         Function(             name=\"lexical\", expression=\"sum(query(q_lexical) * attribute(lexical_rep))\"         ),         Function(             name=\"max_sim\",             expression=\"sum(reduce(sum(query(q_colbert) * attribute(colbert_rep) , x),max, t),qt)/query(q_len_colbert)\",         ),     ],     first_phase=FirstPhaseRanking(         expression=\"0.4*dense + 0.2*lexical +  0.4*max_sim\", rank_score_drop_limit=0.0     ),     match_features=[\"dense\", \"lexical\", \"max_sim\", \"bm25(text)\"], ) m_schema.add_rank_profile(semantic) <p>The <code>m3hybrid</code> rank-profile above defines the query input embedding type and a similarities function that uses a Vespa tensor compute function that calculates the M3 similarities for dense, lexical, and the max_sim for the colbert representations.</p> <p>The profile only defines a single ranking phase, using a linear combination of multiple features using the suggested weighting.</p> <p>Using match-features, Vespa returns selected features along with the hit in the SERP (result page). We also include BM25. We can view BM25 as the fourth dimension. Especially for long-context retrieval, it can be helpful compared to the neural representations.</p> In\u00a0[13]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[14]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 1 of dev-aws-us-east-1c for samples.m. This may take a few minutes the first time.\nINFO    [22:13:09]  Deploying platform version 8.299.14 and application dev build 1 for dev-aws-us-east-1c of default ...\nINFO    [22:13:10]  Using CA signed certificate version 0\nINFO    [22:13:10]  Using 1 nodes in container cluster 'm_container'\nINFO    [22:13:14]  Session 939 for tenant 'samples' prepared and activated.\nINFO    [22:13:17]  ######## Details for all nodes ########\nINFO    [22:13:31]  h88976d.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 &lt;-- :\nINFO    [22:13:31]  --- container-clustercontroller on port 19050 has not started \nINFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:13:31]  h89388b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 &lt;-- :\nINFO    [22:13:31]  --- storagenode on port 19102 has not started \nINFO    [22:13:31]  --- searchnode on port 19107 has not started \nINFO    [22:13:31]  --- distributor on port 19111 has not started \nINFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:13:31]  h90001a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 &lt;-- :\nINFO    [22:13:31]  --- logserver-container on port 4080 has not started \nINFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:13:31]  h90550a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 &lt;-- :\nINFO    [22:13:31]  --- container on port 4080 has not started \nINFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \nINFO    [22:14:31]  Found endpoints:\nINFO    [22:14:31]  - dev.aws-us-east-1c\nINFO    [22:14:31]   |-- https://d29bf3e7.f064e220.z.vespa-app.cloud/ (cluster 'm_container')\nINFO    [22:14:32]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://d29bf3e7.f064e220.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[15]: Copied! <pre>vespa_fields = {\n    \"text\": passage[0],\n    \"lexical_rep\": {\n        key: float(value)\n        for key, value in passage_embeddings[\"lexical_weights\"][0].items()\n    },\n    \"dense_rep\": passage_embeddings[\"dense_vecs\"][0].tolist(),\n    \"colbert_rep\": {\n        index: passage_embeddings[\"colbert_vecs\"][0][index].tolist()\n        for index in range(passage_embeddings[\"colbert_vecs\"][0].shape[0])\n    },\n}\n</pre> vespa_fields = {     \"text\": passage[0],     \"lexical_rep\": {         key: float(value)         for key, value in passage_embeddings[\"lexical_weights\"][0].items()     },     \"dense_rep\": passage_embeddings[\"dense_vecs\"][0].tolist(),     \"colbert_rep\": {         index: passage_embeddings[\"colbert_vecs\"][0][index].tolist()         for index in range(passage_embeddings[\"colbert_vecs\"][0].shape[0])     }, } In\u00a0[17]: Copied! <pre>from vespa.io import VespaResponse\n\nresponse: VespaResponse = app.feed_data_point(\n    schema=\"m\", data_id=0, fields=vespa_fields\n)\nassert response.is_successful()\n</pre> from vespa.io import VespaResponse  response: VespaResponse = app.feed_data_point(     schema=\"m\", data_id=0, fields=vespa_fields ) assert response.is_successful() In\u00a0[\u00a0]: Copied! <pre>query = [\"What is BGE M3?\"]\nquery_embeddings = model.encode(\n    query, return_dense=True, return_sparse=True, return_colbert_vecs=True\n)\n</pre> query = [\"What is BGE M3?\"] query_embeddings = model.encode(     query, return_dense=True, return_sparse=True, return_colbert_vecs=True ) <p>The M3 colbert scoring function needs the query length to normalize the score to the range 0 to 1. This helps when combining the score with the other scoring functions.</p> In\u00a0[19]: Copied! <pre>query_length = query_embeddings[\"colbert_vecs\"][0].shape[0]\n</pre> query_length = query_embeddings[\"colbert_vecs\"][0].shape[0] In\u00a0[20]: Copied! <pre>query_fields = {\n    \"input.query(q_lexical)\": {\n        key: float(value)\n        for key, value in query_embeddings[\"lexical_weights\"][0].items()\n    },\n    \"input.query(q_dense)\": query_embeddings[\"dense_vecs\"][0].tolist(),\n    \"input.query(q_colbert)\": str(\n        {\n            index: query_embeddings[\"colbert_vecs\"][0][index].tolist()\n            for index in range(query_embeddings[\"colbert_vecs\"][0].shape[0])\n        }\n    ),\n    \"input.query(q_len_colbert)\": query_length,\n}\n</pre> query_fields = {     \"input.query(q_lexical)\": {         key: float(value)         for key, value in query_embeddings[\"lexical_weights\"][0].items()     },     \"input.query(q_dense)\": query_embeddings[\"dense_vecs\"][0].tolist(),     \"input.query(q_colbert)\": str(         {             index: query_embeddings[\"colbert_vecs\"][0][index].tolist()             for index in range(query_embeddings[\"colbert_vecs\"][0].shape[0])         }     ),     \"input.query(q_len_colbert)\": query_length, } In\u00a0[21]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select id, text from m where userQuery() or ({targetHits:10}nearestNeighbor(dense_rep,q_dense))\",\n    ranking=\"m3hybrid\",\n    query=query[0],\n    body={**query_fields},\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select id, text from m where userQuery() or ({targetHits:10}nearestNeighbor(dense_rep,q_dense))\",     ranking=\"m3hybrid\",     query=query[0],     body={**query_fields}, ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"index:m_content/0/cfcd2084234135f700f08abf\",\n  \"relevance\": 0.5993361056332731,\n  \"source\": \"m_content\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"bm25(text)\": 0.8630462173553426,\n      \"dense\": 0.6258970723760484,\n      \"lexical\": 0.1941967010498047,\n      \"max_sim\": 0.7753448411822319\n    },\n    \"text\": \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n  }\n}\n</pre> <p>Notice the <code>matchfeatures</code> that returns the configured match-features from the rank-profile. We can use these to compare the torch model scoring with the computations specified in Vespa.</p> <p>Now, we can compare the Vespa computed scores with the model torch code and they line up perfectly</p> In\u00a0[22]: Copied! <pre>model.compute_lexical_matching_score(\n    passage_embeddings[\"lexical_weights\"][0], query_embeddings[\"lexical_weights\"][0]\n)\n</pre> model.compute_lexical_matching_score(     passage_embeddings[\"lexical_weights\"][0], query_embeddings[\"lexical_weights\"][0] ) Out[22]: <pre>0.19554455392062664</pre> In\u00a0[23]: Copied! <pre>query_embeddings[\"dense_vecs\"][0] @ passage_embeddings[\"dense_vecs\"][0].T\n</pre> query_embeddings[\"dense_vecs\"][0] @ passage_embeddings[\"dense_vecs\"][0].T Out[23]: <pre>0.6259037</pre> In\u00a0[24]: Copied! <pre>model.colbert_score(\n    query_embeddings[\"colbert_vecs\"][0], passage_embeddings[\"colbert_vecs\"][0]\n)\n</pre> model.colbert_score(     query_embeddings[\"colbert_vecs\"][0], passage_embeddings[\"colbert_vecs\"][0] ) Out[24]: <pre>tensor(0.7797)</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/mother-of-all-embedding-models-cloud.html#bge-m3-the-mother-of-all-embedding-models","title":"BGE-M3 - The Mother of all embedding models\u00b6","text":"<p>BAAI released BGE-M3 on January 30th, a new member of the BGE model series.</p> <p>M3 stands for Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec (colbert) retrieval).</p> <p>This notebook demonstrates how to use the BGE-M3 embeddings and represent all three embedding representations in Vespa! Vespa is the only scalable serving engine that can handle all M3 representations.</p> <p>This code is inspired by the README from the model hub BAAI/bge-m3.</p> <p></p> <p>Let's get started! First, install dependencies:</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#explore-the-multiple-representations-of-m3","title":"Explore the multiple representations of M3\u00b6","text":"<p>When encoding text, we can ask for the representations we want</p> <ul> <li>Sparse vectors with weights for the token IDs (from the multilingual tokenization process)</li> <li>Dense (DPR) regular text embeddings</li> <li>Multi-Dense (ColBERT) - contextualized multi-token vectors</li> </ul> <p>Let us dive into it - To use this model on the CPU we set <code>use_fp16</code> to False, for GPU inference, it is recommended to use <code>use_fp16=True</code> for accelerated inference.</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#a-demo-passage","title":"A demo passage\u00b6","text":"<p>Let us encode a simple passage</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type. We use Vespa tensors to represent the three different M3 representations.</p> <ul> <li>We use a mapped tensor denoted by <code>t{}</code> to represent the sparse lexical representation</li> <li>We use an indexed tensor denoted by <code>x[1024]</code> to represent the dense single vector representation of 1024 dimensions</li> <li>For the colbert_rep (multi-vector), we use a mixed tensor that combines a mapped and an indexed dimension. This mixed tensor allows us to represent variable lengths.</li> </ul> <p>We use <code>bfloat16</code> tensor cell type, saving 50% storage compared to <code>float</code>.</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#feed-the-m3-representations","title":"Feed the M3 representations\u00b6","text":"<p>We convert the three different representations to Vespa feed format</p>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now, we can also query our data.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul>"},{"location":"examples/mother-of-all-embedding-models-cloud.html#that-is-it","title":"That is it!\u00b6","text":"<p>That is how easy it is to represent the brand new M3 FlagEmbedding representations in Vespa! Read more in the M3 technical report.</p> <p>We can go ahead and delete the Vespa cloud instance we deployed by:</p>"},{"location":"examples/multi-vector-indexing.html","title":"Multi vector indexing","text":"Refer to troubleshooting     for any problem when running this guide.  <p>This notebook requires pyvespa &gt;= 0.37.1, ZSTD, and the Vespa CLI.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa\n</pre> !pip3 install pyvespa In\u00a0[1]: Copied! <pre>from vespa.package import (\n    ApplicationPackage,\n    Component,\n    Parameter,\n    Field,\n    HNSW,\n    RankProfile,\n    Function,\n    FirstPhaseRanking,\n    SecondPhaseRanking,\n    FieldSet,\n    DocumentSummary,\n    Summary,\n)\nfrom pathlib import Path\nimport json\n\napp_package = ApplicationPackage(\n    name=\"wiki\",\n    components=[\n        Component(\n            id=\"e5-small-q\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\"transformer-model\", {\"path\": \"model/e5-small-v2-int8.onnx\"}),\n                Parameter(\"tokenizer-model\", {\"path\": \"model/tokenizer.json\"}),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import (     ApplicationPackage,     Component,     Parameter,     Field,     HNSW,     RankProfile,     Function,     FirstPhaseRanking,     SecondPhaseRanking,     FieldSet,     DocumentSummary,     Summary, ) from pathlib import Path import json  app_package = ApplicationPackage(     name=\"wiki\",     components=[         Component(             id=\"e5-small-q\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(\"transformer-model\", {\"path\": \"model/e5-small-v2-int8.onnx\"}),                 Parameter(\"tokenizer-model\", {\"path\": \"model/tokenizer.json\"}),             ],         )     ], ) In\u00a0[2]: Copied! <pre>app_package.schema.add_fields(\n    Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),\n    Field(\n        name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"url\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"\n    ),\n    Field(\n        name=\"paragraphs\",\n        type=\"array&lt;string&gt;\",\n        indexing=[\"index\", \"summary\"],\n        index=\"enable-bm25\",\n        bolding=True,\n    ),\n    Field(\n        name=\"paragraph_embeddings\",\n        type=\"tensor&lt;float&gt;(p{},x[384])\",\n        indexing=[\"input paragraphs\", \"embed\", \"index\", \"attribute\"],\n        ann=HNSW(distance_metric=\"angular\"),\n        is_document_field=False,\n    ),\n    #\n    # Alteratively, for exact distance calculation not using HNSW:\n    #\n    # Field(name=\"paragraph_embeddings\", type=\"tensor&lt;float&gt;(p{},x[384])\",\n    #       indexing=[\"input paragraphs\", \"embed\", \"attribute\"],\n    #       attribute=[\"distance-metric: angular\"],\n    #       is_document_field=False)\n)\n</pre> app_package.schema.add_fields(     Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),     Field(         name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ),     Field(         name=\"url\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"     ),     Field(         name=\"paragraphs\",         type=\"array\",         indexing=[\"index\", \"summary\"],         index=\"enable-bm25\",         bolding=True,     ),     Field(         name=\"paragraph_embeddings\",         type=\"tensor(p{},x[384])\",         indexing=[\"input paragraphs\", \"embed\", \"index\", \"attribute\"],         ann=HNSW(distance_metric=\"angular\"),         is_document_field=False,     ),     #     # Alteratively, for exact distance calculation not using HNSW:     #     # Field(name=\"paragraph_embeddings\", type=\"tensor(p{},x[384])\",     #       indexing=[\"input paragraphs\", \"embed\", \"attribute\"],     #       attribute=[\"distance-metric: angular\"],     #       is_document_field=False) ) <p>One field of particular interest is <code>paragraph_embeddings</code>. Note that we are not feeding embeddings to this instance. Instead, the embeddings are generated by using the embed feature, using the model configured at start. Read more in Text embedding made simple.</p> <p>Looking closely at the code, <code>paragraph_embeddings</code> uses <code>is_document_field=False</code>, meaning it will read another field as input (here <code>paragraph</code>), and run <code>embed</code> on it.</p> <p>As only one model is configured, <code>embed</code> will use that one - it is possible to configure mode models and use <code>embed model-id</code> as well.</p> <p>As the code comment illustrates, there can be different distrance metrics used, as well as using an exact or approximate nearest neighbor search.</p> In\u00a0[3]: Copied! <pre>app_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"semantic\",\n        inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n        inherits=\"default\",\n        first_phase=\"cos(distance(field,paragraph_embeddings))\",\n        match_features=[\"closest(paragraph_embeddings)\"],\n    )\n)\n\napp_package.schema.add_rank_profile(\n    RankProfile(name=\"bm25\", first_phase=\"2*bm25(title) + bm25(paragraphs)\")\n)\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"hybrid\",\n        inherits=\"semantic\",\n        functions=[\n            Function(\n                name=\"avg_paragraph_similarity\",\n                expression=\"\"\"reduce(\n                              sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),\n                              avg,\n                              p\n                          )\"\"\",\n            ),\n            Function(\n                name=\"max_paragraph_similarity\",\n                expression=\"\"\"reduce(\n                              sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),\n                              max,\n                              p\n                          )\"\"\",\n            ),\n            Function(\n                name=\"all_paragraph_similarities\",\n                expression=\"sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x)\",\n            ),\n        ],\n        first_phase=FirstPhaseRanking(\n            expression=\"cos(distance(field,paragraph_embeddings))\"\n        ),\n        second_phase=SecondPhaseRanking(\n            expression=\"firstPhase + avg_paragraph_similarity() + log( bm25(title) + bm25(paragraphs) + bm25(url))\"\n        ),\n        match_features=[\n            \"closest(paragraph_embeddings)\",\n            \"firstPhase\",\n            \"bm25(title)\",\n            \"bm25(paragraphs)\",\n            \"avg_paragraph_similarity\",\n            \"max_paragraph_similarity\",\n            \"all_paragraph_similarities\",\n        ],\n    )\n)\n</pre> app_package.schema.add_rank_profile(     RankProfile(         name=\"semantic\",         inputs=[(\"query(q)\", \"tensor(x[384])\")],         inherits=\"default\",         first_phase=\"cos(distance(field,paragraph_embeddings))\",         match_features=[\"closest(paragraph_embeddings)\"],     ) )  app_package.schema.add_rank_profile(     RankProfile(name=\"bm25\", first_phase=\"2*bm25(title) + bm25(paragraphs)\") )  app_package.schema.add_rank_profile(     RankProfile(         name=\"hybrid\",         inherits=\"semantic\",         functions=[             Function(                 name=\"avg_paragraph_similarity\",                 expression=\"\"\"reduce(                               sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),                               avg,                               p                           )\"\"\",             ),             Function(                 name=\"max_paragraph_similarity\",                 expression=\"\"\"reduce(                               sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),                               max,                               p                           )\"\"\",             ),             Function(                 name=\"all_paragraph_similarities\",                 expression=\"sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x)\",             ),         ],         first_phase=FirstPhaseRanking(             expression=\"cos(distance(field,paragraph_embeddings))\"         ),         second_phase=SecondPhaseRanking(             expression=\"firstPhase + avg_paragraph_similarity() + log( bm25(title) + bm25(paragraphs) + bm25(url))\"         ),         match_features=[             \"closest(paragraph_embeddings)\",             \"firstPhase\",             \"bm25(title)\",             \"bm25(paragraphs)\",             \"avg_paragraph_similarity\",             \"max_paragraph_similarity\",             \"all_paragraph_similarities\",         ],     ) ) In\u00a0[4]: Copied! <pre>app_package.schema.add_field_set(\n    FieldSet(name=\"default\", fields=[\"title\", \"url\", \"paragraphs\"])\n)\n</pre> app_package.schema.add_field_set(     FieldSet(name=\"default\", fields=[\"title\", \"url\", \"paragraphs\"]) ) In\u00a0[5]: Copied! <pre>app_package.schema.add_document_summary(\n    DocumentSummary(\n        name=\"minimal\",\n        summary_fields=[Summary(\"id\", \"int\"), Summary(\"title\", \"string\")],\n    )\n)\n</pre> app_package.schema.add_document_summary(     DocumentSummary(         name=\"minimal\",         summary_fields=[Summary(\"id\", \"int\"), Summary(\"title\", \"string\")],     ) ) In\u00a0[6]: Copied! <pre>Path(\"pkg\").mkdir(parents=True, exist_ok=True)\napp_package.to_files(\"pkg\")\n</pre> Path(\"pkg\").mkdir(parents=True, exist_ok=True) app_package.to_files(\"pkg\") <p>It is a good idea to inspect the files exported into <code>pkg</code> - these are files referred to in the Vespa Documentation.</p> In\u00a0[7]: Copied! <pre>! mkdir -p pkg/model\n! curl -L -o pkg/model/tokenizer.json \\\n  https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\n\n! curl -L -o pkg/model/e5-small-v2-int8.onnx \\\n  https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\n</pre> ! mkdir -p pkg/model ! curl -L -o pkg/model/tokenizer.json \\   https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json  ! curl -L -o pkg/model/e5-small-v2-int8.onnx \\   https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx <pre>  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  694k  100  694k    0     0  2473k      0 --:--:-- --:--:-- --:--:-- 2508k\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100 32.3M  100 32.3M    0     0  27.1M      0  0:00:01  0:00:01 --:--:-- 53.0M\n</pre> In\u00a0[8]: Copied! <pre>from vespa.deployment import VespaDocker\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy_from_disk(application_name=\"wiki\", application_root=\"pkg\")\n</pre> from vespa.deployment import VespaDocker  vespa_docker = VespaDocker() app = vespa_docker.deploy_from_disk(application_name=\"wiki\", application_root=\"pkg\") <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[9]: Copied! <pre>! curl -s -H \"Accept:application/vnd.github.v3.raw\" \\\n  https://api.github.com/repos/vespa-engine/sample-apps/contents/multi-vector-indexing/ext/articles.jsonl.zst | \\\n  zstdcat - &gt; articles.jsonl\n</pre> ! curl -s -H \"Accept:application/vnd.github.v3.raw\" \\   https://api.github.com/repos/vespa-engine/sample-apps/contents/multi-vector-indexing/ext/articles.jsonl.zst | \\   zstdcat - &gt; articles.jsonl <p>I you do not have ZSTD install, get <code>articles.jsonl.zip</code> and unzip it instead.</p> <p>Feed and index the Wikipedia articles using the Vespa CLI. As part of feeding, <code>embed</code> is called on each article, and the output of this is stored in the <code>paragraph_embeddings</code> field:</p> In\u00a0[10]: Copied! <pre>! vespa config set target local\n! vespa feed articles.jsonl\n</pre> ! vespa config set target local ! vespa feed articles.jsonl <pre>{\n  \"feeder.seconds\": 1.448,\n  \"feeder.ok.count\": 8,\n  \"feeder.ok.rate\": 5.524,\n  \"feeder.error.count\": 0,\n  \"feeder.inflight.count\": 0,\n  \"http.request.count\": 8,\n  \"http.request.bytes\": 12958,\n  \"http.request.MBps\": 0.009,\n  \"http.exception.count\": 0,\n  \"http.response.count\": 8,\n  \"http.response.bytes\": 674,\n  \"http.response.MBps\": 0.000,\n  \"http.response.error.count\": 0,\n  \"http.response.latency.millis.min\": 728,\n  \"http.response.latency.millis.avg\": 834,\n  \"http.response.latency.millis.max\": 1446,\n  \"http.response.code.counts\": {\n    \"200\": 8\n  }\n}\n</pre> <p>Note that creating embeddings is computationally expensive, but this is a small dataset with only 8 articles, so will be done in a few seconds.</p> <p>The Vespa instance is now populated with the Wikipedia articles, with generated embeddings, and ready for queries. The next sections have examples of various kinds of queries to run on the dataset.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nresult: VespaQueryResponse = app.query(\n    body={\n        \"yql\": \"select * from wiki where true\",\n        \"ranking.profile\": \"unranked\",\n        \"hits\": 2,\n    }\n)\nif not result.is_successful():\n    raise ValueError(result.get_json())\nif len(result.hits) != 2:\n    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> from vespa.io import VespaQueryResponse  result: VespaQueryResponse = app.query(     body={         \"yql\": \"select * from wiki where true\",         \"ranking.profile\": \"unranked\",         \"hits\": 2,     } ) if not result.is_successful():     raise ValueError(result.get_json()) if len(result.hits) != 2:     raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) In\u00a0[\u00a0]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select * from wiki where userQuery()\",\n        \"query\": 24,\n        \"ranking.profile\": \"bm25\",\n        \"hits\": 2,\n    }\n)\nif len(result.hits) != 2:\n    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select * from wiki where userQuery()\",         \"query\": 24,         \"ranking.profile\": \"bm25\",         \"hits\": 2,     } ) if len(result.hits) != 2:     raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) In\u00a0[14]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select * from wiki where {targetHits:2}nearestNeighbor(paragraph_embeddings,q)\",\n        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n        \"ranking.profile\": \"semantic\",\n        \"presentation.format.tensors\": \"short-value\",\n        \"hits\": 2,\n    }\n)\nresult.hits\nif len(result.hits) != 2:\n    raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select * from wiki where {targetHits:2}nearestNeighbor(paragraph_embeddings,q)\",         \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",         \"ranking.profile\": \"semantic\",         \"presentation.format.tensors\": \"short-value\",         \"hits\": 2,     } ) result.hits if len(result.hits) != 2:     raise ValueError(\"Expected 2 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"id:wikipedia:wiki::9985\",\n        \"relevance\": 0.8807156260391702,\n        \"source\": \"wiki_content\",\n        \"fields\": {\n            \"matchfeatures\": {\n                \"closest(paragraph_embeddings)\": {\n                    \"4\": 1.0\n                }\n            },\n            \"sddocname\": \"wiki\",\n            \"paragraphs\": [\n                \"The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format.\",\n                \"A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under the 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and ends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called 24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you would say \\\"Tuesday at 24:00\\\" and \\\"Wednesday at 00:00\\\" to mean exactly the same time.\",\n                \"However, the US military prefers not to say 24:00 - they do not like to have two names for the same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n                \"24-hour clock time is used in computers, military, public safety, and transport. In many Asian, European and Latin American countries people use it to write the time. Many European people use it in speaking.\",\n                \"In railway timetables 24:00 means the \\\"end\\\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.\"\n            ],\n            \"documentid\": \"id:wikipedia:wiki::9985\",\n            \"title\": \"24-hour clock\",\n            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n        }\n    },\n    {\n        \"id\": \"id:wikipedia:wiki::59079\",\n        \"relevance\": 0.7972394509946005,\n        \"source\": \"wiki_content\",\n        \"fields\": {\n            \"matchfeatures\": {\n                \"closest(paragraph_embeddings)\": {\n                    \"4\": 1.0\n                }\n            },\n            \"sddocname\": \"wiki\",\n            \"paragraphs\": [\n                \"Logic gates are digital components. They normally work at only two levels of voltage, a positive level and zero level. Commonly they work based on two states: \\\"On\\\" and \\\"Off\\\". In the On state, voltage is positive. In the Off state, the voltage is at zero. The On state usually uses a voltage in the range of 3.5 to 5 volts. This range can be lower for some uses.\",\n                \"Logic gates compare the state at their inputs to decide what the state at their output should be. A logic gate is \\\"on\\\" or active when its rules are correctly met. At this time, electricity is flowing through the gate and the voltage at its output is at the level of its On state.\",\n                \"Logic gates are electronic versions of Boolean logic. Truth tables will tell you what the output will be, depending on the inputs.\",\n                \"AND gates have two inputs. The output of an AND gate is on only if both inputs are on. If at least one of the inputs is off, the output will be off.\",\n                \"Using the image at the right, if \\\"A\\\" and \\\"B\\\" are both in an On state, the output (out) will be an On state. If either \\\"A\\\" or \\\"B\\\" is in an Off state, the output will also be in an Off state. \\\"A\\\" and \\\"B\\\" must be On for the output to be On.\",\n                \"OR gates have two inputs. The output of an OR gate will be on if at least one of the inputs are on. If both inputs are off, the output will be off.\",\n                \"Using the image at the right, if either \\\"A\\\" or \\\"B\\\" is On, the output (\\\"out\\\") will also be On. If both \\\"A\\\" and \\\"B\\\" are Off, the output will be Off.\",\n                \"The NOT logic gate has only one input. If the input is On then the output will be Off. In other words, the NOT logic gate changes the signal from On to Off or from Off to On. It is sometimes called an inverter.\",\n                \"XOR (\\\"exclusive or\\\") gates have two inputs. The output of a XOR gate will be true only if the two inputs are different from each other. If both inputs are the same, the output will be off.\",\n                \"NAND means not both. It is called NAND because it means \\\"not and.\\\" This means that it will always output true unless both inputs are on.\",\n                \"XNOR means \\\"not exclusive or.\\\" This means that it will only output true if both inputs are the same. It is the opposite of a XOR logic gate.\"\n            ],\n            \"documentid\": \"id:wikipedia:wiki::59079\",\n            \"title\": \"Logic gate\",\n            \"url\": \"https://simple.wikipedia.org/wiki?curid=59079\"\n        }\n    }\n]\n</pre> <p>An interesting question then is, of the paragraphs in the document, which one was the closest? When analysing ranking, using match-features lets you export the scores used in the ranking calculations, see closest - from the result above:</p> <pre><code> \"matchfeatures\": {\n                \"closest(paragraph_embeddings)\": {\n                    \"4\": 1.0\n                }\n}\n</code></pre> <p>This means, the tensor of index 4 has the closest match. With this, it is straight forward to feed articles with an array of paragraphs and highlight the best matching paragraph in the document!</p> In\u00a0[17]: Copied! <pre>def find_best_paragraph(hit: dict) -&gt; str:\n    paragraphs = hit[\"fields\"][\"paragraphs\"]\n    match_features = hit[\"fields\"][\"matchfeatures\"]\n    index = int(list(match_features[\"closest(paragraph_embeddings)\"].keys())[0])\n    return paragraphs[index]\n</pre> def find_best_paragraph(hit: dict) -&gt; str:     paragraphs = hit[\"fields\"][\"paragraphs\"]     match_features = hit[\"fields\"][\"matchfeatures\"]     index = int(list(match_features[\"closest(paragraph_embeddings)\"].keys())[0])     return paragraphs[index] In\u00a0[18]: Copied! <pre>find_best_paragraph(result.hits[0])\n</pre> find_best_paragraph(result.hits[0]) Out[18]: <pre>'In railway timetables 24:00 means the \"end\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.'</pre> In\u00a0[20]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select * from wiki where userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))\",\n        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n        \"query\": \"what does 24 mean in the context of railways\",\n        \"ranking.profile\": \"hybrid\",\n        \"presentation.format.tensors\": \"short-value\",\n        \"hits\": 1,\n    }\n)\nif len(result.hits) != 1:\n    raise ValueError(\"Expected 1 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select * from wiki where userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))\",         \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",         \"query\": \"what does 24 mean in the context of railways\",         \"ranking.profile\": \"hybrid\",         \"presentation.format.tensors\": \"short-value\",         \"hits\": 1,     } ) if len(result.hits) != 1:     raise ValueError(\"Expected 1 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"id:wikipedia:wiki::9985\",\n        \"relevance\": 4.163399168193791,\n        \"source\": \"wiki_content\",\n        \"fields\": {\n            \"matchfeatures\": {\n                \"bm25(paragraphs)\": 10.468827250036052,\n                \"bm25(title)\": 1.1272217840066168,\n                \"closest(paragraph_embeddings)\": {\n                    \"4\": 1.0\n                },\n                \"firstPhase\": 0.8807156260391702,\n                \"all_paragraph_similarities\": {\n                    \"1\": 0.8030083179473877,\n                    \"2\": 0.7992785573005676,\n                    \"3\": 0.8273358345031738,\n                    \"4\": 0.8807156085968018,\n                    \"0\": 0.849757194519043\n                },\n                \"avg_paragraph_similarity\": 0.8320191025733947,\n                \"max_paragraph_similarity\": 0.8807156085968018\n            },\n            \"sddocname\": \"wiki\",\n            \"paragraphs\": [\n                \"&lt;hi&gt;The&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock is a way &lt;hi&gt;of&lt;/hi&gt; telling &lt;hi&gt;the&lt;/hi&gt; time &lt;hi&gt;in&lt;/hi&gt; which &lt;hi&gt;the&lt;/hi&gt; day runs from midnight to midnight and is divided into &lt;hi&gt;24&lt;/hi&gt; hours, numbered from 0 to 23. It &lt;hi&gt;does&lt;/hi&gt; not use a.m. or p.m. This system is also referred to (only &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; US and &lt;hi&gt;the&lt;/hi&gt; English speaking parts &lt;hi&gt;of&lt;/hi&gt; Canada) as military time or (only &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; United Kingdom and now very rarely) as continental time. &lt;hi&gt;In&lt;/hi&gt; some parts &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; world, it is called &lt;hi&gt;railway&lt;/hi&gt; time. Also, &lt;hi&gt;the&lt;/hi&gt; international standard notation &lt;hi&gt;of&lt;/hi&gt; time (ISO 8601) is based on this format.\",\n                \"A time &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock is written &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero &lt;hi&gt;in&lt;/hi&gt; front (called a leading zero); e.g. 09:07. Under &lt;hi&gt;the&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock system, &lt;hi&gt;the&lt;/hi&gt; day begins at midnight, 00:00, and &lt;hi&gt;the&lt;/hi&gt; last minute &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day begins at 23:59 and ends at &lt;hi&gt;24&lt;/hi&gt;:00, which is identical to 00:00 &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; following day. 12:00 can only be mid-day. Midnight is called &lt;hi&gt;24&lt;/hi&gt;:00 and is used to &lt;hi&gt;mean&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; end &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day and 00:00 is used to &lt;hi&gt;mean&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; beginning &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day. For example, you would say \\\"Tuesday at &lt;hi&gt;24&lt;/hi&gt;:00\\\" and \\\"Wednesday at 00:00\\\" to &lt;hi&gt;mean&lt;/hi&gt; exactly &lt;hi&gt;the&lt;/hi&gt; same time.\",\n                \"However, &lt;hi&gt;the&lt;/hi&gt; US military prefers not to say &lt;hi&gt;24&lt;/hi&gt;:00 - they &lt;hi&gt;do&lt;/hi&gt; not like to have two names for &lt;hi&gt;the&lt;/hi&gt; same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n                \"&lt;hi&gt;24&lt;/hi&gt;-hour clock time is used &lt;hi&gt;in&lt;/hi&gt; computers, military, public safety, and transport. &lt;hi&gt;In&lt;/hi&gt; many Asian, European and Latin American countries people use it to write &lt;hi&gt;the&lt;/hi&gt; time. Many European people use it &lt;hi&gt;in&lt;/hi&gt; speaking.\",\n                \"&lt;hi&gt;In&lt;/hi&gt; &lt;hi&gt;railway&lt;/hi&gt; timetables &lt;hi&gt;24&lt;/hi&gt;:00 means &lt;hi&gt;the&lt;/hi&gt; \\\"end\\\" &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day. For example, a train due to arrive at a station during &lt;hi&gt;the&lt;/hi&gt; last minute &lt;hi&gt;of&lt;/hi&gt; a day arrives at &lt;hi&gt;24&lt;/hi&gt;:00; but trains which depart during &lt;hi&gt;the&lt;/hi&gt; first minute &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day go at 00:00.\"\n            ],\n            \"documentid\": \"id:wikipedia:wiki::9985\",\n            \"title\": \"24-hour clock\",\n            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n        }\n    }\n]\n</pre> <p>This case combines exact search with nearestNeighbor search. The <code>hybrid</code> rank-profile above also calculates several additional features using tensor expressions:</p> <ul> <li><code>firstPhase</code> is the score of the first ranking phase, configured in the hybrid profile as <code>cos(distance(field, paragraph_embeddings))</code>.</li> <li><code>all_paragraph_similarities</code> returns all the similarity scores for all paragraphs.</li> <li><code>avg_paragraph_similarity</code> is the average similarity score across all the paragraphs.</li> <li><code>max_paragraph_similarity</code> is the same as <code>firstPhase</code>, but computed using a tensor expression.</li> </ul> <p>These additional features are calculated during second-phase ranking to limit the number of vector computations.</p> <p>The Tensor Playground is useful to play with tensor expressions.</p> <p>The Hybrid Search blog post series is a good read to learn more about hybrid ranking!</p> In\u00a0[23]: Copied! <pre>def find_paragraph_scores(hit: dict) -&gt; str:\n    paragraphs = hit[\"fields\"][\"paragraphs\"]\n    match_features = hit[\"fields\"][\"matchfeatures\"]\n    indexes = [int(v) for v in match_features[\"all_paragraph_similarities\"]]\n    scores = list(match_features[\"all_paragraph_similarities\"].values())\n    return list(zip([paragraphs[i] for i in indexes], scores))\n</pre> def find_paragraph_scores(hit: dict) -&gt; str:     paragraphs = hit[\"fields\"][\"paragraphs\"]     match_features = hit[\"fields\"][\"matchfeatures\"]     indexes = [int(v) for v in match_features[\"all_paragraph_similarities\"]]     scores = list(match_features[\"all_paragraph_similarities\"].values())     return list(zip([paragraphs[i] for i in indexes], scores)) In\u00a0[24]: Copied! <pre>find_paragraph_scores(result.hits[0])\n</pre> find_paragraph_scores(result.hits[0]) Out[24]: <pre>[('A time &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock is written &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero &lt;hi&gt;in&lt;/hi&gt; front (called a leading zero); e.g. 09:07. Under &lt;hi&gt;the&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock system, &lt;hi&gt;the&lt;/hi&gt; day begins at midnight, 00:00, and &lt;hi&gt;the&lt;/hi&gt; last minute &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day begins at 23:59 and ends at &lt;hi&gt;24&lt;/hi&gt;:00, which is identical to 00:00 &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; following day. 12:00 can only be mid-day. Midnight is called &lt;hi&gt;24&lt;/hi&gt;:00 and is used to &lt;hi&gt;mean&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; end &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day and 00:00 is used to &lt;hi&gt;mean&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; beginning &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day. For example, you would say \"Tuesday at &lt;hi&gt;24&lt;/hi&gt;:00\" and \"Wednesday at 00:00\" to &lt;hi&gt;mean&lt;/hi&gt; exactly &lt;hi&gt;the&lt;/hi&gt; same time.',\n  0.8030083179473877),\n ('However, &lt;hi&gt;the&lt;/hi&gt; US military prefers not to say &lt;hi&gt;24&lt;/hi&gt;:00 - they &lt;hi&gt;do&lt;/hi&gt; not like to have two names for &lt;hi&gt;the&lt;/hi&gt; same thing, so they always say \"23:59\", which is one minute before midnight.',\n  0.7992785573005676),\n ('&lt;hi&gt;24&lt;/hi&gt;-hour clock time is used &lt;hi&gt;in&lt;/hi&gt; computers, military, public safety, and transport. &lt;hi&gt;In&lt;/hi&gt; many Asian, European and Latin American countries people use it to write &lt;hi&gt;the&lt;/hi&gt; time. Many European people use it &lt;hi&gt;in&lt;/hi&gt; speaking.',\n  0.8273358345031738),\n ('&lt;hi&gt;In&lt;/hi&gt; &lt;hi&gt;railway&lt;/hi&gt; timetables &lt;hi&gt;24&lt;/hi&gt;:00 means &lt;hi&gt;the&lt;/hi&gt; \"end\" &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day. For example, a train due to arrive at a station during &lt;hi&gt;the&lt;/hi&gt; last minute &lt;hi&gt;of&lt;/hi&gt; a day arrives at &lt;hi&gt;24&lt;/hi&gt;:00; but trains which depart during &lt;hi&gt;the&lt;/hi&gt; first minute &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; day go at 00:00.',\n  0.8807156085968018),\n ('&lt;hi&gt;The&lt;/hi&gt; &lt;hi&gt;24&lt;/hi&gt;-hour clock is a way &lt;hi&gt;of&lt;/hi&gt; telling &lt;hi&gt;the&lt;/hi&gt; time &lt;hi&gt;in&lt;/hi&gt; which &lt;hi&gt;the&lt;/hi&gt; day runs from midnight to midnight and is divided into &lt;hi&gt;24&lt;/hi&gt; hours, numbered from 0 to 23. It &lt;hi&gt;does&lt;/hi&gt; not use a.m. or p.m. This system is also referred to (only &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; US and &lt;hi&gt;the&lt;/hi&gt; English speaking parts &lt;hi&gt;of&lt;/hi&gt; Canada) as military time or (only &lt;hi&gt;in&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; United Kingdom and now very rarely) as continental time. &lt;hi&gt;In&lt;/hi&gt; some parts &lt;hi&gt;of&lt;/hi&gt; &lt;hi&gt;the&lt;/hi&gt; world, it is called &lt;hi&gt;railway&lt;/hi&gt; time. Also, &lt;hi&gt;the&lt;/hi&gt; international standard notation &lt;hi&gt;of&lt;/hi&gt; time (ISO 8601) is based on this format.',\n  0.849757194519043)]</pre> In\u00a0[25]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": 'select * from wiki where url contains \"9985\" and userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))',\n        \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",\n        \"query\": \"what does 24 mean in the context of railways\",\n        \"ranking.profile\": \"hybrid\",\n        \"bolding\": False,\n        \"presentation.format.tensors\": \"short-value\",\n        \"hits\": 1,\n    }\n)\nif len(result.hits) != 1:\n    raise ValueError(\"Expected one hit, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": 'select * from wiki where url contains \"9985\" and userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))',         \"input.query(q)\": \"embed(what does 24 mean in the context of railways)\",         \"query\": \"what does 24 mean in the context of railways\",         \"ranking.profile\": \"hybrid\",         \"bolding\": False,         \"presentation.format.tensors\": \"short-value\",         \"hits\": 1,     } ) if len(result.hits) != 1:     raise ValueError(\"Expected one hit, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"id:wikipedia:wiki::9985\",\n        \"relevance\": 4.307079208249452,\n        \"source\": \"wiki_content\",\n        \"fields\": {\n            \"matchfeatures\": {\n                \"bm25(paragraphs)\": 10.468827250036052,\n                \"bm25(title)\": 1.1272217840066168,\n                \"closest(paragraph_embeddings)\": {\n                    \"type\": \"tensor&lt;float&gt;(p{})\",\n                    \"cells\": {\n                        \"4\": 1.0\n                    }\n                },\n                \"firstPhase\": 0.8807156260391702,\n                \"all_paragraph_similarities\": {\n                    \"type\": \"tensor&lt;float&gt;(p{})\",\n                    \"cells\": {\n                        \"1\": 0.8030083179473877,\n                        \"2\": 0.7992785573005676,\n                        \"3\": 0.8273358345031738,\n                        \"4\": 0.8807156085968018,\n                        \"0\": 0.849757194519043\n                    }\n                },\n                \"avg_paragraph_similarity\": 0.8320191025733947,\n                \"max_paragraph_similarity\": 0.8807156085968018\n            },\n            \"sddocname\": \"wiki\",\n            \"paragraphs\": [\n                \"The 24-hour clock is a way of telling the time in which the day runs from midnight to midnight and is divided into 24 hours, numbered from 0 to 23. It does not use a.m. or p.m. This system is also referred to (only in the US and the English speaking parts of Canada) as military time or (only in the United Kingdom and now very rarely) as continental time. In some parts of the world, it is called railway time. Also, the international standard notation of time (ISO 8601) is based on this format.\",\n                \"A time in the 24-hour clock is written in the form hours:minutes (for example, 01:23), or hours:minutes:seconds (01:23:45). Numbers under 10 have a zero in front (called a leading zero); e.g. 09:07. Under the 24-hour clock system, the day begins at midnight, 00:00, and the last minute of the day begins at 23:59 and ends at 24:00, which is identical to 00:00 of the following day. 12:00 can only be mid-day. Midnight is called 24:00 and is used to mean the end of the day and 00:00 is used to mean the beginning of the day. For example, you would say \\\"Tuesday at 24:00\\\" and \\\"Wednesday at 00:00\\\" to mean exactly the same time.\",\n                \"However, the US military prefers not to say 24:00 - they do not like to have two names for the same thing, so they always say \\\"23:59\\\", which is one minute before midnight.\",\n                \"24-hour clock time is used in computers, military, public safety, and transport. In many Asian, European and Latin American countries people use it to write the time. Many European people use it in speaking.\",\n                \"In railway timetables 24:00 means the \\\"end\\\" of the day. For example, a train due to arrive at a station during the last minute of a day arrives at 24:00; but trains which depart during the first minute of the day go at 00:00.\"\n            ],\n            \"documentid\": \"id:wikipedia:wiki::9985\",\n            \"title\": \"24-hour clock\",\n            \"url\": \"https://simple.wikipedia.org/wiki?curid=9985\"\n        }\n    }\n]\n</pre> <p>In short, the above query demonstrates how easy it is to combine various ranking strategies, and also combine with filters.</p> <p>To learn more about pre-filtering vs post-filtering, read Filtering strategies and serving performance. Semantic search with multi-vector indexing is a great read overall for this domain.</p> In\u00a0[26]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/multi-vector-indexing.html#multi-vector-indexing-with-hnsw","title":"Multi-vector indexing with HNSW\u00b6","text":"<p>This is the pyvespa steps of the multi-vector-indexing sample application. Go to the source for a full description and prerequisites, and read the blog post. Highlighted features:</p> <ul> <li>Approximate Nearest Neighbor Search - using HNSW or exact</li> <li>Use a Component to configure the Huggingface embedder.</li> <li>Using synthetic fields with auto-generated embeddings in data and query flow.</li> <li>Application package file export, model files in the application package, deployment from files.</li> <li>Multiphased ranking.</li> <li>How to control text search result highlighting.</li> </ul> <p>For simpler examples, see text search and pyvespa examples.</p> <p>Pyvespa is an add-on to Vespa, and this guide will export the application package containing <code>services.xml</code> and <code>wiki.sd</code>. The latter is the schema file for this application - knowing services.xml and schema files is useful when reading Vespa documentation.</p>"},{"location":"examples/multi-vector-indexing.html#create-the-application","title":"Create the application\u00b6","text":"<p>Configure the Vespa instance with a component loading the E5-small model. Components are used to plug in code and models to a Vespa application - read more:</p>"},{"location":"examples/multi-vector-indexing.html#configure-fields","title":"Configure fields\u00b6","text":"<p>Vespa has a variety of basic and complex field types. This application uses a combination of integer, text and tensor fields, making it easy to implement hybrid ranking use cases:</p>"},{"location":"examples/multi-vector-indexing.html#configure-rank-profiles","title":"Configure rank profiles\u00b6","text":"<p>A rank profile defines the computation for the ranking, with a wide range of possible features as input. Below you will find <code>first_phase</code> ranking using text ranking (<code>bm</code>), semantic ranking using vector distance (consider a tensor a vector here), and combinations of the two:</p>"},{"location":"examples/multi-vector-indexing.html#configure-fieldset","title":"Configure fieldset\u00b6","text":"<p>A fieldset is a way to configure search in multiple fields:</p>"},{"location":"examples/multi-vector-indexing.html#configure-document-summary","title":"Configure document summary\u00b6","text":"<p>A document summary is the collection of fields to return in query results - the default summary is used unless other specified in the query. Here we configure a <code>minimal</code> fieldset without the larger paragraph text/embedding fields:</p>"},{"location":"examples/multi-vector-indexing.html#export-the-configuration","title":"Export the configuration\u00b6","text":"<p>At this point, the application is well defined. Remember that the Component configuration at start configures model files to be found in a <code>model</code> directory. We must therefore export the configuration and add the models, before we can deploy to the Vespa instance. Export the application package:</p>"},{"location":"examples/multi-vector-indexing.html#download-model-files","title":"Download model files\u00b6","text":"<p>At this point, we can save the model files into the application package:</p>"},{"location":"examples/multi-vector-indexing.html#deploy-the-application","title":"Deploy the application\u00b6","text":"<p>As all the files in the app package are ready, we can start a Vespa instance - here using Docker. Deploy the app package:</p>"},{"location":"examples/multi-vector-indexing.html#feed-documents","title":"Feed documents\u00b6","text":"<p>Download the Wikipedia articles:</p>"},{"location":"examples/multi-vector-indexing.html#simple-retrieve-all-articles-with-undefined-ranking","title":"Simple retrieve all articles with undefined ranking\u00b6","text":"<p>Run a query selecting all documents, returning two of them. The rank profile is the built-in <code>unranked</code> which means no ranking calculations are done, the results are returned in random order:</p>"},{"location":"examples/multi-vector-indexing.html#traditional-keyword-search-with-bm25-ranking-on-the-article-level","title":"Traditional keyword search with BM25 ranking on the article level\u00b6","text":"<p>Run a text-search query and use the bm25 ranking profile configured at the start of this guide: <code>2*bm25(title) + bm25(paragraphs)</code>. Here, we use BM25 on the <code>title</code> and <code>paragraph</code> text fields, giving more weight to matches in title:</p>"},{"location":"examples/multi-vector-indexing.html#semantic-vector-search-on-the-paragraph-level","title":"Semantic vector search on the paragraph level\u00b6","text":"<p>This query creates an embedding of the query \"what does 24 mean in the context of railways\" and specifies the <code>semantic</code> ranking profile: <code>cos(distance(field,paragraph_embeddings))</code>. This will hence compute the distance between the vector in the query and the vectors computed when indexing: <code>\"input paragraphs\", \"embed\", \"index\", \"attribute\"</code>:</p>"},{"location":"examples/multi-vector-indexing.html#hybrid-search-and-ranking","title":"Hybrid search and ranking\u00b6","text":"<p>Hybrid combining keyword search on the article level with vector search in the paragraph index:</p>"},{"location":"examples/multi-vector-indexing.html#hybrid-search-and-filter","title":"Hybrid search and filter\u00b6","text":"<p>YQL is a structured query langauge. In the query examples, the user input is fed as-is using the <code>userQuery()</code> operator.</p> <p>Filters are normally separate from the user input, below is an example of adding a filter <code>url contains \"9985\"</code> to the YQL string.</p> <p>Finally, the use the Query API for other options, like highlighting - here disable bolding:</p>"},{"location":"examples/multi-vector-indexing.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html","title":"Multilingual multi vector reps with cohere cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa cohere==4.57 datasets vespacli\n</pre> !pip3 install -U pyvespa cohere==4.57 datasets vespacli In\u00a0[\u00a0]: Copied! <pre>from datasets import load_dataset\n\nlang = \"de\"  # Use the first 10K chunks from the German Wikipedia subset\ndocs = load_dataset(\n    \"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\",\n    lang,\n    split=\"train\",\n    streaming=True,\n).take(10000)\n</pre> from datasets import load_dataset  lang = \"de\"  # Use the first 10K chunks from the German Wikipedia subset docs = load_dataset(     \"Cohere/wikipedia-2023-11-embed-multilingual-v3-int8-binary\",     lang,     split=\"train\",     streaming=True, ).take(10000) In\u00a0[160]: Copied! <pre>pages = dict()\nfor d in docs:\n    url = d[\"url\"]\n    if url not in pages:\n        pages[url] = [d]\n    else:\n        pages[url].append(d)\n</pre> pages = dict() for d in docs:     url = d[\"url\"]     if url not in pages:         pages[url] = [d]     else:         pages[url].append(d) In\u00a0[173]: Copied! <pre>print(len(list(pages.keys())))\n</pre> print(len(list(pages.keys()))) <pre>1866\n</pre> In\u00a0[174]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet\n\nmy_schema = Schema(\n    name=\"page\",\n    mode=\"index\",\n    document=Document(\n        fields=[\n            Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"language\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\", \"set_language\"],\n                match=[\"word\"],\n                rank=\"filter\",\n            ),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"chunks\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"url\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"binary_vectors\",\n                type=\"tensor&lt;int8&gt;(chunk{}, x[128])\",\n                indexing=[\"attribute\", \"index\"],\n                attribute=[\"distance-metric: hamming\"],\n            ),\n            Field(\n                name=\"int8_vectors\",\n                type=\"tensor&lt;int8&gt;(chunk{}, x[1024])\",\n                indexing=[\"attribute\"],\n                attribute=[\"paged\"],\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"chunks\", \"title\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet  my_schema = Schema(     name=\"page\",     mode=\"index\",     document=Document(         fields=[             Field(name=\"doc_id\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"language\",                 type=\"string\",                 indexing=[\"summary\", \"index\", \"set_language\"],                 match=[\"word\"],                 rank=\"filter\",             ),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"chunks\",                 type=\"array\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"url\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"binary_vectors\",                 type=\"tensor(chunk{}, x[128])\",                 indexing=[\"attribute\", \"index\"],                 attribute=[\"distance-metric: hamming\"],             ),             Field(                 name=\"int8_vectors\",                 type=\"tensor(chunk{}, x[1024])\",                 indexing=[\"attribute\"],                 attribute=[\"paged\"],             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"chunks\", \"title\"])], ) <p>We must add the schema to a Vespa application package. This consists of configuration files, schemas, models, and possibly even custom code (plugins).</p> In\u00a0[9]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"wikipedia\"\nvespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema])\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"wikipedia\" vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[my_schema]) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p><code>unpack_bits</code> unpacks the binary representation into a 1024-dimensional float vector doc.</p> <p>We define two tensor inputs, one compact binary representation that is used for the nearestNeighbor search and one full version that is used in ranking.</p> In\u00a0[138]: Copied! <pre>from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function\n\n\nrerank = RankProfile(\n    name=\"rerank\",\n    inputs=[\n        (\"query(q_binary)\", \"tensor&lt;int8&gt;(x[128])\"),\n        (\"query(q_int8)\", \"tensor&lt;int8&gt;(x[1024])\"),\n        (\"query(q_full)\", \"tensor&lt;float&gt;(x[1024])\"),\n    ],\n    functions=[\n        Function(  # this returns a tensor&lt;float&gt;(chunk{}, x[1024]) with values -1 or 1\n            name=\"unpack_binary_representation\",\n            expression=\"2*unpack_bits(attribute(binary_vectors)) -1\",\n        ),\n        Function(\n            name=\"all_chunks_cosine\",\n            expression=\"cosine_similarity(query(q_int8), attribute(int8_vectors),x)\",\n        ),\n        Function(\n            name=\"int8_float_dot_products\",\n            expression=\"sum(query(q_full)*unpack_binary_representation,x)\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"reduce(int8_float_dot_products, max, chunk)\"\n    ),\n    second_phase=SecondPhaseRanking(\n        expression=\"reduce(all_chunks_cosine, max, chunk)\"  # rescoring using the full query and a unpacked binary_vector\n    ),\n    match_features=[\n        \"distance(field, binary_vectors)\",\n        \"all_chunks_cosine\",\n        \"firstPhase\",\n        \"bm25(title)\",\n        \"bm25(chunks)\",\n    ],\n)\nmy_schema.add_rank_profile(rerank)\n</pre> from vespa.package import RankProfile, FirstPhaseRanking, SecondPhaseRanking, Function   rerank = RankProfile(     name=\"rerank\",     inputs=[         (\"query(q_binary)\", \"tensor(x[128])\"),         (\"query(q_int8)\", \"tensor(x[1024])\"),         (\"query(q_full)\", \"tensor(x[1024])\"),     ],     functions=[         Function(  # this returns a tensor(chunk{}, x[1024]) with values -1 or 1             name=\"unpack_binary_representation\",             expression=\"2*unpack_bits(attribute(binary_vectors)) -1\",         ),         Function(             name=\"all_chunks_cosine\",             expression=\"cosine_similarity(query(q_int8), attribute(int8_vectors),x)\",         ),         Function(             name=\"int8_float_dot_products\",             expression=\"sum(query(q_full)*unpack_binary_representation,x)\",         ),     ],     first_phase=FirstPhaseRanking(         expression=\"reduce(int8_float_dot_products, max, chunk)\"     ),     second_phase=SecondPhaseRanking(         expression=\"reduce(all_chunks_cosine, max, chunk)\"  # rescoring using the full query and a unpacked binary_vector     ),     match_features=[         \"distance(field, binary_vectors)\",         \"all_chunks_cosine\",         \"firstPhase\",         \"bm25(title)\",         \"bm25(chunks)\",     ], ) my_schema.add_rank_profile(rerank) In\u00a0[24]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[153]: Copied! <pre>def generate_vespa_feed_documents(pages):\n    for url, chunks in pages.items():\n        title = None\n        text_chunks = []\n        binary_vectors = {}\n        int8_vectors = {}\n        for chunk_id, chunk in enumerate(chunks):\n            title = chunk[\"title\"]\n            text = chunk[\"text\"]\n            text_chunks.append(text)\n            emb_ubinary = chunk[\"emb_ubinary\"]\n            emb_ubinary = [x - 128 for x in emb_ubinary]\n            emb_int8 = chunk[\"emb_int8\"]\n\n            binary_vectors[chunk_id] = emb_ubinary\n            int8_vectors[chunk_id] = emb_int8\n\n        vespa_json = {\n            \"id\": url,\n            \"fields\": {\n                \"doc_id\": url,\n                \"url\": url,\n                \"language\": lang,  # Assuming `lang` is defined somewhere\n                \"title\": title,\n                \"chunks\": text_chunks,\n                \"binary_vectors\": binary_vectors,\n                \"int8_vectors\": int8_vectors,\n            },\n        }\n        yield vespa_json\n</pre> def generate_vespa_feed_documents(pages):     for url, chunks in pages.items():         title = None         text_chunks = []         binary_vectors = {}         int8_vectors = {}         for chunk_id, chunk in enumerate(chunks):             title = chunk[\"title\"]             text = chunk[\"text\"]             text_chunks.append(text)             emb_ubinary = chunk[\"emb_ubinary\"]             emb_ubinary = [x - 128 for x in emb_ubinary]             emb_int8 = chunk[\"emb_int8\"]              binary_vectors[chunk_id] = emb_ubinary             int8_vectors[chunk_id] = emb_int8          vespa_json = {             \"id\": url,             \"fields\": {                 \"doc_id\": url,                 \"url\": url,                 \"language\": lang,  # Assuming `lang` is defined somewhere                 \"title\": title,                 \"chunks\": text_chunks,                 \"binary_vectors\": binary_vectors,                 \"int8_vectors\": int8_vectors,             },         }         yield vespa_json In\u00a0[154]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         ) In\u00a0[156]: Copied! <pre>app.feed_iterable(\n    iter=generate_vespa_feed_documents(pages),\n    schema=\"page\",\n    callback=callback,\n    max_queue_size=4000,\n    max_workers=16,\n    max_connections=16,\n)\n</pre> app.feed_iterable(     iter=generate_vespa_feed_documents(pages),     schema=\"page\",     callback=callback,     max_queue_size=4000,     max_workers=16,     max_connections=16, ) In\u00a0[48]: Copied! <pre>import cohere\n\n# Make sure that the environment variable CO_API_KEY is set to your API key\nco = cohere.Client()\n</pre> import cohere  # Make sure that the environment variable CO_API_KEY is set to your API key co = cohere.Client() In\u00a0[175]: Copied! <pre>query = 'Welche britische Rockband hat das Lied \"Spread Your Wings\"?'\n# Make sure to set input_type=\"search_query\" when getting the embeddings for the query.\n# We ask for 3 types of embeddings: float, binary, and int8\nquery_emb = co.embed(\n    [query],\n    model=\"embed-multilingual-v3.0\",\n    input_type=\"search_query\",\n    embedding_types=[\"float\", \"binary\", \"int8\"],\n)\n</pre> query = 'Welche britische Rockband hat das Lied \"Spread Your Wings\"?' # Make sure to set input_type=\"search_query\" when getting the embeddings for the query. # We ask for 3 types of embeddings: float, binary, and int8 query_emb = co.embed(     [query],     model=\"embed-multilingual-v3.0\",     input_type=\"search_query\",     embedding_types=[\"float\", \"binary\", \"int8\"], ) <p>Now, we use the nearestNeighbor query operator to to retrieve 1000 pages using hamming distance. This phase uses the minimum chunk-level distance for selecting pages. Essentially finding the best chunk in the page. This ensures diversity as we retrieve pages, not chunks.</p> <p>These hits are exposed to the configured ranking phases that perform the re-ranking.</p> <p>Notice the language parameter, for language-specific processing of the query.</p> In\u00a0[158]: Copied! <pre>from vespa.io import VespaQueryResponse\n\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select * from page where userQuery() or ({targetHits:1000, approximate:true}nearestNeighbor(binary_vectors,q_binary))\",\n    ranking=\"rerank\",\n    query=query,\n    language=\"de\",  # don't guess the language of the query\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(q_binary)\": query_emb.embeddings.binary[0],\n        \"input.query(q_full)\": query_emb.embeddings.float[0],\n        \"input.query(q_int8)\": query_emb.embeddings.int8[0],\n    },\n)\nassert response.is_successful()\nresponse.hits[0]\n</pre> from vespa.io import VespaQueryResponse   response: VespaQueryResponse = app.query(     yql=\"select * from page where userQuery() or ({targetHits:1000, approximate:true}nearestNeighbor(binary_vectors,q_binary))\",     ranking=\"rerank\",     query=query,     language=\"de\",  # don't guess the language of the query     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(q_binary)\": query_emb.embeddings.binary[0],         \"input.query(q_full)\": query_emb.embeddings.float[0],         \"input.query(q_int8)\": query_emb.embeddings.int8[0],     }, ) assert response.is_successful() response.hits[0] Out[158]: <pre>{'id': 'id:page:page::https:/de.wikipedia.org/wiki/Spread Your Wings',\n 'relevance': 0.8184863924980164,\n 'source': 'wikipedia_content',\n 'fields': {'matchfeatures': {'bm25(chunks)': 28.125529605038967,\n   'bm25(title)': 7.345395294159827,\n   'distance(field,binary_vectors)': 170.0,\n   'firstPhase': 8.274434089660645,\n   'all_chunks_cosine': {'0': 0.8184863924980164,\n    '1': 0.6203299760818481,\n    '2': 0.643619954586029,\n    '3': 0.6706648468971252,\n    '4': 0.524447500705719,\n    '5': 0.6730406880378723}},\n  'sddocname': 'page',\n  'documentid': 'id:page:page::https:/de.wikipedia.org/wiki/Spread Your Wings',\n  'doc_id': 'https://de.wikipedia.org/wiki/Spread%20Your%20Wings',\n  'language': 'de',\n  'title': 'Spread Your Wings',\n  'chunks': ['Spread Your Wings ist ein Lied der britischen Rockband Queen, das von deren Bassisten John Deacon geschrieben wurde. Es ist auf dem im Oktober 1977 erschienenen Album News of the World enthalten und wurde am 10. Februar 1978 in Europa als Single mit Sheer Heart Attack als B-Seite ver\u00f6ffentlicht. In Nordamerika wurde es nicht als Single ver\u00f6ffentlicht, sondern erschien stattdessen 1980 als B-Seite des Billboard Nummer-1-Hits Crazy Little Thing Called Love. Das Lied wurde zwar kein gro\u00dfer Hit in den Charts, ist aber unter Queen-Fans sehr beliebt.',\n   'Der Text beschreibt einen jungen Mann namens Sammy, der in einer Bar zum Putzen arbeitet (\u201cYou should\u2019ve been sweeping/up the Emerald bar\u201d). W\u00e4hrend sein Chef ihn in den Strophen beschimpft und sagt, er habe keinerlei Ambitionen und solle sich mit dem zufriedengeben, was er hat (\u201cYou\u2019ve got no real ambition,/you won\u2019t get very far/Sammy boy don\u2019t you know who you are/Why can\u2019t you be happy/at the Emerald bar\u201d), ermuntert ihn der Erz\u00e4hler im Refrain, seinen Tr\u00e4umen nachzugehen (\u201cspread your wings and fly away/Fly away, far away/Pull yourself together \u2018cause you know you should do better/That\u2019s because you\u2019re a free man.\u201d).',\n   'Das Lied ist im 4/4-Takt geschrieben, beginnt in der Tonart D-Dur, wechselt in der Bridge zu deren Paralleltonart h-Moll und endet wieder mit D-Dur. Es beginnt mit einem kurzen Piano-Intro, gefolgt von der ersten Strophe, die nur mit einer akustischen Gitarre, Piano und Hi-Hats begleitet wird, und dem Refrain, in dem die E-Gitarre und das Schlagzeug hinzukommen. Die Bridge besteht aus kurzen, langsamen Gitarrent\u00f6nen. Die zweite Strophe enth\u00e4lt im Gegensatz zur ersten beinahe von Anfang an E-Gitarren-Kl\u00e4nge und Schlagzeugt\u00f6ne. Darauf folgt nochmals der Refrain. Das Outro ist \u2013 abgesehen von zwei kurzen Rufen \u2013 instrumental. Es besteht aus einem l\u00e4ngeren Gitarrensolo, in dem \u2013 was f\u00fcr Queen \u00e4u\u00dferst ungew\u00f6hnlich ist \u2013 dieselbe Akkordfolge mehrere Male wiederholt wird und ab dem vierten Mal langsam ausblendet. Das ganze Lied enth\u00e4lt keinerlei Hintergrundgesang, sondern nur den Leadgesang von Freddie Mercury.',\n   'Das Musikvideo wurde ebenso wie das zu We Will Rock You im Januar 1978 im Garten von Roger Taylors damaligen Anwesen Millhanger House gedreht, welches sich im Dorf Thursley im S\u00fcdwesten der englischen Grafschaft Surrey befindet. Der Boden ist dabei von einer Eis- und Schneeschicht \u00fcberzogen, auf der die Musiker spielten.',\n   \"Brian May sagte dazu sp\u00e4ter: \u201cLooking back, it couldn't be done there \u2013 you couldn't do that!\u201d (\u201eWenn ich zur\u00fcckschaue, h\u00e4tte es nicht dort gemacht werden d\u00fcrfen \u2013 man konnte das nicht tun!\u201c)\",\n   'Das Lied wurde mehrfach gecovert, unter anderem von der deutschen Metal-Band Blind Guardian auf ihrem 1992 erschienenen Album Somewhere Far Beyond. Weitere Coverversionen gibt es u. a. von Jeff Scott Soto und Shawn Mars.'],\n  'url': 'https://de.wikipedia.org/wiki/Spread%20Your%20Wings'}}</pre> <p>Notice the returned hits. The <code>relevance</code> is the score assigned by the second-phase expression. Also notice, that we included bm25 scores in the match-features. In this case, they do not influence ranking. The bm25 over chunks is calculated across all the elements, like if it was a bag of words or a single field string.</p> <p>We now have the full Wikipedia context for all the retrieved pages. We have all the chunks and all the cosine similarity scores for all the chunks in the wikipedia page, and no need to duplicate title and url into separate retrievable units like with single-vector databases.</p> <p>In RAG applications, we can now choose how much context we want to input to the generative step:</p> <ul> <li>All the chunks</li> <li>Only the best k chunks with a threshold on the cosine similarity</li> <li>The adjacent chunks of the best chunk</li> </ul> <p>Or combinations of the above.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#multilingual-hybrid-search-with-cohere-binary-embeddings-and-vespa","title":"Multilingual Hybrid Search with Cohere binary embeddings and Vespa\u00b6","text":"<p>Cohere just released a new embedding API supporting binary vectors. Read the announcement in the blog post: Cohere int8 &amp; binary Embeddings - Scale Your Vector Database to Large Datasets.</p> <p>We are excited to announce that Cohere Embed is the first embedding model that natively supports int8 and binary embeddings.</p> <p>This notebook demonstrates:</p> <ul> <li>Building a multilingual search application over a sample of the German split of Wikipedia using binarized cohere embeddings</li> <li>Indexing multiple binary embeddings per document; without having to split the chunks across multiple retrievable units</li> <li>Hybrid search, combining the lexical matching capabilities of Vespa with Cohere binary embeddings</li> <li>Re-scoring the binarized vectors for improved accuracy</li> </ul> <p></p> <p>Install the dependencies:</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#dataset-exploration","title":"Dataset exploration\u00b6","text":"<p>Cohere has released a large Wikipedia dataset</p> <p>This dataset contains the wikimedia/wikipedia dataset dump from 2023-11-01 from Wikipedia in all 300+ languages. The embeddings are provided as int8 and ubinary that allow quick search and reduction of your vector index size up to 32.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#aggregate-from-chunks-to-pages","title":"Aggregate from chunks to pages\u00b6","text":"<p>We want to aggregate the chunk &lt;&gt; vector representations into their natural retrievable unit - a Wikipedia page. We can still search the chunks and the chunk vector representation but retrieve pages instead of chunks. This avoids duplicating page-level metadata like url and title, while still being able to have meaningful semantic search representations. For RAG applications, this also means that we have the full page level context available when we retrieve information for the generative phase.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>First, we define a Vespa schema with the fields we want to store and their type.</p> <p>We use Vespa's multi-vector indexing support - See Revolutionizing Semantic Search with Multi-Vector HNSW Indexing in Vespa for details. Highlights</p> <ul> <li>language for language-specific linguistic processing for keyword search</li> <li>Two named multi-vector representations with different precision and in-memory versus off-memory</li> <li>The named multi-vector representations holds the chunk-level embeddings</li> <li>Chunks is an array of string where we enable BM25</li> <li>Metadata for the page (url, title)</li> </ul>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#feed-the-wikipedia-pages-and-the-embedding-representations","title":"Feed the Wikipedia pages and the embedding representations\u00b6","text":"<p>Read more about feeding with pyvespa in PyVespa:reads and writes.</p> <p>In this case, we use a generator to yield document operations</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> <li>Practical Nearest Neighbor Search Guide</li> </ul> <p>To obtain the query embedding we use the Cohere embed API.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#conclusions","title":"Conclusions\u00b6","text":"<p>These new Cohere binary embeddings are a huge step forward for cost-efficient vector search at scale and integrate perfectly with the rich feature set in Vespa. Including multilingual text search capabilities and hybrid search.</p>"},{"location":"examples/multilingual-multi-vector-reps-with-cohere-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>We can now delete the cloud instance:</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html","title":"pdf retrieval with ColQwen2 vlm Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y\n</pre> !sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y <p>Now install the required python packages:</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install colpali-engine==0.3.1 pdf2image pypdf pyvespa vespacli requests numpy tqdm\n</pre> !pip3 install colpali-engine==0.3.1 pdf2image pypdf pyvespa vespacli requests numpy tqdm In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom io import BytesIO\nfrom colpali_engine.models import ColQwen2, ColQwen2Processor\n</pre> import torch from torch.utils.data import DataLoader from tqdm import tqdm from io import BytesIO from colpali_engine.models import ColQwen2, ColQwen2Processor In\u00a0[\u00a0]: Copied! <pre>model_name = \"vidore/colqwen2-v0.1\"\n\nmodel = ColQwen2.from_pretrained(\n    model_name, torch_dtype=torch.bfloat16, device_map=\"auto\"\n)\nprocessor = ColQwen2Processor.from_pretrained(model_name)\nmodel = model.eval()\n</pre> model_name = \"vidore/colqwen2-v0.1\"  model = ColQwen2.from_pretrained(     model_name, torch_dtype=torch.bfloat16, device_map=\"auto\" ) processor = ColQwen2Processor.from_pretrained(model_name) model = model.eval() In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader\n\n\ndef download_pdf(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return BytesIO(response.content)\n    else:\n        raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")\n\n\ndef get_pdf_images(pdf_url):\n    # Download the PDF\n    pdf_file = download_pdf(pdf_url)\n    # Save the PDF temporarily to disk (pdf2image requires a file path)\n    temp_file = \"temp.pdf\"\n    with open(temp_file, \"wb\") as f:\n        f.write(pdf_file.read())\n    reader = PdfReader(temp_file)\n    page_texts = []\n    for page_number in range(len(reader.pages)):\n        page = reader.pages[page_number]\n        text = page.extract_text()\n        page_texts.append(text)\n    images = convert_from_path(temp_file)\n    assert len(images) == len(page_texts)\n    return (images, page_texts)\n</pre> import requests from pdf2image import convert_from_path from pypdf import PdfReader   def download_pdf(url):     response = requests.get(url)     if response.status_code == 200:         return BytesIO(response.content)     else:         raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")   def get_pdf_images(pdf_url):     # Download the PDF     pdf_file = download_pdf(pdf_url)     # Save the PDF temporarily to disk (pdf2image requires a file path)     temp_file = \"temp.pdf\"     with open(temp_file, \"wb\") as f:         f.write(pdf_file.read())     reader = PdfReader(temp_file)     page_texts = []     for page_number in range(len(reader.pages)):         page = reader.pages[page_number]         text = page.extract_text()         page_texts.append(text)     images = convert_from_path(temp_file)     assert len(images) == len(page_texts)     return (images, page_texts) <p>We define a few sample PDFs to work with. The PDFs are discovered from this url.</p> In\u00a0[\u00a0]: Copied! <pre>sample_pdfs = [\n    {\n        \"title\": \"ConocoPhillips Sustainability Highlights - Nature (24-0976)\",\n        \"url\": \"https://static.conocophillips.com/files/resources/24-0976-sustainability-highlights_nature.pdf\",\n    },\n    {\n        \"title\": \"ConocoPhillips Managing Climate Related Risks\",\n        \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-managing-climate-related-risks.pdf\",\n    },\n    {\n        \"title\": \"ConocoPhillips 2023 Sustainability Report\",\n        \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-sustainability-report.pdf\",\n    },\n]\n</pre> sample_pdfs = [     {         \"title\": \"ConocoPhillips Sustainability Highlights - Nature (24-0976)\",         \"url\": \"https://static.conocophillips.com/files/resources/24-0976-sustainability-highlights_nature.pdf\",     },     {         \"title\": \"ConocoPhillips Managing Climate Related Risks\",         \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-managing-climate-related-risks.pdf\",     },     {         \"title\": \"ConocoPhillips 2023 Sustainability Report\",         \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-sustainability-report.pdf\",     }, ] <p>Now we can convert the PDFs to images and also extract the text content.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_images, page_texts = get_pdf_images(pdf[\"url\"])\n    pdf[\"images\"] = page_images\n    pdf[\"texts\"] = page_texts\n</pre> for pdf in sample_pdfs:     page_images, page_texts = get_pdf_images(pdf[\"url\"])     pdf[\"images\"] = page_images     pdf[\"texts\"] = page_texts <p>Let us look at the extracted image of the first PDF page. This is the document side input to ColPali, one image per page.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display\n\n\ndef resize_image(image, max_height=800):\n    width, height = image.size\n    if height &gt; max_height:\n        ratio = max_height / height\n        new_width = int(width * ratio)\n        new_height = int(height * ratio)\n        return image.resize((new_width, new_height))\n    return image\n\n\ndisplay(resize_image(sample_pdfs[0][\"images\"][0]))\n</pre> from IPython.display import display   def resize_image(image, max_height=800):     width, height = image.size     if height &gt; max_height:         ratio = max_height / height         new_width = int(width * ratio)         new_height = int(height * ratio)         return image.resize((new_width, new_height))     return image   display(resize_image(sample_pdfs[0][\"images\"][0])) <p>Let us also look at the extracted text content of the first PDF page.</p> In\u00a0[\u00a0]: Copied! <pre>print(sample_pdfs[0][\"texts\"][0])\n</pre> print(sample_pdfs[0][\"texts\"][0]) <p>Notice how the layout and order of the text is different from the image representation. Note that</p> <ul> <li>The headlines NATURE and Sustainability have been combined into one word (NATURESustainability).</li> <li>The 0.03% has been converted to 0.03 and order is not preserved in the text representation.</li> <li>The data in the infographics is not represented in the text representation.</li> </ul> <p>Now we use the ColPali model to generate embeddings of the images.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_embeddings = []\n    dataloader = DataLoader(\n        pdf[\"images\"],\n        batch_size=2,\n        shuffle=False,\n        collate_fn=lambda x: processor.process_images(x),\n    )\n\n    for batch_doc in tqdm(dataloader):\n        with torch.no_grad():\n            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n            embeddings_doc = model(**batch_doc)\n            page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n    pdf[\"embeddings\"] = page_embeddings\n</pre> for pdf in sample_pdfs:     page_embeddings = []     dataloader = DataLoader(         pdf[\"images\"],         batch_size=2,         shuffle=False,         collate_fn=lambda x: processor.process_images(x),     )      for batch_doc in tqdm(dataloader):         with torch.no_grad():             batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}             embeddings_doc = model(**batch_doc)             page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))     pdf[\"embeddings\"] = page_embeddings <p>Now we are done with the document side embeddings, we convert the embeddings to Vespa JSON format so we can store (and index) them in Vespa. Details in Vespa JSON feed format doc.</p> <p>We use binary quantization (BQ) of the page level ColPali vector embeddings to reduce their size by 32x.</p> <p>Read more about binarization of multi-vector representations in the colbert blog post.</p> <p>The binarization step maps 128 dimensional floats to 128 bits, or 16 bytes per vector.</p> <p>Reducing the size by 32x. On the DocVQA benchmark, binarization results in a small drop in ranking accuracy.</p> <p>We also demonstrate how to store the image data in Vespa using the raw type for binary data. To encode the binary data in JSON, we use base64 encoding.</p> In\u00a0[\u00a0]: Copied! <pre>import base64\n\n\ndef get_base64_image(image):\n    buffered = BytesIO()\n    image.save(buffered, format=\"JPEG\")\n    return str(base64.b64encode(buffered.getvalue()), \"utf-8\")\n</pre> import base64   def get_base64_image(image):     buffered = BytesIO()     image.save(buffered, format=\"JPEG\")     return str(base64.b64encode(buffered.getvalue()), \"utf-8\") In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nvespa_feed = []\nfor pdf in sample_pdfs:\n    url = pdf[\"url\"]\n    title = pdf[\"title\"]\n    for page_number, (page_text, embedding, image) in enumerate(\n        zip(pdf[\"texts\"], pdf[\"embeddings\"], pdf[\"images\"])\n    ):\n        base_64_image = get_base64_image(resize_image(image, 640))\n        embedding_dict = dict()\n        for idx, patch_embedding in enumerate(embedding):\n            binary_vector = (\n                np.packbits(np.where(patch_embedding &gt; 0, 1, 0))\n                .astype(np.int8)\n                .tobytes()\n                .hex()\n            )\n            embedding_dict[idx] = binary_vector\n        page = {\n            \"id\": hash(url + str(page_number)),\n            \"url\": url,\n            \"title\": title,\n            \"page_number\": page_number,\n            \"image\": base_64_image,\n            \"text\": page_text,\n            \"embedding\": embedding_dict,\n        }\n        vespa_feed.append(page)\n</pre> import numpy as np  vespa_feed = [] for pdf in sample_pdfs:     url = pdf[\"url\"]     title = pdf[\"title\"]     for page_number, (page_text, embedding, image) in enumerate(         zip(pdf[\"texts\"], pdf[\"embeddings\"], pdf[\"images\"])     ):         base_64_image = get_base64_image(resize_image(image, 640))         embedding_dict = dict()         for idx, patch_embedding in enumerate(embedding):             binary_vector = (                 np.packbits(np.where(patch_embedding &gt; 0, 1, 0))                 .astype(np.int8)                 .tobytes()                 .hex()             )             embedding_dict[idx] = binary_vector         page = {             \"id\": hash(url + str(page_number)),             \"url\": url,             \"title\": title,             \"page_number\": page_number,             \"image\": base_64_image,             \"text\": page_text,             \"embedding\": embedding_dict,         }         vespa_feed.append(page) In\u00a0[\u00a0]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\ncolpali_schema = Schema(\n    name=\"pdf_page\",\n    document=Document(\n        fields=[\n            Field(\n                name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"], match=[\"word\"]\n            ),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(name=\"image\", type=\"raw\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;int8&gt;(patch{}, v[16])\",\n                indexing=[\n                    \"attribute\",\n                    \"index\",\n                ],  # adds HNSW index for candidate retrieval.\n                ann=HNSW(\n                    distance_metric=\"hamming\",\n                    max_links_per_node=32,\n                    neighbors_to_explore_at_insert=400,\n                ),\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  colpali_schema = Schema(     name=\"pdf_page\",     document=Document(         fields=[             Field(                 name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"], match=[\"word\"]             ),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(name=\"image\", type=\"raw\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"embedding\",                 type=\"tensor(patch{}, v[16])\",                 indexing=[                     \"attribute\",                     \"index\",                 ],  # adds HNSW index for candidate retrieval.                 ann=HNSW(                     distance_metric=\"hamming\",                     max_links_per_node=32,                     neighbors_to_explore_at_insert=400,                 ),             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])], ) <p>Notice the <code>embedding</code> field which is a tensor field with the type <code>tensor&lt;int8&gt;(patch{}, v[16])</code>. This is the field we use to represent the page level patch embeddings from ColPali.</p> <p>We also enable HNSW indexing for this field to enable fast nearest neighbor search which is used for candidate retrieval.</p> <p>We use binary hamming distance as an approximation of the cosine similarity. Hamming distance is a good approximation for binary representations, and it is much faster to compute than cosine similarity/dot product.</p> <p>The <code>embedding</code> field is an example of a mixed tensor where we combine one mapped (sparse) dimensions with a dense dimension.</p> <p>Read more in Tensor guide. We also enable BM25 for the <code>title</code> and <code>texts</code>\u00a0fields. Notice that the <code>image</code> field use type <code>raw</code> to store the binary image data, encoded with as a base64 string.</p> <p>Create the Vespa application package:</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"visionrag6\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colpali_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"visionrag6\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colpali_schema] ) <p>Now we define how we want to rank the pages for a query. We use Vespa's support for BM25 for the text, and late interaction with Max Sim for the image embeddings.</p> <p>This means that we use the the text representations as a candidate retrieval phase, then we use the ColPALI embeddings with MaxSim to rerank the pages.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolpali_profile = RankProfile(\n    name=\"default\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(text)\"),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"bm25_score\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=100),\n)\ncolpali_schema.add_rank_profile(colpali_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colpali_profile = RankProfile(     name=\"default\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(text)\"),     ],     first_phase=FirstPhaseRanking(expression=\"bm25_score\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=100), ) colpali_schema.add_rank_profile(colpali_profile) <p>The first phase uses a linear combination of BM25 scores for the text fields, and the second phase uses the MaxSim function with the image embeddings. Notice that Vespa supports a <code>unpack_bits</code> function to convert the 16 compressed binary vectors to 128-dimensional floats for the MaxSim function. The query input tensor is not compressed and using full float resolution.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[\u00a0]: Copied! <pre>print(\"Number of PDF pages:\", len(vespa_feed))\n</pre> print(\"Number of PDF pages:\", len(vespa_feed)) <p>Index the documents in Vespa using the Vespa HTTP API.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\nasync with app.asyncio(connections=1, timeout=180) as session:\n    for page in tqdm(vespa_feed):\n        response: VespaResponse = await session.feed_data_point(\n            data_id=page[\"id\"], fields=page, schema=\"pdf_page\"\n        )\n        if not response.is_successful():\n            print(response.json())\n</pre> from vespa.io import VespaResponse  async with app.asyncio(connections=1, timeout=180) as session:     for page in tqdm(vespa_feed):         response: VespaResponse = await session.feed_data_point(             data_id=page[\"id\"], fields=page, schema=\"pdf_page\"         )         if not response.is_successful():             print(response.json()) <p>Now we can query Vespa with the text query and rerank the results using the ColPali embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>queries = [\n    \"Percentage of non-fresh water as source?\",\n    \"Policies related to nature risk?\",\n    \"How much of produced water is recycled?\",\n]\n</pre> queries = [     \"Percentage of non-fresh water as source?\",     \"Policies related to nature risk?\",     \"How much of produced water is recycled?\", ] <p>Obtain the query embeddings using the ColPali model:</p> In\u00a0[\u00a0]: Copied! <pre>dataloader = DataLoader(\n    queries,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor.process_queries(x),\n)\nqs = []\nfor batch_query in dataloader:\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query)\n        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> dataloader = DataLoader(     queries,     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor.process_queries(x), ) qs = [] for batch_query in dataloader:     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query)         qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <p>We create a simple routine to display the results. We render the image and the title of the retrieved page/document.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, HTML\n\n\ndef display_query_results(query, response, hits=5):\n    query_time = response.json.get(\"timing\", {}).get(\"searchtime\", -1)\n    query_time = round(query_time, 2)\n    count = response.json.get(\"root\", {}).get(\"fields\", {}).get(\"totalCount\", 0)\n    html_content = f\"&lt;h3&gt;Query text: '{query}', query time {query_time}s, count={count}, top results:&lt;/h3&gt;\"\n\n    for i, hit in enumerate(response.hits[:hits]):\n        title = hit[\"fields\"][\"title\"]\n        url = hit[\"fields\"][\"url\"]\n        page = hit[\"fields\"][\"page_number\"]\n        image = hit[\"fields\"][\"image\"]\n        score = hit[\"relevance\"]\n\n        html_content += f\"&lt;h4&gt;PDF Result {i + 1}&lt;/h4&gt;\"\n        html_content += f'&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; &lt;a href=\"{url}\"&gt;{title}&lt;/a&gt;, page {page+1} with score {score:.2f}&lt;/p&gt;'\n        html_content += (\n            f'&lt;img src=\"data:image/png;base64,{image}\" style=\"max-width:100%;\"&gt;'\n        )\n\n    display(HTML(html_content))\n</pre> from IPython.display import display, HTML   def display_query_results(query, response, hits=5):     query_time = response.json.get(\"timing\", {}).get(\"searchtime\", -1)     query_time = round(query_time, 2)     count = response.json.get(\"root\", {}).get(\"fields\", {}).get(\"totalCount\", 0)     html_content = f\"Query text: '{query}', query time {query_time}s, count={count}, top results:\"      for i, hit in enumerate(response.hits[:hits]):         title = hit[\"fields\"][\"title\"]         url = hit[\"fields\"][\"url\"]         page = hit[\"fields\"][\"page_number\"]         image = hit[\"fields\"][\"image\"]         score = hit[\"relevance\"]          html_content += f\"PDF Result {i + 1}\"         html_content += f'<p>Title: {title}, page {page+1} with score {score:.2f}</p>'         html_content += (             f''         )      display(HTML(html_content)) <p>Query Vespa with the queries and display the results, here we are using the <code>default</code> rank profile.</p> <p>Note that we retrieve using textual representation with <code>userInput(@userQuery)</code>, this means that we use the BM25 ranking for the extracted text in the first ranking phase and then re-rank the top-k pages using the ColPali embeddings.</p> <p>Later in this notebook we will use Vespa's support for approximate nearest neighbor search (<code>nearestNeighbor</code>) to retrieve directly using the ColPali embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nasync with app.asyncio(connections=1, timeout=120) as session:\n    for idx, query in enumerate(queries):\n        query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}\n        response: VespaQueryResponse = await session.query(\n            yql=\"select title,url,image,page_number from pdf_page where userInput(@userQuery)\",\n            ranking=\"default\",\n            userQuery=query,\n            timeout=120,\n            hits=3,\n            body={\"input.query(qt)\": query_embedding, \"presentation.timing\": True},\n        )\n        assert response.is_successful()\n        display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  async with app.asyncio(connections=1, timeout=120) as session:     for idx, query in enumerate(queries):         query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}         response: VespaQueryResponse = await session.query(             yql=\"select title,url,image,page_number from pdf_page where userInput(@userQuery)\",             ranking=\"default\",             userQuery=query,             timeout=120,             hits=3,             body={\"input.query(qt)\": query_embedding, \"presentation.timing\": True},         )         assert response.is_successful()         display_query_results(query, response) In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ninput_query_tensors = []\nMAX_QUERY_TERMS = 64\nfor i in range(MAX_QUERY_TERMS):\n    input_query_tensors.append((f\"query(rq{i})\", \"tensor&lt;int8&gt;(v[16])\"))\n\ninput_query_tensors.append((\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"))\ninput_query_tensors.append((\"query(qtb)\", \"tensor&lt;int8&gt;(querytoken{}, v[16])\"))\n\ncolpali_retrieval_profile = RankProfile(\n    name=\"retrieval-and-rerank\",\n    inputs=input_query_tensors,\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim_binary\",\n            expression=\"\"\"\n                sum(\n                  reduce(\n                    1/(1 + sum(\n                        hamming(query(qtb), attribute(embedding)) ,v)\n                    ),\n                    max,\n                    patch\n                  ),\n                  querytoken\n                )\n            \"\"\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim_binary\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),\n)\ncolpali_schema.add_rank_profile(colpali_retrieval_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  input_query_tensors = [] MAX_QUERY_TERMS = 64 for i in range(MAX_QUERY_TERMS):     input_query_tensors.append((f\"query(rq{i})\", \"tensor(v[16])\"))  input_query_tensors.append((\"query(qt)\", \"tensor(querytoken{}, v[128])\")) input_query_tensors.append((\"query(qtb)\", \"tensor(querytoken{}, v[16])\"))  colpali_retrieval_profile = RankProfile(     name=\"retrieval-and-rerank\",     inputs=input_query_tensors,     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim_binary\",             expression=\"\"\"                 sum(                   reduce(                     1/(1 + sum(                         hamming(query(qtb), attribute(embedding)) ,v)                     ),                     max,                     patch                   ),                   querytoken                 )             \"\"\",         ),     ],     first_phase=FirstPhaseRanking(expression=\"max_sim_binary\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10), ) colpali_schema.add_rank_profile(colpali_retrieval_profile) <p>We define two functions, one for the first phase and one for the second phase. Instead of the float representations, we use the binary representations with inverted hamming distance in the first phase. Now, we need to re-deploy the application to Vespa Cloud.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>Now we can query Vespa with the text queries and use the <code>nearestNeighbor</code> operator to retrieve the most similar pages to the query and pass the different query tensors.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\ntarget_hits_per_query_tensor = (\n    20  # this is a hyper parameter that can be tuned for speed versus accuracy\n)\nasync with app.asyncio(connections=1, timeout=180) as session:\n    for idx, query in enumerate(queries):\n        float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}\n        binary_query_embeddings = dict()\n        for k, v in float_query_embedding.items():\n            binary_query_embeddings[k] = (\n                np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()\n            )\n\n        # The mixed tensors used in MaxSim calculations\n        # We use both binary and float representations\n        query_tensors = {\n            \"input.query(qtb)\": binary_query_embeddings,\n            \"input.query(qt)\": float_query_embedding,\n        }\n        # The query tensors used in the nearest neighbor calculations\n        for i in range(0, len(binary_query_embeddings)):\n            query_tensors[f\"input.query(rq{i})\"] = binary_query_embeddings[i]\n        nn = []\n        for i in range(0, len(binary_query_embeddings)):\n            nn.append(\n                f\"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))\"\n            )\n        # We use a OR operator to combine the nearest neighbor operator\n        nn = \" OR \".join(nn)\n        response: VespaQueryResponse = await session.query(\n            yql=f\"select title, url, image, page_number from pdf_page where {nn}\",\n            ranking=\"retrieval-and-rerank\",\n            timeout=120,\n            hits=3,\n            body={**query_tensors, \"presentation.timing\": True},\n        )\n        assert response.is_successful()\n        display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  target_hits_per_query_tensor = (     20  # this is a hyper parameter that can be tuned for speed versus accuracy ) async with app.asyncio(connections=1, timeout=180) as session:     for idx, query in enumerate(queries):         float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}         binary_query_embeddings = dict()         for k, v in float_query_embedding.items():             binary_query_embeddings[k] = (                 np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()             )          # The mixed tensors used in MaxSim calculations         # We use both binary and float representations         query_tensors = {             \"input.query(qtb)\": binary_query_embeddings,             \"input.query(qt)\": float_query_embedding,         }         # The query tensors used in the nearest neighbor calculations         for i in range(0, len(binary_query_embeddings)):             query_tensors[f\"input.query(rq{i})\"] = binary_query_embeddings[i]         nn = []         for i in range(0, len(binary_query_embeddings)):             nn.append(                 f\"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))\"             )         # We use a OR operator to combine the nearest neighbor operator         nn = \" OR \".join(nn)         response: VespaQueryResponse = await session.query(             yql=f\"select title, url, image, page_number from pdf_page where {nn}\",             ranking=\"retrieval-and-rerank\",             timeout=120,             hits=3,             body={**query_tensors, \"presentation.timing\": True},         )         assert response.is_successful()         display_query_results(query, response) <p>Depending on the scale, we can evaluate changing different number of targetHits per nearestNeighbor operator and the ranking depths in the two phases. We can also parallelize the ranking phases by using more threads per query request to reduce latency.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#pdf-retrieval-using-colqwen2-colpali-with-vespa","title":"PDF-Retrieval using ColQWen2 (ColPali) with Vespa\u00b6","text":"<p>This notebook is a continuation of our notebooks related to the ColPali models for complex document retrieval.</p> <p>This notebook demonstrates using the new ColQWen2 model checkpoint.</p> <p>ColQwen is a model based on a novel model architecture and training strategy based on Vision Language Models (VLMs) to efficiently index documents from their visual features. It is a Qwen2-VL-2B extension that generates ColBERT- style multi-vector representations of text and images. It was introduced in the paper ColPali: Efficient Document Retrieval with Vision Language Models and first released in this repository</p> <p>ColQWen2 is better than the previous ColPali model in the following ways:</p> <ul> <li>Its more accurate on the ViDoRe dataset (+5 nDCCG@5 points)</li> <li>It's permissive licensed as both the base model and adapter is using open-source licences (Apache 2.0 and MIT)</li> <li>It uses fewer patch embeddings than ColPaliGemma (from 1024 to 768), this reduces both compute and storage.</li> </ul> <p>See also Scaling ColPali to billions of PDFs with Vespa</p> <p>The TLDR; of this notebook:</p> <ul> <li>Generate an image per PDF page using pdf2image and also extract the text using pypdf.</li> <li>For each page image, use ColPali to obtain the visual multi-vector embeddings</li> </ul> <p>Then we store visual embeddings in Vespa as a <code>int8</code> tensor, where we use a binary compression technique to reduce the storage footprint by 32x compared to float representations. See Scaling ColPali to billions of PDFs with Vespa for details on binarization and using hamming distance for retrieval.</p> <p>During retrieval time, we use the same ColPali model to generate embeddings for the query and then use Vespa's <code>nearestNeighbor</code> query to retrieve the most similar documents per query vector token, using binary representation with hamming distance. Then we re-rank the results in two phases:</p> <ul> <li>In the 0-phase we use hamming distance to retrieve the k closest pages per query token vector representation, this is expressed by using multiple nearestNeighbor query operators in Vespa.</li> <li>The nearestNeighbor operators exposes pages to the first-phase ranking function, which uses an approximate MaxSim using inverted hamming distance insted of cosine similarity. This is done to reduce the number of pages that are re-ranked in the second phase.</li> <li>In the second phase, we perform the full MaxSim operation, using float representations of the embeddings to re-rank the top-k pages from the first phase.</li> </ul> <p>This allows us to scale ColPali to very large collections of PDF pages, while still providing accurate and fast retrieval.</p> <p>Let us get started.</p> <p></p> <p>Install dependencies:</p> <p>Note that the python pdf2image package requires poppler-utils, see other installation options here.</p> <p>For MacOs, the simplest install option is <code>brew install poppler</code> if you are using Homebrew.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#load-the-model","title":"Load the model\u00b6","text":"<p>We use device map auto to load the model on the available GPU if available, otherwise on the CPU or MPS if available.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#working-with-pdfs","title":"Working with pdfs\u00b6","text":"<p>We need to convert a PDF to an array of images. One image per page. We use the <code>pdf2image</code> library for this task. Secondary, we also extract the text contents of the PDF using <code>pypdf</code>.</p> <p>NOTE: This step requires that you have <code>poppler</code> installed on your system. Read more in pdf2image docs.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#configure-vespa","title":"Configure Vespa\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Ok, so now we have indexed the PDF pages in Vespa. Let us now obtain ColPali embeddings for a few text queries and use it during ranking of the indexed pdf pages.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#using-nearestneighbor-for-retrieval","title":"Using nearestNeighbor for retrieval\u00b6","text":"<p>In the above example, we used the ColPali embeddings in ranking, but using the text query for retrieval. This is a reasonable approach for text-heavy documents where the text representation is the most important and where ColPali embeddings are used to re-rank the top-k documents from the text retrieval phase.</p> <p>In some cases, the ColPali embeddings are the most important and we want to demonstrate how we can use HNSW indexing with binary hamming distance to retrieve the most similar pages to a query and then have two steps of re-ranking using the ColPali embeddings.</p> <p>All the phases here are executed locally inside the Vespa content node(s) so that no vector data needs to cross the network.</p> <p>Let us add a new rank-profile to the schema, the <code>nearestNeighbor</code> operator takes a query tensor and a field tensor as argument and we need to define the query tensors types in the rank-profile.</p>"},{"location":"examples/pdf-retrieval-with-ColQwen2-vlm_Vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>In this notebook, we have demonstrated how to represent the new ColQwen2 in Vespa. We have generated embeddings for images of PDF pages using ColQwen2 and stored the embeddings in Vespa using mixed tensors.</p> <p>We demonstrated how to store the base64 encoded image using the <code>raw</code> Vespa field type, plus meta data like title and url. We have demonstrated how to retrieve relevant pages for a query using the embeddings generated by ColPali.</p> <p>This notebook can be extended to include more complex ranking models, more complex queries, and more complex data structures, including metadata and other fields which can be filtered on or used for ranking.</p>"},{"location":"examples/pyvespa-examples.html","title":"Pyvespa examples","text":"Refer to troubleshooting     for any problem when running this guide.  <p>Refer to troubleshooting, which also has utilies for debugging.</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install pyvespa\n</pre> !pip3 install pyvespa In\u00a0[14]: Copied! <pre>from vespa.package import ApplicationPackage, Field, RankProfile\nfrom vespa.deployment import VespaDocker\nfrom vespa.io import VespaResponse\n\napp_package = ApplicationPackage(name=\"neighbors\")\n\napp_package.schema.add_fields(\n    Field(name=\"point\", type=\"tensor&lt;float&gt;(d[3])\", indexing=[\"attribute\", \"summary\"])\n)\n\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"max_distance\",\n        inputs=[(\"query(qpoint)\", \"tensor&lt;float&gt;(d[3])\")],\n        first_phase=\"euclidean_distance(attribute(point), query(qpoint), d)\",\n    )\n)\n\nvespa_docker = VespaDocker()\napp = vespa_docker.deploy(application_package=app_package)\n</pre> from vespa.package import ApplicationPackage, Field, RankProfile from vespa.deployment import VespaDocker from vespa.io import VespaResponse  app_package = ApplicationPackage(name=\"neighbors\")  app_package.schema.add_fields(     Field(name=\"point\", type=\"tensor(d[3])\", indexing=[\"attribute\", \"summary\"]) )  app_package.schema.add_rank_profile(     RankProfile(         name=\"max_distance\",         inputs=[(\"query(qpoint)\", \"tensor(d[3])\")],         first_phase=\"euclidean_distance(attribute(point), query(qpoint), d)\",     ) )  vespa_docker = VespaDocker() app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 15/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 20/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 25/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Feed points in 3d space using a 3-dimensional indexed tensor. Pyvespa feeds using the /document/v1/ API, refer to document format:</p> In\u00a0[15]: Copied! <pre>def get_feed(field_name):\n    return [\n        {\"id\": 0, \"fields\": {field_name: [0.0, 1.0, 2.0]}},\n        {\"id\": 1, \"fields\": {field_name: [1.0, 2.0, 3.0]}},\n        {\"id\": 2, \"fields\": {field_name: [2.0, 3.0, 4.0]}},\n    ]\n\n\nwith app.syncio(connections=1) as session:\n    for u in get_feed(\"point\"):\n        response: VespaResponse = session.update_data(\n            data_id=u[\"id\"], schema=\"neighbors\", fields=u[\"fields\"], create=True\n        )\n        if not response.is_successful():\n            print(\n                \"Update failed for document {}\".format(u[\"id\"])\n                + \" with status code {}\".format(response.status_code)\n                + \" with response {}\".format(response.get_json())\n            )\n</pre> def get_feed(field_name):     return [         {\"id\": 0, \"fields\": {field_name: [0.0, 1.0, 2.0]}},         {\"id\": 1, \"fields\": {field_name: [1.0, 2.0, 3.0]}},         {\"id\": 2, \"fields\": {field_name: [2.0, 3.0, 4.0]}},     ]   with app.syncio(connections=1) as session:     for u in get_feed(\"point\"):         response: VespaResponse = session.update_data(             data_id=u[\"id\"], schema=\"neighbors\", fields=u[\"fields\"], create=True         )         if not response.is_successful():             print(                 \"Update failed for document {}\".format(u[\"id\"])                 + \" with status code {}\".format(response.status_code)                 + \" with response {}\".format(response.get_json())             ) <p>Note:  The feed above uses create-if-nonexistent, i.e. update a document, create it if it does not exists. Later in this notebook we will add a field and update it, so using an update to feed data makes it easier.</p> <p>Query from origo using YQL. The rank profile will rank the most distant points highest, here <code>sqrt(2*2 + 3*3 + 4*4) = 5.385</code>:</p> In\u00a0[16]: Copied! <pre>import json\nfrom vespa.io import VespaQueryResponse\n\nresult: VespaQueryResponse = app.query(\n    body={\n        \"yql\": \"select point from neighbors where true\",\n        \"input.query(qpoint)\": \"[0.0, 0.0, 0.0]\",\n        \"ranking.profile\": \"max_distance\",\n        \"presentation.format.tensors\": \"short-value\",\n    }\n)\n\nif not response.is_successful():\n    print(\n        \"Query failed with status code {}\".format(response.status_code)\n        + \" with response {}\".format(response.get_json())\n    )\n    raise Exception(\"Query failed\")\nif len(result.hits) != 3:\n    raise Exception(\"Expected 3 hits, got {}\".format(len(result.hits)))\nprint(json.dumps(result.hits, indent=4))\n</pre> import json from vespa.io import VespaQueryResponse  result: VespaQueryResponse = app.query(     body={         \"yql\": \"select point from neighbors where true\",         \"input.query(qpoint)\": \"[0.0, 0.0, 0.0]\",         \"ranking.profile\": \"max_distance\",         \"presentation.format.tensors\": \"short-value\",     } )  if not response.is_successful():     print(         \"Query failed with status code {}\".format(response.status_code)         + \" with response {}\".format(response.get_json())     )     raise Exception(\"Query failed\") if len(result.hits) != 3:     raise Exception(\"Expected 3 hits, got {}\".format(len(result.hits))) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"index:neighbors_content/0/c81e728dfde15fa4e8dfb3d3\",\n        \"relevance\": 5.385164807134504,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                2.0,\n                3.0,\n                4.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/c4ca4238db266f395150e961\",\n        \"relevance\": 3.7416573867739413,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                1.0,\n                2.0,\n                3.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/cfcd20845b10b1420c6cdeca\",\n        \"relevance\": 2.23606797749979,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                0.0,\n                1.0,\n                2.0\n            ]\n        }\n    }\n]\n</pre> <p>Query from <code>[1.0, 2.0, 2.9]</code> - find that <code>[2.0, 3.0, 4.0]</code> is most distant:</p> In\u00a0[17]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select point from neighbors where true\",\n        \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",\n        \"ranking.profile\": \"max_distance\",\n        \"presentation.format.tensors\": \"short-value\",\n    }\n)\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select point from neighbors where true\",         \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",         \"ranking.profile\": \"max_distance\",         \"presentation.format.tensors\": \"short-value\",     } ) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"index:neighbors_content/0/c81e728dfde15fa4e8dfb3d3\",\n        \"relevance\": 1.7916472308265357,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                2.0,\n                3.0,\n                4.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/cfcd20845b10b1420c6cdeca\",\n        \"relevance\": 1.6763055154708881,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                0.0,\n                1.0,\n                2.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/c4ca4238db266f395150e961\",\n        \"relevance\": 0.09999990575011103,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                1.0,\n                2.0,\n                3.0\n            ]\n        }\n    }\n]\n</pre> In\u00a0[18]: Copied! <pre>app_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"nearest_neighbor\",\n        inputs=[(\"query(qpoint)\", \"tensor&lt;float&gt;(d[3])\")],\n        first_phase=\"closeness(field, point)\",\n    )\n)\n\napp = vespa_docker.deploy(application_package=app_package)\n</pre> app_package.schema.add_rank_profile(     RankProfile(         name=\"nearest_neighbor\",         inputs=[(\"query(qpoint)\", \"tensor(d[3])\")],         first_phase=\"closeness(field, point)\",     ) )  app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Read more in nearest neighbor search.</p> <p>Query using nearestNeighbor query operator:</p> In\u00a0[19]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select point from neighbors where {targetHits: 3}nearestNeighbor(point, qpoint)\",\n        \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",\n        \"ranking.profile\": \"nearest_neighbor\",\n        \"presentation.format.tensors\": \"short-value\",\n    }\n)\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select point from neighbors where {targetHits: 3}nearestNeighbor(point, qpoint)\",         \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",         \"ranking.profile\": \"nearest_neighbor\",         \"presentation.format.tensors\": \"short-value\",     } ) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"index:neighbors_content/0/c4ca4238db266f395150e961\",\n        \"relevance\": 0.9090909879069752,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                1.0,\n                2.0,\n                3.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/cfcd20845b10b1420c6cdeca\",\n        \"relevance\": 0.37364941905256455,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                0.0,\n                1.0,\n                2.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/c81e728dfde15fa4e8dfb3d3\",\n        \"relevance\": 0.35821144946644456,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point\": [\n                2.0,\n                3.0,\n                4.0\n            ]\n        }\n    }\n]\n</pre> In\u00a0[20]: Copied! <pre>app_package.schema.add_fields(\n    Field(\n        name=\"point_angular\",\n        type=\"tensor&lt;float&gt;(d[3])\",\n        indexing=[\"attribute\", \"summary\"],\n        attribute=[\"distance-metric: angular\"],\n    )\n)\napp_package.schema.add_rank_profile(\n    RankProfile(\n        name=\"nearest_neighbor_angular\",\n        inputs=[(\"query(qpoint)\", \"tensor&lt;float&gt;(d[3])\")],\n        first_phase=\"closeness(field, point_angular)\",\n    )\n)\n\napp = vespa_docker.deploy(application_package=app_package)\n</pre> app_package.schema.add_fields(     Field(         name=\"point_angular\",         type=\"tensor(d[3])\",         indexing=[\"attribute\", \"summary\"],         attribute=[\"distance-metric: angular\"],     ) ) app_package.schema.add_rank_profile(     RankProfile(         name=\"nearest_neighbor_angular\",         inputs=[(\"query(qpoint)\", \"tensor(d[3])\")],         first_phase=\"closeness(field, point_angular)\",     ) )  app = vespa_docker.deploy(application_package=app_package) <pre>Waiting for configuration server, 0/300 seconds...\nWaiting for configuration server, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 0/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 5/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nWaiting for application status, 10/300 seconds...\nUsing plain http against endpoint http://localhost:8080/ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> <p>Feed the same data to the <code>point_angular</code> field:</p> In\u00a0[21]: Copied! <pre>for u in get_feed(\"point_angular\"):\n    response: VespaResponse = session.update_data(\n        data_id=u[\"id\"], schema=\"neighbors\", fields=u[\"fields\"]\n    )\n    if not response.is_successful():\n        print(\n            \"Update failed for document {}\".format(u[\"id\"])\n            + \" with status code {}\".format(response.status_code)\n            + \" with response {}\".format(response.get_json())\n        )\n</pre> for u in get_feed(\"point_angular\"):     response: VespaResponse = session.update_data(         data_id=u[\"id\"], schema=\"neighbors\", fields=u[\"fields\"]     )     if not response.is_successful():         print(             \"Update failed for document {}\".format(u[\"id\"])             + \" with status code {}\".format(response.status_code)             + \" with response {}\".format(response.get_json())         ) <p>Observe the documents now have two vectors</p> <p>Notice that we pass native Vespa document v1 api parameters to reduce the tensor verbosity.</p> In\u00a0[24]: Copied! <pre>from vespa.io import VespaResponse\n\nresponse: VespaResponse = app.get_data(\n    schema=\"neighbors\", data_id=0, **{\"format.tensors\": \"short-value\"}\n)\nprint(json.dumps(response.get_json(), indent=4))\n</pre> from vespa.io import VespaResponse  response: VespaResponse = app.get_data(     schema=\"neighbors\", data_id=0, **{\"format.tensors\": \"short-value\"} ) print(json.dumps(response.get_json(), indent=4)) <pre>{\n    \"pathId\": \"/document/v1/neighbors/neighbors/docid/0\",\n    \"id\": \"id:neighbors:neighbors::0\",\n    \"fields\": {\n        \"point\": [\n            0.0,\n            1.0,\n            2.0\n        ],\n        \"point_angular\": [\n            0.0,\n            1.0,\n            2.0\n        ]\n    }\n}\n</pre> In\u00a0[25]: Copied! <pre>result = app.query(\n    body={\n        \"yql\": \"select point_angular from neighbors where {targetHits: 3}nearestNeighbor(point_angular, qpoint)\",\n        \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",\n        \"ranking.profile\": \"nearest_neighbor_angular\",\n        \"presentation.format.tensors\": \"short-value\",\n    }\n)\nprint(json.dumps(result.hits, indent=4))\n</pre> result = app.query(     body={         \"yql\": \"select point_angular from neighbors where {targetHits: 3}nearestNeighbor(point_angular, qpoint)\",         \"input.query(qpoint)\": \"[1.0, 2.0, 2.9]\",         \"ranking.profile\": \"nearest_neighbor_angular\",         \"presentation.format.tensors\": \"short-value\",     } ) print(json.dumps(result.hits, indent=4)) <pre>[\n    {\n        \"id\": \"index:neighbors_content/0/c4ca4238db266f395150e961\",\n        \"relevance\": 0.983943389010042,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point_angular\": [\n                1.0,\n                2.0,\n                3.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/c81e728dfde15fa4e8dfb3d3\",\n        \"relevance\": 0.9004871017951954,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point_angular\": [\n                2.0,\n                3.0,\n                4.0\n            ]\n        }\n    },\n    {\n        \"id\": \"index:neighbors_content/0/cfcd20845b10b1420c6cdeca\",\n        \"relevance\": 0.7638041096953281,\n        \"source\": \"neighbors_content\",\n        \"fields\": {\n            \"point_angular\": [\n                0.0,\n                1.0,\n                2.0\n            ]\n        }\n    }\n]\n</pre> <p>In the output above, observe the different in \"relevance\", compared to the query using <code>'ranking.profile': 'nearest_neighbor'</code> above - this is the difference in <code>closeness()</code> using different distance metrics.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_docker.container.stop()\nvespa_docker.container.remove()\n</pre> vespa_docker.container.stop() vespa_docker.container.remove()"},{"location":"examples/pyvespa-examples.html#pyvespa-examples","title":"Pyvespa examples\u00b6","text":"<p>This is a notebook with short examples one can build applications from.</p>"},{"location":"examples/pyvespa-examples.html#neighbors","title":"Neighbors\u00b6","text":"<p>Explore distance between points in 3D vector space.</p> <p>These are simple examples, feeding documents with a tensor representing a point in space, and a rank profile calculating the distance between a point in the query and the point in the documents.</p> <p>The examples start with using simple ranking expressions like euclidean-distance, then rank features like closeness()) and setting different distance-metrics.</p>"},{"location":"examples/pyvespa-examples.html#distant-neighbor","title":"Distant neighbor\u00b6","text":"<p>First, find the point that is  most  distant from a point in query - deploy the Application Package:</p>"},{"location":"examples/pyvespa-examples.html#nearest-neighbor","title":"Nearest neighbor\u00b6","text":"<p>The nearestNeighbor query operator calculates distances between points in vector space. Here, we are using the default distance metric (euclidean), as it is not specified. The closeness()) rank feature can be used to rank results - add a new rank profile:</p>"},{"location":"examples/pyvespa-examples.html#nearest-neighbor-angular","title":"Nearest neighbor - angular\u00b6","text":"<p>So far, we have used the default distance-metric which is euclidean - now try with another. Add new few field with \"angular\" distance metric:</p>"},{"location":"examples/pyvespa-examples.html#next-steps","title":"Next steps\u00b6","text":"<ul> <li>Try the multi-vector-indexing notebook to explore using an HNSW-index for approximate nearest neighbor search.</li> <li>Explore using the distance()) rank feature - this should give the same results as the ranking expressions using <code>euclidean-distance</code> above.</li> <li><code>label</code> is useful when having more vector fields - read more about the nearestNeighbor query operator.</li> </ul>"},{"location":"examples/pyvespa-examples.html#cleanup","title":"Cleanup\u00b6","text":""},{"location":"examples/rag-blueprint-vespa-cloud.html","title":"Rag blueprint vespa cloud","text":"In\u00a0[1]: Copied! <pre>!pip3 install pyvespa&gt;=0.58.0 vespacli scikit-learn lightgbm pandas\n</pre> !pip3 install pyvespa&gt;=0.58.0 vespacli scikit-learn lightgbm pandas <pre>zsh:1: 0.58.0 not found\n</pre> In\u00a0[1]: Copied! <pre># Clone the RAG Blueprint sample application\n!git clone --depth 1 --filter=blob:none --sparse https://github.com/vespa-engine/sample-apps.git src &amp;&amp; cd src &amp;&amp; git sparse-checkout set rag-blueprint\n</pre> # Clone the RAG Blueprint sample application !git clone --depth 1 --filter=blob:none --sparse https://github.com/vespa-engine/sample-apps.git src &amp;&amp; cd src &amp;&amp; git sparse-checkout set rag-blueprint <pre>Cloning into 'src'...\nremote: Enumerating objects: 640, done.\nremote: Counting objects: 100% (640/640), done.\nremote: Compressing objects: 100% (350/350), done.\nremote: Total 640 (delta 7), reused 557 (delta 5), pack-reused 0 (from 0)\nReceiving objects: 100% (640/640), 62.63 KiB | 1.01 MiB/s, done.\nResolving deltas: 100% (7/7), done.\nremote: Enumerating objects: 15, done.\nremote: Counting objects: 100% (15/15), done.\nremote: Compressing objects: 100% (13/13), done.\nremote: Total 15 (delta 2), reused 8 (delta 2), pack-reused 0 (from 0)\nReceiving objects: 100% (15/15), 92.91 KiB | 318.00 KiB/s, done.\nResolving deltas: 100% (2/2), done.\nUpdating files: 100% (15/15), done.\nremote: Enumerating objects: 37, done.\nremote: Counting objects: 100% (37/37), done.\nremote: Compressing objects: 100% (30/30), done.\nremote: Total 37 (delta 8), reused 21 (delta 6), pack-reused 0 (from 0)\nReceiving objects: 100% (37/37), 111.45 KiB | 401.00 KiB/s, done.\nResolving deltas: 100% (8/8), done.\nUpdating files: 100% (37/37), done.\n</pre> In\u00a0[2]: Copied! <pre>from pathlib import Path\n\n\ndef tree(\n    root: str | Path = \".\", *, show_hidden: bool = False, max_depth: int | None = None\n) -&gt; str:\n    \"\"\"\n    Return a Unix\u2010style 'tree' listing for *root*.\n\n    Parameters\n    ----------\n    root : str | Path\n        Directory to walk (default: \".\")\n    show_hidden : bool\n        Include dotfiles and dot-dirs? (default: False)\n    max_depth : int | None\n        Limit recursion depth; None = no limit.\n\n    Returns\n    -------\n    str\n        A newline-joined string identical to `tree` output.\n    \"\"\"\n    root_path = Path(root).resolve()\n    lines = [root_path.as_posix()]\n\n    def _walk(current: Path, prefix: str = \"\", depth: int = 0) -&gt; None:\n        if max_depth is not None and depth &gt;= max_depth:\n            return\n\n        entries = sorted(\n            (e for e in current.iterdir() if show_hidden or not e.name.startswith(\".\")),\n            key=lambda p: (not p.is_dir(), p.name.lower()),\n        )\n        last = len(entries) - 1\n\n        for idx, entry in enumerate(entries):\n            connector = \"\u2514\u2500\u2500 \" if idx == last else \"\u251c\u2500\u2500 \"\n            lines.append(f\"{prefix}{connector}{entry.name}\")\n            if entry.is_dir():\n                extension = \"    \" if idx == last else \"\u2502   \"\n                _walk(entry, prefix + extension, depth + 1)\n\n    _walk(root_path)\n    return \"\\n\".join(lines)\n</pre> from pathlib import Path   def tree(     root: str | Path = \".\", *, show_hidden: bool = False, max_depth: int | None = None ) -&gt; str:     \"\"\"     Return a Unix\u2010style 'tree' listing for *root*.      Parameters     ----------     root : str | Path         Directory to walk (default: \".\")     show_hidden : bool         Include dotfiles and dot-dirs? (default: False)     max_depth : int | None         Limit recursion depth; None = no limit.      Returns     -------     str         A newline-joined string identical to `tree` output.     \"\"\"     root_path = Path(root).resolve()     lines = [root_path.as_posix()]      def _walk(current: Path, prefix: str = \"\", depth: int = 0) -&gt; None:         if max_depth is not None and depth &gt;= max_depth:             return          entries = sorted(             (e for e in current.iterdir() if show_hidden or not e.name.startswith(\".\")),             key=lambda p: (not p.is_dir(), p.name.lower()),         )         last = len(entries) - 1          for idx, entry in enumerate(entries):             connector = \"\u2514\u2500\u2500 \" if idx == last else \"\u251c\u2500\u2500 \"             lines.append(f\"{prefix}{connector}{entry.name}\")             if entry.is_dir():                 extension = \"    \" if idx == last else \"\u2502   \"                 _walk(entry, prefix + extension, depth + 1)      _walk(root_path)     return \"\\n\".join(lines) In\u00a0[3]: Copied! <pre># Let's explore the RAG Blueprint application structure\nprint(tree(\"src/rag-blueprint\"))\n</pre> # Let's explore the RAG Blueprint application structure print(tree(\"src/rag-blueprint\")) <pre>/Users/thomas/Repos/pyvespa/docs/sphinx/source/examples/src/rag-blueprint\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u2514\u2500\u2500 lightgbm_model.json\n\u2502   \u251c\u2500\u2500 schemas\n\u2502   \u2502   \u251c\u2500\u2500 doc\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base-features.profile\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 collect-second-phase.profile\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 collect-training-data.profile\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 learned-linear.profile\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 match-only.profile\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 second-with-gbdt.profile\n\u2502   \u2502   \u2514\u2500\u2500 doc.sd\n\u2502   \u251c\u2500\u2500 search\n\u2502   \u2502   \u2514\u2500\u2500 query-profiles\n\u2502   \u2502       \u251c\u2500\u2500 deepresearch-with-gbdt.xml\n\u2502   \u2502       \u251c\u2500\u2500 deepresearch.xml\n\u2502   \u2502       \u251c\u2500\u2500 hybrid-with-gbdt.xml\n\u2502   \u2502       \u251c\u2500\u2500 hybrid.xml\n\u2502   \u2502       \u251c\u2500\u2500 rag-with-gbdt.xml\n\u2502   \u2502       \u2514\u2500\u2500 rag.xml\n\u2502   \u2514\u2500\u2500 services.xml\n\u251c\u2500\u2500 dataset\n\u2502   \u2514\u2500\u2500 docs.jsonl\n\u251c\u2500\u2500 eval\n\u2502   \u251c\u2500\u2500 output\n\u2502   \u2502   \u251c\u2500\u2500 Vespa-training-data_match_first_phase_20250623_133241.csv\n\u2502   \u2502   \u251c\u2500\u2500 Vespa-training-data_match_first_phase_20250623_133241_logreg_coefficients.txt\n\u2502   \u2502   \u251c\u2500\u2500 Vespa-training-data_match_rank_second_phase_20250623_135819.csv\n\u2502   \u2502   \u2514\u2500\u2500 Vespa-training-data_match_rank_second_phase_20250623_135819_feature_importance.csv\n\u2502   \u251c\u2500\u2500 collect_pyvespa.py\n\u2502   \u251c\u2500\u2500 evaluate_match_phase.py\n\u2502   \u251c\u2500\u2500 evaluate_ranking.py\n\u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 resp.json\n\u2502   \u251c\u2500\u2500 train_lightgbm.py\n\u2502   \u2514\u2500\u2500 train_logistic_regression.py\n\u251c\u2500\u2500 queries\n\u2502   \u251c\u2500\u2500 queries.json\n\u2502   \u2514\u2500\u2500 test_queries.json\n\u251c\u2500\u2500 deploy-locally.md\n\u251c\u2500\u2500 generation.md\n\u251c\u2500\u2500 query-profiles.md\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 relevance.md\n</pre> <p>We can see that the RAG Blueprint includes a complete application package with:</p> <ul> <li><code>schemas/doc.sd</code> - The document schema with chunking and embeddings</li> <li><code>schemas/doc/*.profile</code> - Ranking profiles for collecting training data, first-phase ranking, and second-phase ranking</li> <li><code>services.xml</code> - Services configuration with embedder and LLM integration</li> <li><code>search/query-profiles/*.xml</code> - Pre-configured query profiles for different use cases</li> <li><code>models/</code> - Pre-trained ranking models</li> </ul> In\u00a0[5]: Copied! <pre>from vespa.deployment import VespaCloud\nfrom vespa.application import Vespa\nfrom pathlib import Path\nimport os\nimport json\n</pre> from vespa.deployment import VespaCloud from vespa.application import Vespa from pathlib import Path import os import json In\u00a0[6]: Copied! <pre>VESPA_TENANT_NAME = \"vespa-team\"  # Replace with your tenant name\n</pre> VESPA_TENANT_NAME = \"vespa-team\"  # Replace with your tenant name <p>Here, set your desired application name. (Will be created in later steps) Note that you can not have hyphen <code>-</code> or underscore <code>_</code> in the application name.</p> In\u00a0[7]: Copied! <pre>VESPA_APPLICATION_NAME = \"rag-blueprint\"  # No hyphens or underscores allowed\nVESPA_SCHEMA_NAME = \"doc\"  # RAG Blueprint uses 'doc' schema\n</pre> VESPA_APPLICATION_NAME = \"rag-blueprint\"  # No hyphens or underscores allowed VESPA_SCHEMA_NAME = \"doc\"  # RAG Blueprint uses 'doc' schema In\u00a0[8]: Copied! <pre>repo_root = Path(\"src/rag-blueprint\")\napplication_root = repo_root / \"app\"\n</pre> repo_root = Path(\"src/rag-blueprint\") application_root = repo_root / \"app\" <p>Note, you could also enable a token endpoint, for easier connection after deployment, see Authenticating to Vespa Cloud for details. We will stick to the default MTLS key/cert authentication for this notebook.</p> <p>Let us first take a look at the original <code>services.xml</code> file, which contains the configuration for the Vespa application services, including the LLM integration and embedder.</p> <p>!!! note It is also possible to define the services.xml-configuration in python code, see Advanced Configuration.</p> In\u00a0[9]: Copied! <pre>from IPython.display import display, Markdown\n\n\ndef display_md(text: str, tag: str = \"txt\"):\n    text = text.rstrip()\n    md = f\"\"\"```{tag}\n{text}\n```\"\"\"\n    display(Markdown(md))\n\n\nservices_content = (application_root / \"services.xml\").read_text()\ndisplay_md(services_content, \"xml\")\n</pre> from IPython.display import display, Markdown   def display_md(text: str, tag: str = \"txt\"):     text = text.rstrip()     md = f\"\"\"```{tag} {text} ```\"\"\"     display(Markdown(md))   services_content = (application_root / \"services.xml\").read_text() display_md(services_content, \"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;services version=\"1.0\" xmlns:deploy=\"vespa\" xmlns:preprocess=\"properties\"\n    minimum-required-vespa-version=\"8.519.55\"&gt;\n\n    &lt;container id=\"default\" version=\"1.0\"&gt;\n        &lt;document-processing /&gt;\n        &lt;document-api /&gt;\n        &lt;!-- Uncomment this to use secret from Vespa Cloud Secret Store --&gt;\n        &lt;secrets&gt;\n            &lt;openai-api-key vault=\"sample-apps\" name=\"openai-dev\" /&gt;\n        &lt;/secrets&gt;\n        &lt;!-- Setup the client to OpenAI --&gt;\n        &lt;component id=\"openai\" class=\"ai.vespa.llm.clients.OpenAI\"&gt;\n            &lt;config name=\"ai.vespa.llm.clients.llm-client\"&gt;\n                &lt;!-- Uncomment this to use secret from Vespa Cloud Secret Store --&gt;\n                &lt;apiKeySecretName&gt;openai-api-key&lt;/apiKeySecretName&gt;\n            &lt;/config&gt;\n        &lt;/component&gt;\n\n        &lt;component id=\"nomicmb\" type=\"hugging-face-embedder\"&gt;\n            &lt;transformer-model\n                url=\"https://data.vespa-cloud.com/onnx_models/nomic-ai-modernbert-embed-base/model.onnx\" /&gt;\n            &lt;transformer-token-type-ids /&gt;\n            &lt;tokenizer-model\n                url=\"https://data.vespa-cloud.com/onnx_models/nomic-ai-modernbert-embed-base/tokenizer.json\" /&gt;\n            &lt;transformer-output&gt;token_embeddings&lt;/transformer-output&gt;\n            &lt;max-tokens&gt;8192&lt;/max-tokens&gt;\n            &lt;prepend&gt;\n                &lt;query&gt;search_query:&lt;/query&gt;\n                &lt;document&gt;search_document:&lt;/document&gt;\n            &lt;/prepend&gt;\n        &lt;/component&gt;\n        &lt;search&gt;\n            &lt;chain id=\"openai\" inherits=\"vespa\"&gt;\n                &lt;searcher id=\"ai.vespa.search.llm.RAGSearcher\"&gt;\n                    &lt;config name=\"ai.vespa.search.llm.llm-searcher\"&gt;\n                        &lt;providerId&gt;openai&lt;/providerId&gt;\n                    &lt;/config&gt;\n                &lt;/searcher&gt;\n            &lt;/chain&gt;\n        &lt;/search&gt;\n        &lt;nodes&gt;\n            &lt;node hostalias=\"node1\" /&gt;\n        &lt;/nodes&gt;\n    &lt;/container&gt;\n\n    &lt;!-- See https://docs.vespa.ai/en/reference/services-content.html --&gt;\n    &lt;content id=\"content\" version=\"1.0\"&gt;\n        &lt;min-redundancy&gt;2&lt;/min-redundancy&gt;\n        &lt;documents&gt;\n            &lt;document type=\"doc\" mode=\"index\" /&gt;\n        &lt;/documents&gt;\n        &lt;nodes&gt;\n            &lt;node hostalias=\"node1\" distribution-key=\"0\" /&gt;\n        &lt;/nodes&gt;\n    &lt;/content&gt;\n\n&lt;/services&gt;\n</pre> In\u00a0[10]: Copied! <pre># This is only needed for CI.\nVESPA_TEAM_API_KEY = os.getenv(\"VESPA_TEAM_API_KEY\", None)\n</pre> # This is only needed for CI. VESPA_TEAM_API_KEY = os.getenv(\"VESPA_TEAM_API_KEY\", None) In\u00a0[11]: Copied! <pre>vespa_cloud = VespaCloud(\n    tenant=VESPA_TENANT_NAME,\n    application=VESPA_APPLICATION_NAME,\n    key_content=VESPA_TEAM_API_KEY,\n    application_root=application_root,\n)\n</pre> vespa_cloud = VespaCloud(     tenant=VESPA_TENANT_NAME,     application=VESPA_APPLICATION_NAME,     key_content=VESPA_TEAM_API_KEY,     application_root=application_root, ) <pre>Setting application...\nRunning: vespa config set application vespa-team.rag-blueprint.default\nSetting target cloud...\nRunning: vespa config set target cloud\n\nApi-key found for control plane access. Using api-key.\n</pre> <p>Now, we will deploy the application to Vespa Cloud. This will take a few minutes, so feel free to skip ahead to the next section while waiting for the deployment to complete.</p> In\u00a0[12]: Copied! <pre># Deploy the application\napp: Vespa = vespa_cloud.deploy(disk_folder=application_root)\n</pre> # Deploy the application app: Vespa = vespa_cloud.deploy(disk_folder=application_root) <pre>Deployment started in run 85 of dev-aws-us-east-1c for vespa-team.rag-blueprint. This may take a few minutes the first time.\nINFO    [09:40:36]  Deploying platform version 8.586.25 and application dev build 85 for dev-aws-us-east-1c of default ...\nINFO    [09:40:36]  Using CA signed certificate version 5\nINFO    [09:40:43]  Session 379704 for tenant 'vespa-team' prepared and activated.\nINFO    [09:40:43]  ######## Details for all nodes ########\nINFO    [09:40:43]  h125699b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:40:43]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:40:43]  --- storagenode on port 19102 has config generation 379704, wanted is 379704\nINFO    [09:40:43]  --- searchnode on port 19107 has config generation 379704, wanted is 379704\nINFO    [09:40:43]  --- distributor on port 19111 has config generation 379699, wanted is 379704\nINFO    [09:40:43]  --- metricsproxy-container on port 19092 has config generation 379704, wanted is 379704\nINFO    [09:40:43]  h125755a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:40:43]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:40:43]  --- container on port 4080 has config generation 379699, wanted is 379704\nINFO    [09:40:43]  --- metricsproxy-container on port 19092 has config generation 379704, wanted is 379704\nINFO    [09:40:43]  h97530b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:40:43]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:40:43]  --- logserver-container on port 4080 has config generation 379704, wanted is 379704\nINFO    [09:40:43]  --- metricsproxy-container on port 19092 has config generation 379704, wanted is 379704\nINFO    [09:40:43]  h119190c.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:40:43]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:40:43]  --- container-clustercontroller on port 19050 has config generation 379699, wanted is 379704\nINFO    [09:40:43]  --- metricsproxy-container on port 19092 has config generation 379699, wanted is 379704\nINFO    [09:40:51]  Found endpoints:\nINFO    [09:40:51]  - dev.aws-us-east-1c\nINFO    [09:40:51]   |-- https://fe5fe13c.fe19121d.z.vespa-app.cloud/ (cluster 'default')\nINFO    [09:40:51]  Deployment of new application revision complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for default\nURL: https://fe5fe13c.fe19121d.z.vespa-app.cloud/\nApplication is up!\n</pre> <p>Now is the time to assign permissions for this application (in this case <code>ragblueprintdemo</code>) to access the secret.</p> <p></p> In\u00a0[13]: Copied! <pre>def uncomment_secrets(xml_content: str) -&gt; str:\n    \"\"\"\n    Uncomments specific secret-related blocks in an XML string.\n\n    This function finds XML blocks that are commented out and contain either:\n    - &lt;secrets&gt;...&lt;/secrets&gt; tags\n    - &lt;apiKeySecretName&gt;...&lt;/apiKeySecretName&gt; tags\n\n    Args:\n        xml_content: A string containing the XML data with commented sections.\n\n    Returns:\n        The XML string with secret sections uncommented.\n\n    Example:\n        Input:  \"&lt;!-- &lt;secrets&gt;&lt;key&gt;value&lt;/key&gt;&lt;/secrets&gt; --&gt;\"\n        Output: \"&lt;secrets&gt;&lt;key&gt;value&lt;/key&gt;&lt;/secrets&gt;\"\n    \"\"\"\n    import re\n\n    # Pattern to find commented-out &lt;secrets&gt; blocks\n    # Matches: &lt;!-- &lt;secrets&gt;content&lt;/secrets&gt; --&gt;\n    secrets_pattern = re.compile(r\"&lt;!--\\s*(&lt;secrets&gt;.*?&lt;/secrets&gt;)\\s*--&gt;\", re.DOTALL)\n\n    # Pattern to find commented-out &lt;apiKeySecretName&gt; blocks\n    # Matches: &lt;!-- &lt;apiKeySecretName&gt;content&lt;/apiKeySecretName&gt; --&gt;\n    api_key_pattern = re.compile(\n        r\"&lt;!--\\s*(&lt;apiKeySecretName&gt;.*?&lt;/apiKeySecretName&gt;)\\s*--&gt;\", re.DOTALL\n    )\n\n    # Uncomment the blocks by replacing with just the XML content\n    uncommented_content = secrets_pattern.sub(r\"\\1\", xml_content)\n    uncommented_content = api_key_pattern.sub(r\"\\1\", uncommented_content)\n\n    return uncommented_content\n\n\nuncommented_services_content = uncomment_secrets(services_content)\ndisplay_md(uncommented_services_content, \"xml\")\n</pre> def uncomment_secrets(xml_content: str) -&gt; str:     \"\"\"     Uncomments specific secret-related blocks in an XML string.      This function finds XML blocks that are commented out and contain either:     - ... tags     - ... tags      Args:         xml_content: A string containing the XML data with commented sections.      Returns:         The XML string with secret sections uncommented.      Example:         Input:  \"\"         Output: \"value\"     \"\"\"     import re      # Pattern to find commented-out  blocks     # Matches:      secrets_pattern = re.compile(r\"\", re.DOTALL)      # Pattern to find commented-out  blocks     # Matches:      api_key_pattern = re.compile(         r\"\", re.DOTALL     )      # Uncomment the blocks by replacing with just the XML content     uncommented_content = secrets_pattern.sub(r\"\\1\", xml_content)     uncommented_content = api_key_pattern.sub(r\"\\1\", uncommented_content)      return uncommented_content   uncommented_services_content = uncomment_secrets(services_content) display_md(uncommented_services_content, \"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;services version=\"1.0\" xmlns:deploy=\"vespa\" xmlns:preprocess=\"properties\"\n    minimum-required-vespa-version=\"8.519.55\"&gt;\n\n    &lt;container id=\"default\" version=\"1.0\"&gt;\n        &lt;document-processing /&gt;\n        &lt;document-api /&gt;\n        &lt;!-- Uncomment this to use secret from Vespa Cloud Secret Store --&gt;\n        &lt;secrets&gt;\n            &lt;openai-api-key vault=\"sample-apps\" name=\"openai-dev\" /&gt;\n        &lt;/secrets&gt;\n        &lt;!-- Setup the client to OpenAI --&gt;\n        &lt;component id=\"openai\" class=\"ai.vespa.llm.clients.OpenAI\"&gt;\n            &lt;config name=\"ai.vespa.llm.clients.llm-client\"&gt;\n                &lt;!-- Uncomment this to use secret from Vespa Cloud Secret Store --&gt;\n                &lt;apiKeySecretName&gt;openai-api-key&lt;/apiKeySecretName&gt;\n            &lt;/config&gt;\n        &lt;/component&gt;\n\n        &lt;component id=\"nomicmb\" type=\"hugging-face-embedder\"&gt;\n            &lt;transformer-model\n                url=\"https://data.vespa-cloud.com/onnx_models/nomic-ai-modernbert-embed-base/model.onnx\" /&gt;\n            &lt;transformer-token-type-ids /&gt;\n            &lt;tokenizer-model\n                url=\"https://data.vespa-cloud.com/onnx_models/nomic-ai-modernbert-embed-base/tokenizer.json\" /&gt;\n            &lt;transformer-output&gt;token_embeddings&lt;/transformer-output&gt;\n            &lt;max-tokens&gt;8192&lt;/max-tokens&gt;\n            &lt;prepend&gt;\n                &lt;query&gt;search_query:&lt;/query&gt;\n                &lt;document&gt;search_document:&lt;/document&gt;\n            &lt;/prepend&gt;\n        &lt;/component&gt;\n        &lt;search&gt;\n            &lt;chain id=\"openai\" inherits=\"vespa\"&gt;\n                &lt;searcher id=\"ai.vespa.search.llm.RAGSearcher\"&gt;\n                    &lt;config name=\"ai.vespa.search.llm.llm-searcher\"&gt;\n                        &lt;providerId&gt;openai&lt;/providerId&gt;\n                    &lt;/config&gt;\n                &lt;/searcher&gt;\n            &lt;/chain&gt;\n        &lt;/search&gt;\n        &lt;nodes&gt;\n            &lt;node hostalias=\"node1\" /&gt;\n        &lt;/nodes&gt;\n    &lt;/container&gt;\n\n    &lt;!-- See https://docs.vespa.ai/en/reference/services-content.html --&gt;\n    &lt;content id=\"content\" version=\"1.0\"&gt;\n        &lt;min-redundancy&gt;2&lt;/min-redundancy&gt;\n        &lt;documents&gt;\n            &lt;document type=\"doc\" mode=\"index\" /&gt;\n        &lt;/documents&gt;\n        &lt;nodes&gt;\n            &lt;node hostalias=\"node1\" distribution-key=\"0\" /&gt;\n        &lt;/nodes&gt;\n    &lt;/content&gt;\n\n&lt;/services&gt;\n</pre> <p>Let us write the uncommented <code>services.xml</code> file to the application package directory:</p> In\u00a0[14]: Copied! <pre>(application_root / \"services.xml\").write_text(uncommented_services_content)\n</pre> (application_root / \"services.xml\").write_text(uncommented_services_content) Out[14]: <pre>2398</pre> <p>Now, we can redeploy the application to Vespa Cloud with the secret reference included in the <code>services.xml</code> file:</p> In\u00a0[15]: Copied! <pre>app: Vespa = vespa_cloud.deploy(disk_folder=application_root)\n</pre> app: Vespa = vespa_cloud.deploy(disk_folder=application_root) <pre>Deployment started in run 86 of dev-aws-us-east-1c for vespa-team.rag-blueprint. This may take a few minutes the first time.\nINFO    [09:40:56]  Deploying platform version 8.586.25 and application dev build 86 for dev-aws-us-east-1c of default ...\nINFO    [09:40:56]  Using CA signed certificate version 5\nINFO    [09:41:01]  Session 379705 for tenant 'vespa-team' prepared and activated.\nINFO    [09:41:01]  ######## Details for all nodes ########\nINFO    [09:41:01]  h125699b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:41:01]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:41:01]  --- storagenode on port 19102 has config generation 379704, wanted is 379705\nINFO    [09:41:01]  --- searchnode on port 19107 has config generation 379705, wanted is 379705\nINFO    [09:41:01]  --- distributor on port 19111 has config generation 379704, wanted is 379705\nINFO    [09:41:01]  --- metricsproxy-container on port 19092 has config generation 379705, wanted is 379705\nINFO    [09:41:01]  h125755a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:41:01]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:41:01]  --- container on port 4080 has config generation 379705, wanted is 379705\nINFO    [09:41:01]  --- metricsproxy-container on port 19092 has config generation 379705, wanted is 379705\nINFO    [09:41:01]  h97530b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:41:01]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:41:01]  --- logserver-container on port 4080 has config generation 379705, wanted is 379705\nINFO    [09:41:01]  --- metricsproxy-container on port 19092 has config generation 379705, wanted is 379705\nINFO    [09:41:01]  h119190c.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:41:01]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:41:01]  --- container-clustercontroller on port 19050 has config generation 379705, wanted is 379705\nINFO    [09:41:01]  --- metricsproxy-container on port 19092 has config generation 379705, wanted is 379705\nINFO    [09:41:08]  Found endpoints:\nINFO    [09:41:08]  - dev.aws-us-east-1c\nINFO    [09:41:08]   |-- https://fe5fe13c.fe19121d.z.vespa-app.cloud/ (cluster 'default')\nINFO    [09:41:08]  Deployment of new application revision complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for default\nURL: https://fe5fe13c.fe19121d.z.vespa-app.cloud/\nApplication is up!\n</pre> In\u00a0[16]: Copied! <pre>doc_file = repo_root / \"dataset\" / \"docs.jsonl\"\nwith open(doc_file, \"r\") as f:\n    docs = [json.loads(line) for line in f.readlines()]\n</pre> doc_file = repo_root / \"dataset\" / \"docs.jsonl\" with open(doc_file, \"r\") as f:     docs = [json.loads(line) for line in f.readlines()] In\u00a0[17]: Copied! <pre>docs[:2]\n</pre> docs[:2] Out[17]: <pre>[{'put': 'id:doc:doc::1',\n  'fields': {'created_timestamp': 1675209600,\n   'modified_timestamp': 1675296000,\n   'text': '# SynapseCore Module: Custom Attention Implementation\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass CustomAttention(nn.Module):\\n    def __init__(self, hidden_dim):\\n        super(CustomAttention, self).__init__()\\n        self.hidden_dim = hidden_dim\\n        self.query_layer = nn.Linear(hidden_dim, hidden_dim)\\n        self.key_layer = nn.Linear(hidden_dim, hidden_dim)\\n        self.value_layer = nn.Linear(hidden_dim, hidden_dim)\\n        # More layers and logic here\\n\\n    def forward(self, query_input, key_input, value_input, mask=None):\\n        # Q, K, V projections\\n        Q = self.query_layer(query_input)\\n        K = self.key_layer(key_input)\\n        V = self.value_layer(value_input)\\n\\n        # Scaled Dot-Product Attention\\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.hidden_dim ** 0.5)\\n        if mask is not None:\\n            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\\n        \\n        attention_probs = F.softmax(attention_scores, dim=-1)\\n        context_vector = torch.matmul(attention_probs, V)\\n        return context_vector, attention_probs\\n\\n# Example Usage:\\n# attention_module = CustomAttention(hidden_dim=512)\\n# output, probs = attention_module(q_tensor, k_tensor, v_tensor)\\n```\\n\\n## Design Notes:\\n- Optimized for speed with batched operations.\\n- Includes optional masking for variable sequence lengths.\\n## &lt;MORE_TEXT:HERE&gt;',\n   'favorite': True,\n   'last_opened_timestamp': 1717308000,\n   'open_count': 25,\n   'title': 'custom_attention_impl.py.md',\n   'id': '1'}},\n {'put': 'id:doc:doc::2',\n  'fields': {'created_timestamp': 1709251200,\n   'modified_timestamp': 1709254800,\n   'text': \"# YC Workshop Notes: Scaling B2B Sales (W25)\\nDate: 2025-03-01\\nSpeaker: [YC Partner Name]\\n\\n## Key Takeaways:\\n1.  **ICP Definition is Crucial:** Don't try to sell to everyone. Narrow down your Ideal Customer Profile.\\n    -   Characteristics: Industry, company size, pain points, decision-maker roles.\\n2.  **Outbound Strategy:**\\n    -   Personalized outreach &gt; Mass emails.\\n    -   Tools mentioned: Apollo.io, Outreach.io.\\n    -   Metrics: Open rates, reply rates, meeting booked rates.\\n3.  **Sales Process Stages:**\\n    -   Prospecting -&gt; Qualification -&gt; Demo -&gt; Proposal -&gt; Negotiation -&gt; Close.\\n    -   Define clear entry/exit criteria for each stage.\\n4.  **Value Proposition:** Clearly articulate how you solve the customer's pain and deliver ROI.\\n5.  **Early Hires:** First sales hire should be a 'hunter-farmer' hybrid if possible, or a strong individual contributor.\\n\\n## Action Items for SynapseFlow:\\n-   [ ] Refine ICP based on beta user feedback.\\n-   [ ] Experiment with a small, targeted outbound campaign for 2 specific verticals.\\n-   [ ] Draft initial sales playbook outline.\\n## &lt;MORE_TEXT:HERE&gt;\",\n   'favorite': True,\n   'last_opened_timestamp': 1717000000,\n   'open_count': 12,\n   'title': 'yc_b2b_sales_workshop_notes.md',\n   'id': '2'}}]</pre> In\u00a0[18]: Copied! <pre>vespa_feed = []\nfor doc in docs:\n    vespa_doc = doc.copy()\n    vespa_doc[\"id\"] = doc[\"fields\"][\"id\"]\n    vespa_doc.pop(\"put\")\n    vespa_feed.append(vespa_doc)\nvespa_feed[:2]\n</pre> vespa_feed = [] for doc in docs:     vespa_doc = doc.copy()     vespa_doc[\"id\"] = doc[\"fields\"][\"id\"]     vespa_doc.pop(\"put\")     vespa_feed.append(vespa_doc) vespa_feed[:2] Out[18]: <pre>[{'fields': {'created_timestamp': 1675209600,\n   'modified_timestamp': 1675296000,\n   'text': '# SynapseCore Module: Custom Attention Implementation\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass CustomAttention(nn.Module):\\n    def __init__(self, hidden_dim):\\n        super(CustomAttention, self).__init__()\\n        self.hidden_dim = hidden_dim\\n        self.query_layer = nn.Linear(hidden_dim, hidden_dim)\\n        self.key_layer = nn.Linear(hidden_dim, hidden_dim)\\n        self.value_layer = nn.Linear(hidden_dim, hidden_dim)\\n        # More layers and logic here\\n\\n    def forward(self, query_input, key_input, value_input, mask=None):\\n        # Q, K, V projections\\n        Q = self.query_layer(query_input)\\n        K = self.key_layer(key_input)\\n        V = self.value_layer(value_input)\\n\\n        # Scaled Dot-Product Attention\\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.hidden_dim ** 0.5)\\n        if mask is not None:\\n            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\\n        \\n        attention_probs = F.softmax(attention_scores, dim=-1)\\n        context_vector = torch.matmul(attention_probs, V)\\n        return context_vector, attention_probs\\n\\n# Example Usage:\\n# attention_module = CustomAttention(hidden_dim=512)\\n# output, probs = attention_module(q_tensor, k_tensor, v_tensor)\\n```\\n\\n## Design Notes:\\n- Optimized for speed with batched operations.\\n- Includes optional masking for variable sequence lengths.\\n## &lt;MORE_TEXT:HERE&gt;',\n   'favorite': True,\n   'last_opened_timestamp': 1717308000,\n   'open_count': 25,\n   'title': 'custom_attention_impl.py.md',\n   'id': '1'},\n  'id': '1'},\n {'fields': {'created_timestamp': 1709251200,\n   'modified_timestamp': 1709254800,\n   'text': \"# YC Workshop Notes: Scaling B2B Sales (W25)\\nDate: 2025-03-01\\nSpeaker: [YC Partner Name]\\n\\n## Key Takeaways:\\n1.  **ICP Definition is Crucial:** Don't try to sell to everyone. Narrow down your Ideal Customer Profile.\\n    -   Characteristics: Industry, company size, pain points, decision-maker roles.\\n2.  **Outbound Strategy:**\\n    -   Personalized outreach &gt; Mass emails.\\n    -   Tools mentioned: Apollo.io, Outreach.io.\\n    -   Metrics: Open rates, reply rates, meeting booked rates.\\n3.  **Sales Process Stages:**\\n    -   Prospecting -&gt; Qualification -&gt; Demo -&gt; Proposal -&gt; Negotiation -&gt; Close.\\n    -   Define clear entry/exit criteria for each stage.\\n4.  **Value Proposition:** Clearly articulate how you solve the customer's pain and deliver ROI.\\n5.  **Early Hires:** First sales hire should be a 'hunter-farmer' hybrid if possible, or a strong individual contributor.\\n\\n## Action Items for SynapseFlow:\\n-   [ ] Refine ICP based on beta user feedback.\\n-   [ ] Experiment with a small, targeted outbound campaign for 2 specific verticals.\\n-   [ ] Draft initial sales playbook outline.\\n## &lt;MORE_TEXT:HERE&gt;\",\n   'favorite': True,\n   'last_opened_timestamp': 1717000000,\n   'open_count': 12,\n   'title': 'yc_b2b_sales_workshop_notes.md',\n   'id': '2'},\n  'id': '2'}]</pre> <p>Now, let us feed the data to Vespa. If you have a large dataset, you could also do this async, with <code>feed_async_iterable()</code>, see Feeding Vespa cloud for a detailed comparison.</p> In\u00a0[19]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\n# Feed data into Vespa synchronously\napp.feed_iterable(vespa_feed, schema=VESPA_SCHEMA_NAME, callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   # Feed data into Vespa synchronously app.feed_iterable(vespa_feed, schema=VESPA_SCHEMA_NAME, callback=callback) <p>Let us test some queries to see if the application is working as expected. We will use one of the pre-configured query profiles, which we will explain in more detail later. For now, let us just see that we can get some results back from the application.</p> In\u00a0[20]: Copied! <pre>query = \"What is SynapseFlows strategy\"\nbody = {\n    \"query\": query,\n    \"queryProfile\": \"hybrid\",\n    \"hits\": 2,\n}\nwith app.syncio() as sess:\n    response = sess.query(body)\nresponse.json\n</pre> query = \"What is SynapseFlows strategy\" body = {     \"query\": query,     \"queryProfile\": \"hybrid\",     \"hits\": 2, } with app.syncio() as sess:     response = sess.query(body) response.json Out[20]: <pre>{'root': {'id': 'toplevel',\n  'relevance': 1.0,\n  'fields': {'totalCount': 100},\n  'coverage': {'coverage': 100,\n   'documents': 100,\n   'full': True,\n   'nodes': 1,\n   'results': 1,\n   'resultsFull': 1},\n  'children': [{'id': 'index:content/0/e369853debf684767dff1f16',\n    'relevance': 1.7111883427143333,\n    'source': 'content',\n    'fields': {'sddocname': 'doc',\n     'chunks_top3': ['# YC Application Draft Snippets - SynapseFlow (Late 2024)\\n\\n**Q: Describe what your company does in 50 characters or less.**\\n- AI model deployment made easy for developers.\\n- Effortless MLOps for startups.\\n- Deploy ML models in minutes, not weeks.\\n\\n**Q: What is your company going to make?**\\nSynapseFlow is building a PaaS solution that radically simplifies the deployment, management, and scaling of machine learning models. We provide a developer-first API and intuitive UI that abstracts away the complexities of MLOps infrastructure (Kubernetes, model servers, monitoring), allowing data scientists and developers ',\n      \"to focus on building models, not wrestling with ops. Our vision is to be the Heroku for AI.\\n\\n**Q: Why did you pick this idea to work on?**\\nAs an AI engineer, I've experienced firsthand the immense friction and time wasted in operationalizing ML models. Existing solutions are often too complex for smaller teams (e.g., full SageMaker/Vertex AI) or lack the flexibility needed for custom model development. We believe there's a huge unmet need for a simple, powerful, and affordable MLOps platform.\\n\\n## &lt;MORE_TEXT:HERE&gt; (More Q&amp;A drafts, team background notes)\"],\n     'summaryfeatures': {'top_3_chunk_sim_scores': {'type': 'tensor&lt;float&gt;(chunk{})',\n       'cells': {'0': 0.36166757345199585, '1': 0.21831661462783813}},\n      'vespa.summaryFeatures.cached': 0.0}}},\n   {'id': 'index:content/0/98f13708aca18c358d9d52d0',\n    'relevance': 1.309791587164871,\n    'source': 'content',\n    'fields': {'sddocname': 'doc',\n     'chunks_top3': [\"# Ideas for SynapseFlow Blog Post - 'Demystifying MLOps'\\n\\n**Target Audience:** Developers, data scientists new to MLOps, product managers.\\n**Goal:** Explain what MLOps is, why it's important, and how SynapseFlow helps.\\n\\n## Outline:\\n1.  **Introduction: The AI/ML Development Lifecycle is More Than Just Model Training**\\n    * Analogy: Building a model is like writing code; MLOps is like DevOps for ML.\\n2.  **What is MLOps? (The Core Pillars)**\\n    * Data Management (Versioning, Lineage, Quality)\\n    * Experiment Tracking &amp; Model Versioning\\n    * CI/CD for ML (Continuous Integration, Continuous Delivery, Continuous Training)\\n    * Model Deployment &amp; Serving\\n    * Monitoring &amp; Observability (Performance, Drift, Data Quality)\\n    * Governance &amp; Reproducibility\\n3.  **Why is MLOps Hard? (The Challenges)\",\n      \"**\\n    * Complexity of the ML lifecycle.\\n    * Bridging the gap between data science and engineering.\\n    * Tooling fragmentation.\\n    * Need for specialized skills.\\n4.  **How SynapseFlow Addresses These Challenges (Subtle Product Weave-in)**\\n    * Focus on ease of deployment (our current strength).\\n    * Streamlined workflow from experiment to production (our vision).\\n    * (Mention specific features that align with MLOps pillars without being overly salesy).\\n5.  **Getting Started with MLOps - Practical Tips**\\n    * Start simple, iterate.\\n    * Focus on automation early.\\n    * Choose tools that fit your team's scale and expertise.\\n6.  **Conclusion: MLOps is an Enabler for Realizing AI Value**\\n\\n## &lt;MORE_TEXT:HERE&gt; (Draft paragraphs, links to reference articles, potential graphics ideas)\"],\n     'summaryfeatures': {'top_3_chunk_sim_scores': {'type': 'tensor&lt;float&gt;(chunk{})',\n       'cells': {'0': 0.3064674735069275, '1': 0.29259079694747925}},\n      'vespa.summaryFeatures.cached': 0.0}}}]}}</pre> <p>And by changing to the <code>rag</code> query profile, and adding the <code>streaming=True</code> parameter, we can stream the results from the LLM as server-sent events (SSE).</p> In\u00a0[21]: Copied! <pre>query = \"What is SynapseFlows strategy\"\nbody = {\n    \"query\": query,\n    \"queryProfile\": \"rag\",\n    \"hits\": 2,\n}\nresp_string = \"\"  # Adding a string variable to use for asserting the response in CI.\nwith app.syncio() as sess:\n    stream_resp = sess.query(\n        body,\n        streaming=True,\n    )\n    for line in stream_resp:\n        if line.startswith(\"data: \"):\n            event = json.loads(line[6:])\n            token = event.get(\"token\", \"\")\n            resp_string += token\n            print(token, end=\"\")\nassert len(resp_string) &gt; 10, \"Response string should be longer than 10 characters.\"\n</pre> query = \"What is SynapseFlows strategy\" body = {     \"query\": query,     \"queryProfile\": \"rag\",     \"hits\": 2, } resp_string = \"\"  # Adding a string variable to use for asserting the response in CI. with app.syncio() as sess:     stream_resp = sess.query(         body,         streaming=True,     )     for line in stream_resp:         if line.startswith(\"data: \"):             event = json.loads(line[6:])             token = event.get(\"token\", \"\")             resp_string += token             print(token, end=\"\") assert len(resp_string) &gt; 10, \"Response string should be longer than 10 characters.\" <pre>SynapseFlow's strategy revolves around simplifying the deployment, management, and scaling of machine learning (ML) models through a developer-first platform-as-a-service (PaaS) solution. The key elements of their strategy include:\n\n1. **Developer-Focused Solution:** SynapseFlow aims to provide a user-friendly API and intuitive user interface that abstracts the complexities associated with MLOps infrastructure (such as Kubernetes and model servers). This allows developers and data scientists to focus primarily on building models rather than dealing with operational challenges.\n\n2. **Addressing Market Gaps:** The founders identified a significant pain point in the existing MLOps landscape, particularly for smaller teams. Many current solutions are either too complex or not flexible enough for custom model development. SynapseFlow targets this unmet need for a straightforward, powerful, and cost-effective platform.\n\n3. **Vision of Simplified MLOps:** By positioning itself as \"the Heroku for AI,\" SynapseFlow aims to offer an all-in-one solution that streamlines the workflow from experimentation to production, thus enhancing efficiency and speed in ML project deployment.\n\n4. **Education and Support:** Their strategy also includes educational initiatives, as outlined in their blog post ideas targeting developers and product managers new to MLOps. By demystifying MLOps and discussing its challenges and the way SynapseFlow addresses them, they plan to enhance user understanding and adoption of their platform.\n\n5. **Continuous Improvement:** SynapseFlow emphasizes a relentless focus on ease of deployment and improving automation capabilities, suggesting an iterative approach to platform development that responds to user feedback and evolving industry needs.\n\nOverall, SynapseFlow's strategy is centered on providing user-friendly solutions that reduce operational complexity, enabling faster deployment of machine learning models and supporting teams in successfully realizing the value of AI.</pre> <p>Great, we got some results. The quality is not very good yet, but we will show how to improve it in the next steps.</p> <p>But first, let us explain the use case we are trying to solve with this RAG application.</p> In\u00a0[22]: Copied! <pre>import json\n\ndocs_file = repo_root / \"dataset\" / \"docs.jsonl\"\n\nwith open(docs_file) as f:\n    docs = [json.loads(line) for line in f]\n\ndocs[10]\n</pre> import json  docs_file = repo_root / \"dataset\" / \"docs.jsonl\"  with open(docs_file) as f:     docs = [json.loads(line) for line in f]  docs[10] Out[22]: <pre>{'put': 'id:doc:doc::11',\n 'fields': {'created_timestamp': 1698796800,\n  'modified_timestamp': 1698796800,\n  'text': \"# Journal Entry - 2024-11-01\\n\\nFeeling the YC pressure cooker, but in a good way. The pace is insane. It reminds me of peaking for a powerlifting meet \u2013 everything has to be precise, every session counts, and you're constantly pushing your limits.\\n\\nThinking about **periodization** in lifting \u2013 how you structure macrocycles, mesocycles, and microcycles. Can this apply to startup sprints? We have our big YC Demo Day goal (macro), then maybe 2-week sprints are mesocycles, and daily tasks are microcycles. Need to ensure we're not just redlining constantly but building in phases of intense work, focused development, and even 'deload' (strategic rest/refinement) to avoid burnout and make sustainable progress.\\n\\n**RPE (Rate of Perceived Exertion)** is another concept. In the gym, it helps auto-regulate training based on how you feel. For the startup, maybe we need an RPE check for the team? Are we pushing too hard on a feature that's yielding low returns (high RPE, low ROI)? Can we adjust the 'load' (scope) or 'reps' (iterations) based on team capacity and feedback?\\n\\nIt's interesting how the discipline and structured thinking from strength training can offer mental models for tackling the chaos of a startup. Both require consistency, grit, and a willingness to fail and learn.\\n\\n## &lt;MORE_TEXT:HERE&gt; (More reflections on YC, specific project challenges)\",\n  'favorite': False,\n  'last_opened_timestamp': 1700000000,\n  'open_count': 5,\n  'title': 'journal_2024_11_01_yc_and_lifting.md',\n  'id': '11'}}</pre> <p>In order to evaluate the quality of the RAG application, we also need a set of representative queries, with annotated relevant documents. Crucially, you need a set of representative queries that thoroughly cover your expected use case. More is better, but some eval is always better than none.</p> <p>We used <code>gemini-2.5-pro</code> to create our queries and relevant document labels. Please check out our blog post to learn more about using LLM-as-a-judge.</p> <p>We decided to generate some queries that need several documents to provide a good answer, and some that only need one document.</p> <p>If these queries are representative of the use case, we will show that they can be a great starting point for creating an (initial) ranking expression that can be used for retrieving and ranking candidate documents. But, it can (and should) also be improved, for example by collecting user interaction data, human labeling and/ or using an LLM to generate relevance feedback following the initial ranking expression.</p> In\u00a0[\u00a0]: Copied! <pre>queries_file = repo_root / \"queries\" / \"queries.json\"\n\nwith open(queries_file) as f:\n    queries = json.load(f)\n\nqueries[10]\n</pre> queries_file = repo_root / \"queries\" / \"queries.json\"  with open(queries_file) as f:     queries = json.load(f)  queries[10] Out[\u00a0]: <pre>{'query_id': 'alex_q_11',\n 'query_text': \"Where's that journal entry where I compared YC to powerlifting?\",\n 'category': 'Navigational - Personal',\n 'description': 'Finding a specific personal reflection in his journal.',\n 'relevant_document_ids': ['11', '58', '100']}</pre> In\u00a0[24]: Copied! <pre>schema_file = repo_root / \"app\" / \"schemas\" / \"doc.sd\"\nschema_content = schema_file.read_text()\n\ndisplay_md(schema_content)\n</pre> schema_file = repo_root / \"app\" / \"schemas\" / \"doc.sd\" schema_content = schema_file.read_text()  display_md(schema_content) <pre><code>txt\n# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\nschema doc {\n\n    document doc {\n\n        field id type string {\n            indexing: summary | attribute\n        }\n\n        field title type string {\n            indexing: index | summary\n            index: enable-bm25\n        }\n\n        field text type string {\n            \n        }\n\n        field created_timestamp type long {\n            indexing: attribute | summary\n        }\n        field modified_timestamp type long {\n            indexing: attribute | summary\n        }\n        \n        field last_opened_timestamp type long {\n            indexing: attribute | summary\n        }\n        field open_count type int {\n            indexing: attribute | summary\n        }\n        field favorite type bool {\n            indexing: attribute | summary\n        }\n\n    }\n\n    field title_embedding type tensor&lt;int8&gt;(x[96]) {\n        indexing: input title | embed | pack_bits | attribute | index\n        attribute {\n            distance-metric: hamming\n        }\n    }\n\n    field chunks type array&lt;string&gt; {\n        indexing: input text | chunk fixed-length 1024 | summary | index\n        index: enable-bm25\n    }\n\n    field chunk_embeddings type tensor&lt;int8&gt;(chunk{}, x[96]) {\n        indexing: input text | chunk fixed-length 1024 | embed | pack_bits | attribute | index\n        attribute {\n            distance-metric: hamming\n        }\n    }\n\n    fieldset default {\n        fields: title, chunks\n    }\n\n    document-summary no-chunks {\n        summary id {}\n        summary title {}\n        summary created_timestamp {}\n        summary modified_timestamp {}\n        summary last_opened_timestamp {}\n        summary open_count {}\n        summary favorite {}\n        summary chunks {}\n    }\n\n    document-summary top_3_chunks {\n        from-disk\n        summary chunks_top3 {\n            source: chunks\n            select-elements-by: top_3_chunk_sim_scores #this needs to be added a summary-feature to the rank-profile\n        }\n    }\n}\n</code></pre> <p>Keep reading for an explanation and reasoning behind the choices in the schema.</p> In\u00a0[25]: Copied! <pre># Let's explore the RAG Blueprint application structure\nprint(tree(\"src/rag-blueprint\"))\n</pre> # Let's explore the RAG Blueprint application structure print(tree(\"src/rag-blueprint\")) <pre>/Users/thomas/Repos/pyvespa/docs/sphinx/source/examples/src/rag-blueprint\n\u251c\u2500\u2500 app\n\u2502   \u251c\u2500\u2500 models\n\u2502   \u2502   \u2514\u2500\u2500 lightgbm_model.json\n\u2502   \u251c\u2500\u2500 schemas\n\u2502   \u2502   \u251c\u2500\u2500 doc\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 base-features.profile\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 collect-second-phase.profile\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 collect-training-data.profile\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 learned-linear.profile\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 match-only.profile\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 second-with-gbdt.profile\n\u2502   \u2502   \u2514\u2500\u2500 doc.sd\n\u2502   \u251c\u2500\u2500 search\n\u2502   \u2502   \u2514\u2500\u2500 query-profiles\n\u2502   \u2502       \u251c\u2500\u2500 deepresearch-with-gbdt.xml\n\u2502   \u2502       \u251c\u2500\u2500 deepresearch.xml\n\u2502   \u2502       \u251c\u2500\u2500 hybrid-with-gbdt.xml\n\u2502   \u2502       \u251c\u2500\u2500 hybrid.xml\n\u2502   \u2502       \u251c\u2500\u2500 rag-with-gbdt.xml\n\u2502   \u2502       \u2514\u2500\u2500 rag.xml\n\u2502   \u251c\u2500\u2500 security\n\u2502   \u2502   \u2514\u2500\u2500 clients.pem\n\u2502   \u2514\u2500\u2500 services.xml\n\u251c\u2500\u2500 dataset\n\u2502   \u251c\u2500\u2500 docs.jsonl\n\u2502   \u251c\u2500\u2500 queries.json\n\u2502   \u2514\u2500\u2500 test_queries.json\n\u251c\u2500\u2500 eval\n\u2502   \u251c\u2500\u2500 output\n\u2502   \u2502   \u251c\u2500\u2500 Vespa-training-data_match_first_phase_20250623_133241.csv\n\u2502   \u2502   \u251c\u2500\u2500 Vespa-training-data_match_first_phase_20250623_133241_logreg_coefficients.txt\n\u2502   \u2502   \u251c\u2500\u2500 Vespa-training-data_match_rank_second_phase_20250623_135819.csv\n\u2502   \u2502   \u2514\u2500\u2500 Vespa-training-data_match_rank_second_phase_20250623_135819_feature_importance.csv\n\u2502   \u251c\u2500\u2500 collect_pyvespa.py\n\u2502   \u251c\u2500\u2500 evaluate_match_phase.py\n\u2502   \u251c\u2500\u2500 evaluate_ranking.py\n\u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 resp.json\n\u2502   \u251c\u2500\u2500 train_lightgbm.py\n\u2502   \u2514\u2500\u2500 train_logistic_regression.py\n\u251c\u2500\u2500 deploy-locally.md\n\u251c\u2500\u2500 generation.md\n\u251c\u2500\u2500 query-profiles.md\n\u251c\u2500\u2500 README.md\n\u2514\u2500\u2500 relevance.md\n</pre> <p>You can see that we have separated the query profiles, and rank profiles into their own directories.</p> In\u00a0[26]: Copied! <pre>qp_dir = repo_root / \"app\" / \"search\" / \"query-profiles\"\nhybrid_qp = (qp_dir / \"hybrid.xml\").read_text()\n\ndisplay_md(hybrid_qp, tag=\"xml\")\n</pre> qp_dir = repo_root / \"app\" / \"search\" / \"query-profiles\" hybrid_qp = (qp_dir / \"hybrid.xml\").read_text()  display_md(hybrid_qp, tag=\"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;!--\nmatch_avg_top_3_chunk_sim_scores   : 13.383840\nmatch_avg_top_3_chunk_text_scores  : 0.203145\nmatch_bm25(chunks)                 : 0.159914\nmatch_bm25(title)                  : 0.191867\nmatch_max_chunk_sim_scores         : 10.067169\nmatch_max_chunk_text_scores        : 0.153392\nIntercept                          : -7.798639\n--&gt;\n&lt;query-profile id=\"hybrid\"&gt;\n    &lt;field name=\"schema\"&gt;doc&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(embedding)\"&gt;embed(@query)&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(float_embedding)\"&gt;embed(@query)&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(intercept)\"&gt;-7.798639&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(avg_top_3_chunk_sim_scores_param)\"&gt;13.383840&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(avg_top_3_chunk_text_scores_param)\"&gt;0.203145&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(bm25_chunks_param)\"&gt;0.159914&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(bm25_title_param)\"&gt;0.191867&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(max_chunk_sim_scores_param)\"&gt;10.067169&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(max_chunk_text_scores_param)\"&gt;0.153392&lt;/field&gt;\n    &lt;field name=\"yql\"&gt;\n        select *\n        from %{schema}\n        where userInput(@query) or\n        ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n        ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))\n    &lt;/field&gt;\n    &lt;field name=\"hits\"&gt;10&lt;/field&gt;\n    &lt;field name=\"ranking.profile\"&gt;learned-linear&lt;/field&gt;\n    &lt;field name=\"presentation.summary\"&gt;top_3_chunks&lt;/field&gt;\n&lt;/query-profile&gt;\n</pre> In\u00a0[27]: Copied! <pre>rag_blueprint_qp = (qp_dir / \"rag.xml\").read_text()\ndisplay_md(rag_blueprint_qp, tag=\"xml\")\n</pre> rag_blueprint_qp = (qp_dir / \"rag.xml\").read_text() display_md(rag_blueprint_qp, tag=\"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;query-profile id=\"rag\" inherits=\"hybrid\"&gt;\n  &lt;field name=\"hits\"&gt;50&lt;/field&gt;\n  &lt;field name=\"searchChain\"&gt;openai&lt;/field&gt;\n  &lt;field name=\"presentation.format\"&gt;sse&lt;/field&gt;\n&lt;/query-profile&gt;\n</pre> In\u00a0[28]: Copied! <pre>deep_qp = (qp_dir / \"deepresearch.xml\").read_text()\ndisplay_md(deep_qp, tag=\"xml\")\n</pre> deep_qp = (qp_dir / \"deepresearch.xml\").read_text() display_md(deep_qp, tag=\"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;query-profile id=\"deepresearch\" inherits=\"hybrid\"&gt;\n  &lt;field name=\"yql\"&gt;\n    select *\n    from %{schema}\n    where userInput(@query) or\n    ({label:\"title_label\", targetHits:10000}nearestNeighbor(title_embedding, embedding)) or\n    ({label:\"chunks_label\", targetHits:10000}nearestNeighbor(chunk_embeddings, embedding))\n  &lt;/field&gt;\n  &lt;field name=\"hits\"&gt;100&lt;/field&gt;\n  &lt;field name=\"timeout\"&gt;5s&lt;/field&gt;\n&lt;/query-profile&gt;\n</pre> <p>We will leave out the LLM-generation for this one, and let an LLM agent on the client side be responsible for using this API call as a tool, and to determine whether enough relevant context to answer has been retrieved. Note that the <code>targetHits</code> parameter set here does not really makes sense until your dataset reach a certain scale.</p> <p>As we add more rank-profiles, we can also inherit the existing query profiles, only to override the <code>ranking.profile</code> field to use a different rank profile. This is what we have done for the <code>rag-with-gbdt</code> and <code>deepresearch-with-gbdt</code> query profiles, which will use the <code>second-with-gbdt</code> rank profile instead of the <code>learned-linear</code> rank profile.</p> In\u00a0[29]: Copied! <pre>rag_gbdt_qp = (qp_dir / \"rag-with-gbdt.xml\").read_text()\ndisplay_md(rag_gbdt_qp, tag=\"xml\")\n</pre> rag_gbdt_qp = (qp_dir / \"rag-with-gbdt.xml\").read_text() display_md(rag_gbdt_qp, tag=\"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;query-profile id=\"rag-with-gbdt\" inherits=\"hybrid-with-gbdt\"&gt;\n  &lt;field name=\"hits\"&gt;50&lt;/field&gt;\n  &lt;field name=\"searchChain\"&gt;openai&lt;/field&gt;\n  &lt;field name=\"presentation.format\"&gt;sse&lt;/field&gt;\n&lt;/query-profile&gt;\n</pre> <p>Run the cells below to evaluate all three retrieval strategies on your dataset.</p> In\u00a0[30]: Copied! <pre>ids_to_query = {query[\"query_id\"]: query[\"query_text\"] for query in queries}\nrelevant_docs = {\n    query[\"query_id\"]: set(query[\"relevant_document_ids\"])\n    for query in queries\n    if \"relevant_document_ids\" in query\n}\n</pre> ids_to_query = {query[\"query_id\"]: query[\"query_text\"] for query in queries} relevant_docs = {     query[\"query_id\"]: set(query[\"relevant_document_ids\"])     for query in queries     if \"relevant_document_ids\" in query } In\u00a0[31]: Copied! <pre>from vespa.evaluation import VespaMatchEvaluator\nfrom vespa.application import Vespa\nimport vespa.querybuilder as qb\nimport json\nfrom pathlib import Path\n\n\ndef match_weakand_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\").from_(VESPA_SCHEMA_NAME).where(qb.userQuery(query_text))\n        ),\n        \"query\": query_text,\n        \"ranking\": \"match-only\",\n        \"input.query(embedding)\": f\"embed({query_text})\",\n        \"presentation.summary\": \"no-chunks\",\n    }\n\n\ndef match_hybrid_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(VESPA_SCHEMA_NAME)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"title_embedding\",\n                    query_vector=\"embedding\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.nearestNeighbor(\n                    field=\"chunk_embeddings\",\n                    query_vector=\"embedding\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.userQuery(\n                    query_text,\n                )\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"match-only\",\n        \"input.query(embedding)\": f\"embed({query_text})\",\n        \"presentation.summary\": \"no-chunks\",\n    }\n\n\ndef match_semantic_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(VESPA_SCHEMA_NAME)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"title_embedding\",\n                    query_vector=\"embedding\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.nearestNeighbor(\n                    field=\"chunk_embeddings\",\n                    query_vector=\"embedding\",\n                    annotations={\"targetHits\": 100},\n                )\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"match-only\",\n        \"input.query(embedding)\": f\"embed({query_text})\",\n        \"presentation.summary\": \"no-chunks\",\n    }\n\n\nmatch_results = {}\nfor evaluator_name, query_fn in [\n    (\"semantic\", match_semantic_query_fn),\n    (\"weakand\", match_weakand_query_fn),\n    (\"hybrid\", match_hybrid_query_fn),\n]:\n    print(f\"Evaluating {evaluator_name}...\")\n\n    match_evaluator = VespaMatchEvaluator(\n        queries=ids_to_query,\n        relevant_docs=relevant_docs,\n        vespa_query_fn=query_fn,\n        app=app,\n        name=\"test-run\",\n        id_field=\"id\",\n        write_csv=False,\n        write_verbose=False,  # optionally write verbose metrics to CSV\n    )\n\n    results = match_evaluator()\n    match_results[evaluator_name] = results\n</pre> from vespa.evaluation import VespaMatchEvaluator from vespa.application import Vespa import vespa.querybuilder as qb import json from pathlib import Path   def match_weakand_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\").from_(VESPA_SCHEMA_NAME).where(qb.userQuery(query_text))         ),         \"query\": query_text,         \"ranking\": \"match-only\",         \"input.query(embedding)\": f\"embed({query_text})\",         \"presentation.summary\": \"no-chunks\",     }   def match_hybrid_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(VESPA_SCHEMA_NAME)             .where(                 qb.nearestNeighbor(                     field=\"title_embedding\",                     query_vector=\"embedding\",                     annotations={\"targetHits\": 100},                 )                 | qb.nearestNeighbor(                     field=\"chunk_embeddings\",                     query_vector=\"embedding\",                     annotations={\"targetHits\": 100},                 )                 | qb.userQuery(                     query_text,                 )             )         ),         \"query\": query_text,         \"ranking\": \"match-only\",         \"input.query(embedding)\": f\"embed({query_text})\",         \"presentation.summary\": \"no-chunks\",     }   def match_semantic_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(VESPA_SCHEMA_NAME)             .where(                 qb.nearestNeighbor(                     field=\"title_embedding\",                     query_vector=\"embedding\",                     annotations={\"targetHits\": 100},                 )                 | qb.nearestNeighbor(                     field=\"chunk_embeddings\",                     query_vector=\"embedding\",                     annotations={\"targetHits\": 100},                 )             )         ),         \"query\": query_text,         \"ranking\": \"match-only\",         \"input.query(embedding)\": f\"embed({query_text})\",         \"presentation.summary\": \"no-chunks\",     }   match_results = {} for evaluator_name, query_fn in [     (\"semantic\", match_semantic_query_fn),     (\"weakand\", match_weakand_query_fn),     (\"hybrid\", match_hybrid_query_fn), ]:     print(f\"Evaluating {evaluator_name}...\")      match_evaluator = VespaMatchEvaluator(         queries=ids_to_query,         relevant_docs=relevant_docs,         vespa_query_fn=query_fn,         app=app,         name=\"test-run\",         id_field=\"id\",         write_csv=False,         write_verbose=False,  # optionally write verbose metrics to CSV     )      results = match_evaluator()     match_results[evaluator_name] = results <pre>Evaluating semantic...\nEvaluating weakand...\nEvaluating hybrid...\n</pre> In\u00a0[32]: Copied! <pre>import pandas as pd\n\ndf = pd.DataFrame(match_results)\ndf\n</pre> import pandas as pd  df = pd.DataFrame(match_results) df Out[32]: semantic weakand hybrid match_recall 1.00000 1.0000 1.00000 avg_recall_per_query 1.00000 1.0000 1.00000 total_relevant_docs 51.00000 51.0000 51.00000 total_matched_relevant 51.00000 51.0000 51.00000 avg_matched_per_query 100.00000 88.7500 100.00000 total_queries 20.00000 20.0000 20.00000 searchtime_avg 0.06275 0.0330 0.04395 searchtime_q50 0.03200 0.0290 0.03750 searchtime_q90 0.06400 0.0511 0.08500 searchtime_q95 0.10055 0.0703 0.08800 In\u00a0[33]: Copied! <pre>from vespa.application import Vespa\nfrom vespa.evaluation import VespaFeatureCollector\nfrom typing import Dict, Any\nimport json\nfrom pathlib import Path\n\n\ndef feature_collection_second_phase_query_fn(\n    query_text: str, top_k: int = 10, query_id: str = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert plain text into a JSON body for Vespa query with 'feature-collection' rank profile.\n    Includes both semantic similarity and BM25 matching with match features.\n    \"\"\"\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(\"doc\")\n            .where(\n                (\n                    qb.nearestNeighbor(\n                        field=\"title_embedding\",\n                        query_vector=\"embedding\",\n                        annotations={\n                            \"targetHits\": 100,\n                            \"label\": \"title_label\",\n                        },\n                    )\n                    | qb.nearestNeighbor(\n                        field=\"chunk_embeddings\",\n                        query_vector=\"embedding\",\n                        annotations={\n                            \"targetHits\": 100,\n                            \"label\": \"chunk_label\",\n                        },\n                    )\n                    | qb.userQuery(\n                        query_text,\n                    )\n                )\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"collect-second-phase\",\n        \"input.query(embedding)\": f\"embed({query_text})\",\n        \"input.query(float_embedding)\": f\"embed({query_text})\",\n        \"hits\": top_k,\n        \"timeout\": \"10s\",\n        \"presentation.summary\": \"no-chunks\",\n        \"presentation.timing\": True,\n    }\n\n\ndef feature_collection_first_phase_query_fn(\n    query_text: str, top_k: int = 10, query_id: str = None\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Convert plain text into a JSON body for Vespa query with 'feature-collection' rank profile.\n    Includes both semantic similarity and BM25 matching with match features.\n    \"\"\"\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(\"doc\")\n            .where(\n                (\n                    qb.nearestNeighbor(\n                        field=\"title_embedding\",\n                        query_vector=\"embedding\",\n                        annotations={\n                            \"targetHits\": 100,\n                            \"label\": \"title_label\",\n                        },\n                    )\n                    | qb.nearestNeighbor(\n                        field=\"chunk_embeddings\",\n                        query_vector=\"embedding\",\n                        annotations={\n                            \"targetHits\": 100,\n                            \"label\": \"chunk_label\",\n                        },\n                    )\n                    | qb.userQuery(\n                        query_text,\n                    )\n                )\n            )\n        ),\n        \"query\": query_text,\n        \"ranking\": \"collect-training-data\",\n        \"input.query(embedding)\": f\"embed({query_text})\",\n        \"input.query(float_embedding)\": f\"embed({query_text})\",\n        \"hits\": top_k,\n        \"timeout\": \"10s\",\n        \"presentation.summary\": \"no-chunks\",\n        \"presentation.timing\": True,\n    }\n\n\ndef generate_collector_name(\n    collect_matchfeatures: bool,\n    collect_rankfeatures: bool,\n    collect_summaryfeatures: bool,\n    second_phase: bool,\n) -&gt; str:\n    \"\"\"\n    Generate a collector name based on feature collection settings and phase.\n\n    Args:\n        collect_matchfeatures: Whether match features are being collected\n        collect_rankfeatures: Whether rank features are being collected\n        collect_summaryfeatures: Whether summary features are being collected\n        second_phase: Whether using second phase (True) or first phase (False)\n\n    Returns:\n        Generated collector name string\n    \"\"\"\n    features = []\n    if collect_matchfeatures:\n        features.append(\"match\")\n    if collect_rankfeatures:\n        features.append(\"rank\")\n    if collect_summaryfeatures:\n        features.append(\"summary\")\n\n    features_str = \"_\".join(features) if features else \"nofeatures\"\n    phase_str = \"second_phase\" if second_phase else \"first_phase\"\n    return f\"{features_str}_{phase_str}\"\n\n\nfeature_collector = VespaFeatureCollector(\n    queries=ids_to_query,\n    relevant_docs=relevant_docs,\n    vespa_query_fn=feature_collection_first_phase_query_fn,\n    app=app,\n    name=\"first-phase\",\n    id_field=\"id\",\n    collect_matchfeatures=True,\n    collect_summaryfeatures=False,\n    collect_rankfeatures=False,\n    write_csv=False,\n    random_hits_strategy=\"ratio\",\n    random_hits_value=1,\n)\nresults = feature_collector.collect()\n</pre> from vespa.application import Vespa from vespa.evaluation import VespaFeatureCollector from typing import Dict, Any import json from pathlib import Path   def feature_collection_second_phase_query_fn(     query_text: str, top_k: int = 10, query_id: str = None ) -&gt; Dict[str, Any]:     \"\"\"     Convert plain text into a JSON body for Vespa query with 'feature-collection' rank profile.     Includes both semantic similarity and BM25 matching with match features.     \"\"\"     return {         \"yql\": str(             qb.select(\"*\")             .from_(\"doc\")             .where(                 (                     qb.nearestNeighbor(                         field=\"title_embedding\",                         query_vector=\"embedding\",                         annotations={                             \"targetHits\": 100,                             \"label\": \"title_label\",                         },                     )                     | qb.nearestNeighbor(                         field=\"chunk_embeddings\",                         query_vector=\"embedding\",                         annotations={                             \"targetHits\": 100,                             \"label\": \"chunk_label\",                         },                     )                     | qb.userQuery(                         query_text,                     )                 )             )         ),         \"query\": query_text,         \"ranking\": \"collect-second-phase\",         \"input.query(embedding)\": f\"embed({query_text})\",         \"input.query(float_embedding)\": f\"embed({query_text})\",         \"hits\": top_k,         \"timeout\": \"10s\",         \"presentation.summary\": \"no-chunks\",         \"presentation.timing\": True,     }   def feature_collection_first_phase_query_fn(     query_text: str, top_k: int = 10, query_id: str = None ) -&gt; Dict[str, Any]:     \"\"\"     Convert plain text into a JSON body for Vespa query with 'feature-collection' rank profile.     Includes both semantic similarity and BM25 matching with match features.     \"\"\"     return {         \"yql\": str(             qb.select(\"*\")             .from_(\"doc\")             .where(                 (                     qb.nearestNeighbor(                         field=\"title_embedding\",                         query_vector=\"embedding\",                         annotations={                             \"targetHits\": 100,                             \"label\": \"title_label\",                         },                     )                     | qb.nearestNeighbor(                         field=\"chunk_embeddings\",                         query_vector=\"embedding\",                         annotations={                             \"targetHits\": 100,                             \"label\": \"chunk_label\",                         },                     )                     | qb.userQuery(                         query_text,                     )                 )             )         ),         \"query\": query_text,         \"ranking\": \"collect-training-data\",         \"input.query(embedding)\": f\"embed({query_text})\",         \"input.query(float_embedding)\": f\"embed({query_text})\",         \"hits\": top_k,         \"timeout\": \"10s\",         \"presentation.summary\": \"no-chunks\",         \"presentation.timing\": True,     }   def generate_collector_name(     collect_matchfeatures: bool,     collect_rankfeatures: bool,     collect_summaryfeatures: bool,     second_phase: bool, ) -&gt; str:     \"\"\"     Generate a collector name based on feature collection settings and phase.      Args:         collect_matchfeatures: Whether match features are being collected         collect_rankfeatures: Whether rank features are being collected         collect_summaryfeatures: Whether summary features are being collected         second_phase: Whether using second phase (True) or first phase (False)      Returns:         Generated collector name string     \"\"\"     features = []     if collect_matchfeatures:         features.append(\"match\")     if collect_rankfeatures:         features.append(\"rank\")     if collect_summaryfeatures:         features.append(\"summary\")      features_str = \"_\".join(features) if features else \"nofeatures\"     phase_str = \"second_phase\" if second_phase else \"first_phase\"     return f\"{features_str}_{phase_str}\"   feature_collector = VespaFeatureCollector(     queries=ids_to_query,     relevant_docs=relevant_docs,     vespa_query_fn=feature_collection_first_phase_query_fn,     app=app,     name=\"first-phase\",     id_field=\"id\",     collect_matchfeatures=True,     collect_summaryfeatures=False,     collect_rankfeatures=False,     write_csv=False,     random_hits_strategy=\"ratio\",     random_hits_value=1, ) results = feature_collector.collect() In\u00a0[34]: Copied! <pre>feature_df = pd.DataFrame(results[\"results\"])\nfeature_df\n</pre> feature_df = pd.DataFrame(results[\"results\"]) feature_df Out[34]: query_id doc_id relevance_label relevance_score match_avg_top_3_chunk_sim_scores match_avg_top_3_chunk_text_scores match_bm25(chunks) match_bm25(title) match_max_chunk_sim_scores match_max_chunk_text_scores 0 alex_q_01 1 1.0 0.734995 0.358027 15.100841 23.010389 4.333828 0.391143 20.582403 1 alex_q_01 82 1.0 0.262686 0.225300 12.327676 18.611592 2.453409 0.258905 15.644889 2 alex_q_01 50 1.0 0.060615 0.248329 8.444725 7.717984 0.000000 0.268457 8.444725 3 alex_q_01 64 0.0 0.994799 0.238926 3.608304 4.940433 0.000000 0.262717 4.063323 4 alex_q_01 21 0.0 0.986948 0.265199 3.424351 3.615531 0.000000 0.265199 3.424351 ... ... ... ... ... ... ... ... ... ... ... 97 alex_q_19 4 0.0 0.958641 0.210284 1.256423 2.238139 0.000000 0.229001 1.967774 98 alex_q_20 20 1.0 0.656100 0.337411 8.959117 12.534452 9.865092 0.402615 12.799867 99 alex_q_20 35 1.0 0.306241 0.227978 8.462585 13.478890 0.000000 0.239757 13.353056 100 alex_q_20 2 0.0 0.999038 0.200672 0.942418 0.871042 0.000000 0.206993 0.942418 101 alex_q_20 45 0.0 0.964807 0.151361 2.288041 2.695306 0.000000 0.151361 2.288041 <p>102 rows \u00d7 10 columns</p> <p>Note that the <code>relevance_score</code> in this table is just the random expression we used in the <code>second-phase</code> of the <code>collect-training-data</code> rank profile, and will be dropped before training the model.</p> In\u00a0[35]: Copied! <pre>import pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    log_loss,\n    roc_auc_score,\n    average_precision_score,\n)\n\n\ndef get_coefficients_info(model, features, intercept, scaler):\n    \"\"\"\n    Returns the model coefficients as a dictionary that accounts for standardization.\n    The transformation allows the model to be expressed in terms of the original, unscaled features.\n    \"\"\"\n    # For standardized features, the transformation is z = (x - mean) / std.\n    # The original expression 'coef * z + intercept' becomes:\n    # (coef / std) * x + (intercept - coef * mean / std)\n    transformed_coefs = model.coef_[0] / scaler.scale_\n    transformed_intercept = intercept - np.sum(\n        model.coef_[0] * scaler.mean_ / scaler.scale_\n    )\n\n    # Create a mathematical expression for the model using original (unscaled) features\n    expression_parts = [f\"{transformed_intercept:.6f}\"]\n    for feature, coef in zip(features, transformed_coefs):\n        expression_parts.append(f\"{coef:+.6f}*{feature}\")\n    expression = \"\".join(expression_parts)\n\n    # Return a dictionary containing scaling parameters and coefficient information\n    return {\n        \"expression\": expression,\n        \"feature_means\": dict(zip(features, scaler.mean_)),\n        \"feature_stds\": dict(zip(features, scaler.scale_)),\n        \"original_coefficients\": dict(zip(features, model.coef_[0])),\n        \"original_intercept\": float(intercept),\n        \"transformed_coefficients\": dict(zip(features, transformed_coefs)),\n        \"transformed_intercept\": float(transformed_intercept),\n    }\n\n\ndef perform_cross_validation(df: pd.DataFrame):\n    \"\"\"\n    Loads data, applies standardization, and performs 5-fold stratified cross-validation.\n\n    Args:\n        df: A pandas DataFrame with features and a 'relevance_label' target column.\n\n    Returns:\n        A tuple containing two pandas DataFrames:\n        - cv_results_df: The mean and standard deviation of evaluation metrics.\n        - coef_df: The model coefficients for both scaled and unscaled features.\n    \"\"\"\n    # Define and drop irrelevant columns\n    columns_to_drop = [\"doc_id\", \"query_id\", \"relevance_score\"]\n    # Drop only the columns that exist in the DataFrame\n    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n    df[\"relevance_label\"] = df[\"relevance_label\"].astype(int)\n\n    # Define features (X) and target (y)\n    X = df.drop(columns=[\"relevance_label\"])\n    features = X.columns.tolist()\n    y = df[\"relevance_label\"]\n\n    # Initialize StandardScaler, model, and cross-validator\n    scaler = StandardScaler()\n    N_SPLITS = 5\n    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n    model = LogisticRegression(C=0.001, tol=1e-2, random_state=42)\n\n    # Lists to store metrics for each fold\n    metrics = {\n        \"Accuracy\": [],\n        \"Precision\": [],\n        \"Recall\": [],\n        \"F1-Score\": [],\n        \"Log Loss\": [],\n        \"ROC AUC\": [],\n        \"Avg Precision\": [],\n    }\n\n    # Perform 5-Fold Stratified Cross-Validation\n    for train_index, test_index in skf.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n        # Fit scaler on training data and transform both sets\n        X_train_scaled = scaler.fit_transform(X_train)\n        X_test_scaled = scaler.transform(X_test)\n\n        # Train the model and make predictions\n        model.fit(X_train_scaled, y_train)\n        y_pred = model.predict(X_test_scaled)\n        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n\n        # Calculate and store metrics for the fold\n        metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n        metrics[\"Precision\"].append(precision_score(y_test, y_pred, zero_division=0))\n        metrics[\"Recall\"].append(recall_score(y_test, y_pred, zero_division=0))\n        metrics[\"F1-Score\"].append(f1_score(y_test, y_pred, zero_division=0))\n        metrics[\"Log Loss\"].append(log_loss(y_test, y_pred_proba))\n        metrics[\"ROC AUC\"].append(roc_auc_score(y_test, y_pred_proba))\n        metrics[\"Avg Precision\"].append(average_precision_score(y_test, y_pred_proba))\n\n    # --- Prepare Results DataFrames ---\n\n    # Create DataFrame for cross-validation results\n    cv_results = {\n        \"Metric\": list(metrics.keys()),\n        \"Mean\": [np.mean(v) for v in metrics.values()],\n        \"Std Dev\": [np.std(v) for v in metrics.values()],\n    }\n    cv_results_df = pd.DataFrame(cv_results)\n\n    # Retrain on full standardized data to get final coefficients\n    X_scaled = scaler.fit_transform(X)\n    model.fit(X_scaled, y)\n\n    # Get transformed coefficients for original (unscaled) features\n    coef_info = get_coefficients_info(model, features, model.intercept_[0], scaler)\n\n    # Create DataFrame for coefficients\n    coef_data = {\n        \"Feature\": features + [\"Intercept\"],\n        \"Coefficient (Standardized)\": np.append(model.coef_[0], model.intercept_[0]),\n        \"Coefficient (Original)\": np.append(\n            list(coef_info[\"transformed_coefficients\"].values()),\n            coef_info[\"transformed_intercept\"],\n        ),\n    }\n    coef_df = pd.DataFrame(coef_data)\n\n    return cv_results_df, coef_df\n\n\n# Perform cross-validation and get the results\ncv_results_df, coefficients_df = perform_cross_validation(feature_df)\n\n# Print the results\nprint(\"--- Cross-Validation Results ---\")\nprint(cv_results_df.to_string(index=False))\nprint(\"\\n\" + \"=\" * 40 + \"\\n\")\nprint(\"--- Model Coefficients ---\")\nprint(coefficients_df.to_string(index=False))\n</pre> import pandas as pd import numpy as np from sklearn.linear_model import LogisticRegression from sklearn.model_selection import StratifiedKFold from sklearn.preprocessing import StandardScaler from sklearn.metrics import (     accuracy_score,     precision_score,     recall_score,     f1_score,     log_loss,     roc_auc_score,     average_precision_score, )   def get_coefficients_info(model, features, intercept, scaler):     \"\"\"     Returns the model coefficients as a dictionary that accounts for standardization.     The transformation allows the model to be expressed in terms of the original, unscaled features.     \"\"\"     # For standardized features, the transformation is z = (x - mean) / std.     # The original expression 'coef * z + intercept' becomes:     # (coef / std) * x + (intercept - coef * mean / std)     transformed_coefs = model.coef_[0] / scaler.scale_     transformed_intercept = intercept - np.sum(         model.coef_[0] * scaler.mean_ / scaler.scale_     )      # Create a mathematical expression for the model using original (unscaled) features     expression_parts = [f\"{transformed_intercept:.6f}\"]     for feature, coef in zip(features, transformed_coefs):         expression_parts.append(f\"{coef:+.6f}*{feature}\")     expression = \"\".join(expression_parts)      # Return a dictionary containing scaling parameters and coefficient information     return {         \"expression\": expression,         \"feature_means\": dict(zip(features, scaler.mean_)),         \"feature_stds\": dict(zip(features, scaler.scale_)),         \"original_coefficients\": dict(zip(features, model.coef_[0])),         \"original_intercept\": float(intercept),         \"transformed_coefficients\": dict(zip(features, transformed_coefs)),         \"transformed_intercept\": float(transformed_intercept),     }   def perform_cross_validation(df: pd.DataFrame):     \"\"\"     Loads data, applies standardization, and performs 5-fold stratified cross-validation.      Args:         df: A pandas DataFrame with features and a 'relevance_label' target column.      Returns:         A tuple containing two pandas DataFrames:         - cv_results_df: The mean and standard deviation of evaluation metrics.         - coef_df: The model coefficients for both scaled and unscaled features.     \"\"\"     # Define and drop irrelevant columns     columns_to_drop = [\"doc_id\", \"query_id\", \"relevance_score\"]     # Drop only the columns that exist in the DataFrame     df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])     df[\"relevance_label\"] = df[\"relevance_label\"].astype(int)      # Define features (X) and target (y)     X = df.drop(columns=[\"relevance_label\"])     features = X.columns.tolist()     y = df[\"relevance_label\"]      # Initialize StandardScaler, model, and cross-validator     scaler = StandardScaler()     N_SPLITS = 5     skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)     model = LogisticRegression(C=0.001, tol=1e-2, random_state=42)      # Lists to store metrics for each fold     metrics = {         \"Accuracy\": [],         \"Precision\": [],         \"Recall\": [],         \"F1-Score\": [],         \"Log Loss\": [],         \"ROC AUC\": [],         \"Avg Precision\": [],     }      # Perform 5-Fold Stratified Cross-Validation     for train_index, test_index in skf.split(X, y):         X_train, X_test = X.iloc[train_index], X.iloc[test_index]         y_train, y_test = y.iloc[train_index], y.iloc[test_index]          # Fit scaler on training data and transform both sets         X_train_scaled = scaler.fit_transform(X_train)         X_test_scaled = scaler.transform(X_test)          # Train the model and make predictions         model.fit(X_train_scaled, y_train)         y_pred = model.predict(X_test_scaled)         y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]          # Calculate and store metrics for the fold         metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred))         metrics[\"Precision\"].append(precision_score(y_test, y_pred, zero_division=0))         metrics[\"Recall\"].append(recall_score(y_test, y_pred, zero_division=0))         metrics[\"F1-Score\"].append(f1_score(y_test, y_pred, zero_division=0))         metrics[\"Log Loss\"].append(log_loss(y_test, y_pred_proba))         metrics[\"ROC AUC\"].append(roc_auc_score(y_test, y_pred_proba))         metrics[\"Avg Precision\"].append(average_precision_score(y_test, y_pred_proba))      # --- Prepare Results DataFrames ---      # Create DataFrame for cross-validation results     cv_results = {         \"Metric\": list(metrics.keys()),         \"Mean\": [np.mean(v) for v in metrics.values()],         \"Std Dev\": [np.std(v) for v in metrics.values()],     }     cv_results_df = pd.DataFrame(cv_results)      # Retrain on full standardized data to get final coefficients     X_scaled = scaler.fit_transform(X)     model.fit(X_scaled, y)      # Get transformed coefficients for original (unscaled) features     coef_info = get_coefficients_info(model, features, model.intercept_[0], scaler)      # Create DataFrame for coefficients     coef_data = {         \"Feature\": features + [\"Intercept\"],         \"Coefficient (Standardized)\": np.append(model.coef_[0], model.intercept_[0]),         \"Coefficient (Original)\": np.append(             list(coef_info[\"transformed_coefficients\"].values()),             coef_info[\"transformed_intercept\"],         ),     }     coef_df = pd.DataFrame(coef_data)      return cv_results_df, coef_df   # Perform cross-validation and get the results cv_results_df, coefficients_df = perform_cross_validation(feature_df)  # Print the results print(\"--- Cross-Validation Results ---\") print(cv_results_df.to_string(index=False)) print(\"\\n\" + \"=\" * 40 + \"\\n\") print(\"--- Model Coefficients ---\") print(coefficients_df.to_string(index=False)) <pre>--- Cross-Validation Results ---\n       Metric     Mean  Std Dev\n     Accuracy 0.659524 0.115234\n    Precision 0.623102 0.085545\n       Recall 1.000000 0.000000\n     F1-Score 0.764337 0.065585\n     Log Loss 0.639436 0.014668\n      ROC AUC 0.974949 0.019901\nAvg Precision 0.979207 0.018465\n\n========================================\n\n--- Model Coefficients ---\n                          Feature  Coefficient (Standardized)  Coefficient (Original)\n match_avg_top_3_chunk_sim_scores                    0.034383                0.421609\nmatch_avg_top_3_chunk_text_scores                    0.031768                0.006793\n               match_bm25(chunks)                    0.031909                0.004862\n                match_bm25(title)                    0.021095                0.008671\n       match_max_chunk_sim_scores                    0.034131                0.352846\n      match_max_chunk_text_scores                    0.032141                0.005228\n                        Intercept                    0.158401               -0.143366\n</pre> In\u00a0[36]: Copied! <pre>coefficients_df\n</pre> coefficients_df Out[36]: Feature Coefficient (Standardized) Coefficient (Original) 0 match_avg_top_3_chunk_sim_scores 0.034383 0.421609 1 match_avg_top_3_chunk_text_scores 0.031768 0.006793 2 match_bm25(chunks) 0.031909 0.004862 3 match_bm25(title) 0.021095 0.008671 4 match_max_chunk_sim_scores 0.034131 0.352846 5 match_max_chunk_text_scores 0.032141 0.005228 6 Intercept 0.158401 -0.143366 <p>Which seems quite good. With such a small dataset however, it is easy to overfit. Let us evaluate on the unseen test queries to see how well the model generalizes.</p> <p>First, we need to add the learned coefficients as inputs to a new rank profile in our schema, so that we can use them in Vespa.</p> In\u00a0[37]: Copied! <pre>learned_linear_rp = (\n    repo_root / \"app\" / \"schemas\" / \"doc\" / \"learned-linear.profile\"\n).read_text()\ndisplay_md(learned_linear_rp, tag=\"txt\")\n</pre> learned_linear_rp = (     repo_root / \"app\" / \"schemas\" / \"doc\" / \"learned-linear.profile\" ).read_text() display_md(learned_linear_rp, tag=\"txt\") <pre><code>txt\nrank-profile learned-linear inherits base-features {\n        match-features: \n        inputs {\n            query(embedding) tensor&lt;int8&gt;(x[96])\n            query(float_embedding) tensor&lt;float&gt;(x[768])\n            query(intercept) double\n            query(avg_top_3_chunk_sim_scores_param) double\n            query(avg_top_3_chunk_text_scores_param) double\n            query(bm25_chunks_param) double\n            query(bm25_title_param) double\n            query(max_chunk_sim_scores_param) double\n            query(max_chunk_text_scores_param) double\n        }\n        first-phase {\n            expression {\n                query(intercept) + \n                query(avg_top_3_chunk_sim_scores_param) * avg_top_3_chunk_sim_scores() +\n                query(avg_top_3_chunk_text_scores_param) * avg_top_3_chunk_text_scores() +\n                query(bm25_title_param) * bm25(title) + \n                query(bm25_chunks_param) * bm25(chunks) +\n                query(max_chunk_sim_scores_param) * max_chunk_sim_scores() +\n                query(max_chunk_text_scores_param) * max_chunk_text_scores()\n            }\n        }\n        summary-features {\n            top_3_chunk_sim_scores\n        }\n        \n    }\n</code></pre> <p>To allow for changing the parameters without redeploying the application, we will also add the values of the coefficients as query parameters to a new query profile.</p> In\u00a0[38]: Copied! <pre>display_md(hybrid_qp, tag=\"xml\")\n</pre> display_md(hybrid_qp, tag=\"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;!--\nmatch_avg_top_3_chunk_sim_scores   : 13.383840\nmatch_avg_top_3_chunk_text_scores  : 0.203145\nmatch_bm25(chunks)                 : 0.159914\nmatch_bm25(title)                  : 0.191867\nmatch_max_chunk_sim_scores         : 10.067169\nmatch_max_chunk_text_scores        : 0.153392\nIntercept                          : -7.798639\n--&gt;\n&lt;query-profile id=\"hybrid\"&gt;\n    &lt;field name=\"schema\"&gt;doc&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(embedding)\"&gt;embed(@query)&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(float_embedding)\"&gt;embed(@query)&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(intercept)\"&gt;-7.798639&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(avg_top_3_chunk_sim_scores_param)\"&gt;13.383840&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(avg_top_3_chunk_text_scores_param)\"&gt;0.203145&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(bm25_chunks_param)\"&gt;0.159914&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(bm25_title_param)\"&gt;0.191867&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(max_chunk_sim_scores_param)\"&gt;10.067169&lt;/field&gt;\n    &lt;field name=\"ranking.features.query(max_chunk_text_scores_param)\"&gt;0.153392&lt;/field&gt;\n    &lt;field name=\"yql\"&gt;\n        select *\n        from %{schema}\n        where userInput(@query) or\n        ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n        ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))\n    &lt;/field&gt;\n    &lt;field name=\"hits\"&gt;10&lt;/field&gt;\n    &lt;field name=\"ranking.profile\"&gt;learned-linear&lt;/field&gt;\n    &lt;field name=\"presentation.summary\"&gt;top_3_chunks&lt;/field&gt;\n&lt;/query-profile&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre>test_queries_file = repo_root / \"queries\" / \"test_queries.json\"\n\nwith open(test_queries_file) as f:\n    test_queries = json.load(f)\n\ntest_ids_to_query = {query[\"query_id\"]: query[\"query_text\"] for query in test_queries}\ntest_relevant_docs = {\n    query[\"query_id\"]: set(query[\"relevant_document_ids\"])\n    for query in test_queries\n    if \"relevant_document_ids\" in query\n}\n</pre> test_queries_file = repo_root / \"queries\" / \"test_queries.json\"  with open(test_queries_file) as f:     test_queries = json.load(f)  test_ids_to_query = {query[\"query_id\"]: query[\"query_text\"] for query in test_queries} test_relevant_docs = {     query[\"query_id\"]: set(query[\"relevant_document_ids\"])     for query in test_queries     if \"relevant_document_ids\" in query } <p>We need to parse the coefficients into the required format for input.</p> In\u00a0[40]: Copied! <pre>coefficients_df\n</pre> coefficients_df Out[40]: Feature Coefficient (Standardized) Coefficient (Original) 0 match_avg_top_3_chunk_sim_scores 0.034383 0.421609 1 match_avg_top_3_chunk_text_scores 0.031768 0.006793 2 match_bm25(chunks) 0.031909 0.004862 3 match_bm25(title) 0.021095 0.008671 4 match_max_chunk_sim_scores 0.034131 0.352846 5 match_max_chunk_text_scores 0.032141 0.005228 6 Intercept 0.158401 -0.143366 In\u00a0[41]: Copied! <pre>coef_dict = coefficients_df.to_dict()\ncoef_dict\n</pre> coef_dict = coefficients_df.to_dict() coef_dict Out[41]: <pre>{'Feature': {0: 'match_avg_top_3_chunk_sim_scores',\n  1: 'match_avg_top_3_chunk_text_scores',\n  2: 'match_bm25(chunks)',\n  3: 'match_bm25(title)',\n  4: 'match_max_chunk_sim_scores',\n  5: 'match_max_chunk_text_scores',\n  6: 'Intercept'},\n 'Coefficient (Standardized)': {0: 0.03438259396169029,\n  1: 0.031767760839597856,\n  2: 0.03190853104175455,\n  3: 0.021094809721098663,\n  4: 0.03413143203194206,\n  5: 0.0321408033796812,\n  6: 0.1584007329169953},\n 'Coefficient (Original)': {0: 0.421609061801165,\n  1: 0.0067931485936015825,\n  2: 0.004861617295220699,\n  3: 0.008671224628375315,\n  4: 0.3528463496849927,\n  5: 0.005227988942349101,\n  6: -0.14336597939520906}}</pre> In\u00a0[42]: Copied! <pre>def format_key(feature):\n    \"\"\"Formats the feature string into the desired key format.\"\"\"\n    if feature == \"Intercept\":\n        return \"input.query(intercept)\"\n    name = feature.removeprefix(\"match_\").replace(\"(\", \"_\").replace(\")\", \"\")\n    return f\"input.query({name}_param)\"\n\n\nlinear_params = {\n    format_key(feature): coef_dict[\"Coefficient (Original)\"][i]\n    for i, feature in enumerate(coef_dict[\"Feature\"].values())\n}\nlinear_params\n</pre> def format_key(feature):     \"\"\"Formats the feature string into the desired key format.\"\"\"     if feature == \"Intercept\":         return \"input.query(intercept)\"     name = feature.removeprefix(\"match_\").replace(\"(\", \"_\").replace(\")\", \"\")     return f\"input.query({name}_param)\"   linear_params = {     format_key(feature): coef_dict[\"Coefficient (Original)\"][i]     for i, feature in enumerate(coef_dict[\"Feature\"].values()) } linear_params Out[42]: <pre>{'input.query(avg_top_3_chunk_sim_scores_param)': 0.421609061801165,\n 'input.query(avg_top_3_chunk_text_scores_param)': 0.0067931485936015825,\n 'input.query(bm25_chunks_param)': 0.004861617295220699,\n 'input.query(bm25_title_param)': 0.008671224628375315,\n 'input.query(max_chunk_sim_scores_param)': 0.3528463496849927,\n 'input.query(max_chunk_text_scores_param)': 0.005227988942349101,\n 'input.query(intercept)': -0.14336597939520906}</pre> <p>We run the evaluation script on a set of unseen test queries, and get the following output:</p> In\u00a0[43]: Copied! <pre># Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\nfrom vespa.evaluation import VespaEvaluator\nfrom vespa.application import Vespa\nimport json\nfrom pathlib import Path\n\n\ndef rank_first_phase_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(VESPA_SCHEMA_NAME)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"title_embedding\",\n                    query_vector=\"embedding\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.nearestNeighbor(\n                    field=\"chunk_embeddings\",\n                    query_vector=\"embedding\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.userQuery(\n                    query_text,\n                )\n            )\n        ),\n        \"hits\": top_k,\n        \"query\": query_text,\n        \"ranking.profile\": \"learned-linear\",\n        \"input.query(embedding)\": f\"embed({query_text})\",\n        \"input.query(float_embedding)\": f\"embed({query_text})\",\n        \"presentation.summary\": \"no-chunks\",\n    } | linear_params\n\n\nfirst_phase_evaluator = VespaEvaluator(\n    queries=test_ids_to_query,\n    relevant_docs=test_relevant_docs,\n    vespa_query_fn=rank_first_phase_query_fn,\n    id_field=\"id\",\n    app=app,\n    name=\"first-phase-evaluation\",\n    write_csv=False,\n    precision_recall_at_k=[10, 20],\n)\n\nfirst_phase_results = first_phase_evaluator()\n</pre> # Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root. from vespa.evaluation import VespaEvaluator from vespa.application import Vespa import json from pathlib import Path   def rank_first_phase_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(VESPA_SCHEMA_NAME)             .where(                 qb.nearestNeighbor(                     field=\"title_embedding\",                     query_vector=\"embedding\",                     annotations={\"targetHits\": 100},                 )                 | qb.nearestNeighbor(                     field=\"chunk_embeddings\",                     query_vector=\"embedding\",                     annotations={\"targetHits\": 100},                 )                 | qb.userQuery(                     query_text,                 )             )         ),         \"hits\": top_k,         \"query\": query_text,         \"ranking.profile\": \"learned-linear\",         \"input.query(embedding)\": f\"embed({query_text})\",         \"input.query(float_embedding)\": f\"embed({query_text})\",         \"presentation.summary\": \"no-chunks\",     } | linear_params   first_phase_evaluator = VespaEvaluator(     queries=test_ids_to_query,     relevant_docs=test_relevant_docs,     vespa_query_fn=rank_first_phase_query_fn,     id_field=\"id\",     app=app,     name=\"first-phase-evaluation\",     write_csv=False,     precision_recall_at_k=[10, 20], )  first_phase_results = first_phase_evaluator() In\u00a0[44]: Copied! <pre>first_phase_results\n</pre> first_phase_results Out[44]: <pre>{'accuracy@1': 1.0,\n 'accuracy@3': 1.0,\n 'accuracy@5': 1.0,\n 'accuracy@10': 1.0,\n 'precision@10': 0.23500000000000001,\n 'recall@10': 0.9405303030303032,\n 'precision@20': 0.1275,\n 'recall@20': 0.990909090909091,\n 'mrr@10': 1.0,\n 'ndcg@10': 0.8893451868887793,\n 'map@100': 0.8183245416199961,\n 'searchtime_avg': 0.04085000000000001,\n 'searchtime_q50': 0.0425,\n 'searchtime_q90': 0.06040000000000004,\n 'searchtime_q95': 0.08305000000000001}</pre> In\u00a0[45]: Copied! <pre>first_phase_df = pd.DataFrame(first_phase_results, index=[\"value\"]).T\nfirst_phase_df\n</pre> first_phase_df = pd.DataFrame(first_phase_results, index=[\"value\"]).T first_phase_df Out[45]: value accuracy@1 1.000000 accuracy@3 1.000000 accuracy@5 1.000000 accuracy@10 1.000000 precision@10 0.235000 recall@10 0.940530 precision@20 0.127500 recall@20 0.990909 mrr@10 1.000000 ndcg@10 0.889345 map@100 0.818325 searchtime_avg 0.040850 searchtime_q50 0.042500 searchtime_q90 0.060400 searchtime_q95 0.083050 <p>For the first phase ranking, we care most about recall, as we just want to make sure that the candidate documents are ranked high enough to be included in the second-phase ranking. (the default number of documents that will be exposed to second-phase is 10 000, but can be controlled by the <code>rerank-count</code> parameter).</p> <p>We can see that our results are already very good. This is of course due to the fact that we have a small,synthetic dataset. In reality, you should align the metric expectations with your dataset and test queries.</p> <p>We can also see that our search time is quite fast, with an average of 22ms. You should consider whether this is well within your latency budget, as you want some headroom for second-phase ranking.</p> In\u00a0[46]: Copied! <pre>second_phase_collector = VespaFeatureCollector(\n    queries=ids_to_query,\n    relevant_docs=relevant_docs,\n    vespa_query_fn=feature_collection_second_phase_query_fn,\n    app=app,\n    name=\"second-phase\",\n    id_field=\"id\",\n    collect_matchfeatures=True,\n    collect_summaryfeatures=False,\n    collect_rankfeatures=True,\n    write_csv=False,\n    random_hits_strategy=\"ratio\",\n    random_hits_value=1,\n)\nsecond_phase_features = second_phase_collector.collect()\n</pre> second_phase_collector = VespaFeatureCollector(     queries=ids_to_query,     relevant_docs=relevant_docs,     vespa_query_fn=feature_collection_second_phase_query_fn,     app=app,     name=\"second-phase\",     id_field=\"id\",     collect_matchfeatures=True,     collect_summaryfeatures=False,     collect_rankfeatures=True,     write_csv=False,     random_hits_strategy=\"ratio\",     random_hits_value=1, ) second_phase_features = second_phase_collector.collect() In\u00a0[47]: Copied! <pre>second_phase_df = pd.DataFrame(second_phase_features[\"results\"])\nsecond_phase_df\n</pre> second_phase_df = pd.DataFrame(second_phase_features[\"results\"]) second_phase_df Out[47]: query_id doc_id relevance_label relevance_score match_avg_top_3_chunk_sim_scores match_avg_top_3_chunk_text_scores match_bm25(chunks) match_bm25(title) match_is_favorite match_max_chunk_sim_scores ... rank_term(3).significance rank_term(3).weight rank_term(4).connectedness rank_term(4).significance rank_term(4).weight rank_textSimilarity(title).fieldCoverage rank_textSimilarity(title).order rank_textSimilarity(title).proximity rank_textSimilarity(title).queryCoverage rank_textSimilarity(title).score 0 alex_q_01 1 1.0 0.928815 0.358027 15.100841 23.010389 4.333828 1.0 0.391143 ... 0.524369 100.0 0.1 0.560104 100.0 0.400000 1.0 1.00 0.133333 0.620000 1 alex_q_01 50 1.0 0.791824 0.248329 8.444725 7.717984 0.000000 0.0 0.268457 ... 0.524369 100.0 0.1 0.560104 100.0 0.000000 0.0 0.00 0.000000 0.000000 2 alex_q_01 82 1.0 0.271836 0.225300 12.327676 18.611592 2.453409 1.0 0.258905 ... 0.524369 100.0 0.1 0.560104 100.0 0.200000 0.0 0.75 0.066667 0.322500 3 alex_q_01 34 0.0 0.982272 0.231970 5.111429 7.128779 0.000000 0.0 0.257180 ... 0.524369 100.0 0.1 0.560104 100.0 0.000000 0.0 0.00 0.000000 0.000000 4 alex_q_01 24 0.0 0.975659 0.201503 2.404518 2.680087 0.000000 1.0 0.201503 ... 0.524369 100.0 0.1 0.560104 100.0 0.000000 0.0 0.00 0.000000 0.000000 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 97 alex_q_19 58 0.0 0.990156 0.136911 2.231116 2.606189 0.000000 0.0 0.136911 ... 0.548752 100.0 0.1 0.558248 100.0 0.000000 0.0 0.00 0.000000 0.000000 98 alex_q_20 20 1.0 0.618527 0.337411 8.959117 12.534452 9.865092 0.0 0.402615 ... 0.558248 100.0 0.1 0.524369 100.0 0.833333 1.0 1.00 0.555556 0.833333 99 alex_q_20 35 1.0 0.617958 0.227978 8.462585 13.478890 0.000000 0.0 0.239757 ... 0.558248 100.0 0.1 0.524369 100.0 0.000000 0.0 0.00 0.000000 0.000000 100 alex_q_20 63 0.0 0.979987 0.182378 3.131521 5.032468 0.000000 1.0 0.183292 ... 0.558248 100.0 0.1 0.524369 100.0 0.000000 0.0 0.00 0.000000 0.000000 101 alex_q_20 32 0.0 0.977501 0.157868 2.246247 2.442976 1.388680 0.0 0.157868 ... 0.558248 100.0 0.1 0.524369 100.0 0.200000 0.0 0.75 0.111111 0.335833 <p>102 rows \u00d7 198 columns</p> <p>This collects 195 features (excluding ids and labels), providing a rich feature set for training more sophisticated ranking models.</p> In\u00a0[48]: Copied! <pre>import json\nimport re\nfrom typing import Dict, Any, Tuple\n\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.preprocessing import LabelEncoder\n\n\ndef strip_feature_prefix(feature_name: str) -&gt; str:\n    \"\"\"Strips 'rank_' or 'match_' prefix from a feature name.\"\"\"\n    return re.sub(r\"^(rank_|match_)\", \"\", feature_name)\n\n\ndef calculate_mean_importance(\n    importance_frames: list,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculates and returns the mean feature importance from all folds.\"\"\"\n    if not importance_frames:\n        return pd.DataFrame(columns=[\"feature\", \"gain\"])\n    imp_all = pd.concat(importance_frames, axis=0)\n    imp_mean = (\n        imp_all.groupby(\"feature\")[\"gain\"]\n        .mean()\n        .sort_values(ascending=False)\n        .reset_index()\n    )\n    return imp_mean\n\n\ndef perform_cross_validation(\n    df: pd.DataFrame, args: Dict[str, Any]\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n    \"\"\"\n    Performs stratified cross-validation with LightGBM on a DataFrame.\n\n    Args:\n        df: Input pandas DataFrame containing features and the target column.\n        args: A dictionary of parameters for the training process.\n\n    Returns:\n        A tuple containing:\n        - cv_results_df: DataFrame with the cross-validation metrics (Mean and Std Dev).\n        - feature_importance_df: DataFrame with the mean feature importance (gain).\n        - final_model_dict: The final trained LightGBM model, exported as a dictionary.\n    \"\"\"\n    # --- Parameter setup ---\n    target_col = args.get(\"target\", \"relevance_label\")\n    drop_cols = args.get(\"drop_cols\", [\"query_id\", \"doc_id\", \"relevance_score\"])\n    folds = args.get(\"folds\", 5)\n    seed = args.get(\"seed\", 42)\n    max_rounds = args.get(\"max_rounds\", 1000)\n    early_stop = args.get(\"early_stop\", 50)\n    learning_rate = args.get(\"learning_rate\", 0.05)\n\n    np.random.seed(seed)\n\n    # --- Data Cleaning ---\n    df = df.copy()\n    constant_cols = [c for c in df.columns if df[c].nunique(dropna=False) &lt;= 1]\n    cols_to_drop = [c for c in drop_cols if c in df.columns]\n    feature_cols = df.columns.difference(\n        constant_cols + cols_to_drop + [target_col]\n    ).tolist()\n\n    # Strip prefixes from feature names and rename columns\n    stripped_feature_mapping = {\n        original_col: strip_feature_prefix(original_col)\n        for original_col in feature_cols\n    }\n    df = df.rename(columns=stripped_feature_mapping)\n    feature_cols = list(stripped_feature_mapping.values())\n\n    # --- Handle Categorical Variables ---\n    cat_cols = [\n        c\n        for c in df.select_dtypes(include=[\"object\", \"category\"]).columns\n        if c in feature_cols\n    ]\n    for c in cat_cols:\n        df[c] = df[c].astype(str)\n        df[c] = LabelEncoder().fit_transform(df[c])\n    categorical_feature_idx = [feature_cols.index(c) for c in cat_cols]\n\n    # --- Prepare X and y ---\n    X = df[feature_cols]\n    y = df[target_col].astype(int)\n\n    # Store original names and rename columns for LightGBM compatibility\n    original_feature_names = X.columns.tolist()\n    X.columns = [f\"feature_{i}\" for i in range(len(X.columns))]\n    feature_name_mapping = dict(zip(X.columns, original_feature_names))\n\n    # --- Stratified K-Fold Cross-Validation ---\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n    oof_pred = np.zeros(len(df))\n    importance_frames = []\n    fold_metrics = {\"Accuracy\": [], \"ROC AUC\": []}\n    best_iterations = []\n\n    print(f\"Performing {folds}-Fold Stratified Cross-Validation...\")\n\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n\n        lgb_train = lgb.Dataset(\n            X_train, y_train, categorical_feature=categorical_feature_idx\n        )\n        lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n\n        params = dict(\n            objective=\"binary\",\n            metric=\"auc\",\n            seed=seed,\n            verbose=-1,\n            learning_rate=learning_rate,\n            num_leaves=10,\n            max_depth=3,\n            feature_fraction=0.8,\n            bagging_fraction=0.8,\n            bagging_freq=5,\n        )\n        callbacks = [lgb.early_stopping(early_stop, verbose=False)]\n\n        model = lgb.train(\n            params,\n            lgb_train,\n            num_boost_round=max_rounds,\n            valid_sets=[lgb_val],\n            callbacks=callbacks,\n        )\n\n        best_iterations.append(model.best_iteration)\n        val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n        oof_pred[val_idx] = val_preds\n\n        fold_metrics[\"ROC AUC\"].append(roc_auc_score(y_val, val_preds))\n        fold_metrics[\"Accuracy\"].append(\n            accuracy_score(y_val, (val_preds &gt; 0.5).astype(int))\n        )\n\n        print(\n            f\"Fold {fold}: AUC = {fold_metrics['ROC AUC'][-1]:.4f}, ACC = {fold_metrics['Accuracy'][-1]:.4f}\"\n        )\n\n        importance_frames.append(\n            pd.DataFrame(\n                {\n                    \"feature\": original_feature_names,\n                    \"gain\": model.feature_importance(importance_type=\"gain\"),\n                }\n            )\n        )\n\n    # --- Compile Results ---\n    cv_results_df = pd.DataFrame(\n        {\n            \"Metric\": list(fold_metrics.keys()),\n            \"Mean\": [np.mean(v) for v in fold_metrics.values()],\n            \"Std Dev\": [np.std(v) for v in fold_metrics.values()],\n        }\n    )\n\n    feature_importance_df = calculate_mean_importance(importance_frames)\n\n    # --- Train Final Model ---\n    final_features = feature_importance_df[feature_importance_df[\"gain\"] &gt; 0][\n        \"feature\"\n    ].tolist()\n    print(\n        f\"\\nTraining final model on {len(final_features)} features with non-zero importance.\"\n    )\n\n    # Map selected original names back to 'feature_i' names\n    final_feature_indices = [\n        key for key, val in feature_name_mapping.items() if val in final_features\n    ]\n    X_final = X[final_feature_indices]\n\n    final_categorical_idx = [\n        X_final.columns.get_loc(c)\n        for c in X_final.columns\n        if feature_name_mapping[c] in cat_cols\n    ]\n\n    full_dataset = lgb.Dataset(X_final, y, categorical_feature=final_categorical_idx)\n    final_boost_rounds = int(np.mean(best_iterations))\n\n    final_model = lgb.train(params, full_dataset, num_boost_round=final_boost_rounds)\n\n    # Export model with original feature names\n    model_json = final_model.dump_model()\n    model_json_str = json.dumps(model_json)\n    for renamed_feature, original_feature in feature_name_mapping.items():\n        model_json_str = model_json_str.replace(\n            f'\"{renamed_feature}\"', f'\"{original_feature}\"'\n        )\n    final_model_dict = json.loads(model_json_str)\n\n    print(\"Training completed successfully!\")\n    return cv_results_df, feature_importance_df, final_model_dict\n\n\n# 2. Define arguments as a dictionary\ntraining_args = {\n    \"target\": \"relevance_label\",\n    \"drop_cols\": [\"query_id\", \"doc_id\", \"relevance_score\"],\n    \"folds\": 5,\n    \"seed\": 42,\n    \"max_rounds\": 500,\n    \"early_stop\": 25,\n    \"learning_rate\": 0.05,\n}\n\n# 3. Run the cross-validation and get the results\ncv_results, feature_importance, final_model = perform_cross_validation(\n    df=second_phase_df, args=training_args\n)\n</pre> import json import re from typing import Dict, Any, Tuple  import pandas as pd import lightgbm as lgb from sklearn.preprocessing import LabelEncoder   def strip_feature_prefix(feature_name: str) -&gt; str:     \"\"\"Strips 'rank_' or 'match_' prefix from a feature name.\"\"\"     return re.sub(r\"^(rank_|match_)\", \"\", feature_name)   def calculate_mean_importance(     importance_frames: list, ) -&gt; pd.DataFrame:     \"\"\"Calculates and returns the mean feature importance from all folds.\"\"\"     if not importance_frames:         return pd.DataFrame(columns=[\"feature\", \"gain\"])     imp_all = pd.concat(importance_frames, axis=0)     imp_mean = (         imp_all.groupby(\"feature\")[\"gain\"]         .mean()         .sort_values(ascending=False)         .reset_index()     )     return imp_mean   def perform_cross_validation(     df: pd.DataFrame, args: Dict[str, Any] ) -&gt; Tuple[pd.DataFrame, pd.DataFrame, Dict]:     \"\"\"     Performs stratified cross-validation with LightGBM on a DataFrame.      Args:         df: Input pandas DataFrame containing features and the target column.         args: A dictionary of parameters for the training process.      Returns:         A tuple containing:         - cv_results_df: DataFrame with the cross-validation metrics (Mean and Std Dev).         - feature_importance_df: DataFrame with the mean feature importance (gain).         - final_model_dict: The final trained LightGBM model, exported as a dictionary.     \"\"\"     # --- Parameter setup ---     target_col = args.get(\"target\", \"relevance_label\")     drop_cols = args.get(\"drop_cols\", [\"query_id\", \"doc_id\", \"relevance_score\"])     folds = args.get(\"folds\", 5)     seed = args.get(\"seed\", 42)     max_rounds = args.get(\"max_rounds\", 1000)     early_stop = args.get(\"early_stop\", 50)     learning_rate = args.get(\"learning_rate\", 0.05)      np.random.seed(seed)      # --- Data Cleaning ---     df = df.copy()     constant_cols = [c for c in df.columns if df[c].nunique(dropna=False) &lt;= 1]     cols_to_drop = [c for c in drop_cols if c in df.columns]     feature_cols = df.columns.difference(         constant_cols + cols_to_drop + [target_col]     ).tolist()      # Strip prefixes from feature names and rename columns     stripped_feature_mapping = {         original_col: strip_feature_prefix(original_col)         for original_col in feature_cols     }     df = df.rename(columns=stripped_feature_mapping)     feature_cols = list(stripped_feature_mapping.values())      # --- Handle Categorical Variables ---     cat_cols = [         c         for c in df.select_dtypes(include=[\"object\", \"category\"]).columns         if c in feature_cols     ]     for c in cat_cols:         df[c] = df[c].astype(str)         df[c] = LabelEncoder().fit_transform(df[c])     categorical_feature_idx = [feature_cols.index(c) for c in cat_cols]      # --- Prepare X and y ---     X = df[feature_cols]     y = df[target_col].astype(int)      # Store original names and rename columns for LightGBM compatibility     original_feature_names = X.columns.tolist()     X.columns = [f\"feature_{i}\" for i in range(len(X.columns))]     feature_name_mapping = dict(zip(X.columns, original_feature_names))      # --- Stratified K-Fold Cross-Validation ---     skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)     oof_pred = np.zeros(len(df))     importance_frames = []     fold_metrics = {\"Accuracy\": [], \"ROC AUC\": []}     best_iterations = []      print(f\"Performing {folds}-Fold Stratified Cross-Validation...\")      for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):         X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]         X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]          lgb_train = lgb.Dataset(             X_train, y_train, categorical_feature=categorical_feature_idx         )         lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)          params = dict(             objective=\"binary\",             metric=\"auc\",             seed=seed,             verbose=-1,             learning_rate=learning_rate,             num_leaves=10,             max_depth=3,             feature_fraction=0.8,             bagging_fraction=0.8,             bagging_freq=5,         )         callbacks = [lgb.early_stopping(early_stop, verbose=False)]          model = lgb.train(             params,             lgb_train,             num_boost_round=max_rounds,             valid_sets=[lgb_val],             callbacks=callbacks,         )          best_iterations.append(model.best_iteration)         val_preds = model.predict(X_val, num_iteration=model.best_iteration)         oof_pred[val_idx] = val_preds          fold_metrics[\"ROC AUC\"].append(roc_auc_score(y_val, val_preds))         fold_metrics[\"Accuracy\"].append(             accuracy_score(y_val, (val_preds &gt; 0.5).astype(int))         )          print(             f\"Fold {fold}: AUC = {fold_metrics['ROC AUC'][-1]:.4f}, ACC = {fold_metrics['Accuracy'][-1]:.4f}\"         )          importance_frames.append(             pd.DataFrame(                 {                     \"feature\": original_feature_names,                     \"gain\": model.feature_importance(importance_type=\"gain\"),                 }             )         )      # --- Compile Results ---     cv_results_df = pd.DataFrame(         {             \"Metric\": list(fold_metrics.keys()),             \"Mean\": [np.mean(v) for v in fold_metrics.values()],             \"Std Dev\": [np.std(v) for v in fold_metrics.values()],         }     )      feature_importance_df = calculate_mean_importance(importance_frames)      # --- Train Final Model ---     final_features = feature_importance_df[feature_importance_df[\"gain\"] &gt; 0][         \"feature\"     ].tolist()     print(         f\"\\nTraining final model on {len(final_features)} features with non-zero importance.\"     )      # Map selected original names back to 'feature_i' names     final_feature_indices = [         key for key, val in feature_name_mapping.items() if val in final_features     ]     X_final = X[final_feature_indices]      final_categorical_idx = [         X_final.columns.get_loc(c)         for c in X_final.columns         if feature_name_mapping[c] in cat_cols     ]      full_dataset = lgb.Dataset(X_final, y, categorical_feature=final_categorical_idx)     final_boost_rounds = int(np.mean(best_iterations))      final_model = lgb.train(params, full_dataset, num_boost_round=final_boost_rounds)      # Export model with original feature names     model_json = final_model.dump_model()     model_json_str = json.dumps(model_json)     for renamed_feature, original_feature in feature_name_mapping.items():         model_json_str = model_json_str.replace(             f'\"{renamed_feature}\"', f'\"{original_feature}\"'         )     final_model_dict = json.loads(model_json_str)      print(\"Training completed successfully!\")     return cv_results_df, feature_importance_df, final_model_dict   # 2. Define arguments as a dictionary training_args = {     \"target\": \"relevance_label\",     \"drop_cols\": [\"query_id\", \"doc_id\", \"relevance_score\"],     \"folds\": 5,     \"seed\": 42,     \"max_rounds\": 500,     \"early_stop\": 25,     \"learning_rate\": 0.05, }  # 3. Run the cross-validation and get the results cv_results, feature_importance, final_model = perform_cross_validation(     df=second_phase_df, args=training_args ) <pre>Performing 5-Fold Stratified Cross-Validation...\nFold 1: AUC = 0.9727, ACC = 0.8095\nFold 2: AUC = 0.9636, ACC = 0.8571\nFold 3: AUC = 0.9798, ACC = 0.9000\nFold 4: AUC = 0.9798, ACC = 0.8500\nFold 5: AUC = 1.0000, ACC = 0.8000\n\nTraining final model on 14 features with non-zero importance.\nTraining completed successfully!\n</pre> In\u00a0[49]: Copied! <pre>cv_results\n</pre> cv_results Out[49]: Metric Mean Std Dev 0 Accuracy 0.843333 0.035964 1 ROC AUC 0.979192 0.011979 In\u00a0[50]: Copied! <pre>feature_importance[:15]\n</pre> feature_importance[:15] Out[50]: feature gain 0 nativeProximity 183.686466 1 firstPhase 131.138263 2 avg_top_3_chunk_sim_scores 58.646572 3 max_chunk_sim_scores 40.141040 4 elementCompleteness(chunks).queryCompleteness 37.331087 5 nativeRank 13.850518 6 avg_top_3_chunk_text_scores 1.838134 7 bm25(chunks) 0.463590 8 modified_freshness 0.386416 9 fieldMatch(title).absoluteProximity 0.374392 10 fieldMatch(title).orderness 0.363286 11 elementSimilarity(chunks) 0.214760 12 max_chunk_text_scores 0.183127 13 nativeFieldMatch 0.119759 14 fieldTermMatch(title,3).weight 0.000000 In\u00a0[51]: Copied! <pre>final_model\n</pre> final_model Out[51]: <pre>{'name': 'tree',\n 'version': 'v4',\n 'num_class': 1,\n 'num_tree_per_iteration': 1,\n 'label_index': 0,\n 'max_feature_idx': 16,\n 'objective': 'binary sigmoid:1',\n 'average_output': False,\n 'feature_names': ['avg_top_3_chunk_sim_scores',\n  'avg_top_3_chunk_text_scores',\n  'bm25(chunks)',\n  'bm25(chunks)',\n  'max_chunk_sim_scores',\n  'max_chunk_text_scores',\n  'modified_freshness',\n  'bm25(chunks)',\n  'bm25(chunks)',\n  'elementCompleteness(chunks).queryCompleteness',\n  'elementSimilarity(chunks)',\n  'fieldMatch(title).absoluteProximity',\n  'fieldMatch(title).orderness',\n  'firstPhase',\n  'nativeFieldMatch',\n  'nativeProximity',\n  'nativeRank'],\n 'monotone_constraints': [],\n 'feature_infos': {'avg_top_3_chunk_sim_scores': {'min_value': 0.08106629550457,\n   'max_value': 0.4134707450866699,\n   'values': []},\n  'avg_top_3_chunk_text_scores': {'min_value': 0,\n   'max_value': 20.105823516845703,\n   'values': []},\n  'bm25(chunks)': {'min_value': 0,\n   'max_value': 25.04552896302937,\n   'values': []},\n  'max_chunk_sim_scores': {'min_value': 0.08106629550457,\n   'max_value': 0.4462931454181671,\n   'values': []},\n  'max_chunk_text_scores': {'min_value': 0,\n   'max_value': 21.62700843811035,\n   'values': []},\n  'modified_freshness': {'min_value': 0,\n   'max_value': 0.5671891292958484,\n   'values': []},\n  'elementCompleteness(chunks).queryCompleteness': {'min_value': 0,\n   'max_value': 0.7777777777777778,\n   'values': []},\n  'elementSimilarity(chunks)': {'min_value': 0,\n   'max_value': 0.7162878787878787,\n   'values': []},\n  'fieldMatch(title).absoluteProximity': {'min_value': 0,\n   'max_value': 0.10000000149011612,\n   'values': []},\n  'fieldMatch(title).orderness': {'min_value': 0,\n   'max_value': 1,\n   'values': []},\n  'firstPhase': {'min_value': -5.438998465840945,\n   'max_value': 14.07283096376979,\n   'values': []},\n  'nativeFieldMatch': {'min_value': 0,\n   'max_value': 0.3354072940571937,\n   'values': []},\n  'nativeProximity': {'min_value': 0,\n   'max_value': 0.1963793884211417,\n   'values': []},\n  'nativeRank': {'min_value': 0.0017429193899782137,\n   'max_value': 0.17263275990663562,\n   'values': []}},\n 'tree_info': [{'tree_index': 0,\n   'num_leaves': 2,\n   'num_cat': 0,\n   'shrinkage': 1,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 50.4098014831543,\n    'threshold': 0.02084435169178268,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.165181,\n    'internal_weight': 18.8831,\n    'internal_count': 76,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': 0.08130811914532406,\n     'leaf_weight': 9.193098649382593,\n     'leaf_count': 37},\n    'right_child': {'leaf_index': 1,\n     'leaf_value': 0.24475291179584288,\n     'leaf_weight': 9.690022900700567,\n     'leaf_count': 39}}},\n  {'tree_index': 1,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 0,\n    'split_gain': 44.23429870605469,\n    'threshold': 0.18672376126050952,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.00762683,\n    'internal_weight': 18.8402,\n    'internal_count': 76,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.10463142349527131,\n     'leaf_weight': 5.986800223588946,\n     'leaf_count': 24},\n    'right_child': {'split_index': 1,\n     'split_feature': 9,\n     'split_gain': 7.076389789581299,\n     'threshold': 0.44949494949494956,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0599142,\n     'internal_weight': 12.8534,\n     'internal_count': 52,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.013179562064110115,\n      'leaf_weight': 4.968685954809187,\n      'leaf_count': 20},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.08936491628319639,\n      'leaf_weight': 7.884672373533249,\n      'leaf_count': 32}}}},\n  {'tree_index': 2,\n   'num_leaves': 2,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 42.20650100708008,\n    'threshold': 0.02084435169178268,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.00729477,\n    'internal_weight': 18.7478,\n    'internal_count': 76,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.06880462126513588,\n     'leaf_weight': 9.240163266658785,\n     'leaf_count': 37},\n    'right_child': {'leaf_index': 1,\n     'leaf_value': 0.08125312744778718,\n     'leaf_weight': 9.507659405469893,\n     'leaf_count': 39}}},\n  {'tree_index': 3,\n   'num_leaves': 2,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 38.436100006103516,\n    'threshold': 0.02084435169178268,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.00699584,\n    'internal_weight': 18.6093,\n    'internal_count': 76,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.06538935309867093,\n     'leaf_weight': 9.236633136868479,\n     'leaf_count': 37},\n    'right_child': {'leaf_index': 1,\n     'leaf_value': 0.07833036395826393,\n     'leaf_weight': 9.372678577899931,\n     'leaf_count': 39}}},\n  {'tree_index': 4,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 0,\n    'split_gain': 35.5458984375,\n    'threshold': 0.18672376126050952,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.00672514,\n    'internal_weight': 18.4298,\n    'internal_count': 76,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.09372889424381685,\n     'leaf_weight': 5.958949193358424,\n     'leaf_count': 24},\n    'right_child': {'split_index': 1,\n     'split_feature': 9,\n     'split_gain': 5.318920135498047,\n     'threshold': 0.44949494949494956,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0547252,\n     'internal_weight': 12.4708,\n     'internal_count': 52,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.014303727398432995,\n      'leaf_weight': 4.924616768956183,\n      'leaf_count': 20},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.08110403985734628,\n      'leaf_weight': 7.546211168169975,\n      'leaf_count': 32}}}},\n  {'tree_index': 5,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 0,\n    'split_gain': 38.138301849365234,\n    'threshold': 0.18672376126050952,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.00466505,\n    'internal_weight': 17.5394,\n    'internal_count': 73,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.08973068432306786,\n     'leaf_weight': 6.64585913717747,\n     'leaf_count': 27},\n    'right_child': {'split_index': 1,\n     'split_feature': 9,\n     'split_gain': 1.3554699420928955,\n     'threshold': 0.4641025641025642,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0622534,\n     'internal_weight': 10.8935,\n     'internal_count': 46,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.04350337739463364,\n      'leaf_weight': 5.113931432366369,\n      'leaf_count': 21},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.07884389694057212,\n      'leaf_weight': 5.779602885246277,\n      'leaf_count': 25}}}},\n  {'tree_index': 6,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 0,\n    'split_gain': 34.902099609375,\n    'threshold': 0.18672376126050952,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.004498,\n    'internal_weight': 17.3039,\n    'internal_count': 73,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.08633609429142271,\n     'leaf_weight': 6.563828170299533,\n     'leaf_count': 27},\n    'right_child': {'split_index': 1,\n     'split_feature': 15,\n     'split_gain': 1.338919997215271,\n     'threshold': 0.04231842199421151,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0600115,\n     'internal_weight': 10.7401,\n     'internal_count': 46,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.04135593626110073,\n      'leaf_weight': 5.074008285999296,\n      'leaf_count': 21},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.07671780288029927,\n      'leaf_weight': 5.66606205701828,\n      'leaf_count': 25}}}},\n  {'tree_index': 7,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 0,\n    'split_gain': 32.02009963989258,\n    'threshold': 0.18672376126050952,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.00434441,\n    'internal_weight': 17.0374,\n    'internal_count': 73,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.08334419516313175,\n     'leaf_weight': 6.4620268940925625,\n     'leaf_count': 27},\n    'right_child': {'split_index': 1,\n     'split_feature': 13,\n     'split_gain': 1.350219964981079,\n     'threshold': 2.3306006116972546,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0579262,\n     'internal_weight': 10.5754,\n     'internal_count': 46,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.039874616438302576,\n      'leaf_weight': 5.23301127552986,\n      'leaf_count': 22},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.075608344236657,\n      'leaf_weight': 5.342339798808098,\n      'leaf_count': 24}}}},\n  {'tree_index': 8,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 0,\n    'split_gain': 29.436899185180664,\n    'threshold': 0.18672376126050952,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.00420139,\n    'internal_weight': 16.7481,\n    'internal_count': 73,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.08069001048178517,\n     'leaf_weight': 6.343828111886981,\n     'leaf_count': 27},\n    'right_child': {'split_index': 1,\n     'split_feature': 9,\n     'split_gain': 1.3577200174331665,\n     'threshold': 0.4641025641025642,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0559624,\n     'internal_weight': 10.4043,\n     'internal_count': 46,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.03721400081314201,\n      'leaf_weight': 5.008224830031393,\n      'leaf_count': 21},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.07336338756704952,\n      'leaf_weight': 5.396055206656456,\n      'leaf_count': 25}}}},\n  {'tree_index': 9,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 0,\n    'split_gain': 27.117399215698242,\n    'threshold': 0.18672376126050952,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': 0.00406947,\n    'internal_weight': 16.4361,\n    'internal_count': 73,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.0783218588683625,\n     'leaf_weight': 6.212180107831958,\n     'leaf_count': 27},\n    'right_child': {'split_index': 1,\n     'split_feature': 13,\n     'split_gain': 1.3397400379180908,\n     'threshold': 2.3306006116972546,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0541313,\n     'internal_weight': 10.2239,\n     'internal_count': 46,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.03614212999194114,\n      'leaf_weight': 5.143270537257193,\n      'leaf_count': 22},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.07234219952515168,\n      'leaf_weight': 5.080672308802605,\n      'leaf_count': 24}}}},\n  {'tree_index': 10,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 24.532800674438477,\n    'threshold': 0.02681743703534994,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.0040159,\n    'internal_weight': 17.9796,\n    'internal_count': 81,\n    'left_child': {'split_index': 1,\n     'split_feature': 1,\n     'split_gain': 7.316380023956299,\n     'threshold': 3.092608213424683,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': -0.0496308,\n     'internal_weight': 11.1677,\n     'internal_count': 48,\n     'left_child': {'leaf_index': 0,\n      'leaf_value': -0.0856005281817455,\n      'leaf_weight': 6.239090889692308,\n      'leaf_count': 27},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': -0.004096688964982691,\n      'leaf_weight': 4.92857152223587,\n      'leaf_count': 21}},\n    'right_child': {'leaf_index': 1,\n     'leaf_value': 0.07076665519154234,\n     'leaf_weight': 6.811910331249236,\n     'leaf_count': 33}}},\n  {'tree_index': 11,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 13,\n    'split_gain': 23.044300079345703,\n    'threshold': -0.9175117702774908,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.00387752,\n    'internal_weight': 17.6602,\n    'internal_count': 81,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.07470333072738213,\n     'leaf_weight': 6.959094658493998,\n     'leaf_count': 31},\n    'right_child': {'split_index': 1,\n     'split_feature': 13,\n     'split_gain': 3.699049949645996,\n     'threshold': 1.8772808596672073,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0421818,\n     'internal_weight': 10.7011,\n     'internal_count': 50,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.011210025880369016,\n      'leaf_weight': 5.071562081575392,\n      'leaf_count': 22},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.07008390819526038,\n      'leaf_weight': 5.629503101110458,\n      'leaf_count': 28}}}},\n  {'tree_index': 12,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 21.399799346923828,\n    'threshold': 0.02681743703534994,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.00374963,\n    'internal_weight': 17.3372,\n    'internal_count': 81,\n    'left_child': {'split_index': 1,\n     'split_feature': 2,\n     'split_gain': 5.836999893188477,\n     'threshold': 3.5472756680480115,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': -0.046492,\n     'internal_weight': 10.89,\n     'internal_count': 48,\n     'left_child': {'leaf_index': 0,\n      'leaf_value': -0.08218247103000542,\n      'leaf_weight': 5.5828584283590335,\n      'leaf_count': 25},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': -0.008947176009566292,\n      'leaf_weight': 5.307131439447403,\n      'leaf_count': 23}},\n    'right_child': {'leaf_index': 1,\n     'leaf_value': 0.06844637890571116,\n     'leaf_weight': 6.447218477725982,\n     'leaf_count': 33}}},\n  {'tree_index': 13,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 13,\n    'split_gain': 19.988399505615234,\n    'threshold': -0.9175117702774908,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.00362511,\n    'internal_weight': 17.0069,\n    'internal_count': 81,\n    'left_child': {'leaf_index': 0,\n     'leaf_value': -0.07099178346638545,\n     'leaf_weight': 6.683696135878566,\n     'leaf_count': 31},\n    'right_child': {'split_index': 1,\n     'split_feature': 13,\n     'split_gain': 3.370919942855835,\n     'threshold': 1.8772808596672073,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0399912,\n     'internal_weight': 10.3232,\n     'internal_count': 50,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.010651145320500731,\n      'leaf_weight': 5.024654343724249,\n      'leaf_count': 22},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': 0.06781494678857775,\n      'leaf_weight': 5.298499584197998,\n      'leaf_count': 28}}}},\n  {'tree_index': 14,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 18.75670051574707,\n    'threshold': 0.02681743703534994,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.00351166,\n    'internal_weight': 16.6706,\n    'internal_count': 81,\n    'left_child': {'split_index': 1,\n     'split_feature': 1,\n     'split_gain': 5.915229797363281,\n     'threshold': 3.092608213424683,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': -0.0436897,\n     'internal_weight': 10.592,\n     'internal_count': 48,\n     'left_child': {'leaf_index': 0,\n      'leaf_value': -0.07794227861102893,\n      'leaf_weight': 5.755450502038004,\n      'leaf_count': 27},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': -0.002928969366291567,\n      'leaf_weight': 4.836504548788071,\n      'leaf_count': 21}},\n    'right_child': {'leaf_index': 1,\n     'leaf_value': 0.06649753931977596,\n     'leaf_weight': 6.0786804407835,\n     'leaf_count': 33}}},\n  {'tree_index': 15,\n   'num_leaves': 3,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 9,\n    'split_gain': 19.521400451660156,\n    'threshold': 0.44949494949494956,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.00670763,\n    'internal_weight': 16.4224,\n    'internal_count': 83,\n    'left_child': {'split_index': 1,\n     'split_feature': 1,\n     'split_gain': 2.7174599170684814,\n     'threshold': 2.4830845594406132,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': -0.0505802,\n     'internal_weight': 9.96688,\n     'internal_count': 47,\n     'left_child': {'leaf_index': 0,\n      'leaf_value': -0.0748234090211124,\n      'leaf_weight': 5.352049484848978,\n      'leaf_count': 26},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': -0.022464201444285303,\n      'leaf_weight': 4.614828139543533,\n      'leaf_count': 21}},\n    'right_child': {'leaf_index': 1,\n     'leaf_value': 0.06102882167971128,\n     'leaf_weight': 6.455503240227698,\n     'leaf_count': 36}}},\n  {'tree_index': 16,\n   'num_leaves': 4,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 20.915599822998047,\n    'threshold': 0.02084435169178268,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.00650951,\n    'internal_weight': 16.0734,\n    'internal_count': 83,\n    'left_child': {'split_index': 1,\n     'split_feature': 1,\n     'split_gain': 0.7167580127716064,\n     'threshold': 2.181384921073914,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': -0.0584402,\n     'internal_weight': 8.78815,\n     'internal_count': 42,\n     'left_child': {'leaf_index': 0,\n      'leaf_value': -0.07283105883520614,\n      'leaf_weight': 4.359884411096575,\n      'leaf_count': 22},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': -0.044271557748743806,\n      'leaf_weight': 4.428262785077095,\n      'leaf_count': 20}},\n    'right_child': {'split_index': 2,\n     'split_feature': 6,\n     'split_gain': 0.27922600507736206,\n     'threshold': 0.48491415168876384,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0561343,\n     'internal_weight': 7.28523,\n     'internal_count': 41,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.046566645885268335,\n      'leaf_weight': 3.725804477930068,\n      'leaf_count': 21},\n     'right_child': {'leaf_index': 3,\n      'leaf_value': 0.06614921330100301,\n      'leaf_weight': 3.559424474835396,\n      'leaf_count': 20}}}},\n  {'tree_index': 17,\n   'num_leaves': 4,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 19.341999053955078,\n    'threshold': 0.02084435169178268,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.0063281,\n    'internal_weight': 15.7046,\n    'internal_count': 83,\n    'left_child': {'split_index': 1,\n     'split_feature': 1,\n     'split_gain': 0.7211930155754089,\n     'threshold': 2.181384921073914,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': -0.0566291,\n     'internal_weight': 8.62062,\n     'internal_count': 42,\n     'left_child': {'leaf_index': 0,\n      'leaf_value': -0.07136146887938256,\n      'leaf_weight': 4.2304699271917325,\n      'leaf_count': 22},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': -0.04243262371555604,\n      'leaf_weight': 4.390146732330322,\n      'leaf_count': 20}},\n    'right_child': {'split_index': 2,\n     'split_feature': 4,\n     'split_gain': 0.17738600075244904,\n     'threshold': 0.3187254816293717,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.054884,\n     'internal_weight': 7.08399,\n     'internal_count': 41,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.04723412069233138,\n      'leaf_weight': 3.6613249629735956,\n      'leaf_count': 20},\n     'right_child': {'leaf_index': 3,\n      'leaf_value': 0.06306728950350501,\n      'leaf_weight': 3.4226654171943665,\n      'leaf_count': 21}}}},\n  {'tree_index': 18,\n   'num_leaves': 4,\n   'num_cat': 0,\n   'shrinkage': 0.05,\n   'tree_structure': {'split_index': 0,\n    'split_feature': 15,\n    'split_gain': 17.89940071105957,\n    'threshold': 0.02084435169178268,\n    'decision_type': '&lt;=',\n    'default_left': True,\n    'missing_type': 'None',\n    'internal_value': -0.00615586,\n    'internal_weight': 15.3347,\n    'internal_count': 83,\n    'left_child': {'split_index': 1,\n     'split_feature': 9,\n     'split_gain': 0.660440981388092,\n     'threshold': 0.22649572649572652,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': -0.0549116,\n     'internal_weight': 8.45071,\n     'internal_count': 42,\n     'left_child': {'leaf_index': 0,\n      'leaf_value': -0.06930617045321565,\n      'leaf_weight': 4.101271376013754,\n      'leaf_count': 22},\n     'right_child': {'leaf_index': 2,\n      'leaf_value': -0.04133840308087882,\n      'leaf_weight': 4.349441319704056,\n      'leaf_count': 20}},\n    'right_child': {'split_index': 2,\n     'split_feature': 15,\n     'split_gain': 0.189178004860878,\n     'threshold': 0.05606487282356567,\n     'decision_type': '&lt;=',\n     'default_left': True,\n     'missing_type': 'None',\n     'internal_value': 0.0536959,\n     'internal_weight': 6.88402,\n     'internal_count': 41,\n     'left_child': {'leaf_index': 1,\n      'leaf_value': 0.04578324148730414,\n      'leaf_weight': 3.6016914695501336,\n      'leaf_count': 20},\n     'right_child': {'leaf_index': 3,\n      'leaf_value': 0.062378436439081024,\n      'leaf_weight': 3.282333254814148,\n      'leaf_count': 21}}}}],\n 'feature_importances': {'avg_top_3_chunk_sim_scores': 7,\n  'avg_top_3_chunk_text_scores': 5,\n  'bm25(chunks)': 1,\n  'max_chunk_sim_scores': 1,\n  'modified_freshness': 1,\n  'elementCompleteness(chunks).queryCompleteness': 6,\n  'firstPhase': 6,\n  'nativeProximity': 11},\n 'pandas_categorical': []}</pre> In\u00a0[52]: Copied! <pre># Write the final model to a file\nmodel_file = repo_root / \"app\" / \"models\" / \"lightgbm_model.json\"\nwith open(model_file, \"w\") as f:\n    json.dump(final_model, f, indent=2)\n</pre> # Write the final model to a file model_file = repo_root / \"app\" / \"models\" / \"lightgbm_model.json\" with open(model_file, \"w\") as f:     json.dump(final_model, f, indent=2) <p>Create a new rank profile that uses this model:</p> In\u00a0[53]: Copied! <pre>second_gbdt_rp = (\n    repo_root / \"app\" / \"schemas\" / \"doc\" / \"second-with-gbdt.profile\"\n).read_text()\ndisplay_md(second_gbdt_rp, tag=\"txt\")\n</pre> second_gbdt_rp = (     repo_root / \"app\" / \"schemas\" / \"doc\" / \"second-with-gbdt.profile\" ).read_text() display_md(second_gbdt_rp, tag=\"txt\") <pre><code>txt\nrank-profile second-with-gbdt inherits collect-second-phase {\n    match-features {\n        max_chunk_sim_scores\n        max_chunk_text_scores\n        avg_top_3_chunk_text_scores\n        avg_top_3_chunk_sim_scores\n        bm25(title)\n        modified_freshness\n        open_count\n        firstPhase\n    }\n    # nativeProximity,168.84977385997772\n    # firstPhase,151.73823466300965\n    # max_chunk_sim_scores,69.43774781227111\n    # avg_top_3_chunk_text_scores,56.507930064201354\n    # avg_top_3_chunk_sim_scores,31.87002867460251\n    # nativeRank,20.071615393646063\n    # nativeFieldMatch,15.991393876075744\n    # elementSimilarity(chunks),9.700291919708253\n    # bm25(chunks),3.8777143508195877\n    # max_chunk_text_scores,3.6405647873878477\n    # \"fieldTermMatch(chunks,4).firstPosition\",1.2615019798278808\n    # \"fieldTermMatch(chunks,4).occurrences\",1.0542740106582642\n    # \"fieldTermMatch(chunks,4).weight\",0.7263560056686401\n    # term(3).significance,0.5077840089797974\n    rank-features {\n        nativeProximity\n        nativeFieldMatch\n        nativeRank\n        elementSimilarity(chunks)\n        fieldTermMatch(chunks, 4).firstPosition\n        fieldTermMatch(chunks, 4).occurrences\n        fieldTermMatch(chunks, 4).weight\n        term(3).significance\n    }\n    second-phase {\n        expression: lightgbm(\"lightgbm_model.json\")\n    }\n\n    summary-features: top_3_chunk_sim_scores\n}\n</code></pre> <p>And redeploy your application. We add a try/except block to this in case your authentication token has expired.</p> In\u00a0[54]: Copied! <pre>try:\n    app: Vespa = vespa_cloud.deploy(disk_folder=application_root)\nexcept Exception:\n    vespa_cloud = VespaCloud(\n        tenant=VESPA_TENANT_NAME,\n        application=VESPA_APPLICATION_NAME,\n        key_content=VESPA_TEAM_API_KEY,\n        application_root=application_root,\n    )\n    app: Vespa = vespa_cloud.deploy(disk_folder=application_root)\n</pre> try:     app: Vespa = vespa_cloud.deploy(disk_folder=application_root) except Exception:     vespa_cloud = VespaCloud(         tenant=VESPA_TENANT_NAME,         application=VESPA_APPLICATION_NAME,         key_content=VESPA_TEAM_API_KEY,         application_root=application_root,     )     app: Vespa = vespa_cloud.deploy(disk_folder=application_root) <pre>Deployment started in run 87 of dev-aws-us-east-1c for vespa-team.rag-blueprint. This may take a few minutes the first time.\nINFO    [09:43:43]  Deploying platform version 8.586.25 and application dev build 87 for dev-aws-us-east-1c of default ...\nINFO    [09:43:43]  Using CA signed certificate version 5\nINFO    [09:43:52]  Session 379708 for tenant 'vespa-team' prepared and activated.\nINFO    [09:43:52]  ######## Details for all nodes ########\nINFO    [09:43:52]  h125699b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:43:52]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:43:52]  --- storagenode on port 19102 has config generation 379705, wanted is 379708\nINFO    [09:43:52]  --- searchnode on port 19107 has config generation 379708, wanted is 379708\nINFO    [09:43:52]  --- distributor on port 19111 has config generation 379708, wanted is 379708\nINFO    [09:43:52]  --- metricsproxy-container on port 19092 has config generation 379708, wanted is 379708\nINFO    [09:43:52]  h125755a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:43:52]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:43:52]  --- container on port 4080 has config generation 379708, wanted is 379708\nINFO    [09:43:52]  --- metricsproxy-container on port 19092 has config generation 379708, wanted is 379708\nINFO    [09:43:52]  h97530b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:43:52]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:43:52]  --- logserver-container on port 4080 has config generation 379708, wanted is 379708\nINFO    [09:43:52]  --- metricsproxy-container on port 19092 has config generation 379708, wanted is 379708\nINFO    [09:43:52]  h119190c.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [09:43:52]  --- platform vespa/cloud-tenant-rhel8:8.586.25\nINFO    [09:43:52]  --- container-clustercontroller on port 19050 has config generation 379708, wanted is 379708\nINFO    [09:43:52]  --- metricsproxy-container on port 19092 has config generation 379708, wanted is 379708\nINFO    [09:43:59]  Found endpoints:\nINFO    [09:43:59]  - dev.aws-us-east-1c\nINFO    [09:43:59]   |-- https://fe5fe13c.fe19121d.z.vespa-app.cloud/ (cluster 'default')\nINFO    [09:43:59]  Deployment of new application revision complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for default\nURL: https://fe5fe13c.fe19121d.z.vespa-app.cloud/\nApplication is up!\n</pre> In\u00a0[55]: Copied! <pre>def rank_second_phase_query_fn(query_text: str, top_k: int) -&gt; dict:\n    return {\n        \"yql\": str(\n            qb.select(\"*\")\n            .from_(VESPA_SCHEMA_NAME)\n            .where(\n                qb.nearestNeighbor(\n                    field=\"title_embedding\",\n                    query_vector=\"embedding\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.nearestNeighbor(\n                    field=\"chunk_embeddings\",\n                    query_vector=\"embedding\",\n                    annotations={\"targetHits\": 100},\n                )\n                | qb.userQuery(\n                    query_text,\n                )\n            )\n        ),\n        \"hits\": top_k,\n        \"query\": query_text,\n        \"ranking\": \"second-with-gbdt\",\n        \"input.query(embedding)\": f\"embed({query_text})\",\n        \"input.query(float_embedding)\": f\"embed({query_text})\",\n        \"presentation.summary\": \"no-chunks\",\n    }\n\n\nsecond_phase_evaluator = VespaEvaluator(\n    queries=test_ids_to_query,\n    relevant_docs=test_relevant_docs,\n    vespa_query_fn=rank_second_phase_query_fn,\n    id_field=\"id\",\n    app=app,\n    name=\"second-phase-evaluation\",\n    write_csv=False,\n    precision_recall_at_k=[10, 20],\n)\n\nsecond_phase_results = second_phase_evaluator()\n</pre> def rank_second_phase_query_fn(query_text: str, top_k: int) -&gt; dict:     return {         \"yql\": str(             qb.select(\"*\")             .from_(VESPA_SCHEMA_NAME)             .where(                 qb.nearestNeighbor(                     field=\"title_embedding\",                     query_vector=\"embedding\",                     annotations={\"targetHits\": 100},                 )                 | qb.nearestNeighbor(                     field=\"chunk_embeddings\",                     query_vector=\"embedding\",                     annotations={\"targetHits\": 100},                 )                 | qb.userQuery(                     query_text,                 )             )         ),         \"hits\": top_k,         \"query\": query_text,         \"ranking\": \"second-with-gbdt\",         \"input.query(embedding)\": f\"embed({query_text})\",         \"input.query(float_embedding)\": f\"embed({query_text})\",         \"presentation.summary\": \"no-chunks\",     }   second_phase_evaluator = VespaEvaluator(     queries=test_ids_to_query,     relevant_docs=test_relevant_docs,     vespa_query_fn=rank_second_phase_query_fn,     id_field=\"id\",     app=app,     name=\"second-phase-evaluation\",     write_csv=False,     precision_recall_at_k=[10, 20], )  second_phase_results = second_phase_evaluator() In\u00a0[56]: Copied! <pre>second_phase_results\n</pre> second_phase_results Out[56]: <pre>{'accuracy@1': 0.75,\n 'accuracy@3': 0.95,\n 'accuracy@5': 0.95,\n 'accuracy@10': 1.0,\n 'precision@10': 0.24000000000000005,\n 'recall@10': 0.9651515151515152,\n 'precision@20': 0.12999999999999998,\n 'recall@20': 0.9954545454545455,\n 'mrr@10': 0.8404761904761905,\n 'ndcg@10': 0.8391408637111896,\n 'map@100': 0.7673197781750414,\n 'searchtime_avg': 0.03360000000000001,\n 'searchtime_q50': 0.0285,\n 'searchtime_q90': 0.05120000000000001,\n 'searchtime_q95': 0.0534}</pre> In\u00a0[57]: Copied! <pre>second_phase_df = pd.DataFrame(second_phase_results, index=[\"value\"]).T\nsecond_phase_df\n</pre> second_phase_df = pd.DataFrame(second_phase_results, index=[\"value\"]).T second_phase_df Out[57]: value accuracy@1 0.750000 accuracy@3 0.950000 accuracy@5 0.950000 accuracy@10 1.000000 precision@10 0.240000 recall@10 0.965152 precision@20 0.130000 recall@20 0.995455 mrr@10 0.840476 ndcg@10 0.839141 map@100 0.767320 searchtime_avg 0.033600 searchtime_q50 0.028500 searchtime_q90 0.051200 searchtime_q95 0.053400 <p>Expected results show significant improvement over first-phase ranking:</p> In\u00a0[58]: Copied! <pre>total_df = pd.concat(\n    [\n        first_phase_df.rename(columns={\"value\": \"first_phase\"}),\n        second_phase_df.rename(columns={\"value\": \"second_phase\"}),\n    ],\n    axis=1,\n)\n# Add diff\ntotal_df[\"diff\"] = total_df[\"second_phase\"] - total_df[\"first_phase\"]\ntotal_df = total_df.round(4)\n\n\n# highlight recall@10 row and recall@20 row\n# Define a function to apply the style\ndef highlight_rows_by_index(row, indices_to_highlight):\n    if row.name in indices_to_highlight:\n        return [\"background-color: lightblue; color: black\"] * len(row)\n    return [\"\"] * len(row)\n\n\ntotal_df.style.apply(\n    highlight_rows_by_index,\n    indices_to_highlight=[\"recall@10\", \"recall@20\"],\n    axis=1,\n)\n</pre> total_df = pd.concat(     [         first_phase_df.rename(columns={\"value\": \"first_phase\"}),         second_phase_df.rename(columns={\"value\": \"second_phase\"}),     ],     axis=1, ) # Add diff total_df[\"diff\"] = total_df[\"second_phase\"] - total_df[\"first_phase\"] total_df = total_df.round(4)   # highlight recall@10 row and recall@20 row # Define a function to apply the style def highlight_rows_by_index(row, indices_to_highlight):     if row.name in indices_to_highlight:         return [\"background-color: lightblue; color: black\"] * len(row)     return [\"\"] * len(row)   total_df.style.apply(     highlight_rows_by_index,     indices_to_highlight=[\"recall@10\", \"recall@20\"],     axis=1, ) Out[58]: first_phase second_phase diff accuracy@1 1.000000 0.750000 -0.250000 accuracy@3 1.000000 0.950000 -0.050000 accuracy@5 1.000000 0.950000 -0.050000 accuracy@10 1.000000 1.000000 0.000000 precision@10 0.235000 0.240000 0.005000 recall@10 0.940500 0.965200 0.024600 precision@20 0.127500 0.130000 0.002500 recall@20 0.990900 0.995500 0.004500 mrr@10 1.000000 0.840500 -0.159500 ndcg@10 0.889300 0.839100 -0.050200 map@100 0.818300 0.767300 -0.051000 searchtime_avg 0.040900 0.033600 -0.007200 searchtime_q50 0.042500 0.028500 -0.014000 searchtime_q90 0.060400 0.051200 -0.009200 searchtime_q95 0.083100 0.053400 -0.029700 <p>For a larger dataset, we would expect to see significant improvement over first-phase ranking. Since our first-phase ranking is already quite good, we can not see this here, but we will leave the comparison code for you to run on a real-world dataset.</p> <p>We also observe a slight increase in search time (from 22ms to 35ms average), which is expected due to the additional complexity of the GBDT model.</p> In\u00a0[59]: Copied! <pre>hybrid_with_gbdt_qp = (qp_dir / \"hybrid-with-gbdt.xml\").read_text()\ndisplay_md(hybrid_with_gbdt_qp, tag=\"xml\")\n</pre> hybrid_with_gbdt_qp = (qp_dir / \"hybrid-with-gbdt.xml\").read_text() display_md(hybrid_with_gbdt_qp, tag=\"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;query-profile id=\"hybrid-with-gbdt\" inherits=\"hybrid\"&gt;\n  &lt;field name=\"hits\"&gt;20&lt;/field&gt;\n  &lt;field name=\"ranking.profile\"&gt;second-with-gbdt&lt;/field&gt;\n  &lt;field name=\"presentation.summary\"&gt;top_3_chunks&lt;/field&gt;\n&lt;/query-profile&gt;\n</pre> In\u00a0[60]: Copied! <pre>rag_with_gbdt_qp = (qp_dir / \"rag-with-gbdt.xml\").read_text()\ndisplay_md(rag_with_gbdt_qp, tag=\"xml\")\n</pre> rag_with_gbdt_qp = (qp_dir / \"rag-with-gbdt.xml\").read_text() display_md(rag_with_gbdt_qp, tag=\"xml\") <pre>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\nproject root. --&gt;\n&lt;query-profile id=\"rag-with-gbdt\" inherits=\"hybrid-with-gbdt\"&gt;\n  &lt;field name=\"hits\"&gt;50&lt;/field&gt;\n  &lt;field name=\"searchChain\"&gt;openai&lt;/field&gt;\n  &lt;field name=\"presentation.format\"&gt;sse&lt;/field&gt;\n&lt;/query-profile&gt;\n</pre> <p>Test the improved ranking:</p> In\u00a0[61]: Copied! <pre>query = \"what are key points learned for finetuning llms?\"\nquery_profile = \"hybrid-with-gbdt\"\n\nbody = {\n    \"query\": query,\n    \"queryProfile\": query_profile,\n}\nwith app.syncio() as sess:\n    result = sess.query(body=body)\nresult.hits[0]\n</pre> query = \"what are key points learned for finetuning llms?\" query_profile = \"hybrid-with-gbdt\"  body = {     \"query\": query,     \"queryProfile\": query_profile, } with app.syncio() as sess:     result = sess.query(body=body) result.hits[0] Out[61]: <pre>{'id': 'index:content/0/a3f390d8c35680335e3aebe1',\n 'relevance': 0.8034803261636057,\n 'source': 'content',\n 'fields': {'matchfeatures': {'bm25(title)': 0.0,\n   'firstPhase': 1.9722333906160157,\n   'avg_top_3_chunk_sim_scores': 0.2565740570425987,\n   'avg_top_3_chunk_text_scores': 4.844822406768799,\n   'max_chunk_sim_scores': 0.2736895978450775,\n   'max_chunk_text_scores': 7.804652690887451,\n   'modified_freshness': 0.5275786815220422,\n   'open_count': 7.0},\n  'sddocname': 'doc',\n  'chunks_top3': [\"# Parameter-Efficient Fine-Tuning (PEFT) Techniques - Overview\\n\\n**Goal:** Fine-tune large pre-trained models with significantly fewer trainable parameters, reducing computational cost and memory footprint.\\n\\n**Key Techniques I've Researched/Used:**\\n\\n1.  **LoRA (Low-Rank Adaptation):**\\n    * Freezes pre-trained model weights.\\n    * Injects trainable rank decomposition matrices into Transformer layers.\\n    * Significantly reduces trainable parameters.\\n    * My default starting point for LLM fine-tuning (see `llm_finetuning_pitfalls_best_practices.md`).\\n\\n2.  **QLoRA:**\\n    * Builds on LoRA.\\n    * Quantizes pre-trained model to 4-bit.\\n    * Uses LoRA for fine-tuning the quantized model.\\n    * Further reduces memory usage, enabling fine-tuning of larger models on \",\n   'consumer GPUs.\\n\\n3.  **Adapter Modules:**\\n    * Inserts small, trainable neural network modules (adapters) between existing layers of the pre-trained model.\\n    * Only adapters are trained.\\n\\n4.  **Prompt Tuning / Prefix Tuning:**\\n    * Keeps model parameters frozen.\\n    * Learns a small set of continuous prompt embeddings (virtual tokens) that are prepended to the input sequence.\\n\\n**Benefits for SynapseFlow (Internal Model Dev):**\\n- Faster iteration on fine-tuning tasks.\\n- Ability to experiment with larger models on available hardware.\\n- Easier to manage multiple fine-tuned model versions (smaller delta to store).\\n\\n## &lt;MORE_TEXT:HERE&gt; (Links to papers, Hugging Face PEFT library notes)'],\n  'summaryfeatures': {'top_3_chunk_sim_scores': {'type': 'tensor&lt;float&gt;(chunk{})',\n    'cells': {'0': 0.2736895978450775, '1': 0.23945851624011993}},\n   'vespa.summaryFeatures.cached': 0.0}}}</pre> <p>Let us summarize our best practices for second-phase ranking.</p> In\u00a0[62]: Copied! <pre>from vespa.io import VespaResponse\nimport json\n\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"answer\": {\n            \"type\": \"string\",\n            \"description\": \"The answer to the query if it is contained in the documents. If not, it say that you are not allowed to answer based on the documents.\",\n        },\n        \"citations\": {\n            \"type\": \"array\",\n            \"description\": \"List of returned and cited document IDs\",\n            \"items\": {\"type\": \"string\"},\n        },\n    },\n    \"required\": [\"answer\", \"citations\"],\n    \"additionalProperties\": False,\n}\n\nquery = \"What is SynapseFlows strategy\"\nbody = {\n    \"query\": query,\n    \"queryProfile\": \"hybrid\",\n    \"searchChain\": \"openai\",\n    \"llm.json_schema\": json.dumps(schema),\n    \"presentation.format\": \"json\",\n}\n\nwith app.syncio() as sess:\n    resp = sess.query(body=body)\n\n\ndef response_to_string(response: VespaResponse):\n    \"\"\"\n    Convert a Vespa response to a string of the returned tokens.\n    \"\"\"\n    children = response.json.get(\"root\", {}).get(\"children\", [])\n    tokens = \"\"\n    for child in children:\n        if child.get(\"id\") == \"event_stream\":\n            for stream_child in child.get(\"children\", []):\n                tokens += stream_child.get(\"fields\", {}).get(\"token\", \"\")\n    return tokens\n\n\ntokens = response_to_string(resp)\njson.loads(tokens)\n</pre> from vespa.io import VespaResponse import json  schema = {     \"type\": \"object\",     \"properties\": {         \"answer\": {             \"type\": \"string\",             \"description\": \"The answer to the query if it is contained in the documents. If not, it say that you are not allowed to answer based on the documents.\",         },         \"citations\": {             \"type\": \"array\",             \"description\": \"List of returned and cited document IDs\",             \"items\": {\"type\": \"string\"},         },     },     \"required\": [\"answer\", \"citations\"],     \"additionalProperties\": False, }  query = \"What is SynapseFlows strategy\" body = {     \"query\": query,     \"queryProfile\": \"hybrid\",     \"searchChain\": \"openai\",     \"llm.json_schema\": json.dumps(schema),     \"presentation.format\": \"json\", }  with app.syncio() as sess:     resp = sess.query(body=body)   def response_to_string(response: VespaResponse):     \"\"\"     Convert a Vespa response to a string of the returned tokens.     \"\"\"     children = response.json.get(\"root\", {}).get(\"children\", [])     tokens = \"\"     for child in children:         if child.get(\"id\") == \"event_stream\":             for stream_child in child.get(\"children\", []):                 tokens += stream_child.get(\"fields\", {}).get(\"token\", \"\")     return tokens   tokens = response_to_string(resp) json.loads(tokens) Out[62]: <pre>{'answer': \"SynapseFlow's strategy focuses on simplifying the deployment, management, and scaling of machine learning models for developers and small teams. The key components of their strategy include:\\n\\n1. **Target Audience**: They target individual developers, startups, and SMEs with a particular emphasis on those new to MLOps, allowing them to leverage AI deployment without needing deep Ops knowledge.\\n\\n2. **Customer Pain Points**: SynapseFlow aims to address common challenges such as complex deployment processes, reliance on DevOps teams for model deployment, and slow, bureaucratic workflows. They provide a solution that minimizes infrastructure overhead and streamlines the journey from model experimentation to production.\\n\\n3. **Developer-First Approach**: Offering a developer-first API and intuitive UI, they ensure that users can deploy models quickly, focusing on easing the operational burden of MLOps.\\n\\n4. **Marketing and Outreach**: Their go-to-market strategy includes content marketing to educate potential users, leveraging developer communities, and building relationships through the YC network. They're also focused on SEO for high visibility within relevant search terms.\\n\\n5. **Feature Differentiators**: The platform differentiates itself through ease of deployment, a simple user interface, and a transparent pricing model tailored for startups and small businesses, making it more accessible than traditional MLOps solutions like SageMaker or Vertex AI.\\n\\n6. **Feedback and Iteration**: SynapseFlow is committed to continuous improvement based on user feedback, refining their offerings, and iteratively enhancing their product based on real-world user experiences and needs.  \\n\\n7. **Future Growth**: Plans for future growth include targeting additional user segments and functionalities, such as integrating advanced monitoring solutions and data drift detection.\\n\\nOverall, SynapseFlow's strategy is to be the go-to platform for AI deployment, with a focus on simplifying processes for those who may not have extensive technical resources, thereby enabling more teams to harness the power of AI effectively.\",\n 'citations': ['1', '4', '5', '8', '9']}</pre> <p></p> In\u00a0[63]: Copied! <pre>if os.getenv(\"CI\", \"false\") == \"true\":\n    vespa_cloud.delete()\n</pre> if os.getenv(\"CI\", \"false\") == \"true\":     vespa_cloud.delete()"},{"location":"examples/rag-blueprint-vespa-cloud.html#rag-blueprint-tutorial","title":"RAG Blueprint tutorial\u00b6","text":"<p>Many of our users use Vespa to power large scale RAG Applications.</p> <p>This blueprint aims to exemplify many of the best practices we have learned while supporting these users.</p> <p>While many RAG tutorials exist, this blueprint provides a customizable template that:</p> <ul> <li>Can (auto)scale with your data size and/or query load.</li> <li>Is fast and production grade.</li> <li>Enables you to build RAG applications with state-of-the-art quality.</li> </ul> <p>This tutorial will show how we can develop a high-quality RAG application with an evaluation-driven mindset, while being a resource you can revisit for making informed choices for your own use case.</p> <p>We will guide you through the following steps:</p> <ol> <li>Installing dependencies</li> <li>Cloning the RAG Blueprint</li> <li>Inspecting the RAG Blueprint</li> <li>Deploying to Vespa Cloud</li> <li>Our use case</li> <li>Data modeling</li> <li>Structuring your Vespa application</li> <li>Configuring match-phase (retrieval)</li> <li>First-phase ranking</li> <li>Second-phase ranking</li> <li>(Optional) Global-phase reranking</li> </ol> <p>All the accompanying code can be found in our sample app repo, but we will also clone the repo and run the code in this notebook.</p> <p>Some of the python scripts from the sample app will be adapted and shown inline in this notebook instead of running them separately.</p> <p>Each step will contain reasoning behind the choices and design of the blueprint, as well as pointers for customizing to your own application.</p> <p>This is not a 'Deploy RAG in 5 minutes' tutorial (although you can technically do that by just running the notebook). This focus is more about providing you with the insights and tools for you to apply it to your own use case. Therefore we suggest taking your time to look at the code in the sample app, and run the described steps.\"</p> <p></p> <p>Here is an overview of the retrieval and ranking pipeline we will build in this tutorial:</p> <p></p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#installing-dependencies","title":"Installing dependencies\u00b6","text":""},{"location":"examples/rag-blueprint-vespa-cloud.html#cloning-the-rag-blueprint","title":"Cloning the RAG Blueprint\u00b6","text":"<p>Although you could define all components of the application with python code only from pyvespa, this would go against our advise on  or the Advanced Configuration notebook for a guide if you want to do that.</p> <p>Here, we will use pyvespa to deploy an application package from the existing files. Let us start by cloning the RAG Blueprint application from the Vespa sample-apps repository.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#inspecting-the-rag-blueprint","title":"Inspecting the RAG Blueprint\u00b6","text":"<p>First, let's examine the structure of the RAG Blueprint application we just cloned:</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#deploying-to-vespa-cloud","title":"Deploying to Vespa Cloud\u00b6","text":""},{"location":"examples/rag-blueprint-vespa-cloud.html#create-a-free-trial","title":"Create a free trial\u00b6","text":"<p>Create a tenant from here. The trial includes $300 credit. Take note of your tenant name, and input it below.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#adding-secret-to-vespa-cloud-secret-store","title":"Adding secret to Vespa Cloud Secret Store\u00b6","text":"<p>In order to use the LLM integration, you need to add your OpenAI API key to the Vespa Cloud Secret Store.</p> <p>Then, we can reference this secret in our <code>services.xml</code> file, so that Vespa can use it to access the OpenAI API. Below we have added a vault called <code>sample-apps</code> and a secret named <code>openai-dev</code> that contains the OpenAI API key.</p> <p></p> <p>We also need to assign permissions for our application to access this secret, but this can not be done until the application is deployed.</p> <pre>        &lt;!-- Uncomment this to use secret from Vespa Cloud Secret Store --&gt;\n        &lt;secrets&gt;\n            &lt;openai-api-key vault=\"sample-apps\" name=\"openai-dev\" /&gt;\n        &lt;/secrets&gt;\n</pre>"},{"location":"examples/rag-blueprint-vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>Now let's deploy the RAG Blueprint application to Vespa Cloud:</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#uncomment-secret-reference-and-redeploy","title":"Uncomment secret reference and redeploy\u00b6","text":""},{"location":"examples/rag-blueprint-vespa-cloud.html#feed-sample-data","title":"Feed Sample Data\u00b6","text":"<p>The RAG Blueprint comes with sample data. Let's download and feed it to test our deployment:</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#test-a-query-to-the-vespa-application","title":"Test a query to the Vespa application\u00b6","text":""},{"location":"examples/rag-blueprint-vespa-cloud.html#our-use-case","title":"Our use case\u00b6","text":"<p>The sample use case is a document search application, for a user who wants to get answers and insights quickly from a document collection containing company documents, notes, learning material, training logs. To make the blueprint more realistic, we required a dataset with more structured fields than are commonly found in public datasets. Therefore, we used a Large Language Model (LLM) to generate a custom one.</p> <p>It is a toy example, with only 100 documents, but we think it will illustrate the necessary concepts. You can also feel confident that the blueprint will provide a starting point that can scale as you want, with minimal changes.</p> <p>Below you can see a sample document from the dataset.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#data-modeling","title":"Data modeling\u00b6","text":"<p>Here is the schema that we will use for our sample application.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#picking-your-searchable-unit","title":"Picking your searchable unit\u00b6","text":"<p>When building a RAG application, your first key decision is choosing the \"searchable unit.\" This is the basic block of information your system will search through and return as context to the LLM. For instance, if you have millions of documents, some hundreds of pages long, what should be your searchable unit?</p> <p>Consider these points when selecting your searchable unit:</p> <ul> <li>Too fine-grained (e.g., individual sentences or very small paragraphs):<ul> <li>Leads to duplication of context and metadata across many small units.</li> <li>May result in units lacking sufficient context for the LLM to make good selections or generate relevant responses.</li> <li>Increases overhead for managing many small document units.</li> </ul> </li> <li>Too coarse-grained (e.g., very long chapters or entire large documents):<ul> <li>Can cause performance issues due to the size of the units being processed.</li> <li>May lead to some large documents appearing relevant to too many queries, reducing precision.</li> <li>If you embed the whole document, a too large context will lead to reduced retrieval quality.</li> </ul> </li> </ul> <p>We recommend erring on the side of using slightly larger units.</p> <ul> <li>LLMs are increasingly capable of handling larger contexts.</li> <li>In Vespa, you can index larger units, while avoiding data duplication and performance issues, by returning only the most relevant parts.</li> </ul> <p>With Vespa, it is now possible to return only the top k most relevant chunks of a document, and include and combine both document-level and chunk-level features in ranking.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#chunk-selection","title":"Chunk selection\u00b6","text":"<p>Assume you have chosen a document as your searchable unit. Your documents may then contain text index fields of highly variable lengths. Consider for example a corpus of web pages. Some might be very long, while the average is well within the recommended size. See scaling retrieval size for more details.</p> <p>While we recommend implementing guards against too long documents in your feeding pipeline, you still probably do not want to return every chunk of the top k documents to an LLM for RAG.</p> <p>In Vespa, we now have a solution for this problem. Below, we show how you can score both documents as well as individual chunks, and use that score to select the best chunks to be returned in a summary, instead of returning all chunks belonging to the top k ranked documents.</p> <p>Compute closeness per chunk in a ranking function; use <code>elementwise(bm25(chunks), i, double)</code> for a per-chunk text signal. See rank feature reference</p> <p>This allows you to pick a large document as the searchable unit, while still addressing the potential drawbacks many encounter as follows:</p> <ul> <li>Pick your (larger) document as your searchable unit.</li> <li>Chunk the text-fields automatically on indexing.</li> <li>Embed each chunk (enabled through Vespa's multivector support)</li> <li>Calculate chunk-level features (e.g. bm25 and embedding similarity) and document-level features. Combine as you want.</li> <li>Limit the actual chunks that are returned to the ones that are actually relevant context for the LLM.</li> </ul> <p>This allows you to index larger units, while avoiding data duplication and performance issues, by returning only the most relevant parts.</p> <p>Vespa also supports automatic chunking in the indexing language.</p> <p>Here are the parts of the schema, which defines the searchable unit as a document with a text field, and automatically chunks it into smaller parts of 1024 characters, which each are embedded and indexed separately:</p> <pre><code>txt\nfield chunks type array&lt;string&gt; {\n    indexing: input text | chunk fixed-length 1024 | summary | index\n    index: enable-bm25\n}\n\nfield chunk_embeddings type tensor&lt;int8&gt;(chunk{}, x[96]) {\n    indexing: input text | chunk fixed-length 1024 | embed | pack_bits | attribute | index\n    attribute {\n        distance-metric: hamming\n    }\n}\n</code></pre> <p>In Vespa, we can specify which chunks to be returned with a summary feature, see docs for details. For this blueprint, we will return the top 3 chunks based on the similarity score of the chunk embeddings, which is calculated in the ranking phase. Note that this feature could be any chunk-level summary feature defined in your rank-profile.</p> <p>Here is how the summary feature is calculated in the rank-profile:</p> <pre><code>txt\n# This function unpack the bits of each dimenrion of the mapped chunk_embeddings attribute tensor\nfunction chunk_emb_vecs() {\n    expression: unpack_bits(attribute(chunk_embeddings))\n}\n\n# This function calculate the dot product between the query embedding vector and the chunk embeddings (both are now float) over the x dimension\nfunction chunk_dot_prod() {\n    expression: reduce(query(float_embedding) * chunk_emb_vecs(), sum, x)\n}\n\n# This function calculate the L2 normalized length of an input tensor\nfunction vector_norms(t) {\n    expression: sqrt(sum(pow(t, 2), x))\n}\n\n# Here we calculate cosine similarity by dividing the dot product by the product of the L2 normalized query embedding and document embeddings\nfunction chunk_sim_scores() {\n    expression: chunk_dot_prod() / (vector_norms(chunk_emb_vecs()) * vector_norms(query(float_embedding)))\n}\n\nfunction top_3_chunk_text_scores() {\n    expression: top(3, chunk_text_scores())\n}\n\nfunction top_3_chunk_sim_scores() {\n        expression: top(3, chunk_sim_scores())\n    }\n\nsummary-features {\n        top_3_chunk_sim_scores\n    }\n</code></pre> <p>The ranking expression may seem a bit complex, as we chose to embed each chunk independently, store the embeddings in a binarized format, and then unpack them to calculate similarity based on their float representations. For single dimension dense vector similarity between same-precision embeddings, this can be simplified significantly using the closeness) convenience function.</p> <p>Note that we want to use the float-representation of the query-embedding, and thus also need to convert the binary embedding of the chunks to float. After that, we can calculate the similarity score between the query embedding and the chunk embeddings using cosine similarity (the dot product, and then normalize it by the norms of the embeddings).</p> <p>See ranking expressions for more details on the <code>top</code>-function, and other functions available for ranking expressions.</p> <p>Now, we can use this summary feature in our document summary to return the top 3 chunks of the document, which will be used as context for the LLM. Note that we can also define a document summary that returns all chunks, which might be useful for another use case, such as deep research.</p> <pre><code>txt\ndocument-summary top_3_chunks {\n      from-disk\n      summary chunks_top3 {\n          source: chunks\n          select-elements-by: top_3_chunk_sim_scores #this needs to be added a summary-feature to the rank-profile\n      }\n  }\n</code></pre>"},{"location":"examples/rag-blueprint-vespa-cloud.html#use-multiple-text-fields-consider-multiple-embeddings","title":"Use multiple text fields, consider multiple embeddings\u00b6","text":"<p>We recommend indexing different textual content as separate indexes. These can be searched together, using field-sets</p> <p>In our schema, this is exemplified by the sections below, which define the <code>title</code> and <code>chunks</code> fields as separate indexed text fields.</p> <pre><code>txt\n...\nfield title type title {\n    indexing: index | summary\n    index: enable-bm25\n}\nfield chunks type array&lt;string&gt; {\n    indexing: input text | chunk fixed-length 1024 | summary | index\n    index: enable-bm25\n}\n</code></pre> <p>Whether you should have separate embedding fields, depends on whether the added memory usage is justified by the quality improvement you could get from the additional embedding field.</p> <p>We choose to index both a <code>title_embedding</code> and a <code>chunk_embeddings</code> field for this blueprint, as we aim to minimize cost by embedding the binary vectors.</p> <pre><code>txt\nfield title_embedding type tensor&lt;int8&gt;(title{}, x[96]) {\n    indexing: input text | embed | pack_bits | attribute | index\n    attribute {\n        distance-metric: hamming\n    }\n}\nfield chunk_embeddings type tensor&lt;int8&gt;(chunk{}, x[96]) {\n    indexing: input text | chunk fixed-length 1024 | embed | pack_bits | attribute | index\n    attribute {\n        distance-metric: hamming\n    }\n}\n</code></pre> <p>Indexing several embedding fields may not be worth the cost for you. Evaluate whether the cost-quality trade-off is worth it for your application.</p> <p>If you have different vector space representations of your document (e.g images), indexing them separately is likely worth it, as they are likely to provide signals that are complementary to the text-based embeddings.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#model-metadata-and-signals-using-structured-fields","title":"Model Metadata and Signals Using Structured Fields\u00b6","text":"<p>We recommend modeling metadata and signals as structured fields in your schema. Below are some general recommendations, as well as the implementation in our blueprint schema.</p> <p>Metadata \u2014 knowledge about your data:</p> <ul> <li>Authors, publish time, source, links, category, price, \u2026</li> <li>Usage: filters, ranking, grouping/aggregation</li> <li>Index only metadata that are strong filters</li> </ul> <p>In our blueprint schema, we include these metadata fields to demonstrate these concepts:</p> <ul> <li><code>id</code> - document identifier</li> <li><code>title</code> - document name/filename for display and text matching</li> <li><code>created_timestamp</code>, <code>modified_timestamp</code> - temporal metadata for filtering and ranking by recency</li> </ul> <p>Signals \u2014 observations about your data:</p> <ul> <li>Popularity, quality, spam probability, click_probability, \u2026</li> <li>Usage: ranking</li> <li>Often updated separately via partial updates</li> <li>Multiple teams can add their own signals independently</li> </ul> <p>In our blueprint schema, we include several of these signals:</p> <ul> <li><code>last_opened_timestamp</code> - user engagement signal for personalization</li> <li><code>open_count</code> - popularity signal indicating document importance</li> <li><code>favorite</code> - explicit user preference signal, can be used for boosting relevant content</li> </ul> <p>These fields are configured as <code>attribute | summary</code> to enable efficient filtering, sorting, and grouping operations while being returned in search results. The timestamp fields allow for temporal filtering (e.g., \"recent documents\") and recency-based ranking, while usage signals like <code>open_count</code> and <code>favorite</code> can boost frequently accessed or explicitly marked important documents.</p> <p>Consider parent-child relationships for low-cardinality metadata. Most large scale RAG application schemas contain at least a hundred structured fields.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#llm-generation-with-openai-client","title":"LLM-generation with OpenAI-client\u00b6","text":"<p>Vespa supports both Local LLMs, and any OpenAI-compatible API for LLM generation. For details, see LLMs in Vespa</p> <p>The recommended way to provide an API key is by using the secret store in Vespa Cloud.</p> <p>To enable this, you need to create a vault (if you don't have one already) and a secret through the Vespa Cloud console. If your vault is named <code>sample-apps</code> and contains a secret with the name <code>openai-api-key</code>, you would use the following configuration in your <code>services.xml</code> to set up the OpenAI client to use that secret:</p> <pre>  &lt;secrets&gt;\n      &lt;openai-api-key vault=\"sample-apps\" name=\"openai-dev\" /&gt;\n  &lt;/secrets&gt;\n  &lt;!-- Setup the client to OpenAI --&gt;\n  &lt;component id=\"openai\" class=\"ai.vespa.llm.clients.OpenAI\"&gt;\n      &lt;config name=\"ai.vespa.llm.clients.llm-client\"&gt;\n          &lt;apiKeySecretRef&gt;openai-api-key&lt;/apiKeySecretRef&gt;\n      &lt;/config&gt;\n  &lt;/component&gt;\n</pre> <p>Alternatively, for local deployments, you can set the <code>X-LLM-API-KEY</code> header in your query to use the OpenAI client for generation.</p> <p>To test generation using the OpenAI client, post a query that runs the <code>openai</code> search chain, with <code>format=sse</code>. (Use <code>format=json</code> for a streaming json response including both the search hits and the LLM-generated tokens.)</p> <pre>vespa query \\\n    --timeout 60 \\\n    --header=\"X-LLM-API-KEY:&lt;your-api-key&gt;\" \\\n    yql='select *\n    from doc\n    where userInput(@query) or\n    ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n    ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))' \\\n    query=\"Summarize the key architectural decisions documented for SynapseFlow's v0.2 release.\" \\\n    searchChain=openai \\\n    format=sse \\\n    hits=5\n</pre>"},{"location":"examples/rag-blueprint-vespa-cloud.html#structuring-your-vespa-application","title":"Structuring your vespa application\u00b6","text":"<p>This section provides recommendations for structuring your Vespa application package. See also the application package docs for more details on the application package structure. Note that this is not mandatory, and it might be simpler to start without query profiles and rank profiles, but as you scale out your application, it will be beneficial to have a well-structured application package.</p> <p>Consider the following structure for our application package:</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#manage-queries-in-query-profiles","title":"Manage queries in query profiles\u00b6","text":"<p>Query profiles let you maintain collections of query parameters in one file. Clients choose a query profile \u2192 the profile sets everything else. This lets us change behavior for a use case without involving clients.</p> <p>Let us take a closer look at 3 of the query profiles in our sample application.</p> <ol> <li><code>hybrid</code></li> <li><code>rag</code></li> <li><code>deepresearch</code></li> </ol>"},{"location":"examples/rag-blueprint-vespa-cloud.html#hybrid-query-profile","title":"hybrid query profile\u00b6","text":"<p>This query profile will be the one used by clients for traditional search, where the user is presented a limited number of hits. Our other query profiles will inherit this one (but may override some fields).</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#rag-query-profile","title":"rag query profile\u00b6","text":"<p>This will be the query profile where the <code>openai</code> searchChain will be added, to generate a response based on the retrieved context. Here, we set some configuration that are specific to this use case.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#deepresearch-query-profile","title":"deepresearch query profile\u00b6","text":"<p>Again, we will inherit from the <code>hybrid</code> query profile, but override with a <code>targetHits</code> value of 10 000 (original was 100) that prioritizes recall over latency. We will also increase number of hits to be returned, and increase the timeout to 5 seconds.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#separating-out-rank-profiles","title":"Separating out rank profiles\u00b6","text":"<p>To build a great RAG application, assume you\u2019ll need many ranking models. This will allow you to bucket-test alternatives continuously and to serve different use cases, including data collection for different phases, and the rank profiles to be used in production.</p> <p>Separate common functions/setup into parent rank profiles and use <code>.profile</code> files.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#phased-ranking-in-vespa","title":"Phased ranking in Vespa\u00b6","text":"<p>Before we move on, it might be useful to recap Vespa\u00b4s phased ranking approach.</p> <p>Below is a schematic overview of how to think about retrieval and ranking for this RAG blueprint. Since we are developing this as a tutorial using a small toy dataset, the application can be deployed in a single machine, using a single docker container, where only one container node and one container node will run. This is obviously not the case for most real-world RAG applications, so this is cruical to have in mind as you want to scale your application.</p> <p>It is worth noting that parameters such as <code>targetHits</code> (for the match phase) and <code>rerank-count</code> (for first and second phase) are applied per content node. Also note that the stateless container nodes can also be scaled independently to handle increased query load.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#configuring-match-phase-retrieval","title":"Configuring match-phase (retrieval)\u00b6","text":"<p>This section will contain important considerations for the retrieval-phase of a RAG application in Vespa.</p> <p>The goal of the retrieval phase is to retrieve candidate documents efficiently, and maximize recall, without exposing too many documents to ranking.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#choosing-a-retrieval-strategy-vector-text-or-hybrid","title":"Choosing a Retrieval Strategy: Vector, Text, or Hybrid?\u00b6","text":"<p>As you could see from the schema, we create and index both a text representation and a vector representation for each chunk of the document. This will allow us to use both text-based features and semantic features for both recall and ranking.</p> <p>The text and vector representation complement each other well:</p> <ul> <li>Text-only \u2192 misses recall of semantically similar content</li> <li>Vector-only \u2192 misses recall of specific content not well understood by the embedding models</li> </ul> <p>Our recommendation is to default to hybrid retrieval:</p> <pre>select *\n        from doc\n        where userInput(@query) or\n        ({label:\"title_label\", targetHits:1000}nearestNeighbor(title_embedding, embedding)) or\n        ({label:\"chunks_label\", targetHits:1000}nearestNeighbor(chunk_embeddings, embedding))\n</pre> <p>In generic domains, or if you have fine-tuned an embedding model for your specific data, you might consider a vector-only approach:</p> <pre>select *\n        from doc\n        where rank({targetHits:10000}nearestNeighbor(embeddings_field, query_embedding, userInput(@query)))\n</pre> <p>Notice that only the first argument of the rank-operator will be used to determine if a document is a match, while all arguments are used for calculating rank features. This mean we can do vector only for matching, but still use text-based features such as <code>bm25</code> and <code>nativeRank</code> for ranking. Note that if you do this, it makes sense to increase the number of <code>targetHits</code> for the <code>nearestNeighbor</code>-operator.</p> <p>For our sample application, we add three different retrieval operators (that are combined with <code>OR</code>), one with <code>weakAnd</code> for text matching, and two <code>nearestNeighbor</code> operators for vector matching, one for the title and one for the chunks. This will allow us to retrieve both relevant documents based on text and vector similarity, while also allowing us to return the most relevant chunks of the documents.</p> <pre>select *\n        from doc\n        where userInput(@query) or\n        ({targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n        ({targetHits:100}nearestNeighbor(chunk_embeddings, embedding))\n</pre>"},{"location":"examples/rag-blueprint-vespa-cloud.html#choosing-your-embedding-model-and-strategy","title":"Choosing your embedding model (and strategy)\u00b6","text":"<p>Choice of embedding model will be a trade-off between inference time (both indexing and query time), memory usage (embedding dimensions) and quality. There are many good open-source models available, and we recommend checking out the MTEB leaderboard, and look at the <code>Retrieval</code>-column to gauge performance, while also considering the memory usage, vector dimensions, and context length of the model.</p> <p>See model hub for a list of provided models ready to use with Vespa. See also Huggingface Embedder for details on using other models (exported as ONNX) with Vespa.</p> <p>In addition to dense vector representation, Vespa supports sparse embeddings (token weights) and multi-vector (ColBERT-style) embeddings. See our example notebook of using the bge-m3 model, which supports both, with Vespa.</p> <p>Vespa also supports Matryoshka embeddings, which can be a great way of reducing inference cost for retrieval phases, by using a subset of the embedding dimensions, while using more dimensions for increased precision in the later ranking phases.</p> <p>For domain-specific applications or less popular languages, you may want to consider finetuning a model on your own data.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#consider-binary-vectors-for-recall","title":"Consider binary vectors for recall\u00b6","text":"<p>Another decision to make is which precision you will use for your embeddings. See binarization docs for an introduction to binarization in Vespa.</p> <p>For most cases, binary vectors (in Vespa, packed into <code>int8</code>-representation) will provide an attractive tradeoff, especially for recall during match-phase. Consider these factors to determine whether this holds true for your application:</p> <ul> <li>Reduces memory-vector cost by 5 \u2013 30 \u00d7</li> <li>Reduces query and indexing cost by 30 \u00d7</li> <li>Often reduces quality by only a few percentage points</li> </ul> <pre><code>txt\nfield binary_chunk_embeddings type tensor&lt;int8&gt;(chunk{}, x) {\n  indexing: input text | chunk fixed-length 1024 | embed | pack_bits | attribute | index \n  attribute { distance-metric: hamming }\n}\n</code></pre> <p>If you need higher precision vector similarity, you should use bfloat16 precision, and consider paging these vectors to disk to avoid large memory cost. Note that this means that when accessing this field in ranking, they will also need to be read from disk, so you need to restrict the number of hits that accesses this field to avoid performance issues.</p> <pre><code>txt\nfield chunk_embeddings type tensor&lt;bfloat16&gt;(chunk{}, x) {\n  indexing: input text | chunk fixed-length 1024 | embed | attribute \n  attribute: paged\n}\n</code></pre> <p>For example, if you want to calculate <code>closeness</code> for a paged embedding vector in first-phase, consider configuring your retrieval operators (typically <code>weakAnd</code> and/or <code>nearestNeighbor</code>, optionally combined with filters) so that not too many hits are matched. Another option is to enable match-phase limiting, see match-phase docs. In essence, you restrict the number of matches by specifying an attribute field.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#consider-float-binary-for-ranking","title":"Consider float-binary for ranking\u00b6","text":"<p>In our blueprint, we choose to index binary vectors of the documents. This does not prevent us from using the float-representation of the query embedding though.</p> <p>By unpacking the binary document chunk embeddings to their float representations (using <code>unpack_bits</code>), we can calculate the similarity between query and document with slightly higher precision using a <code>float-binary</code> dot product, instead of hamming distance (<code>binary-binary</code>)</p> <p>Below, you can see how we can do this:</p> <pre><code>txt\nrank-profile collect-training-data {\n \n        inputs {\n            query(embedding) tensor&lt;int8&gt;(x[96])\n            query(float_embedding) tensor&lt;float&gt;(x[768])\n        }\n        \n        function chunk_emb_vecs() {\n            expression: unpack_bits(attribute(chunk_embeddings))\n        }\n\n        function chunk_dot_prod() {\n            expression: reduce(query(float_embedding) * chunk_emb_vecs(), sum, x)\n        }\n\n        function vector_norms(t) {\n            expression: sqrt(sum(pow(t, 2), x))\n        }\n        function chunk_sim_scores() {\n            expression: chunk_dot_prod() / (vector_norms(chunk_emb_vecs()) * vector_norms(query(float_embedding)))\n        }\n\n        function top_3_chunk_text_scores() {\n            expression: top(3, chunk_text_scores())\n        }\n\n        function top_3_chunk_sim_scores() {\n            expression: top(3, chunk_sim_scores())\n        }\n}\n</code></pre>"},{"location":"examples/rag-blueprint-vespa-cloud.html#use-complex-linguisticsrecall-only-for-precision","title":"Use complex linguistics/recall only for precision\u00b6","text":"<p>Vespa gives you extensive control over linguistics. You can decide match mode, stemming, normalization, or control derived tokens.</p> <p>It is also possible to use more specific operators than weakAnd to match only close occurrences (near/ onear), multiple alternatives (equiv), weight items, set connectivity, and apply query-rewrite rules.</p> <p>Don\u2019t use this to increase recall \u2014 improve your embedding model instead.</p> <p>Consider using it to improve precision when needed.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#evaluating-recall-of-the-retrieval-phase","title":"Evaluating recall of the retrieval phase\u00b6","text":"<p>To know whether your retrieval phase is working well, you need to measure recall, number of total matches and the reported time spent.</p> <p>We can use <code>VespaMatchEvaluator</code> from the pyvespa client library to do this.</p> <p>For this sample application, we set up an evaluation script that compares three different retrieval strategies, let us call them \"retrieval arms\":</p> <ol> <li>Semantic-only: Uses only vector similarity through <code>nearestNeighbor</code> operators.</li> <li>WeakAnd-only: Uses only text-based matching with <code>userQuery()</code>.</li> <li>Hybrid: Combines both approaches with OR logic.</li> </ol> <p>Note that this is only generic suggestion for and that you are of course free to include both filter clauses, grouping, predicates, geosearch etc. to support your specific use cases.</p> <p>It is recommended to use a ranking profile that does not use any first-phase ranking, to run the match-phase evaluation faster.</p> <p>The evaluation will output metrics like:</p> <ul> <li>Recall (percentage of relevant documents matched)</li> <li>Total number of matches per query</li> <li>Query latency statistics</li> <li>Per-query detailed results (when <code>write_verbose=True</code>) to identify \"offending\" queries with regards to recall or performance.</li> </ul> <p>This will be valuable input for tuning each of them.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#tuning-the-retrieval-phase","title":"Tuning the retrieval phase\u00b6","text":"<p>We can see that all queries match all relevant documents, which is expected, since we use <code>targetHits:100</code> in the <code>nearestNeighbor</code> operator, and this is also the default for <code>weakAnd</code>(and <code>userQuery</code>). By setting <code>targetHits</code> lower, we can see that recall will drop.</p> <p>In general, you have these options if you want to increase recall:</p> <ol> <li>Increase <code>targetHits</code> in your retrieval operators (e.g., <code>nearestNeighbor</code>, <code>weakAnd</code>).</li> <li>Improve your embedding model (use a better model or finetune it on your data).</li> <li>You can also consider tuning HNSW parameters, see docs on HNSW.</li> </ol> <p>Conversely, if you want to reduce the latency of one of your retrieval 'arms' at the cost of a small trade-off in recall, you can:</p> <ol> <li>Tune <code>weakAnd</code> parameters. This has potential to 3x your performance for the <code>weakAnd</code>-parameter of your query, see blog post.</li> </ol> <p>Below are some empirically found default parameters that work well for most use cases:</p> <pre><code>txt\nrank-profile optimized inherits baseline {\n    filter-threshold: 0.05\n    weakand {\n      stopword-limit: 0.6\n      adjust-target: 0.01\n    }\n  }\n</code></pre> <p>See the reference for more details on the <code>weakAnd</code> parameters. These can also be set as query parameters.</p> <ol> <li>As already mentioned, consider binary vectors for your embeddings.</li> <li>Consider using an embedding model with less dimensions, or using only a subset of the dimensions (e.g., using Matryoshka embeddings).</li> </ol>"},{"location":"examples/rag-blueprint-vespa-cloud.html#first-phase-ranking","title":"First-phase ranking\u00b6","text":"<p>For the first-phase ranking, we must use a computationally cheap function, as it is applied to all documents matched in the retrieval phase. For many applications, this can amount to millions of candidate documents.</p> <p>Common options include (learned) linear combination of features including text similarity features, vector closeness, and metadata. It could also be a heuristic handwritten function.</p> <p>Text features should include nativeRank or bm25 \u2014 not fieldMatch (it is too expensive).</p> <p>Considerations for deciding whether to choose <code>bm25</code> or <code>nativeRank</code>:</p> <ul> <li>bm25: cheapest, strong significance, no proximity, not normalized.</li> <li>nativeRank: 2 \u2013 3 \u00d7 costlier, truncated significance, includes proximity, normalized.</li> </ul> <p>For this blueprint, we opted for using <code>bm25</code> for first phase, but you could evaluate and compare to see whether the additional cost of using <code>nativeRank</code> is justified by increased quality.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#collecting-training-data-for-first-phase-ranking","title":"Collecting training data for first-phase ranking\u00b6","text":"<p>The features we will use for first-phase ranking are not normalized (ie. they have values in different ranges). This means we can't just weight them equally and expect that to be a good proxy for relevance.</p> <p>Below we will show how we can find (learn) optimal weights (coefficients) for each feature, so that we can combine them into a ranking-expression on the format:</p> <pre>a * bm25(title) + b * bm25(chunks) + c * max_chunk_sim_scores() + d * max_chunk_text_scores() + e * avg_top_3_chunk_sim_scores() + f * avg_top_3_chunk_text_scores()\n</pre> <p>The first thing we need to is to collect training data. We do this using the VespaFeatureCollector from the pyvespa library.</p> <p>These are the features we will include:</p> <pre><code>txt\nrank-profile collect-training-data {\n        match-features {\n            bm25(title)\n            bm25(chunks)\n            max_chunk_sim_scores\n            max_chunk_text_scores\n            avg_top_3_chunk_sim_scores\n            avg_top_3_chunk_text_scores\n\n        }\n\n        # Since we need both binary embeddings (for match-phase) and float embeddings (for ranking) we define it as two inputs.\n        inputs {\n            query(embedding) tensor&lt;int8&gt;(x[96])\n            query(float_embedding) tensor&lt;float&gt;(x[768])\n        }\n\n        rank chunks {\n            element-gap: 0 # Fixed length chunking should not cause any positional gap between elements\n        }\n        function chunk_text_scores() {\n            expression: elementwise(bm25(chunks),chunk,float)\n        }\n\n        function chunk_emb_vecs() {\n            expression: unpack_bits(attribute(chunk_embeddings))\n        }\n\n        function chunk_dot_prod() {\n            expression: reduce(query(float_embedding) * chunk_emb_vecs(), sum, x)\n        }\n\n        function vector_norms(t) {\n            expression: sqrt(sum(pow(t, 2), x))\n        }\n        function chunk_sim_scores() {\n            expression: chunk_dot_prod() / (vector_norms(chunk_emb_vecs()) * vector_norms(query(float_embedding)))\n        }\n\n        function top_3_chunk_text_scores() {\n            expression: top(3, chunk_text_scores())\n        }\n\n        function top_3_chunk_sim_scores() {\n            expression: top(3, chunk_sim_scores())\n        }\n\n        function avg_top_3_chunk_text_scores() {\n            expression: reduce(top_3_chunk_text_scores(), avg, chunk)\n        }\n        function avg_top_3_chunk_sim_scores() {\n            expression: reduce(top_3_chunk_sim_scores(), avg, chunk)\n        }\n        \n        function max_chunk_text_scores() {\n            expression: reduce(chunk_text_scores(), max, chunk)\n        }\n\n        function max_chunk_sim_scores() {\n            expression: reduce(chunk_sim_scores(), max, chunk)\n        }\n\n        first-phase {\n            expression {\n                # Not used in this profile\n                bm25(title) + \n                bm25(chunks) +\n                max_chunk_sim_scores() +\n                max_chunk_text_scores()\n            }\n        }\n\n        second-phase {\n            expression: random\n        }\n    }\n</code></pre> <p>As you can see, we rely on the <code>bm25</code> and different vector similarity features (both document-level and chunk-level) for the first-phase ranking. These are relatively cheap to calculate, and will likely provide good enough ranking signals for the first-phase ranking.</p> <p>Running the command below will save a .csv-file with the collected features, which can be used to train a ranking model for the first-phase ranking.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#training-a-first-phase-ranking-model","title":"Training a first-phase ranking model\u00b6","text":"<p>As you recall, a first-phase ranking expression must be cheap to evaluate. This most often means a heuristic handwritten combination of match features, or a linear model trained on match features.</p> <p>We will demonstrate how to train a simple Logistic Regression model to predict relevance based on the collected match features. The full training script can be found in the sample-apps repository.</p> <p>Some \"gotchas\" to be aware of:</p> <ul> <li>We sample an equal number of relevant and random documents for each query, to avoid class imbalance.</li> <li>We make sure that we drop <code>query_id</code> and <code>doc_id</code> columns before training.</li> <li>We apply standard scaling to the features before training the model. We apply the inverse transform to the model coefficients after training, so that we can use them in Vespa.</li> <li>We do 5-fold stratified cross-validation to evaluate the model performance, ensuring that each fold has a balanced number of relevant and random documents.</li> <li>We also make sure to have an unseen set of test queries to evaluate the model on, to avoid overfitting.</li> </ul> <p>Run the cell below to train the model and get the coefficients.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#evaluating-first-phase-ranking","title":"Evaluating first-phase ranking\u00b6","text":"<p>Now we are ready to evaluate our first-phase ranking function. We can use the VespaEvaluator to evaluate the first-phase ranking function on the unseen test queries.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#second-phase-ranking","title":"Second-phase ranking\u00b6","text":"<p>For the second-phase ranking, we can afford to use a more expensive ranking expression, since we will only run it on the top-k documents from the first-phase ranking (defined by the <code>rerank-count</code> parameter, which defaults to 10,000 documents).</p> <p>This is where we can significantly improve ranking quality by using more sophisticated models and features that would be too expensive to compute for all matched documents.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#collecting-features-for-second-phase-ranking","title":"Collecting features for second-phase ranking\u00b6","text":"<p>For second-phase ranking, we request Vespa's default set of rank features, which includes a comprehensive set of text features. See the rank features documentation for complete details.</p> <p>We can collect both match features and rank features by running the same code as we did for first-phase ranking, with  some additional parameters to collect rank features as well.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#training-a-gbdt-model-for-second-phase-ranking","title":"Training a GBDT model for second-phase ranking\u00b6","text":"<p>With the expanded feature set, we can train a Gradient Boosted Decision Tree (GBDT) model to predict document relevance. We use LightGBM for this purpose.</p> <p>Vespa also supports XGBoost and ONNX models.</p> <p>To train the model, run the following command (link to training script):</p> <p>The training process includes several important considerations:</p> <ul> <li>Cross-validation: We use 5-fold stratified cross-validation to evaluate model performance and prevent overfitting</li> <li>Hyperparameter tuning: We set conservative hyperparameters to prevent growing overly large and deep trees, especially important for smaller datasets</li> <li>Feature selection: Features with zero importance during cross-validation are excluded from the final model</li> <li>Early stopping: Training stops when validation scores don't improve for 50 rounds</li> </ul>"},{"location":"examples/rag-blueprint-vespa-cloud.html#feature-importance-analysis","title":"Feature importance analysis\u00b6","text":"<p>The trained model reveals which features are most important for ranking quality. (As this notebook runs in CI, and not everything from data_collection and training is deterministic, the exact feature importances may vary, but we expect the observations below to hold for most runs.)</p> <p>Key observations:</p> <ul> <li>Text proximity features (nativeProximity) are highly valuable for understanding query-document relevance</li> <li>First-phase score (<code>firstPhase</code>) being important validates that our first-phase ranking provides a good foundation</li> <li>Chunk-level features (both text and semantic) contribute significantly to ranking quality</li> <li>Traditional text features like nativeRank and bm25 remain important</li> </ul>"},{"location":"examples/rag-blueprint-vespa-cloud.html#integrating-the-gbdt-model-into-vespa","title":"Integrating the GBDT model into Vespa\u00b6","text":"<p>The trained LightGBM model can be exported and added to your Vespa application package:</p> <pre><code>txt\napp/\n\u251c\u2500\u2500 models/\n\u2502   \u2514\u2500\u2500 lightgbm_model.json\n</code></pre>"},{"location":"examples/rag-blueprint-vespa-cloud.html#evaluating-second-phase-ranking-performance","title":"Evaluating second-phase ranking performance\u00b6","text":"<p>Let us run the ranking evaluation to evaluate the GBDT-powered second-phase ranking on unseen test queries:</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#query-profiles-with-gbdt-ranking","title":"Query profiles with GBDT ranking\u00b6","text":"<p>Create new query profiles that leverage the improved ranking:</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#best-practices-for-second-phase-ranking","title":"Best practices for second-phase ranking\u00b6","text":"<p>Model complexity considerations:</p> <ul> <li>Use more sophisticated models (GBDT, neural networks) that would be too expensive for first-phase</li> <li>Take advantage of the reduced candidate set (typically 100-10,000 documents)</li> <li>Include expensive text features like <code>nativeProximity</code> and <code>fieldMatch</code></li> </ul> <p>Feature engineering:</p> <ul> <li>Combine first-phase scores with additional text and semantic features</li> <li>Use chunk-level aggregations (max, average, top-k) to capture document structure</li> <li>Include metadata signals</li> </ul> <p>Training data quality:</p> <ul> <li>Use the first-phase ranking to generate better training data</li> <li>Consider having LLMs generate relevance judgments for top-k results</li> <li>Iteratively improve with user interaction data when available</li> </ul> <p>Performance monitoring:</p> <ul> <li>Monitor latency impact of second-phase ranking</li> <li>Adjust <code>rerank-count</code> based on quality vs. performance trade-offs</li> <li>Consider using different models for different query types or use cases</li> </ul> <p>The second-phase ranking represents a crucial step in building high-quality RAG applications, providing the precision needed for effective LLM context while maintaining reasonable query latencies.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#optional-global-phase-ranking","title":"(Optional) Global-phase ranking\u00b6","text":"<p>We also have the option of configuring global-phase ranking, which can rerank the top k (as set by <code>rerank-count</code> parameter) documents from the second-phase ranking.</p> <p>Common options for global-phase are cross-encoders or another GBDT model, trained for better separating top ranked documents on objectives such as LambdaMart. For RAG applications, we consider this less important than for search applications where the results are mainly consumed by an human, as LLMs don't care that much about the ordering of the results.</p> <p>See also our notebook on using cross-encoders for global reranking</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#further-improvements","title":"Further improvements\u00b6","text":"<p>Finally, we will sketch out some opportunities for further improvements. As you have seen, we started out with only binary relevance labels for a few queries, and trained a model based on the relevant docs and a set of random documents.</p> <p>As you may have noted, we have not discussed what most people think about when discussing RAG evals, evaluating the \"Generation\"-step. There are several tools available to do this, for example ragas and ARES. We refer to other sources for details on this, as this tutorial is probably enough to digest as it is.</p> <p>This was useful initially, as we had no better way to retrieve the candidate documents. Now, that we have a reasonably good second-phase ranking, we could potentially generate a new set of relevance labels for queries that we did not have labels for by having an LLM do relevance judgments of the top k returned hits. This training dataset would likely be even better in separating the top documents.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#structured-output-from-the-llm","title":"Structured output from the LLM\u00b6","text":"<p>Let us also show how we can request structured JSON output from the LLM, which can be useful for several reasons, the most common probably being citations.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>In this tutorial, we have built a complete RAG application using Vespa, providing our recommendations for how to approach both retrieval phase with binary vectors and text matching, first-phase ranking with a linear combination of relatively cheap features to a more sophisticated second-phase ranking system with more expensive features and a GBDT model.</p> <p>We hope that this tutorial, along with the provided code in our sample-apps repository, will serve as a useful reference for building your own RAG applications, with an evaluation-driven approach.</p> <p>By using the principles demonstrated in this tutorial, you are empowered to build high-quality RAG applications that can scale to any dataset size, and any query load.</p>"},{"location":"examples/rag-blueprint-vespa-cloud.html#faq","title":"FAQ\u00b6","text":"<ul> <li><p>Q: Which embedding models can I use with Vespa? A: Vespa supports a variety of embedding models. For a list of vespa provided models on Vespa Cloud, see Model hub. See also embedding reference for how to use embedders. You can also use private models (gated by authentication with Bearer token from Vespa Cloud secret store).</p> </li> <li><p>Q: Why don't you use ColBERT for ranking? A: We love ColBERT, and it has shown great performance. We do support ColBERT-style models in Vespa. The challenge is the added cost in memory storage, especially for large-scale applications. If you use it, we recommend consider binarizing the vectors to reduce memory usage 32x compared to float. If you want to improve the ranking quality and accept the additional cost, we encourage you to evaluate and try. Here are some resources if you want to learn more about using ColBERT with Vespa:</p> <ul> <li>Announcing ColBERT embedder</li> <li>Long context ColBERT</li> <li>Long context ColBERT sample app</li> <li>ColBERT sample app</li> <li>ColBERT embedder reference</li> <li>ColBERT standalone python example notebook</li> <li>ColBERT standalone long context example notebook</li> </ul> </li> <li><p>Q: Do I need to use an LLM with Vespa? A: No, you are free to use Vespa as a search engine. We provide the option of calling out to LLMs from within a Vespa application for reduced latency compared to sending large search results sets several times over network as well as the option to deploy Local LLMs, optionally in your own infrastructure if you prefer. See Vespa Cloud Enclave</p> </li> <li><p>Q: Why do we use binary vectors for the document embeddings? A: Binary vectors takes up a lot less memory and are faster to compute distances on, with only a slight reduction in quality. See blog post for details.</p> </li> <li><p>Q: How can you say that Vespa can scale to any data and query load? A: Vespa can scale both the stateless container nodes and content nodes of your application. See overview and elasticity for details.</p> </li> </ul>"},{"location":"examples/rag-blueprint-vespa-cloud.html#clean-up","title":"Clean up\u00b6","text":"<p>As this tutorial is running in a CI environment, we will clean up the resources created.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html","title":"Scaling personal ai assistants with streaming mode cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa llama-index vespacli\n</pre> !pip3 install -U pyvespa llama-index vespacli In\u00a0[2]: Copied! <pre>from typing import List\n\n\ndef synthetic_mail_data_generator() -&gt; List[dict]:\n    synthetic_mails = [\n        {\n            \"id\": 1,\n            \"groupname\": \"bergum@vespa.ai\",\n            \"fields\": {\n                \"subject\": \"LlamaIndex news, 2023-11-14\",\n                \"to\": \"bergum@vespa.ai\",\n                \"body\": \"\"\"Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \n                    lane on our blog with twelve milestones from our first year. Be sure to check it out.\"\"\",\n                \"from\": \"news@llamaindex.ai\",\n                \"display_date\": \"2023-11-15T09:00:00Z\",\n            },\n        },\n        {\n            \"id\": 2,\n            \"groupname\": \"bergum@vespa.ai\",\n            \"fields\": {\n                \"subject\": \"Dentist Appointment Reminder\",\n                \"to\": \"bergum@vespa.ai\",\n                \"body\": \"Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist\",\n                \"from\": \"dentist@dentist.no\",\n                \"display_date\": \"2023-11-15T15:30:00Z\",\n            },\n        },\n        {\n            \"id\": 1,\n            \"groupname\": \"giraffe@wildlife.ai\",\n            \"fields\": {\n                \"subject\": \"Wildlife Update: Giraffe Edition\",\n                \"to\": \"giraffe@wildlife.ai\",\n                \"body\": \"Dear Wildlife Enthusiasts \ud83e\udd92, We're thrilled to share the latest insights into giraffe behavior in the wild. Join us on an adventure as we explore their natural habitat and learn more about these majestic creatures.\",\n                \"from\": \"updates@wildlife.ai\",\n                \"display_date\": \"2023-11-12T14:30:00Z\",\n            },\n        },\n        {\n            \"id\": 1,\n            \"groupname\": \"penguin@antarctica.ai\",\n            \"fields\": {\n                \"subject\": \"Antarctica Expedition: Penguin Chronicles\",\n                \"to\": \"penguin@antarctica.ai\",\n                \"body\": \"Greetings Explorers \ud83d\udc27, Our team is embarking on an exciting expedition to Antarctica to study penguin colonies. Stay tuned for live updates and behind-the-scenes footage as we dive into the world of these fascinating birds.\",\n                \"from\": \"expedition@antarctica.ai\",\n                \"display_date\": \"2023-11-11T11:45:00Z\",\n            },\n        },\n        {\n            \"id\": 1,\n            \"groupname\": \"space@exploration.ai\",\n            \"fields\": {\n                \"subject\": \"Space Exploration News: November Edition\",\n                \"to\": \"space@exploration.ai\",\n                \"body\": \"Hello Space Enthusiasts \ud83d\ude80, Join us as we highlight the latest discoveries and breakthroughs in space exploration. From distant galaxies to new technologies, there's a lot to explore!\",\n                \"from\": \"news@exploration.ai\",\n                \"display_date\": \"2023-11-01T16:20:00Z\",\n            },\n        },\n        {\n            \"id\": 1,\n            \"groupname\": \"ocean@discovery.ai\",\n            \"fields\": {\n                \"subject\": \"Ocean Discovery: Hidden Treasures Unveiled\",\n                \"to\": \"ocean@discovery.ai\",\n                \"body\": \"Dear Ocean Explorers \ud83c\udf0a, Dive deep into the secrets of the ocean with our latest discoveries. From undiscovered species to underwater landscapes, our team is uncovering the wonders of the deep blue.\",\n                \"from\": \"discovery@ocean.ai\",\n                \"display_date\": \"2023-10-01T10:15:00Z\",\n            },\n        },\n    ]\n    for mail in synthetic_mails:\n        yield mail\n</pre> from typing import List   def synthetic_mail_data_generator() -&gt; List[dict]:     synthetic_mails = [         {             \"id\": 1,             \"groupname\": \"bergum@vespa.ai\",             \"fields\": {                 \"subject\": \"LlamaIndex news, 2023-11-14\",                 \"to\": \"bergum@vespa.ai\",                 \"body\": \"\"\"Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory                      lane on our blog with twelve milestones from our first year. Be sure to check it out.\"\"\",                 \"from\": \"news@llamaindex.ai\",                 \"display_date\": \"2023-11-15T09:00:00Z\",             },         },         {             \"id\": 2,             \"groupname\": \"bergum@vespa.ai\",             \"fields\": {                 \"subject\": \"Dentist Appointment Reminder\",                 \"to\": \"bergum@vespa.ai\",                 \"body\": \"Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist\",                 \"from\": \"dentist@dentist.no\",                 \"display_date\": \"2023-11-15T15:30:00Z\",             },         },         {             \"id\": 1,             \"groupname\": \"giraffe@wildlife.ai\",             \"fields\": {                 \"subject\": \"Wildlife Update: Giraffe Edition\",                 \"to\": \"giraffe@wildlife.ai\",                 \"body\": \"Dear Wildlife Enthusiasts \ud83e\udd92, We're thrilled to share the latest insights into giraffe behavior in the wild. Join us on an adventure as we explore their natural habitat and learn more about these majestic creatures.\",                 \"from\": \"updates@wildlife.ai\",                 \"display_date\": \"2023-11-12T14:30:00Z\",             },         },         {             \"id\": 1,             \"groupname\": \"penguin@antarctica.ai\",             \"fields\": {                 \"subject\": \"Antarctica Expedition: Penguin Chronicles\",                 \"to\": \"penguin@antarctica.ai\",                 \"body\": \"Greetings Explorers \ud83d\udc27, Our team is embarking on an exciting expedition to Antarctica to study penguin colonies. Stay tuned for live updates and behind-the-scenes footage as we dive into the world of these fascinating birds.\",                 \"from\": \"expedition@antarctica.ai\",                 \"display_date\": \"2023-11-11T11:45:00Z\",             },         },         {             \"id\": 1,             \"groupname\": \"space@exploration.ai\",             \"fields\": {                 \"subject\": \"Space Exploration News: November Edition\",                 \"to\": \"space@exploration.ai\",                 \"body\": \"Hello Space Enthusiasts \ud83d\ude80, Join us as we highlight the latest discoveries and breakthroughs in space exploration. From distant galaxies to new technologies, there's a lot to explore!\",                 \"from\": \"news@exploration.ai\",                 \"display_date\": \"2023-11-01T16:20:00Z\",             },         },         {             \"id\": 1,             \"groupname\": \"ocean@discovery.ai\",             \"fields\": {                 \"subject\": \"Ocean Discovery: Hidden Treasures Unveiled\",                 \"to\": \"ocean@discovery.ai\",                 \"body\": \"Dear Ocean Explorers \ud83c\udf0a, Dive deep into the secrets of the ocean with our latest discoveries. From undiscovered species to underwater landscapes, our team is uncovering the wonders of the deep blue.\",                 \"from\": \"discovery@ocean.ai\",                 \"display_date\": \"2023-10-01T10:15:00Z\",             },         },     ]     for mail in synthetic_mails:         yield mail In\u00a0[3]: Copied! <pre>from typing import List\n\n\ndef synthetic_calendar_data_generator() -&gt; List[dict]:\n    calendar_data = [\n        {\n            \"id\": 1,\n            \"groupname\": \"bergum@vespa.ai\",\n            \"fields\": {\n                \"subject\": \"Dentist Appointment\",\n                \"to\": \"bergum@vespa.ai\",\n                \"body\": \"Dentist appointment at 2023-12-04 at 09:30 - 1 hour duration\",\n                \"from\": \"dentist@dentist.no\",\n                \"display_date\": \"2023-11-15T15:30:00Z\",\n                \"duration\": 60,\n            },\n        },\n        {\n            \"id\": 2,\n            \"groupname\": \"bergum@vespa.ai\",\n            \"fields\": {\n                \"subject\": \"Public Cloud Platform Events\",\n                \"to\": \"bergum@vespa.ai\",\n                \"body\": \"The cloud team continues to push new features and improvements to the platform. Join us for a live demo of the latest updates\",\n                \"from\": \"public-cloud-platform-events\",\n                \"display_date\": \"2023-11-21T09:30:00Z\",\n                \"duration\": 60,\n            },\n        },\n    ]\n    for event in calendar_data:\n        yield event\n</pre> from typing import List   def synthetic_calendar_data_generator() -&gt; List[dict]:     calendar_data = [         {             \"id\": 1,             \"groupname\": \"bergum@vespa.ai\",             \"fields\": {                 \"subject\": \"Dentist Appointment\",                 \"to\": \"bergum@vespa.ai\",                 \"body\": \"Dentist appointment at 2023-12-04 at 09:30 - 1 hour duration\",                 \"from\": \"dentist@dentist.no\",                 \"display_date\": \"2023-11-15T15:30:00Z\",                 \"duration\": 60,             },         },         {             \"id\": 2,             \"groupname\": \"bergum@vespa.ai\",             \"fields\": {                 \"subject\": \"Public Cloud Platform Events\",                 \"to\": \"bergum@vespa.ai\",                 \"body\": \"The cloud team continues to push new features and improvements to the platform. Join us for a live demo of the latest updates\",                 \"from\": \"public-cloud-platform-events\",                 \"display_date\": \"2023-11-21T09:30:00Z\",                 \"duration\": 60,             },         },     ]     for event in calendar_data:         yield event In\u00a0[4]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\nmail_schema = Schema(\n    name=\"mail\",\n    mode=\"streaming\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"subject\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"to\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"from\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"body\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"display_date\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"timestamp\",\n                type=\"long\",\n                indexing=[\n                    \"input display_date\",\n                    \"to_epoch_second\",\n                    \"summary\",\n                    \"attribute\",\n                ],\n                is_document_field=False,\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;bfloat16&gt;(x[384])\",\n                indexing=[\n                    'input subject .\" \". input body',\n                    \"embed e5\",\n                    \"attribute\",\n                    \"index\",\n                ],\n                ann=HNSW(distance_metric=\"angular\"),\n                is_document_field=False,\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"subject\", \"body\", \"to\", \"from\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  mail_schema = Schema(     name=\"mail\",     mode=\"streaming\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"subject\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"to\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"from\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"body\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"display_date\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"timestamp\",                 type=\"long\",                 indexing=[                     \"input display_date\",                     \"to_epoch_second\",                     \"summary\",                     \"attribute\",                 ],                 is_document_field=False,             ),             Field(                 name=\"embedding\",                 type=\"tensor(x[384])\",                 indexing=[                     'input subject .\" \". input body',                     \"embed e5\",                     \"attribute\",                     \"index\",                 ],                 ann=HNSW(distance_metric=\"angular\"),                 is_document_field=False,             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"subject\", \"body\", \"to\", \"from\"])], ) <p>In the <code>mail</code> schema, we have six document fields; these are provided by us when we feed documents of type <code>mail</code> to this app. The fieldset defines which fields are matched against when we do not mention explicit field names when querying. We can add as many fieldsets as we like without duplicating content.</p> <p>In addition to the fields within the <code>document</code>, there are two synthetic fields in the schema, <code>timestamp</code> and <code>embedding</code>, using Vespa indexing expressions taking inputs from the document and performing conversions.</p> <ul> <li>the <code>timestamp</code> field takes the input <code>display_date</code> and uses the to_epoch_second converter to convert the display date into an epoch timestamp. This is useful because we can calculate the document's age and use the <code>freshness(timestamp)</code> rank feature during ranking phases.</li> <li>the <code>embedding</code> tensor field takes the subject and body as input and feeds that into an embed function that uses an embedding model to map the string input into an embedding vector representation using 384 dimensions with <code>bfloat16</code> precision. Vectors in Vespa are represented as Tensors.</li> </ul> In\u00a0[5]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\ncalendar_schema = Schema(\n    name=\"calendar\",\n    inherits=\"mail\",\n    mode=\"streaming\",\n    document=Document(\n        inherits=\"mail\",\n        fields=[\n            Field(name=\"duration\", type=\"int\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"guests\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"location\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"address\", type=\"string\", indexing=[\"summary\", \"index\"]),\n        ],\n    ),\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  calendar_schema = Schema(     name=\"calendar\",     inherits=\"mail\",     mode=\"streaming\",     document=Document(         inherits=\"mail\",         fields=[             Field(name=\"duration\", type=\"int\", indexing=[\"summary\", \"index\"]),             Field(name=\"guests\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(name=\"location\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"address\", type=\"string\", indexing=[\"summary\", \"index\"]),         ],     ), ) <p>The observant reader might have noticed the <code>e5</code> argument to the <code>embed</code> expression in the above <code>embedding</code> field. The <code>e5</code> argument references a component of the type hugging-face-embedder. We configure the application package and its name with the <code>mail</code> schema and the <code>e5</code> embedder component.</p> In\u00a0[6]: Copied! <pre>from vespa.package import ApplicationPackage, Component, Parameter\n\nvespa_app_name = \"assistant\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name,\n    schema=[mail_schema, calendar_schema],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    name=\"transformer-model\",\n                    args={\n                        \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    name=\"tokenizer-model\",\n                    args={\n                        \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"\n                    },\n                ),\n                Parameter(\n                    name=\"prepend\",\n                    args={},\n                    children=[\n                        Parameter(name=\"query\", args={}, children=\"query: \"),\n                        Parameter(name=\"document\", args={}, children=\"passage: \"),\n                    ],\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import ApplicationPackage, Component, Parameter  vespa_app_name = \"assistant\" vespa_application_package = ApplicationPackage(     name=vespa_app_name,     schema=[mail_schema, calendar_schema],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     name=\"transformer-model\",                     args={                         \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"                     },                 ),                 Parameter(                     name=\"tokenizer-model\",                     args={                         \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"                     },                 ),                 Parameter(                     name=\"prepend\",                     args={},                     children=[                         Parameter(name=\"query\", args={}, children=\"query: \"),                         Parameter(name=\"document\", args={}, children=\"passage: \"),                     ],                 ),             ],         )     ], ) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the mail schema.</p> <p>Vespa supports phased ranking and has a rich set of built-in rank-features.</p> <p>Users can also define custom functions with ranking expressions.</p> In\u00a0[7]: Copied! <pre>from vespa.package import RankProfile, Function, GlobalPhaseRanking, FirstPhaseRanking\n\nkeywords_and_freshness = RankProfile(\n    name=\"default\",\n    functions=[\n        Function(\n            name=\"my_function\",\n            expression=\"nativeRank(subject) + nativeRank(body) + freshness(timestamp)\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(expression=\"my_function\", rank_score_drop_limit=0.02),\n    match_features=[\n        \"nativeRank(subject)\",\n        \"nativeRank(body)\",\n        \"my_function\",\n        \"freshness(timestamp)\",\n    ],\n)\n\nsemantic = RankProfile(\n    name=\"semantic\",\n    functions=[\n        Function(name=\"cosine\", expression=\"max(0,cos(distance(field, embedding)))\")\n    ],\n    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],\n    first_phase=FirstPhaseRanking(\n        expression=\"if(cosine &gt; query(threshold), cosine, -1)\",\n        rank_score_drop_limit=0.1,\n    ),\n    match_features=[\n        \"cosine\",\n        \"freshness(timestamp)\",\n        \"distance(field, embedding)\",\n        \"query(threshold)\",\n    ],\n)\n\nfusion = RankProfile(\n    name=\"fusion\",\n    inherits=\"semantic\",\n    functions=[\n        Function(\n            name=\"keywords_and_freshness\",\n            expression=\" nativeRank(subject) + nativeRank(body) + freshness(timestamp)\",\n        ),\n        Function(name=\"semantic\", expression=\"cos(distance(field,embedding))\"),\n    ],\n    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],\n    first_phase=FirstPhaseRanking(\n        expression=\"if(cosine &gt; query(threshold), cosine, -1)\",\n        rank_score_drop_limit=0.1,\n    ),\n    match_features=[\n        \"nativeRank(subject)\",\n        \"keywords_and_freshness\",\n        \"freshness(timestamp)\",\n        \"cosine\",\n        \"query(threshold)\",\n    ],\n    global_phase=GlobalPhaseRanking(\n        rerank_count=1000,\n        expression=\"reciprocal_rank_fusion(semantic, keywords_and_freshness)\",\n    ),\n)\n</pre> from vespa.package import RankProfile, Function, GlobalPhaseRanking, FirstPhaseRanking  keywords_and_freshness = RankProfile(     name=\"default\",     functions=[         Function(             name=\"my_function\",             expression=\"nativeRank(subject) + nativeRank(body) + freshness(timestamp)\",         )     ],     first_phase=FirstPhaseRanking(expression=\"my_function\", rank_score_drop_limit=0.02),     match_features=[         \"nativeRank(subject)\",         \"nativeRank(body)\",         \"my_function\",         \"freshness(timestamp)\",     ], )  semantic = RankProfile(     name=\"semantic\",     functions=[         Function(name=\"cosine\", expression=\"max(0,cos(distance(field, embedding)))\")     ],     inputs=[(\"query(q)\", \"tensor(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],     first_phase=FirstPhaseRanking(         expression=\"if(cosine &gt; query(threshold), cosine, -1)\",         rank_score_drop_limit=0.1,     ),     match_features=[         \"cosine\",         \"freshness(timestamp)\",         \"distance(field, embedding)\",         \"query(threshold)\",     ], )  fusion = RankProfile(     name=\"fusion\",     inherits=\"semantic\",     functions=[         Function(             name=\"keywords_and_freshness\",             expression=\" nativeRank(subject) + nativeRank(body) + freshness(timestamp)\",         ),         Function(name=\"semantic\", expression=\"cos(distance(field,embedding))\"),     ],     inputs=[(\"query(q)\", \"tensor(x[384])\"), (\"query(threshold)\", \"\", \"0.75\")],     first_phase=FirstPhaseRanking(         expression=\"if(cosine &gt; query(threshold), cosine, -1)\",         rank_score_drop_limit=0.1,     ),     match_features=[         \"nativeRank(subject)\",         \"keywords_and_freshness\",         \"freshness(timestamp)\",         \"cosine\",         \"query(threshold)\",     ],     global_phase=GlobalPhaseRanking(         rerank_count=1000,         expression=\"reciprocal_rank_fusion(semantic, keywords_and_freshness)\",     ), ) <p>The <code>default</code> rank profile defines a custom function <code>my_function</code> that computes a linear combination of three different features:</p> <ul> <li><code>nativeRank(subject)</code> Is a text matching feature , scoped to the <code>subject</code> field.</li> <li><code>nativeRank(body)</code> Same, but scoped to the <code>body</code> field.</li> <li><code>freshness(timestamp)</code> This is a built-in rank-feature that returns a number that is close to 1 if the timestamp is recent compared to the current query time.</li> </ul> In\u00a0[8]: Copied! <pre>mail_schema.add_rank_profile(keywords_and_freshness)\nmail_schema.add_rank_profile(semantic)\nmail_schema.add_rank_profile(fusion)\ncalendar_schema.add_rank_profile(keywords_and_freshness)\ncalendar_schema.add_rank_profile(semantic)\ncalendar_schema.add_rank_profile(fusion)\n</pre> mail_schema.add_rank_profile(keywords_and_freshness) mail_schema.add_rank_profile(semantic) mail_schema.add_rank_profile(fusion) calendar_schema.add_rank_profile(keywords_and_freshness) calendar_schema.add_rank_profile(semantic) calendar_schema.add_rank_profile(fusion) <p>Finally, we have our basic Vespa schema and application package.</p> <p>We can serialize the representation to application package files. This is handy when we want to start working with production deployments and when we want to manage the application with version control.</p> In\u00a0[9]: Copied! <pre>import os\n\napplication_directory = \"my-assistant-vespa-app\"\nvespa_application_package.to_files(application_directory)\n\n\ndef print_files_in_directory(directory):\n    for root, _, files in os.walk(directory):\n        for file in files:\n            print(os.path.join(root, file))\n\n\nprint_files_in_directory(application_directory)\n</pre> import os  application_directory = \"my-assistant-vespa-app\" vespa_application_package.to_files(application_directory)   def print_files_in_directory(directory):     for root, _, files in os.walk(directory):         for file in files:             print(os.path.join(root, file))   print_files_in_directory(application_directory) <pre>my-assistant-vespa-app/services.xml\nmy-assistant-vespa-app/schemas/mail.sd\nmy-assistant-vespa-app/schemas/calendar.sd\nmy-assistant-vespa-app/search/query-profiles/default.xml\nmy-assistant-vespa-app/search/query-profiles/types/root.xml\n</pre> In\u00a0[15]: Copied! <pre>from vespa.deployment import VespaCloud\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone. The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy(disk_folder=application_directory)\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy(disk_folder=application_directory) In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(f\"Error {response.url} : {response.get_json()}\")\n    else:\n        print(f\"Success {response.url}\")\n\n\napp.feed_iterable(\n    synthetic_mail_data_generator(),\n    schema=\"mail\",\n    namespace=\"assistant\",\n    callback=callback,\n)\napp.feed_iterable(\n    synthetic_calendar_data_generator(),\n    schema=\"calendar\",\n    namespace=\"assistant\",\n    callback=callback,\n)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(f\"Error {response.url} : {response.get_json()}\")     else:         print(f\"Success {response.url}\")   app.feed_iterable(     synthetic_mail_data_generator(),     schema=\"mail\",     namespace=\"assistant\",     callback=callback, ) app.feed_iterable(     synthetic_calendar_data_generator(),     schema=\"calendar\",     namespace=\"assistant\",     callback=callback, ) In\u00a0[18]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select subject, display_date, to from sources mail where userQuery()\",\n    query=\"when is my dentist appointment\",\n    groupname=\"bergum@vespa.ai\",\n    ranking=\"default\",\n    timeout=\"2s\",\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select subject, display_date, to from sources mail where userQuery()\",     query=\"when is my dentist appointment\",     groupname=\"bergum@vespa.ai\",     ranking=\"default\",     timeout=\"2s\", ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:assistant:mail:g=bergum@vespa.ai:2\",\n  \"relevance\": 1.134783932836458,\n  \"source\": \"assistant_content.mail\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"freshness(timestamp)\": 0.9232458847736625,\n      \"nativeRank(body)\": 0.09246780326887034,\n      \"nativeRank(subject)\": 0.11907024479392506,\n      \"my_function\": 1.134783932836458\n    },\n    \"subject\": \"Dentist Appointment Reminder\",\n    \"to\": \"bergum@vespa.ai\",\n    \"display_date\": \"2023-11-15T15:30:00Z\"\n  }\n}\n</pre> <p>For the above query request, Vespa searched the <code>default</code> fieldset which we defined in the schema to match against several fields including the body and the subject. The <code>default</code> rank-profile calculated the relevance score as the sum of three rank-features: <code>nativeRank(body)</code> + <code>nativeRank(subject)</code> + <code>freshness(</code>timestamp)<code>, and the result of this computation is the </code>relevance<code>score of the hit. In addition, we also asked for Vespa to return</code>matchfeatures<code>that are handy for debugging the final</code>relevance` score or for feature logging.</p> <p>Now, we can try the <code>semantic</code> ranking profile, using Vespa's support for nearestNeighbor search. This also exemplifies using the configured <code>e5</code> embedder to embed the user query into an embedding representation. See embedding a query text for more usage examples of using Vespa embedders.</p> In\u00a0[19]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select subject, display_date from mail where {targetHits:10}nearestNeighbor(embedding,q)\",\n    groupname=\"bergum@vespa.ai\",\n    ranking=\"semantic\",\n    body={\n        \"input.query(q)\": 'embed(e5, \"when is my dentist appointment\")',\n    },\n    timeout=\"2s\",\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select subject, display_date from mail where {targetHits:10}nearestNeighbor(embedding,q)\",     groupname=\"bergum@vespa.ai\",     ranking=\"semantic\",     body={         \"input.query(q)\": 'embed(e5, \"when is my dentist appointment\")',     },     timeout=\"2s\", ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:assistant:mail:g=bergum@vespa.ai:2\",\n  \"relevance\": 0.9079386507883569,\n  \"source\": \"assistant_content.mail\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"distance(field,embedding)\": 0.4324572498488368,\n      \"freshness(timestamp)\": 0.9232457561728395,\n      \"query(threshold)\": 0.75,\n      \"cosine\": 0.9079386507883569\n    },\n    \"subject\": \"Dentist Appointment Reminder\",\n    \"display_date\": \"2023-11-15T15:30:00Z\"\n  }\n}\n</pre> In\u00a0[\u00a0]: Copied! <pre>from llama_index.legacy.core.base_retriever import BaseRetriever\nfrom llama_index.legacy.schema import NodeWithScore, QueryBundle, TextNode\nfrom llama_index.legacy.callbacks.base import CallbackManager\n\nfrom vespa.application import Vespa\nfrom vespa.io import VespaQueryResponse\n\nfrom typing import List, Union, Optional\n\n\nclass PersonalAssistantVespaRetriever(BaseRetriever):\n    def __init__(\n        self,\n        app: Vespa,\n        user: str,\n        hits: int = 5,\n        vespa_rank_profile: str = \"default\",\n        vespa_score_cutoff: float = 0.70,\n        sources: List[str] = [\"mail\"],\n        fields: List[str] = [\"subject\", \"body\"],\n        callback_manager: Optional[CallbackManager] = None,\n    ) -&gt; None:\n        \"\"\"Sample Retriever for a personal assistant application.\n        Args:\n        param: app: Vespa application object\n        param: user: user id to retrieve documents for (used for Vespa streaming groupname)\n        param: hits: number of hits to retrieve from Vespa app\n        param: vespa_rank_profile: Vespa rank profile to use\n        param: vespa_score_cutoff: Vespa score cutoff to use during first-phase ranking\n        param: sources: sources to retrieve documents from\n        param: fields: fields to retrieve\n        \"\"\"\n\n        self.app = app\n        self.hits = hits\n        self.user = user\n        self.vespa_rank_profile = vespa_rank_profile\n        self.vespa_score_cutoff = vespa_score_cutoff\n        self.fields = fields\n        self.summary_fields = \",\".join(fields)\n        self.sources = \",\".join(sources)\n        super().__init__(callback_manager)\n\n    def _retrieve(self, query: Union[str, QueryBundle]) -&gt; List[NodeWithScore]:\n        \"\"\"Retrieve documents from Vespa application.\"\"\"\n        if isinstance(query, QueryBundle):\n            query = query.query_str\n\n        if self.vespa_rank_profile == \"default\":\n            yql: str = f\"select {self.summary_fields} from mail where userQuery()\"\n        else:\n            yql = f\"select {self.summary_fields} from sources {self.sources} where {{targetHits:10}}nearestNeighbor(embedding,q) or userQuery()\"\n        vespa_body_request = {\n            \"yql\": yql,\n            \"query\": query,\n            \"hits\": self.hits,\n            \"ranking.profile\": self.vespa_rank_profile,\n            \"timeout\": \"2s\",\n            \"input.query(threshold)\": self.vespa_score_cutoff,\n        }\n        if self.vespa_rank_profile != \"default\":\n            vespa_body_request[\"input.query(q)\"] = f'embed(e5, \"{query}\")'\n\n        with self.app.syncio(connections=1) as session:\n            response: VespaQueryResponse = session.query(\n                body=vespa_body_request, groupname=self.user\n            )\n            if not response.is_successful():\n                raise ValueError(\n                    f\"Query request failed: {response.status_code}, response payload: {response.get_json()}\"\n                )\n\n        nodes: List[NodeWithScore] = []\n        for hit in response.hits:\n            response_fields: dict = hit.get(\"fields\", {})\n            text: str = \"\"\n            for field in response_fields.keys():\n                if isinstance(response_fields[field], str) and field in self.fields:\n                    text += response_fields[field] + \" \"\n            id = hit[\"id\"]\n            #\n            doc = TextNode(\n                id_=id,\n                text=text,\n                metadata=response_fields,\n            )\n            nodes.append(NodeWithScore(node=doc, score=hit[\"relevance\"]))\n        return nodes\n</pre> from llama_index.legacy.core.base_retriever import BaseRetriever from llama_index.legacy.schema import NodeWithScore, QueryBundle, TextNode from llama_index.legacy.callbacks.base import CallbackManager  from vespa.application import Vespa from vespa.io import VespaQueryResponse  from typing import List, Union, Optional   class PersonalAssistantVespaRetriever(BaseRetriever):     def __init__(         self,         app: Vespa,         user: str,         hits: int = 5,         vespa_rank_profile: str = \"default\",         vespa_score_cutoff: float = 0.70,         sources: List[str] = [\"mail\"],         fields: List[str] = [\"subject\", \"body\"],         callback_manager: Optional[CallbackManager] = None,     ) -&gt; None:         \"\"\"Sample Retriever for a personal assistant application.         Args:         param: app: Vespa application object         param: user: user id to retrieve documents for (used for Vespa streaming groupname)         param: hits: number of hits to retrieve from Vespa app         param: vespa_rank_profile: Vespa rank profile to use         param: vespa_score_cutoff: Vespa score cutoff to use during first-phase ranking         param: sources: sources to retrieve documents from         param: fields: fields to retrieve         \"\"\"          self.app = app         self.hits = hits         self.user = user         self.vespa_rank_profile = vespa_rank_profile         self.vespa_score_cutoff = vespa_score_cutoff         self.fields = fields         self.summary_fields = \",\".join(fields)         self.sources = \",\".join(sources)         super().__init__(callback_manager)      def _retrieve(self, query: Union[str, QueryBundle]) -&gt; List[NodeWithScore]:         \"\"\"Retrieve documents from Vespa application.\"\"\"         if isinstance(query, QueryBundle):             query = query.query_str          if self.vespa_rank_profile == \"default\":             yql: str = f\"select {self.summary_fields} from mail where userQuery()\"         else:             yql = f\"select {self.summary_fields} from sources {self.sources} where {{targetHits:10}}nearestNeighbor(embedding,q) or userQuery()\"         vespa_body_request = {             \"yql\": yql,             \"query\": query,             \"hits\": self.hits,             \"ranking.profile\": self.vespa_rank_profile,             \"timeout\": \"2s\",             \"input.query(threshold)\": self.vespa_score_cutoff,         }         if self.vespa_rank_profile != \"default\":             vespa_body_request[\"input.query(q)\"] = f'embed(e5, \"{query}\")'          with self.app.syncio(connections=1) as session:             response: VespaQueryResponse = session.query(                 body=vespa_body_request, groupname=self.user             )             if not response.is_successful():                 raise ValueError(                     f\"Query request failed: {response.status_code}, response payload: {response.get_json()}\"                 )          nodes: List[NodeWithScore] = []         for hit in response.hits:             response_fields: dict = hit.get(\"fields\", {})             text: str = \"\"             for field in response_fields.keys():                 if isinstance(response_fields[field], str) and field in self.fields:                     text += response_fields[field] + \" \"             id = hit[\"id\"]             #             doc = TextNode(                 id_=id,                 text=text,                 metadata=response_fields,             )             nodes.append(NodeWithScore(node=doc, score=hit[\"relevance\"]))         return nodes <p>The above defines a <code>PersonalAssistantVespaRetriever</code> which accepts most importantly a pyvespa <code>Vespa</code> application instance.</p> <p>The YQL specifies a hybrid retrieval query that retrieves both using embedding-based retrieval (vector search) using Vespa's nearest neighbor search operator in combination with traditional keyword matching.</p> <p>With the above, we can connect to the running Vespa app and initialize the <code>PersonalAssistantVespaRetriever</code> for the user <code>bergum@vespa.ai</code>. The <code>user</code> argument maps to the streaming search groupname parameter.</p> In\u00a0[21]: Copied! <pre>retriever = PersonalAssistantVespaRetriever(\n    app=app, user=\"bergum@vespa.ai\", vespa_rank_profile=\"default\"\n)\nretriever.retrieve(\"When is my dentist appointment?\")\n</pre> retriever = PersonalAssistantVespaRetriever(     app=app, user=\"bergum@vespa.ai\", vespa_rank_profile=\"default\" ) retriever.retrieve(\"When is my dentist appointment?\") Out[21]: <pre>[NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:2', embedding=None, metadata={'matchfeatures': {'freshness(timestamp)': 0.9232454989711935, 'nativeRank(body)': 0.09246780326887034, 'nativeRank(subject)': 0.11907024479392506, 'my_function': 1.1347835470339889}, 'subject': 'Dentist Appointment Reminder', 'body': 'Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='269fe208f8d43a967dc683e1c9b832b18ddfb0b2efd801ab7e428620c8163021', text='Dentist Appointment Reminder Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=1.1347835470339889),\n NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:1', embedding=None, metadata={'matchfeatures': {'freshness(timestamp)': 0.9202362397119341, 'nativeRank(body)': 0.02919821398130037, 'nativeRank(subject)': 1.3512214436142505e-38, 'my_function': 0.9494344536932345}, 'subject': 'LlamaIndex news, 2023-11-14', 'body': \"Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \\n                    lane on our blog with twelve milestones from our first year. Be sure to check it out.\"}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='5e975eaece761d46956c9d301138f29b5c067d3da32fd013bb79c6ee9c033d3d', text=\"LlamaIndex news, 2023-11-14 Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \\n                    lane on our blog with twelve milestones from our first year. Be sure to check it out. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9494344536932345)]</pre> <p>These <code>NodeWithScore</code> retrieved <code>default</code> rank-profile can then be used for the next steps in a generative chain.</p> <p>We can also try the <code>semantic</code> rank-profile, which has rank-score-drop functionality, allowing us to have a per-query time threshold. Altering the threshold will remove context.</p> In\u00a0[22]: Copied! <pre>retriever = PersonalAssistantVespaRetriever(\n    app=app,\n    user=\"bergum@vespa.ai\",\n    vespa_rank_profile=\"semantic\",\n    vespa_score_cutoff=0.6,\n    hits=20,\n)\nretriever.retrieve(\"When is my dentist appointment?\")\n</pre> retriever = PersonalAssistantVespaRetriever(     app=app,     user=\"bergum@vespa.ai\",     vespa_rank_profile=\"semantic\",     vespa_score_cutoff=0.6,     hits=20, ) retriever.retrieve(\"When is my dentist appointment?\") Out[22]: <pre>[NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:2', embedding=None, metadata={'matchfeatures': {'distance(field,embedding)': 0.43945494361938975, 'freshness(timestamp)': 0.9232453703703704, 'query(threshold)': 0.6, 'cosine': 0.9049836898369259}, 'subject': 'Dentist Appointment Reminder', 'body': 'Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='e89f669e6c9cf64ab6a856d9857915481396e2aa84154951327cd889c23f7c4f', text='Dentist Appointment Reminder Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.9049836898369259),\n NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:1', embedding=None, metadata={'matchfeatures': {'distance(field,embedding)': 0.69930099954744, 'freshness(timestamp)': 0.9202361111111111, 'query(threshold)': 0.6, 'cosine': 0.7652923088511814}, 'subject': 'LlamaIndex news, 2023-11-14', 'body': \"Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \\n                    lane on our blog with twelve milestones from our first year. Be sure to check it out.\"}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='cb9b588e5b53dbdd0fbe6f7aadfa689d84a5bea23239293bd299347ee9ecd853', text=\"LlamaIndex news, 2023-11-14 Hello Llama Friends \ud83e\udd99 LlamaIndex is 1 year old this week! \ud83c\udf89 To celebrate, we're taking a stroll down memory \\n                    lane on our blog with twelve milestones from our first year. Be sure to check it out. \", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.7652923088511814)]</pre> <p>Create a new retriever with sources including both mail and calendar data:</p> In\u00a0[23]: Copied! <pre>retriever = PersonalAssistantVespaRetriever(\n    app=app,\n    user=\"bergum@vespa.ai\",\n    vespa_rank_profile=\"fusion\",\n    sources=[\"calendar\", \"mail\"],\n    vespa_score_cutoff=0.80,\n)\nretriever.retrieve(\"When is my dentist appointment?\")\n</pre> retriever = PersonalAssistantVespaRetriever(     app=app,     user=\"bergum@vespa.ai\",     vespa_rank_profile=\"fusion\",     sources=[\"calendar\", \"mail\"],     vespa_score_cutoff=0.80, ) retriever.retrieve(\"When is my dentist appointment?\") Out[23]: <pre>[NodeWithScore(node=TextNode(id_='id:assistant:calendar:g=bergum@vespa.ai:1', embedding=None, metadata={'matchfeatures': {'freshness(timestamp)': 0.9232447273662552, 'nativeRank(subject)': 0.11907024479392506, 'query(threshold)': 0.8, 'cosine': 0.8872983644178517, 'keywords_and_freshness': 1.1606592237923947}, 'subject': 'Dentist Appointment', 'body': 'Dentist appointment at 2023-12-04 at 09:30 - 1 hour duration'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='b30948011cbe9bbf29135384efbc72f85a6eb65113be0eb9762315a022f11ba1', text='Dentist Appointment Dentist appointment at 2023-12-04 at 09:30 - 1 hour duration ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.03278688524590164),\n NodeWithScore(node=TextNode(id_='id:assistant:mail:g=bergum@vespa.ai:2', embedding=None, metadata={'matchfeatures': {'freshness(timestamp)': 0.9232447273662552, 'nativeRank(subject)': 0.11907024479392506, 'query(threshold)': 0.8, 'cosine': 0.9049836898369259, 'keywords_and_freshness': 1.1347827754290507}, 'subject': 'Dentist Appointment Reminder', 'body': 'Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist'}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, hash='21c501ccdc6e4b33d388eefa244c5039a0e1ed4b81e4f038916765e22be24705', text='Dentist Appointment Reminder Dear Jo Kristian ,\\nThis is a reminder for your upcoming dentist appointment on 2023-12-04 at 09:30. Please arrive 15 minutes early.\\nBest regards,\\nDr. Dentist ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.03278688524590164)]</pre> In\u00a0[24]: Copied! <pre>app.query(\n    yql=\"select subject, display_date from calendar where duration &gt; 0\",\n    ranking=\"default\",\n    groupname=\"bergum@vespa.ai\",\n    timeout=\"2s\",\n).json\n</pre> app.query(     yql=\"select subject, display_date from calendar where duration &gt; 0\",     ranking=\"default\",     groupname=\"bergum@vespa.ai\",     timeout=\"2s\", ).json Out[24]: <pre>{'root': {'id': 'toplevel',\n  'relevance': 1.0,\n  'fields': {'totalCount': 2},\n  'coverage': {'coverage': 100,\n   'documents': 2,\n   'full': True,\n   'nodes': 1,\n   'results': 1,\n   'resultsFull': 1},\n  'children': [{'id': 'id:assistant:calendar:g=bergum@vespa.ai:2',\n    'relevance': 0.987133487654321,\n    'source': 'assistant_content.calendar',\n    'fields': {'matchfeatures': {'freshness(timestamp)': 0.987133487654321,\n      'nativeRank(body)': 0.0,\n      'nativeRank(subject)': 0.0,\n      'my_function': 0.987133487654321},\n     'subject': 'Public Cloud Platform Events',\n     'display_date': '2023-11-21T09:30:00Z'}},\n   {'id': 'id:assistant:calendar:g=bergum@vespa.ai:1',\n    'relevance': 0.9232445987654321,\n    'source': 'assistant_content.calendar',\n    'fields': {'matchfeatures': {'freshness(timestamp)': 0.9232445987654321,\n      'nativeRank(body)': 0.0,\n      'nativeRank(subject)': 0.0,\n      'my_function': 0.9232445987654321},\n     'subject': 'Dentist Appointment',\n     'display_date': '2023-11-15T15:30:00Z'}}]}}</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#building-cost-efficient-retrieval-augmented-personal-ai-assistants","title":"Building cost-efficient retrieval-augmented personal AI assistants\u00b6","text":"<p>This notebook demonstrates how to use Vespa streaming mode for cost-efficient retrieval for applications that store and retrieve personal data. You can read more about Vespa vector streaming search in these two blog posts:</p> <ul> <li>Announcing vector streaming search: AI assistants at scale without breaking the bank</li> <li>Yahoo Mail turns to Vespa to do RAG at scale</li> </ul>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#a-summary-of-vespa-streaming-mode","title":"A summary of Vespa streaming mode\u00b6","text":"<p>Vespa\u2019s streaming search solution lets you make the user id a part of the document ID so that Vespa can use it to co-locate the data of each user on a small set of nodes and the same chunk of disk. This allows you to do searches over a user\u2019s data with low latency without keeping any user\u2019s data in memory or paying the cost of managing indexes.</p> <ul> <li>There is no accuracy drop for vector search as it uses exact vector search</li> <li>Several orders of magnitude higher throughput (No expensive index builds to support approximate search)</li> <li>Documents (including vector data) are disk-based.</li> <li>Ultra-low memory requirements (fixed per document)</li> </ul> <p>This notebook connects a custom LlamaIndex Retriever with a Vespa app using streaming mode to retrieve personal data. The focus is on how to use the streaming mode feature.</p> <p></p> <p>First, install dependencies:</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#synthetic-mail-calendar-data","title":"Synthetic Mail &amp; Calendar Data\u00b6","text":"<p>There are few public email datasets because people care about their privacy, so this notebook uses synthetic data to examine how to use Vespa streaming mode. We create two generator functions that returns Python <code>dict</code>s with synthetic mail and calendar data.</p> <p>Notice that the dict has three keys:</p> <ul> <li><code>id</code></li> <li><code>groupname</code></li> <li><code>fields</code></li> </ul> <p>This is the expected feed format for PyVespa feed operations and where PyVespa will use these to build a Vespa document v1 API request(s). The <code>groupname</code> key is only to be used when using streaming mode.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#defining-a-vespa-application","title":"Defining a Vespa application\u00b6","text":"<p>PyVespa help us build the Vespa application package. A Vespa application package consists of configuration files.</p> <p>First, we define a Vespa schema. PyVespa offers a programmatic API for creating the schema. In the end, it is serialized to a file (<code>&lt;schema&gt;.sd</code>) before it can be deployed to Vespa.</p> <p>Vespa is statically typed, so we need to define the fields and their type in the schema before we can start feeding documents. Note that we set <code>mode</code> to <code>streaming</code> which enables Vespa streaming mode for this schema. Other valid modes are <code>indexed</code> and <code>store-only</code>.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#feeding-data-to-vespa","title":"Feeding data to Vespa\u00b6","text":"<p>With the app up and running in Vespa Cloud, we can start feeding and querying our data.</p> <p>We use the feed_iterable API of pyvespa with a custom <code>callback</code> that prints the URL and an error if the operation fails.</p> <p>We pass the <code>synthetic_*generator()</code> and call <code>feed_iterable</code> with the specific <code>schema</code> and <code>namespace</code>.</p> <p>Read more about Vespa document IDs.</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now, we can also query our data. With streaming mode, we must pass the <code>groupname</code> parameter, or the request will fail with an error.</p> <p>The query request uses the Vespa Query API and the <code>Vespa.query()</code> function supports passing any of the Vespa query API parameters.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul> <p>Sample query request for <code>when is my dentist appointment</code> for the user <code>bergum@vespa.ai</code>:</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#llamaindex-retrievers-introduction","title":"LlamaIndex Retrievers Introduction\u00b6","text":"<p>Now, we have a basic Vespa app using streaming mode. We likely want to use an LLM framework like\u00a0LangChain or LLamaIndex to build an end-to-end assistant. In this example notebook, we use LLamaIndex retrievers.</p> <p>LlamaIndex retriever abstraction allows developers to add custom retrievers that retrieve information in Retrieval Augmented Generation (RAG) pipelines.</p> <p>For an excellent introduction to LLamaIndex and its concepts, see LLamaIndex High-Level Concepts.</p> <p>To create a custom LlamaIndex retrieve, we implement a class that inherits from <code>llama_index.retrievers.BaseRetriever.BaseRetriever</code> and which implements <code>_retrieve(query)</code>.</p> <p>A simple <code>PersonalAssistantVespaRetriever</code> could look like the following:</p>"},{"location":"examples/scaling-personal-ai-assistants-with-streaming-mode-cloud.html#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we have demonstrated:</p> <ul> <li>Configuring and using Vespa's streaming mode</li> <li>Using multiple document types and schema to organize our data</li> <li>Running embedding inference in Vespa</li> <li>Hybrid retrieval techniques - combined with score thresholding to filter irrelevant contexts</li> <li>Creating a custom LLamaIndex retriever and connecting it with our Vespa app</li> <li>Vespa Cloud deployments to sandbox/dev zone</li> </ul> <p>We can now delete the cloud instance:</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html","title":"simplified retrieval with colpali vlm Vespa cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y\n</pre> !sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y <p>Now install the required python packages:</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install transformers==4.51.3 accelerate vidore_benchmark==4.0.0 pdf2image pypdf==5.0.1 pyvespa vespacli requests numpy\n</pre> !pip3 install transformers==4.51.3 accelerate vidore_benchmark==4.0.0 pdf2image pypdf==5.0.1 pyvespa vespacli requests numpy In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom io import BytesIO\n\nfrom transformers import ColPaliForRetrieval, ColPaliProcessor\nfrom vidore_benchmark.utils.image_utils import scale_image, get_base64_image\n</pre> import torch from torch.utils.data import DataLoader from tqdm import tqdm from io import BytesIO  from transformers import ColPaliForRetrieval, ColPaliProcessor from vidore_benchmark.utils.image_utils import scale_image, get_base64_image <p>Choose the right device to run the model.</p> In\u00a0[\u00a0]: Copied! <pre># Load model (bfloat16 support is limited; fallback to float32 if needed)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nif torch.backends.mps.is_available():\n    device = \"mps\"  # For Apple Silicon devices\ndtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n</pre> # Load model (bfloat16 support is limited; fallback to float32 if needed) device = \"cuda\" if torch.cuda.is_available() else \"cpu\" if torch.backends.mps.is_available():     device = \"mps\"  # For Apple Silicon devices dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32 <p>Load the model and the processor.</p> In\u00a0[\u00a0]: Copied! <pre>model_name = \"vidore/colpali-v1.2-hf\"\nmodel = ColPaliForRetrieval.from_pretrained(\n    model_name,\n    torch_dtype=dtype,\n    device_map=device,  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\n).eval()\n\nprocessor = ColPaliProcessor.from_pretrained(model_name)\n</pre> model_name = \"vidore/colpali-v1.2-hf\" model = ColPaliForRetrieval.from_pretrained(     model_name,     torch_dtype=dtype,     device_map=device,  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon ).eval()  processor = ColPaliProcessor.from_pretrained(model_name) In\u00a0[\u00a0]: Copied! <pre>import requests\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader\n\n\ndef download_pdf(url):\n    response = requests.get(url)\n    if response.status_code == 200:\n        return BytesIO(response.content)\n    else:\n        raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")\n\n\ndef get_pdf_images(pdf_url):\n    # Download the PDF\n    pdf_file = download_pdf(pdf_url)\n    # Save the PDF temporarily to disk (pdf2image requires a file path)\n    temp_file = \"temp.pdf\"\n    with open(temp_file, \"wb\") as f:\n        f.write(pdf_file.read())\n    reader = PdfReader(temp_file)\n    page_texts = []\n    for page_number in range(len(reader.pages)):\n        page = reader.pages[page_number]\n        text = page.extract_text()\n        page_texts.append(text)\n    images = convert_from_path(temp_file)\n    assert len(images) == len(page_texts)\n    return (images, page_texts)\n</pre> import requests from pdf2image import convert_from_path from pypdf import PdfReader   def download_pdf(url):     response = requests.get(url)     if response.status_code == 200:         return BytesIO(response.content)     else:         raise Exception(f\"Failed to download PDF: Status code {response.status_code}\")   def get_pdf_images(pdf_url):     # Download the PDF     pdf_file = download_pdf(pdf_url)     # Save the PDF temporarily to disk (pdf2image requires a file path)     temp_file = \"temp.pdf\"     with open(temp_file, \"wb\") as f:         f.write(pdf_file.read())     reader = PdfReader(temp_file)     page_texts = []     for page_number in range(len(reader.pages)):         page = reader.pages[page_number]         text = page.extract_text()         page_texts.append(text)     images = convert_from_path(temp_file)     assert len(images) == len(page_texts)     return (images, page_texts) <p>We define a few sample PDFs to work with. The PDFs are discovered from this url.</p> In\u00a0[\u00a0]: Copied! <pre>sample_pdfs = [\n    {\n        \"title\": \"ConocoPhillips Sustainability Highlights - Nature (24-0976)\",\n        \"url\": \"https://static.conocophillips.com/files/resources/24-0976-sustainability-highlights_nature.pdf\",\n    },\n    {\n        \"title\": \"ConocoPhillips Managing Climate Related Risks\",\n        \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-managing-climate-related-risks.pdf\",\n    },\n    {\n        \"title\": \"ConocoPhillips 2023 Sustainability Report\",\n        \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-sustainability-report.pdf\",\n    },\n]\n</pre> sample_pdfs = [     {         \"title\": \"ConocoPhillips Sustainability Highlights - Nature (24-0976)\",         \"url\": \"https://static.conocophillips.com/files/resources/24-0976-sustainability-highlights_nature.pdf\",     },     {         \"title\": \"ConocoPhillips Managing Climate Related Risks\",         \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-managing-climate-related-risks.pdf\",     },     {         \"title\": \"ConocoPhillips 2023 Sustainability Report\",         \"url\": \"https://static.conocophillips.com/files/resources/conocophillips-2023-sustainability-report.pdf\",     }, ] <p>Now we can convert the PDFs to images and also extract the text content.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_images, page_texts = get_pdf_images(pdf[\"url\"])\n\n    pdf[\"images\"] = page_images\n    pdf[\"texts\"] = page_texts\n</pre> for pdf in sample_pdfs:     page_images, page_texts = get_pdf_images(pdf[\"url\"])      pdf[\"images\"] = page_images     pdf[\"texts\"] = page_texts <p>Let us look at the extracted image of the first PDF page. This is the document side input to ColPali, one image per page.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display\n\ndisplay(scale_image(sample_pdfs[0][\"images\"][0], 720))\n</pre> from IPython.display import display  display(scale_image(sample_pdfs[0][\"images\"][0], 720)) <p>Let us also look at the extracted text content of the first PDF page.</p> In\u00a0[\u00a0]: Copied! <pre>print(sample_pdfs[0][\"texts\"][0])\n</pre> print(sample_pdfs[0][\"texts\"][0]) <p>Notice how the layout and order of the text is different from the image representation. Note that</p> <ul> <li>The headlines NATURE and Sustainability have been combined into one word (NATURESustainability).</li> <li>The 0.03% has been converted to 0.03 and order is not preserved in the text representation.</li> <li>The data in the infographics is not represented in the text representation. For example the source water distribution in the infographics is not represented in the text representation.</li> </ul> <p>Now we use the ColPali model to generate embeddings of the images.</p> In\u00a0[\u00a0]: Copied! <pre>for pdf in sample_pdfs:\n    page_embeddings = []\n    dataloader = DataLoader(\n        pdf[\"images\"],\n        batch_size=2,\n        shuffle=False,\n        collate_fn=lambda x: processor(images=x, return_tensors=\"pt\"),\n    )\n    for batch_doc in tqdm(dataloader):\n        with torch.no_grad():\n            batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n            embeddings_doc = model(**batch_doc).embeddings\n            page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n    pdf[\"embeddings\"] = page_embeddings\n</pre> for pdf in sample_pdfs:     page_embeddings = []     dataloader = DataLoader(         pdf[\"images\"],         batch_size=2,         shuffle=False,         collate_fn=lambda x: processor(images=x, return_tensors=\"pt\"),     )     for batch_doc in tqdm(dataloader):         with torch.no_grad():             batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}             embeddings_doc = model(**batch_doc).embeddings             page_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))     pdf[\"embeddings\"] = page_embeddings <p>Now we are done with the document side embeddings, we now convert the embeddings to Vespa JSON format so we can store (and index) them in Vespa. Details in Vespa JSON feed format doc.</p> <p>We use binary quantization (BQ) of the page level ColPali vector embeddings to reduce their size by 32x.</p> <p>Read more about binarization of multi-vector representations in the colbert blog post.</p> <p>The binarization step maps 128 dimensional floats to 128 bits, or 16 bytes per vector. Reducing the size by 32x. On the DocVQA benchmark, binarization results in a small drop in ranking accuracy.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nvespa_feed = []\nfor pdf in sample_pdfs:\n    url = pdf[\"url\"]\n    title = pdf[\"title\"]\n    for page_number, (page_text, embedding, image) in enumerate(\n        zip(pdf[\"texts\"], pdf[\"embeddings\"], pdf[\"images\"])\n    ):\n        base_64_image = get_base64_image(scale_image(image, 640), add_url_prefix=False)\n        embedding_dict = dict()\n        for idx, patch_embedding in enumerate(embedding):\n            binary_vector = (\n                np.packbits(np.where(patch_embedding &gt; 0, 1, 0))\n                .astype(np.int8)\n                .tobytes()\n                .hex()\n            )\n            embedding_dict[idx] = binary_vector\n        page = {\n            \"id\": hash(url + str(page_number)),\n            \"fields\": {\n                \"url\": url,\n                \"title\": title,\n                \"page_number\": page_number,\n                \"image\": base_64_image,\n                \"text\": page_text,\n                \"embedding\": embedding_dict,\n            },\n        }\n        vespa_feed.append(page)\n</pre> import numpy as np  vespa_feed = [] for pdf in sample_pdfs:     url = pdf[\"url\"]     title = pdf[\"title\"]     for page_number, (page_text, embedding, image) in enumerate(         zip(pdf[\"texts\"], pdf[\"embeddings\"], pdf[\"images\"])     ):         base_64_image = get_base64_image(scale_image(image, 640), add_url_prefix=False)         embedding_dict = dict()         for idx, patch_embedding in enumerate(embedding):             binary_vector = (                 np.packbits(np.where(patch_embedding &gt; 0, 1, 0))                 .astype(np.int8)                 .tobytes()                 .hex()             )             embedding_dict[idx] = binary_vector         page = {             \"id\": hash(url + str(page_number)),             \"fields\": {                 \"url\": url,                 \"title\": title,                 \"page_number\": page_number,                 \"image\": base_64_image,                 \"text\": page_text,                 \"embedding\": embedding_dict,             },         }         vespa_feed.append(page) In\u00a0[\u00a0]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\ncolpali_schema = Schema(\n    name=\"pdf_page\",\n    document=Document(\n        fields=[\n            Field(\n                name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"], match=[\"word\"]\n            ),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(name=\"image\", type=\"raw\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;int8&gt;(patch{}, v[16])\",\n                indexing=[\n                    \"attribute\",\n                    \"index\",\n                ],  # adds HNSW index for candidate retrieval.\n                ann=HNSW(\n                    distance_metric=\"hamming\",\n                    max_links_per_node=32,\n                    neighbors_to_explore_at_insert=400,\n                ),\n            ),\n        ]\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  colpali_schema = Schema(     name=\"pdf_page\",     document=Document(         fields=[             Field(                 name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"], match=[\"word\"]             ),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(name=\"image\", type=\"raw\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"embedding\",                 type=\"tensor(patch{}, v[16])\",                 indexing=[                     \"attribute\",                     \"index\",                 ],  # adds HNSW index for candidate retrieval.                 ann=HNSW(                     distance_metric=\"hamming\",                     max_links_per_node=32,                     neighbors_to_explore_at_insert=400,                 ),             ),         ]     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"title\", \"text\"])], ) <p>Notice the <code>embedding</code> field which is a tensor field with the type <code>tensor&lt;int8&gt;(patch{}, v[16])</code>. This is the field we use to represent the patch embeddings from ColPali.</p> <p>We also enable HNSW indexing for this field to enable fast nearest neighbor search which is used for candidate retrieval.</p> <p>We use binary hamming distance as an approximation of the cosine similarity. Hamming distance is a good approximation for binary representations, and it is much faster to compute than cosine similarity/dot product.</p> <p>The <code>embedding</code> field is an example of a mixed tensor where we combine one mapped (sparse) dimensions with a dense dimension.</p> <p>Read more in Tensor guide. We also enable BM25 for the <code>title</code> and <code>texts</code>\u00a0fields. Notice that the <code>image</code> field use type <code>raw</code> to store the binary image data, encoded with as a base64 string.</p> <p>Create the Vespa application package:</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import ApplicationPackage\n\nvespa_app_name = \"visionrag5\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name, schema=[colpali_schema]\n)\n</pre> from vespa.package import ApplicationPackage  vespa_app_name = \"visionrag5\" vespa_application_package = ApplicationPackage(     name=vespa_app_name, schema=[colpali_schema] ) <p>Now we define how we want to rank the pages for a query. We use Vespa's support for BM25 for the text, and late interaction with Max Sim for the image embeddings.</p> <p>This means that we use the the text representations as a candidate retrieval phase,  then we use the ColPALI embeddings with MaxSim to rerank the pages.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ncolpali_profile = RankProfile(\n    name=\"default\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(text)\"),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"bm25_score\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=100),\n)\ncolpali_schema.add_rank_profile(colpali_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  colpali_profile = RankProfile(     name=\"default\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(name=\"bm25_score\", expression=\"bm25(title) + bm25(text)\"),     ],     first_phase=FirstPhaseRanking(expression=\"bm25_score\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=100), ) colpali_schema.add_rank_profile(colpali_profile) <p>The first phase uses a linear combination of BM25 scores for the text fields, and the second phase uses the MaxSim function with the image embeddings. Notice that Vespa supports a <code>unpack_bits</code> function to convert the 16 compressed binary vectors to 128-dimensional floats for the MaxSim function. The query input tensor is not compressed and using full float resolution.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD testing of this notebook. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() In\u00a0[\u00a0]: Copied! <pre>print(\"Number of PDF pages:\", len(vespa_feed))\n</pre> print(\"Number of PDF pages:\", len(vespa_feed)) <p>Index the documents in Vespa using the Vespa HTTP API.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\n# Feed data into Vespa synchronously\napp.feed_iterable(vespa_feed, schema=\"pdf_page\", callback=callback)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   # Feed data into Vespa synchronously app.feed_iterable(vespa_feed, schema=\"pdf_page\", callback=callback) <p>Now we can query Vespa with the text query and rerank the results using the ColPali embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>queries = [\n    \"Percentage of non-fresh water as source?\",\n    \"Policies related to nature risk?\",\n    \"How much of produced water is recycled?\",\n]\ndataloader = DataLoader(\n    queries,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"),\n)\nqs = []\nfor batch_query in dataloader:\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query).embeddings\n        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> queries = [     \"Percentage of non-fresh water as source?\",     \"Policies related to nature risk?\",     \"How much of produced water is recycled?\", ] dataloader = DataLoader(     queries,     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"), ) qs = [] for batch_query in dataloader:     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query).embeddings         qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <p>Obtain the query embeddings using the ColPali model:</p> In\u00a0[\u00a0]: Copied! <pre>dataloader = DataLoader(\n    queries,\n    batch_size=1,\n    shuffle=False,\n    collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"),\n)\nqs = []\nfor batch_query in dataloader:\n    with torch.no_grad():\n        batch_query = {k: v.to(model.device) for k, v in batch_query.items()}\n        embeddings_query = model(**batch_query).embeddings\n        qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\"))))\n</pre> dataloader = DataLoader(     queries,     batch_size=1,     shuffle=False,     collate_fn=lambda x: processor(text=x, return_tensors=\"pt\"), ) qs = [] for batch_query in dataloader:     with torch.no_grad():         batch_query = {k: v.to(model.device) for k, v in batch_query.items()}         embeddings_query = model(**batch_query).embeddings         qs.extend(list(torch.unbind(embeddings_query.to(\"cpu\")))) <p>We create a simple routine to display the results. We render the image and the title of the retrieved page/document.</p> In\u00a0[\u00a0]: Copied! <pre>from IPython.display import display, HTML\n\n\ndef display_query_results(query, response, hits=5):\n    query_time = response.json.get(\"timing\", {}).get(\"searchtime\", -1)\n    query_time = round(query_time, 2)\n    count = response.json.get(\"root\", {}).get(\"fields\", {}).get(\"totalCount\", 0)\n    html_content = f\"&lt;h3&gt;Query text: '{query}', query time {query_time}s, count={count}, top results:&lt;/h3&gt;\"\n\n    for i, hit in enumerate(response.hits[:hits]):\n        title = hit[\"fields\"][\"title\"]\n        url = hit[\"fields\"][\"url\"]\n        page = hit[\"fields\"][\"page_number\"]\n        image = hit[\"fields\"][\"image\"]\n        score = hit[\"relevance\"]\n\n        html_content += f\"&lt;h4&gt;PDF Result {i + 1}&lt;/h4&gt;\"\n        html_content += f'&lt;p&gt;&lt;strong&gt;Title:&lt;/strong&gt; &lt;a href=\"{url}\"&gt;{title}&lt;/a&gt;, page {page+1} with score {score:.2f}&lt;/p&gt;'\n        html_content += (\n            f'&lt;img src=\"data:image/png;base64,{image}\" style=\"max-width:100%;\"&gt;'\n        )\n\n    display(HTML(html_content))\n</pre> from IPython.display import display, HTML   def display_query_results(query, response, hits=5):     query_time = response.json.get(\"timing\", {}).get(\"searchtime\", -1)     query_time = round(query_time, 2)     count = response.json.get(\"root\", {}).get(\"fields\", {}).get(\"totalCount\", 0)     html_content = f\"Query text: '{query}', query time {query_time}s, count={count}, top results:\"      for i, hit in enumerate(response.hits[:hits]):         title = hit[\"fields\"][\"title\"]         url = hit[\"fields\"][\"url\"]         page = hit[\"fields\"][\"page_number\"]         image = hit[\"fields\"][\"image\"]         score = hit[\"relevance\"]          html_content += f\"PDF Result {i + 1}\"         html_content += f'<p>Title: {title}, page {page+1} with score {score:.2f}</p>'         html_content += (             f''         )      display(HTML(html_content)) <p>Query Vespa with the queries and display the results, here we are using the <code>default</code> rank profile.</p> <p>Note that we retrieve using textual representation with <code>userInput(@userQuery)</code>, this means that we use the BM25 ranking for the extracted text in the first ranking phase and then re-rank the top-k pages using the ColPali embeddings.</p> <p>Later in this notebook we will use Vespa's support for approximate nearest neighbor search (<code>nearestNeighbor</code>) to retrieve directly using the ColPali embeddings.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\nasync with app.asyncio(connections=1, timeout=120) as session:\n    for idx, query in enumerate(queries):\n        query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}\n        response: VespaQueryResponse = await session.query(\n            yql=\"select title,url,image,page_number from pdf_page where userInput(@userQuery)\",\n            ranking=\"default\",\n            userQuery=query,\n            timeout=120,\n            hits=3,\n            body={\"input.query(qt)\": query_embedding, \"presentation.timing\": True},\n        )\n        assert response.is_successful()\n        display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  async with app.asyncio(connections=1, timeout=120) as session:     for idx, query in enumerate(queries):         query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}         response: VespaQueryResponse = await session.query(             yql=\"select title,url,image,page_number from pdf_page where userInput(@userQuery)\",             ranking=\"default\",             userQuery=query,             timeout=120,             hits=3,             body={\"input.query(qt)\": query_embedding, \"presentation.timing\": True},         )         assert response.is_successful()         display_query_results(query, response) In\u00a0[\u00a0]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking\n\ninput_query_tensors = []\nMAX_QUERY_TERMS = 64\nfor i in range(MAX_QUERY_TERMS):\n    input_query_tensors.append((f\"query(rq{i})\", \"tensor&lt;int8&gt;(v[16])\"))\n\ninput_query_tensors.append((\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"))\ninput_query_tensors.append((\"query(qtb)\", \"tensor&lt;int8&gt;(querytoken{}, v[16])\"))\n\ncolpali_retrieval_profile = RankProfile(\n    name=\"retrieval-and-rerank\",\n    inputs=input_query_tensors,\n    functions=[\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)) , v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim_binary\",\n            expression=\"\"\"\n                sum(\n                  reduce(\n                    1/(1 + sum(\n                        hamming(query(qtb), attribute(embedding)) ,v)\n                    ),\n                    max,\n                    patch\n                  ),\n                  querytoken\n                )\n            \"\"\",\n        ),\n    ],\n    first_phase=FirstPhaseRanking(expression=\"max_sim_binary\"),\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),\n)\ncolpali_schema.add_rank_profile(colpali_retrieval_profile)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking, SecondPhaseRanking  input_query_tensors = [] MAX_QUERY_TERMS = 64 for i in range(MAX_QUERY_TERMS):     input_query_tensors.append((f\"query(rq{i})\", \"tensor(v[16])\"))  input_query_tensors.append((\"query(qt)\", \"tensor(querytoken{}, v[128])\")) input_query_tensors.append((\"query(qtb)\", \"tensor(querytoken{}, v[16])\"))  colpali_retrieval_profile = RankProfile(     name=\"retrieval-and-rerank\",     inputs=input_query_tensors,     functions=[         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)) , v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim_binary\",             expression=\"\"\"                 sum(                   reduce(                     1/(1 + sum(                         hamming(query(qtb), attribute(embedding)) ,v)                     ),                     max,                     patch                   ),                   querytoken                 )             \"\"\",         ),     ],     first_phase=FirstPhaseRanking(expression=\"max_sim_binary\"),     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10), ) colpali_schema.add_rank_profile(colpali_retrieval_profile) <p>We define two functions, one for the first phase and one for the second phase. Instead of the float representations, we use the binary representations with inverted hamming distance in the first phase. Now, we need to re-deploy the application to Vespa Cloud.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <p>Now we can query Vespa with the text queries and use the <code>nearestNeighbor</code> operator to retrieve the most similar pages to the query and pass the different query tensors.</p> In\u00a0[\u00a0]: Copied! <pre>from vespa.io import VespaQueryResponse\n\ntarget_hits_per_query_tensor = (\n    20  # this is a hyper parameter that can be tuned for speed versus accuracy\n)\nasync with app.asyncio(connections=1, timeout=180) as session:\n    for idx, query in enumerate(queries):\n        float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}\n        binary_query_embeddings = dict()\n        for k, v in float_query_embedding.items():\n            binary_query_embeddings[k] = (\n                np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()\n            )\n\n        # The mixed tensors used in MaxSim calculations\n        # We use both binary and float representations\n        query_tensors = {\n            \"input.query(qtb)\": binary_query_embeddings,\n            \"input.query(qt)\": float_query_embedding,\n        }\n        # The query tensors used in the nearest neighbor calculations\n        for i in range(0, len(binary_query_embeddings)):\n            query_tensors[f\"input.query(rq{i})\"] = binary_query_embeddings[i]\n        nn = []\n        for i in range(0, len(binary_query_embeddings)):\n            nn.append(\n                f\"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))\"\n            )\n        # We use a OR operator to combine the nearest neighbor operator\n        nn = \" OR \".join(nn)\n        response: VespaQueryResponse = await session.query(\n            yql=f\"select title, url, image, page_number from pdf_page where {nn}\",\n            ranking=\"retrieval-and-rerank\",\n            timeout=120,\n            hits=3,\n            body={**query_tensors, \"presentation.timing\": True},\n        )\n        assert response.is_successful()\n        display_query_results(query, response)\n</pre> from vespa.io import VespaQueryResponse  target_hits_per_query_tensor = (     20  # this is a hyper parameter that can be tuned for speed versus accuracy ) async with app.asyncio(connections=1, timeout=180) as session:     for idx, query in enumerate(queries):         float_query_embedding = {k: v.tolist() for k, v in enumerate(qs[idx])}         binary_query_embeddings = dict()         for k, v in float_query_embedding.items():             binary_query_embeddings[k] = (                 np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()             )          # The mixed tensors used in MaxSim calculations         # We use both binary and float representations         query_tensors = {             \"input.query(qtb)\": binary_query_embeddings,             \"input.query(qt)\": float_query_embedding,         }         # The query tensors used in the nearest neighbor calculations         for i in range(0, len(binary_query_embeddings)):             query_tensors[f\"input.query(rq{i})\"] = binary_query_embeddings[i]         nn = []         for i in range(0, len(binary_query_embeddings)):             nn.append(                 f\"({{targetHits:{target_hits_per_query_tensor}}}nearestNeighbor(embedding,rq{i}))\"             )         # We use a OR operator to combine the nearest neighbor operator         nn = \" OR \".join(nn)         response: VespaQueryResponse = await session.query(             yql=f\"select title, url, image, page_number from pdf_page where {nn}\",             ranking=\"retrieval-and-rerank\",             timeout=120,             hits=3,             body={**query_tensors, \"presentation.timing\": True},         )         assert response.is_successful()         display_query_results(query, response) <p>Depending on the scale, we can evaluate changing different number of targetHits per nearestNeighbor operator and the ranking depths in the two phases. We can also parallelize the ranking phases by using more threads per query request to reduce latency.</p> In\u00a0[\u00a0]: Copied! <pre>if os.getenv(\"CI\", \"false\") == \"true\":\n    vespa_cloud.delete()\n</pre> if os.getenv(\"CI\", \"false\") == \"true\":     vespa_cloud.delete()"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#scaling-colpali-vlm-retrieval","title":"Scaling ColPALI (VLM) Retrieval\u00b6","text":"<p>This notebook demonstrates how to represent ColPali in Vespa and to scale to large collections. Also see the blog post: Scaling ColPali to billions of PDFs with Vespa</p> <p>Consider following the ColQWen2 notebook instead as it use a better model with improved performance (Both accuracy and speed).</p> <p>ColPali is a powerful visual language model that can generate embeddings for images (screenshots of PDF pages) and text queries.</p> <p>In this notebook, we will use ColPali to generate embeddings for images of PDF pages and store the embeddings in Vespa. We will also store the base64 encoded image of the PDF page and meta data like title and url.</p> <p>We demonstrate how to retrieve relevant pages for a query using the embeddings generated by ColPali.</p> <p>The TLDR of this notebook:</p> <ul> <li>Generate an image per PDF page using pdf2image and also extract the text using pypdf.</li> <li>For each page image, use ColPali to obtain the visual multi-vector embeddings</li> </ul> <p>Then we store visual embeddings in Vespa as a <code>int8</code> tensor, where we use a binary compression technique to reduce the storage footprint by 32x compared to float representations. See Scaling ColPali to billions of PDFs with Vespa for details on binarization and using hamming distance for retrieval.</p> <p>During retrieval time, we use the same ColPali model to generate embeddings for the query and then use Vespa's <code>nearestNeighbor</code> query to retrieve the most similar documents per query vector token, using binary representation with hamming distance. Then we re-rank the results in two phases:</p> <ul> <li>In the 0-phase we use hamming distance to retrieve the k closest pages per query token vector representation, this is expressed by using multiple nearestNeighbor query operators in Vespa.</li> <li>The nearestNeighbor operators exposes pages to the first-phase ranking function, which uses an approximate MaxSim using inverted hamming distance insted of cosine similarity. This is done to reduce the number of pages that are re-ranked in the second phase.</li> <li>In the second phase, we perform the full MaxSim operation, using float representations of the embeddings to re-rank the top-k pages from the first phase.</li> </ul> <p>This allows us to scale ColPali to very large collections of PDF pages, while still providing accurate and fast retrieval.</p> <p>Let us get started.</p> <p></p> <p>Install dependencies:</p> <p>Note that the python pdf2image package requires poppler-utils, see other installation options here.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#load-the-model","title":"Load the model\u00b6","text":"<p>This requires that the HF_TOKEN environment variable is set as the underlaying PaliGemma model is hosted on Hugging Face and has a restricive licence that requires authentication.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#working-with-pdfs","title":"Working with pdfs\u00b6","text":"<p>We need to convert a PDF to an array of images. One image per page. We use the <code>pdf2image</code> library for this task. Secondary, we also extract the text contents of the PDF using <code>pypdf</code>.</p> <p>NOTE: This step requires that you have <code>poppler</code> installed on your system. Read more in pdf2image docs.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#configure-vespa","title":"Configure Vespa\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#querying-vespa","title":"Querying Vespa\u00b6","text":"<p>Ok, so now we have indexed the PDF pages in Vespa. Let us now obtain ColPali embeddings for a few text queries and use it during ranking of the indexed pdf pages.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#using-nearestneighbor-for-retrieval","title":"Using nearestNeighbor for retrieval\u00b6","text":"<p>In the above example, we used the ColPali embeddings in ranking, but using the text query for retrieval. This is a reasonable approach for text-heavy documents where the text representation is the most important and where ColPali embeddings are used to re-rank the top-k documents from the text retrieval phase.</p> <p>In some cases, the ColPali embeddings are the most important and we want to demonstrate how we can use HNSW indexing with binary hamming distance to retrieve the most similar pages to a query and then have two steps of re-ranking using the ColPali embeddings.</p> <p>All the phases here are executed locally inside the Vespa content node(s) so that no vector data needs to cross the network.</p> <p>Let us add a new rank-profile to the schema, the <code>nearestNeighbor</code> operator takes a query tensor and a field tensor as argument and we need to define the query tensors types in the rank-profile.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#summary","title":"Summary\u00b6","text":"<p>In this notebook, we have demonstrated how to represent ColPali in Vespa. We have generated embeddings for images of PDF pages using ColPali and stored the embeddings in Vespa using mixed tensors.</p> <p>We demonstrated how to store the base64 encoded image using the <code>raw</code> Vespa field type, plus meta data like title and url. We have demonstrated how to retrieve relevant pages for a query using the embeddings generated by ColPali.</p> <p>This notebook can be extended to include more complex ranking models, more complex queries, and more complex data structures, including metadata and other fields which can be filtered on or used for ranking.</p>"},{"location":"examples/simplified-retrieval-with-colpali-vlm_Vespa-cloud.html#cleanup","title":"Cleanup\u00b6","text":"<p>When this notebook is running in CI, we want to delete the application.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html","title":"Turbocharge rag with langchain and vespa streaming mode cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!pip3 install -U pyvespa langchain langchain-community pypdf==5.0.1 openai\n</pre> !pip3 install -U pyvespa langchain langchain-community pypdf==5.0.1 openai In\u00a0[1]: Copied! <pre>def sample_pdfs():\n    return [\n        {\n            \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",\n            \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",\n            \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",\n        },\n        {\n            \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n            \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",\n            \"authors\": \"Omar Khattab, Matei Zaharia\",\n        },\n        {\n            \"title\": \"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\",\n            \"url\": \"https://arxiv.org/pdf/2108.11480.pdf\",\n            \"authors\": \"Craig Macdonald, Nicola Tonellotto\",\n        },\n        {\n            \"title\": \"A Study on Token Pruning for ColBERT\",\n            \"url\": \"https://arxiv.org/pdf/2112.06540.pdf\",\n            \"authors\": \"Carlos Lassance, Maroua Maachou, Joohee Park, St\u00e9phane Clinchant\",\n        },\n        {\n            \"title\": \"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval\",\n            \"url\": \"https://arxiv.org/pdf/2106.11251.pdf\",\n            \"authors\": \"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis\",\n        },\n    ]\n</pre> def sample_pdfs():     return [         {             \"title\": \"ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction\",             \"url\": \"https://arxiv.org/pdf/2112.01488.pdf\",             \"authors\": \"Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, Matei Zaharia\",         },         {             \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",             \"url\": \"https://arxiv.org/pdf/2004.12832.pdf\",             \"authors\": \"Omar Khattab, Matei Zaharia\",         },         {             \"title\": \"On Approximate Nearest Neighbour Selection for Multi-Stage Dense Retrieval\",             \"url\": \"https://arxiv.org/pdf/2108.11480.pdf\",             \"authors\": \"Craig Macdonald, Nicola Tonellotto\",         },         {             \"title\": \"A Study on Token Pruning for ColBERT\",             \"url\": \"https://arxiv.org/pdf/2112.06540.pdf\",             \"authors\": \"Carlos Lassance, Maroua Maachou, Joohee Park, St\u00e9phane Clinchant\",         },         {             \"title\": \"Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval\",             \"url\": \"https://arxiv.org/pdf/2106.11251.pdf\",             \"authors\": \"Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis\",         },     ] In\u00a0[2]: Copied! <pre>from vespa.package import Schema, Document, Field, FieldSet, HNSW\n\npdf_schema = Schema(\n    name=\"pdf\",\n    mode=\"streaming\",\n    document=Document(\n        fields=[\n            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"title\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"authors\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"page\", type=\"int\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"metadata\",\n                type=\"map&lt;string,string&gt;\",\n                indexing=[\"summary\", \"index\"],\n            ),\n            Field(name=\"chunks\", type=\"array&lt;string&gt;\", indexing=[\"summary\", \"index\"]),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;bfloat16&gt;(chunk{}, x[384])\",\n                indexing=[\"input chunks\", \"embed e5\", \"attribute\", \"index\"],\n                ann=HNSW(distance_metric=\"angular\"),\n                is_document_field=False,\n            ),\n        ],\n    ),\n    fieldsets=[FieldSet(name=\"default\", fields=[\"chunks\", \"title\"])],\n)\n</pre> from vespa.package import Schema, Document, Field, FieldSet, HNSW  pdf_schema = Schema(     name=\"pdf\",     mode=\"streaming\",     document=Document(         fields=[             Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"title\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"authors\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(name=\"page\", type=\"int\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"metadata\",                 type=\"map\",                 indexing=[\"summary\", \"index\"],             ),             Field(name=\"chunks\", type=\"array\", indexing=[\"summary\", \"index\"]),             Field(                 name=\"embedding\",                 type=\"tensor(chunk{}, x[384])\",                 indexing=[\"input chunks\", \"embed e5\", \"attribute\", \"index\"],                 ann=HNSW(distance_metric=\"angular\"),                 is_document_field=False,             ),         ],     ),     fieldsets=[FieldSet(name=\"default\", fields=[\"chunks\", \"title\"])], ) <p>The above defines our <code>pdf</code> schema using mode <code>streaming</code>. Most fields are straightforward, but take a note of:</p> <ul> <li><code>metadata</code> using <code>map&lt;string,string&gt;</code> - here we can store and match over page level metadata extracted by the PDF parser.</li> <li><code>chunks</code> using <code>array&lt;string&gt;</code>, these are the text chunks that we use langchain document transformers for</li> <li>The <code>embedding</code> field of type <code>tensor&lt;bfloat16&gt;(chunk{},x[384])</code> allows us to store and search the 384-dimensional embeddings per chunk in the same document</li> </ul> <p>The observant reader might have noticed the <code>e5</code> argument to the <code>embed</code> expression in the above <code>embedding</code> field. The <code>e5</code> argument references a component of the type hugging-face-embedder. We configure the application package and its name with the <code>pdf</code> schema and the <code>e5</code> embedder component.</p> In\u00a0[3]: Copied! <pre>from vespa.package import ApplicationPackage, Component, Parameter\n\nvespa_app_name = \"ragpdfs\"\nvespa_application_package = ApplicationPackage(\n    name=vespa_app_name,\n    schema=[pdf_schema],\n    components=[\n        Component(\n            id=\"e5\",\n            type=\"hugging-face-embedder\",\n            parameters=[\n                Parameter(\n                    \"transformer-model\",\n                    {\n                        \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"\n                    },\n                ),\n                Parameter(\n                    \"tokenizer-model\",\n                    {\n                        \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"\n                    },\n                ),\n            ],\n        )\n    ],\n)\n</pre> from vespa.package import ApplicationPackage, Component, Parameter  vespa_app_name = \"ragpdfs\" vespa_application_package = ApplicationPackage(     name=vespa_app_name,     schema=[pdf_schema],     components=[         Component(             id=\"e5\",             type=\"hugging-face-embedder\",             parameters=[                 Parameter(                     \"transformer-model\",                     {                         \"url\": \"https://github.com/vespa-engine/sample-apps/raw/master/examples/model-exporting/model/e5-small-v2-int8.onnx\"                     },                 ),                 Parameter(                     \"tokenizer-model\",                     {                         \"url\": \"https://raw.githubusercontent.com/vespa-engine/sample-apps/master/examples/model-exporting/model/tokenizer.json\"                     },                 ),             ],         )     ], ) <p>In the last step, we configure ranking by adding <code>rank-profile</code>'s to the schema.</p> <p>Vespa supports phased ranking and has a rich set of built-in rank-features, including many text-matching features such as:</p> <ul> <li>BM25.</li> <li>nativeRank and many more.</li> </ul> <p>Users can also define custom functions using ranking expressions. The following defines a <code>hybrid</code> Vespa ranking profile.</p> In\u00a0[4]: Copied! <pre>from vespa.package import RankProfile, Function, FirstPhaseRanking\n\n\nsemantic = RankProfile(\n    name=\"hybrid\",\n    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[384])\")],\n    functions=[\n        Function(\n            name=\"similarities\",\n            expression=\"cosine_similarity(query(q), attribute(embedding),x)\",\n        )\n    ],\n    first_phase=FirstPhaseRanking(\n        expression=\"nativeRank(title) + nativeRank(chunks) + reduce(similarities, max, chunk)\",\n        rank_score_drop_limit=0.0,\n    ),\n    match_features=[\n        \"closest(embedding)\",\n        \"similarities\",\n        \"nativeRank(chunks)\",\n        \"nativeRank(title)\",\n        \"elementSimilarity(chunks)\",\n    ],\n)\npdf_schema.add_rank_profile(semantic)\n</pre> from vespa.package import RankProfile, Function, FirstPhaseRanking   semantic = RankProfile(     name=\"hybrid\",     inputs=[(\"query(q)\", \"tensor(x[384])\")],     functions=[         Function(             name=\"similarities\",             expression=\"cosine_similarity(query(q), attribute(embedding),x)\",         )     ],     first_phase=FirstPhaseRanking(         expression=\"nativeRank(title) + nativeRank(chunks) + reduce(similarities, max, chunk)\",         rank_score_drop_limit=0.0,     ),     match_features=[         \"closest(embedding)\",         \"similarities\",         \"nativeRank(chunks)\",         \"nativeRank(title)\",         \"elementSimilarity(chunks)\",     ], ) pdf_schema.add_rank_profile(semantic) <p>The <code>hybrid</code> rank-profile above defines the query input embedding type and a similarities function that uses a Vespa tensor compute function that calculates the cosine similarity between all the chunk embeddings and the query embedding.</p> <p>The profile only defines a single ranking phase, using a linear combination of multiple features.</p> <p>Using match-features, Vespa returns selected features along with the hit in the SERP (result page).</p> In\u00a0[8]: Copied! <pre>from vespa.deployment import VespaCloud\nimport os\n\n# Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n\n# Key is only used for CI/CD. Can be removed if logging in interactively\nkey = os.getenv(\"VESPA_TEAM_API_KEY\", None)\nif key is not None:\n    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n\nvespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=vespa_app_name,\n    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n    application_package=vespa_application_package,\n)\n</pre> from vespa.deployment import VespaCloud import os  # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\"  # Key is only used for CI/CD. Can be removed if logging in interactively key = os.getenv(\"VESPA_TEAM_API_KEY\", None) if key is not None:     key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly  vespa_cloud = VespaCloud(     tenant=tenant_name,     application=vespa_app_name,     key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively     application_package=vespa_application_package, ) <p>Now deploy the app to Vespa Cloud dev zone.</p> <p>The first deployment typically takes 2 minutes until the endpoint is up.</p> In\u00a0[18]: Copied! <pre>from vespa.application import Vespa\n\napp: Vespa = vespa_cloud.deploy()\n</pre> from vespa.application import Vespa  app: Vespa = vespa_cloud.deploy() <pre>Deployment started in run 2 of dev-aws-us-east-1c for samples.pdfs. This may take a few minutes the first time.\nINFO    [17:23:35]  Deploying platform version 8.270.8 and application dev build 2 for dev-aws-us-east-1c of default ...\nINFO    [17:23:35]  Using CA signed certificate version 0\nWARNING [17:23:35]  For schema 'pdf', field 'page': Changed to attribute because numerical indexes (field has type int) is not currently supported. Index-only settings may fail. Ignore this warning for streaming search.\nINFO    [17:23:35]  Using 1 nodes in container cluster 'pdfs_container'\nWARNING [17:23:36]  For streaming search cluster 'pdfs_content.pdf', SD field 'embedding': hnsw index is not relevant and not supported, ignoring setting\nWARNING [17:23:36]  For streaming search cluster 'pdfs_content.pdf', SD field 'embedding': hnsw index is not relevant and not supported, ignoring setting\nINFO    [17:23:38]  Deployment successful.\nINFO    [17:23:38]  Session 3239 for tenant 'samples' prepared and activated.\nINFO    [17:23:38]  ######## Details for all nodes ########\nINFO    [17:23:38]  h88963a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [17:23:38]  --- platform vespa/cloud-tenant-rhel8:8.270.8\nINFO    [17:23:38]  --- storagenode on port 19102 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- searchnode on port 19107 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- distributor on port 19111 has config generation 3238, wanted is 3239\nINFO    [17:23:38]  --- metricsproxy-container on port 19092 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  h88969g.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [17:23:38]  --- platform vespa/cloud-tenant-rhel8:8.270.8\nINFO    [17:23:38]  --- logserver-container on port 4080 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- metricsproxy-container on port 19092 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  h88972i.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [17:23:38]  --- platform vespa/cloud-tenant-rhel8:8.270.8\nINFO    [17:23:38]  --- container-clustercontroller on port 19050 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- metricsproxy-container on port 19092 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  h89461a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\nINFO    [17:23:38]  --- platform vespa/cloud-tenant-rhel8:8.270.8\nINFO    [17:23:38]  --- container on port 4080 has config generation 3239, wanted is 3239\nINFO    [17:23:38]  --- metricsproxy-container on port 19092 has config generation 3239, wanted is 3239\nINFO    [17:23:51]  Found endpoints:\nINFO    [17:23:51]  - dev.aws-us-east-1c\nINFO    [17:23:51]   |-- https://c4f42a1b.bfbdb4fd.z.vespa-app.cloud/ (cluster 'pdfs_container')\nINFO    [17:23:52]  Installation succeeded!\nUsing mTLS (key,cert) Authentication against endpoint https://c4f42a1b.bfbdb4fd.z.vespa-app.cloud//ApplicationStatus\nApplication is up!\nFinished deployment.\n</pre> In\u00a0[10]: Copied! <pre>from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1024,  # chars, not llm tokens\n    chunk_overlap=0,\n    length_function=len,\n    is_separator_regex=False,\n)\n</pre> from langchain_community.document_loaders import PyPDFLoader from langchain.text_splitter import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(     chunk_size=1024,  # chars, not llm tokens     chunk_overlap=0,     length_function=len,     is_separator_regex=False, ) <p>The following iterates over the <code>sample_pdfs</code> and performs the following:</p> <ul> <li>Load the URL and extract the text into pages. A page is the retrievable unit we will use in Vespa</li> <li>For each page, use the text splitter to split the text into chunks. The chunks are represented as an <code>array&lt;string&gt;</code> in the Vespa schema</li> <li>Create the page level Vespa <code>fields</code>, note that we duplicate some content like the title and URL into the page level representation.</li> </ul> In\u00a0[11]: Copied! <pre>import hashlib\nimport unicodedata\n\n\ndef remove_control_characters(s):\n    return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")\n\n\nmy_docs_to_feed = []\nfor pdf in sample_pdfs():\n    url = pdf[\"url\"]\n    loader = PyPDFLoader(url)\n    pages = loader.load_and_split()\n    for index, page in enumerate(pages):\n        source = page.metadata[\"source\"]\n        chunks = text_splitter.transform_documents([page])\n        text_chunks = [chunk.page_content for chunk in chunks]\n        text_chunks = [remove_control_characters(chunk) for chunk in text_chunks]\n        page_number = index + 1\n        vespa_id = f\"{url}#{page_number}\"\n        hash_value = hashlib.sha1(vespa_id.encode()).hexdigest()\n        fields = {\n            \"title\": pdf[\"title\"],\n            \"url\": url,\n            \"page\": page_number,\n            \"id\": hash_value,\n            \"authors\": [a.strip() for a in pdf[\"authors\"].split(\",\")],\n            \"chunks\": text_chunks,\n            \"metadata\": page.metadata,\n        }\n        my_docs_to_feed.append(fields)\n</pre> import hashlib import unicodedata   def remove_control_characters(s):     return \"\".join(ch for ch in s if unicodedata.category(ch)[0] != \"C\")   my_docs_to_feed = [] for pdf in sample_pdfs():     url = pdf[\"url\"]     loader = PyPDFLoader(url)     pages = loader.load_and_split()     for index, page in enumerate(pages):         source = page.metadata[\"source\"]         chunks = text_splitter.transform_documents([page])         text_chunks = [chunk.page_content for chunk in chunks]         text_chunks = [remove_control_characters(chunk) for chunk in text_chunks]         page_number = index + 1         vespa_id = f\"{url}#{page_number}\"         hash_value = hashlib.sha1(vespa_id.encode()).hexdigest()         fields = {             \"title\": pdf[\"title\"],             \"url\": url,             \"page\": page_number,             \"id\": hash_value,             \"authors\": [a.strip() for a in pdf[\"authors\"].split(\",\")],             \"chunks\": text_chunks,             \"metadata\": page.metadata,         }         my_docs_to_feed.append(fields) <p>Now that we have parsed the input PDFs and created a list of pages that we want to add to Vespa, we must format the list into the format that PyVespa accepts. Notice the <code>fields</code>, <code>id</code> and <code>groupname</code> keys. The <code>groupname</code> is the key that is used to shard and co-locate the data and is only relevant when using Vespa with streaming mode.</p> In\u00a0[12]: Copied! <pre>from typing import Iterable\n\n\ndef vespa_feed(user: str) -&gt; Iterable[dict]:\n    for doc in my_docs_to_feed:\n        yield {\"fields\": doc, \"id\": doc[\"id\"], \"groupname\": user}\n</pre> from typing import Iterable   def vespa_feed(user: str) -&gt; Iterable[dict]:     for doc in my_docs_to_feed:         yield {\"fields\": doc, \"id\": doc[\"id\"], \"groupname\": user} <p>Now, we can feed to the Vespa instance (<code>app</code>), using the <code>feed_iterable</code> API, using the generator function above as input with a custom <code>callback</code> function. Vespa also performs embedding inference during this step using the built-in Vespa embedding functionality.</p> In\u00a0[13]: Copied! <pre>from vespa.io import VespaResponse\n\n\ndef callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"\n        )\n\n\napp.feed_iterable(\n    schema=\"pdf\", iter=vespa_feed(\"jo-bergum\"), namespace=\"personal\", callback=callback\n)\n</pre> from vespa.io import VespaResponse   def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Document {id} failed to feed with status code {response.status_code}, url={response.url} response={response.json}\"         )   app.feed_iterable(     schema=\"pdf\", iter=vespa_feed(\"jo-bergum\"), namespace=\"personal\", callback=callback ) <p>Notice the <code>schema</code> and <code>namespace</code> arguments. PyVespa transforms the input operations to Vespa document v1 requests.</p> <p></p> In\u00a0[15]: Copied! <pre>from vespa.io import VespaQueryResponse\nimport json\n\nresponse: VespaQueryResponse = app.query(\n    yql=\"select id,title,page,chunks from pdf where userQuery() or ({targetHits:10}nearestNeighbor(embedding,q))\",\n    groupname=\"jo-bergum\",\n    ranking=\"hybrid\",\n    query=\"why is colbert effective?\",\n    body={\n        \"presentation.format.tensors\": \"short-value\",\n        \"input.query(q)\": 'embed(e5, \"why is colbert effective?\")',\n    },\n    timeout=\"2s\",\n)\nassert response.is_successful()\nprint(json.dumps(response.hits[0], indent=2))\n</pre> from vespa.io import VespaQueryResponse import json  response: VespaQueryResponse = app.query(     yql=\"select id,title,page,chunks from pdf where userQuery() or ({targetHits:10}nearestNeighbor(embedding,q))\",     groupname=\"jo-bergum\",     ranking=\"hybrid\",     query=\"why is colbert effective?\",     body={         \"presentation.format.tensors\": \"short-value\",         \"input.query(q)\": 'embed(e5, \"why is colbert effective?\")',     },     timeout=\"2s\", ) assert response.is_successful() print(json.dumps(response.hits[0], indent=2)) <pre>{\n  \"id\": \"id:personal:pdf:g=jo-bergum:a4b2ced87807ee9cb0325b7a1c64a070d05a31f7\",\n  \"relevance\": 1.1412738851962692,\n  \"source\": \"pdfs_content.pdf\",\n  \"fields\": {\n    \"matchfeatures\": {\n      \"closest(embedding)\": {\n        \"0\": 1.0\n      },\n      \"elementSimilarity(chunks)\": 0.5006379585326953,\n      \"nativeRank(chunks)\": 0.15642522855051508,\n      \"nativeRank(title)\": 0.1341324233922751,\n      \"similarities\": {\n        \"1\": 0.7731813192367554,\n        \"2\": 0.8196794986724854,\n        \"3\": 0.796222984790802,\n        \"4\": 0.7699441909790039,\n        \"0\": 0.850716233253479\n      }\n    },\n    \"id\": \"a4b2ced87807ee9cb0325b7a1c64a070d05a31f7\",\n    \"title\": \"ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\",\n    \"page\": 9,\n    \"chunks\": [\n      \"Sq,d:=\\u00d5i\\u2208[|Eq|]maxj\\u2208[|Ed|]Eqi\\u00b7ETdj(3)ColBERT is di\\ufb00erentiable end-to-end. We /f_ine-tune the BERTencoders and train from scratch the additional parameters (i.e., thelinear layer and the [Q] and [D] markers\\u2019 embeddings) using theAdam [ 16] optimizer. Notice that our interaction mechanism hasno trainable parameters. Given a triple \\u27e8q,d+,d\\u2212\\u27e9with query q,positive document d+and negative document d\\u2212, ColBERT is usedto produce a score for each document individually and is optimizedvia pairwise so/f_tmax cross-entropy loss over the computed scoresofd+andd\\u2212.3.4 O\\ufb00line Indexing: Computing &amp; StoringDocument EmbeddingsBy design, ColBERT isolates almost all of the computations betweenqueries and documents, largely to enable pre-computing documentrepresentations o\\ufb04ine. At a high level, our indexing procedure isstraight-forward: we proceed over the documents in the collectionin batches, running our document encoder fDon each batch andstoring the output embeddings per document. Although indexing\",\n      \"a set of documents is an o\\ufb04ine process, we incorporate a fewsimple optimizations for enhancing the throughput of indexing. Aswe show in \\u00a74.5, these optimizations can considerably reduce theo\\ufb04ine cost of indexing.To begin with, we exploit multiple GPUs, if available, for fasterencoding of batches of documents in parallel. When batching, wepad all documents to the maximum length of a document withinthe batch.3To make capping the sequence length on a per-batchbasis more e\\ufb00ective, our indexer proceeds through documents ingroups of B(e.g., B=100,000) documents. It sorts these documentsby length and then feeds batches of b(e.g., b=128) documents ofcomparable length through our encoder. /T_his length-based bucket-ing is sometimes refered to as a BucketIterator in some libraries(e.g., allenNLP). Lastly, while most computations occur on the GPU,we found that a non-trivial portion of the indexing time is spent onpre-processing the text sequences, primarily BERT\\u2019s WordPiece to-\",\n      \"kenization. Exploiting that these operations are independent acrossdocuments in a batch, we parallelize the pre-processing across theavailable CPU cores.Once the document representations are produced, they are savedto disk using 32-bit or 16-bit values to represent each dimension.As we describe in \\u00a73.5 and 3.6, these representations are eithersimply loaded from disk for ranking or are subsequently indexedfor vector-similarity search, respectively.3.5 Top- kRe-ranking with ColBERTRecall that ColBERT can be used for re-ranking the output of an-other retrieval model, typically a term-based model, or directlyfor end-to-end retrieval from a document collection. In this sec-tion, we discuss how we use ColBERT for ranking a small set ofk(e.g., k=1000) documents given a query q. Since kis small, werely on batch computations to exhaustively score each document\",\n      \"3/T_he public BERT implementations we saw simply pad to a pre-de/f_ined length.(unlike our approach in \\u00a73.6). To begin with, our query serving sub-system loads the indexed documents representations into memory,representing each document as a matrix of embeddings.Given a query q, we compute its bag of contextualized embed-dings Eq(Equation 1) and, concurrently, gather the document repre-sentations into a 3-dimensional tensor Dconsisting of kdocumentmatrices. We pad the kdocuments to their maximum length tofacilitate batched operations, and move the tensor Dto the GPU\\u2019smemory. On the GPU, we compute a batch dot-product of EqandD, possibly over multiple mini-batches. /T_he output materializes a3-dimensional tensor that is a collection of cross-match matricesbetween qand each document. To compute the score of each docu-ment, we reduce its matrix across document terms via a max-pool(i.e., representing an exhaustive implementation of our MaxSim\",\n      \"computation) and reduce across query terms via a summation. Fi-nally, we sort the kdocuments by their total scores.\"\n    ]\n  }\n}\n</pre> <p>Notice the <code>matchfeatures</code> that returns the configured match-features from the rank-profile, including all the chunk similarities.</p> In\u00a0[19]: Copied! <pre>from langchain_core.documents import Document\nfrom langchain_core.retrievers import BaseRetriever\nfrom typing import List\n\n\nclass VespaStreamingHybridRetriever(BaseRetriever):\n    app: Vespa\n    user: str\n    pages: int = 5\n    chunks_per_page: int = 3\n    chunk_similarity_threshold: float = 0.8\n\n    def _get_relevant_documents(self, query: str) -&gt; List[Document]:\n        response: VespaQueryResponse = self.app.query(\n            yql=\"select id, url, title, page, authors, chunks from pdf where userQuery() or ({targetHits:20}nearestNeighbor(embedding,q))\",\n            groupname=self.user,\n            ranking=\"hybrid\",\n            query=query,\n            hits=self.pages,\n            body={\n                \"presentation.format.tensors\": \"short-value\",\n                \"input.query(q)\": f'embed(e5, \"query: {query} \")',\n            },\n            timeout=\"2s\",\n        )\n        if not response.is_successful():\n            raise ValueError(\n                f\"Query failed with status code {response.status_code}, url={response.url} response={response.json}\"\n            )\n        return self._parse_response(response)\n\n    def _parse_response(self, response: VespaQueryResponse) -&gt; List[Document]:\n        documents: List[Document] = []\n        for hit in response.hits:\n            fields = hit[\"fields\"]\n            chunks_with_scores = self._get_chunk_similarities(fields)\n            ## Best k chunks from each page\n            best_chunks_on_page = \" ### \".join(\n                [\n                    chunk\n                    for chunk, score in chunks_with_scores[0 : self.chunks_per_page]\n                    if score &gt; self.chunk_similarity_threshold\n                ]\n            )\n            documents.append(\n                Document(\n                    id=fields[\"id\"],\n                    page_content=best_chunks_on_page,\n                    title=fields[\"title\"],\n                    metadata={\n                        \"title\": fields[\"title\"],\n                        \"url\": fields[\"url\"],\n                        \"page\": fields[\"page\"],\n                        \"authors\": fields[\"authors\"],\n                        \"features\": fields[\"matchfeatures\"],\n                    },\n                )\n            )\n        return documents\n\n    def _get_chunk_similarities(self, hit_fields: dict) -&gt; List[tuple]:\n        match_features = hit_fields[\"matchfeatures\"]\n        similarities = match_features[\"similarities\"]\n        chunk_scores = []\n        for i in range(0, len(similarities)):\n            chunk_scores.append(similarities.get(str(i), 0))\n        chunks = hit_fields[\"chunks\"]\n        chunks_with_scores = list(zip(chunks, chunk_scores))\n        return sorted(chunks_with_scores, key=lambda x: x[1], reverse=True)\n</pre> from langchain_core.documents import Document from langchain_core.retrievers import BaseRetriever from typing import List   class VespaStreamingHybridRetriever(BaseRetriever):     app: Vespa     user: str     pages: int = 5     chunks_per_page: int = 3     chunk_similarity_threshold: float = 0.8      def _get_relevant_documents(self, query: str) -&gt; List[Document]:         response: VespaQueryResponse = self.app.query(             yql=\"select id, url, title, page, authors, chunks from pdf where userQuery() or ({targetHits:20}nearestNeighbor(embedding,q))\",             groupname=self.user,             ranking=\"hybrid\",             query=query,             hits=self.pages,             body={                 \"presentation.format.tensors\": \"short-value\",                 \"input.query(q)\": f'embed(e5, \"query: {query} \")',             },             timeout=\"2s\",         )         if not response.is_successful():             raise ValueError(                 f\"Query failed with status code {response.status_code}, url={response.url} response={response.json}\"             )         return self._parse_response(response)      def _parse_response(self, response: VespaQueryResponse) -&gt; List[Document]:         documents: List[Document] = []         for hit in response.hits:             fields = hit[\"fields\"]             chunks_with_scores = self._get_chunk_similarities(fields)             ## Best k chunks from each page             best_chunks_on_page = \" ### \".join(                 [                     chunk                     for chunk, score in chunks_with_scores[0 : self.chunks_per_page]                     if score &gt; self.chunk_similarity_threshold                 ]             )             documents.append(                 Document(                     id=fields[\"id\"],                     page_content=best_chunks_on_page,                     title=fields[\"title\"],                     metadata={                         \"title\": fields[\"title\"],                         \"url\": fields[\"url\"],                         \"page\": fields[\"page\"],                         \"authors\": fields[\"authors\"],                         \"features\": fields[\"matchfeatures\"],                     },                 )             )         return documents      def _get_chunk_similarities(self, hit_fields: dict) -&gt; List[tuple]:         match_features = hit_fields[\"matchfeatures\"]         similarities = match_features[\"similarities\"]         chunk_scores = []         for i in range(0, len(similarities)):             chunk_scores.append(similarities.get(str(i), 0))         chunks = hit_fields[\"chunks\"]         chunks_with_scores = list(zip(chunks, chunk_scores))         return sorted(chunks_with_scores, key=lambda x: x[1], reverse=True) <p>That's it! We can give our newborn retriever a spin for the user\u00a0<code>jo-bergum</code> by</p> In\u00a0[20]: Copied! <pre>vespa_hybrid_retriever = VespaStreamingHybridRetriever(\n    app=app, user=\"jo-bergum\", pages=1, chunks_per_page=1\n)\n</pre> vespa_hybrid_retriever = VespaStreamingHybridRetriever(     app=app, user=\"jo-bergum\", pages=1, chunks_per_page=1 ) In\u00a0[21]: Copied! <pre>vespa_hybrid_retriever.get_relevant_documents(\"what is the maxsim operator in colbert?\")\n</pre> vespa_hybrid_retriever.get_relevant_documents(\"what is the maxsim operator in colbert?\") Out[21]: <pre>[Document(page_content='ture that precisely does so. As illustrated, every query embeddinginteracts with all document embeddings via a MaxSim operator,which computes maximum similarity (e.g., cosine similarity), andthe scalar outputs of these operators are summed across queryterms. /T_his paradigm allows ColBERT to exploit deep LM-basedrepresentations while shi/f_ting the cost of encoding documents of-/f_line and amortizing the cost of encoding the query once acrossall ranked documents. Additionally, it enables ColBERT to lever-age vector-similarity search indexes (e.g., [ 1,15]) to retrieve thetop-kresults directly from a large document collection, substan-tially improving recall over models that only re-rank the output ofterm-based retrieval.As Figure 1 illustrates, ColBERT can serve queries in tens orfew hundreds of milliseconds. For instance, when used for re-ranking as in \u201cColBERT (re-rank)\u201d, it delivers over 170 \u00d7speedup(and requires 14,000 \u00d7fewer FLOPs) relative to existing BERT-based', metadata={'title': 'ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT', 'url': 'https://arxiv.org/pdf/2004.12832.pdf', 'page': 4, 'authors': ['Omar Khattab', 'Matei Zaharia'], 'features': {'closest(embedding)': {'0': 1.0}, 'elementSimilarity(chunks)': 0.41768707482993195, 'nativeRank(chunks)': 0.1401101487033024, 'nativeRank(title)': 0.0520403737720047, 'similarities': {'1': 0.8369992971420288, '0': 0.8730311393737793}}})]</pre> <p>Finally, we can connect our custom retriever with the complete flexibility and power of the [LangChain] LLM framework. The following uses LangChain Expression Language, or LCEL, a declarative way to compose chains.</p> <p>We have several steps composed into a chain:</p> <ul> <li>The prompt template and LLM model, in this case using OpenAI</li> <li>The retriever that provides the retrieved context for the question</li> <li>The formatting of the retrieved context</li> </ul> In\u00a0[22]: Copied! <pre>vespa_hybrid_retriever = VespaStreamingHybridRetriever(\n    app=app, user=\"jo-bergum\", pages=3, chunks_per_page=3\n)\n</pre> vespa_hybrid_retriever = VespaStreamingHybridRetriever(     app=app, user=\"jo-bergum\", pages=3, chunks_per_page=3 ) In\u00a0[25]: Copied! <pre>from langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import ChatPromptTemplate\nfrom langchain.schema import StrOutputParser\nfrom langchain.schema.runnable import RunnablePassthrough\n\nprompt_template = \"\"\"\nAnswer the question based only on the following context. \nCite the page number and the url of the document you are citing.\n\n{context}\nQuestion: {question}\n\"\"\"\nprompt = ChatPromptTemplate.from_template(prompt_template)\nmodel = ChatOpenAI()\n\n\ndef format_prompt_context(docs) -&gt; str:\n    context = []\n    for d in docs:\n        context.append(f\"{d.metadata['title']} by {d.metadata['authors']}\\n\")\n        context.append(f\"url: {d.metadata['url']}\\n\")\n        context.append(f\"page: {d.metadata['page']}\\n\")\n        context.append(f\"{d.page_content}\\n\\n\")\n    return \"\".join(context)\n\n\nchain = (\n    {\n        \"context\": vespa_hybrid_retriever | format_prompt_context,\n        \"question\": RunnablePassthrough(),\n    }\n    | prompt\n    | model\n    | StrOutputParser()\n)\n</pre> from langchain.chat_models import ChatOpenAI from langchain.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain.schema.runnable import RunnablePassthrough  prompt_template = \"\"\" Answer the question based only on the following context.  Cite the page number and the url of the document you are citing.  {context} Question: {question} \"\"\" prompt = ChatPromptTemplate.from_template(prompt_template) model = ChatOpenAI()   def format_prompt_context(docs) -&gt; str:     context = []     for d in docs:         context.append(f\"{d.metadata['title']} by {d.metadata['authors']}\\n\")         context.append(f\"url: {d.metadata['url']}\\n\")         context.append(f\"page: {d.metadata['page']}\\n\")         context.append(f\"{d.page_content}\\n\\n\")     return \"\".join(context)   chain = (     {         \"context\": vespa_hybrid_retriever | format_prompt_context,         \"question\": RunnablePassthrough(),     }     | prompt     | model     | StrOutputParser() ) In\u00a0[26]: Copied! <pre>chain.invoke(\"what is colbert?\")\n</pre> chain.invoke(\"what is colbert?\") Out[26]: <pre>'ColBERT is a ranking model that adapts deep language models, specifically BERT, for efficient retrieval. It introduces a late interaction architecture that independently encodes queries and documents using BERT and then uses a cheap yet powerful interaction step to model their fine-grained similarity. This allows ColBERT to leverage the expressiveness of deep language models while also being able to pre-compute document representations offline, significantly speeding up query processing. ColBERT can be used for re-ranking documents retrieved by a traditional model or for end-to-end retrieval directly from a large document collection. It has been shown to be effective and efficient compared to existing models. (source: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT by Omar Khattab, Matei Zaharia, page 1, url: https://arxiv.org/pdf/2004.12832.pdf)'</pre> In\u00a0[27]: Copied! <pre>chain.invoke(\"what is the colbert maxsim operator\")\n</pre> chain.invoke(\"what is the colbert maxsim operator\") Out[27]: <pre>\"The ColBERT model utilizes the MaxSim operator, which computes the maximum similarity (e.g., cosine similarity) between query embeddings and document embeddings. The scalar outputs of these operators are summed across query terms, allowing ColBERT to exploit deep LM-based representations while reducing the cost of encoding documents offline and amortizing the cost of encoding the query once across all ranked documents.\\n\\nSource: \\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT by ['Omar Khattab', 'Matei Zaharia']\\nURL: https://arxiv.org/pdf/2004.12832.pdf\\nPage: 4\"</pre> In\u00a0[28]: Copied! <pre>chain.invoke(\n    \"What is the difference between colbert and single vector representational models?\"\n)\n</pre> chain.invoke(     \"What is the difference between colbert and single vector representational models?\" ) Out[28]: <pre>'The difference between ColBERT and single vector representational models is that ColBERT utilizes a late interaction architecture that independently encodes the query and the document using BERT, while single vector models use a single embedding vector for both the query and the document. This late interaction mechanism in ColBERT allows for fine-grained similarity estimation, which leads to more effective retrieval. (Source: ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT by Omar Khattab and Matei Zaharia, page 17, url: https://arxiv.org/pdf/2004.12832.pdf)'</pre> In\u00a0[\u00a0]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete()"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#turbocharge-rag-with-langchain-and-vespa-streaming-mode-for-partitioned-data","title":"Turbocharge RAG with LangChain and Vespa Streaming Mode for Partitioned Data\u00b6","text":"<p>This notebook illustrates using Vespa streaming mode to build cost-efficient RAG applications over naturally sharded data.</p> <p>You can read more about Vespa vector streaming search in these blog posts:</p> <ul> <li>Announcing vector streaming search: AI assistants at scale without breaking the bank</li> <li>Yahoo Mail turns to Vespa to do RAG at scale</li> <li>Hands-On RAG guide for personal data with Vespa and LLamaIndex</li> </ul> <p>This notebook is also available in blog form: Turbocharge RAG with LangChain and Vespa Streaming Mode for Sharded Data</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#tldr-vespa-streaming-mode-for-partitioned-data","title":"TLDR; Vespa streaming mode for partitioned data\u00b6","text":"<p>Vespa's streaming search solution enables you to integrate a user ID (or any sharding key) into the Vespa document ID. This setup allows Vespa to efficiently group each user's data on a small set of nodes and the same disk chunk. Streaming mode enables low latency searches on a user's data without keeping data in memory.</p> <p>The key benefits of streaming mode:</p> <ul> <li>Eliminating compromises in precision introduced by approximate algorithms</li> <li>Achieve significantly higher write throughput, thanks to the absence of index builds required for supporting approximate search.</li> <li>Optimize efficiency by storing documents, including tensors and data, on disk, benefiting from the cost-effective economics of storage tiers.</li> <li>Storage cost is the primary cost driver of Vespa streaming mode; no data is in memory. Avoiding memory usage lowers deployment costs significantly.</li> </ul>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#connecting-langchain-retriever-with-vespa-for-context-retrieval-from-pdf-documents","title":"Connecting LangChain Retriever with Vespa for Context Retrieval from PDF Documents\u00b6","text":"<p>In this notebook, we seamlessly integrate a custom LangChain retriever with a Vespa app, leveraging Vespa's streaming mode to extract meaningful context from PDF documents.</p> <p>The workflow</p> <ul> <li>Define and deploy a Vespa application package using PyVespa.</li> <li>Utilize LangChain PDF Loaders to download and parse PDF files.</li> <li>Leverage LangChain Document Transformers to convert each PDF page into multiple text chunks.</li> <li>Feed the transformer representation to the running Vespa instance</li> <li>Employ Vespa's built-in embedder functionality (using an open-source embedding model) for embedding the text chunks per page, resulting in a multi-vector representation.</li> <li>Develop a custom Retriever to enable seamless retrieval for any unstructured text query.</li> </ul> <p></p> <p></p> <p>Let's get started! First, install dependencies:</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#sample-data","title":"Sample data\u00b6","text":"<p>We love ColBERT, so we'll use a few COlBERT related papers as examples of PDFs in this notebook.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#defining-the-vespa-application","title":"Defining the Vespa application\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#deploy-the-application-to-vespa-cloud","title":"Deploy the application to Vespa Cloud\u00b6","text":"<p>With the configured application, we can deploy it to Vespa Cloud.</p> <p>To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:</p> <p>Create a tenant at console.vespa-cloud.com (unless you already have one). This step requires a Google or GitHub account, and will start your free trial.</p> <p>Make note of the tenant name, it is used in the next steps.</p> <p>Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#processing-pdfs-with-langchain","title":"Processing PDFs with LangChain\u00b6","text":"<p>LangChain has a rich set of document loaders that can be used to load and process various file formats. In this notebook, we use the PyPDFLoader.</p> <p>We also want to split the extracted text into chunks using a text splitter. Most text embedding models have limited input lengths (typically less than 512 language model tokens, so splitting the text into multiple chunks that fits into the context limit of the embedding model is a common strategy.</p> <p>For embedding text data, models based on the Transformer architecture have become the de facto standard. A challenge with Transformer-based models is their input length limitation due to the quadratic self-attention computational complexity. For example, a popular open-source text embedding model like e5 has an absolute maximum input length of 512 wordpiece tokens. In addition to the technical limitation, trying to fit more tokens than used during fine-tuning of the model will impact the quality of the vector representation.</p> <p>One can view text embedding encoding as a lossy compression technique, where variable-length texts are compressed into a fixed dimensional vector representation.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#querying-data","title":"Querying data\u00b6","text":"<p>Now, we can also query our data. With streaming mode, we must pass the <code>groupname</code> parameter, or the request will fail with an error.</p> <p>The query request uses the Vespa Query API and the <code>Vespa.query()</code> function supports passing any of the Vespa query API parameters.</p> <p>Read more about querying Vespa in:</p> <ul> <li>Vespa Query API</li> <li>Vespa Query API reference</li> <li>Vespa Query Language API (YQL)</li> </ul> <p>Sample query request for <code>why is colbert effective?</code> for the user <code>bergum@vespa.ai</code>:</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#langchain-retriever","title":"LangChain Retriever\u00b6","text":"<p>We use the LangChain Retriever interface so that we can connect our Vespa app with the flexibility and power of the LangChain LLM framework.</p> <p>A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.</p> <p>The retriever interface fits perfectly with Vespa, as Vespa can support a wide range of features and ways to retrieve and rank content. The following implements a custom retriever <code>VespaStreamingHybridRetriever</code> that takes the following arguments:</p> <ul> <li><code>app:Vespa</code> The Vespa application we retrieve from. This could be a Vespa Cloud instance or a local instance, for example running on a laptop.</li> <li><code>user:str</code> The user that that we want to retrieve for, this argument maps to the Vespa streaming mode groupname parameter</li> <li><code>pages:int</code> The target number of PDF pages we want to retrieve for a given query</li> <li><code>chunks_per_page</code> The is the target number of relevant text chunks that are associated with the page</li> <li><code>chunk_similarity_threshold</code> - The chunk similarity threshold, only chunks with a similarity above this threshold</li> </ul> <p>The core idea is to retrieve pages using maximum chunk similarity as the initial scoring function, then consider other chunks on the same page potentially relevant.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#rag","title":"RAG\u00b6","text":""},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#interact-with-the-chain","title":"Interact with the chain\u00b6","text":"<p>Now, we can start asking questions using the <code>chain</code> define above.</p>"},{"location":"examples/turbocharge-rag-with-langchain-and-vespa-streaming-mode-cloud.html#summary","title":"Summary\u00b6","text":"<p>Vespa\u2019s streaming mode is a game-changer, enabling the creation of highly cost-effective RAG applications for naturally partitioned data.</p> <p>In this notebook, we delved into the hands-on application of LangChain, leveraging document loaders and transformers. Finally, we showcased a custom LangChain retriever that connected all the functionality of LangChain with Vespa.</p> <p>For those interested in learning more about Vespa, join the Vespa community on Slack to exchange ideas, seek assistance, or stay in the loop on the latest Vespa developments.</p> <p>We can now delete the cloud instance:</p>"},{"location":"examples/video_search_twelvelabs_cloud.html","title":"Video search twelvelabs cloud","text":"In\u00a0[1]: Copied! <pre>!python --version\n</pre> !python --version <pre>Python 3.12.4\n</pre> In\u00a0[2]: Copied! <pre>!pip3 install pyvespa vespacli twelvelabs pandas\n</pre> !pip3 install pyvespa vespacli twelvelabs pandas <pre>Requirement already satisfied: pyvespa in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (0.55.0)\nRequirement already satisfied: vespacli in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (8.391.23)\nRequirement already satisfied: twelvelabs in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (0.4.10)\nRequirement already satisfied: pandas in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (2.2.2)\nRequirement already satisfied: requests in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (2.32.3)\nRequirement already satisfied: requests_toolbelt in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (1.0.0)\nRequirement already satisfied: docker in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (7.1.0)\nRequirement already satisfied: jinja2 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (3.1.4)\nRequirement already satisfied: cryptography in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (43.0.3)\nRequirement already satisfied: aiohttp in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (3.10.10)\nRequirement already satisfied: httpx[http2] in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (0.28.1)\nRequirement already satisfied: tenacity&gt;=8.4.1 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (9.0.0)\nRequirement already satisfied: typing_extensions in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (4.12.2)\nRequirement already satisfied: python-dateutil in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (2.9.0.post0)\nRequirement already satisfied: fastcore&gt;=1.7.8 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (1.7.19)\nRequirement already satisfied: lxml in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pyvespa) (5.3.0)\nRequirement already satisfied: pydantic&gt;=2.4.2 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from twelvelabs) (2.10.6)\nRequirement already satisfied: numpy&gt;=1.26.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: pytz&gt;=2020.1 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata&gt;=2022.7 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pandas) (2023.3)\nRequirement already satisfied: packaging in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from fastcore&gt;=1.7.8-&gt;pyvespa) (24.2)\nRequirement already satisfied: anyio in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from httpx[http2]-&gt;pyvespa) (4.8.0)\nRequirement already satisfied: certifi in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from httpx[http2]-&gt;pyvespa) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from httpx[http2]-&gt;pyvespa) (1.0.7)\nRequirement already satisfied: idna in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from httpx[http2]-&gt;pyvespa) (3.10)\nRequirement already satisfied: h11&lt;0.15,&gt;=0.13 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from httpcore==1.*-&gt;httpx[http2]-&gt;pyvespa) (0.14.0)\nRequirement already satisfied: annotated-types&gt;=0.6.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pydantic&gt;=2.4.2-&gt;twelvelabs) (0.7.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from pydantic&gt;=2.4.2-&gt;twelvelabs) (2.27.2)\nRequirement already satisfied: six&gt;=1.5 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from python-dateutil-&gt;pyvespa) (1.16.0)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.3.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from aiohttp-&gt;pyvespa) (2.4.0)\nRequirement already satisfied: aiosignal&gt;=1.1.2 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from aiohttp-&gt;pyvespa) (1.3.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from aiohttp-&gt;pyvespa) (24.2.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from aiohttp-&gt;pyvespa) (1.4.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from aiohttp-&gt;pyvespa) (6.0.4)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.12.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from aiohttp-&gt;pyvespa) (1.15.5)\nRequirement already satisfied: cffi&gt;=1.12 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from cryptography-&gt;pyvespa) (1.17.1)\nRequirement already satisfied: urllib3&gt;=1.26.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from docker-&gt;pyvespa) (2.3.0)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from requests-&gt;pyvespa) (3.4.1)\nRequirement already satisfied: h2&lt;5,&gt;=3 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from httpx[http2]-&gt;pyvespa) (4.1.0)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from jinja2-&gt;pyvespa) (3.0.2)\nRequirement already satisfied: pycparser in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from cffi&gt;=1.12-&gt;cryptography-&gt;pyvespa) (2.22)\nRequirement already satisfied: hyperframe&lt;7,&gt;=6.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]-&gt;pyvespa) (6.0.1)\nRequirement already satisfied: hpack&lt;5,&gt;=4.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from h2&lt;5,&gt;=3-&gt;httpx[http2]-&gt;pyvespa) (4.0.0)\nRequirement already satisfied: propcache&gt;=0.2.0 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from yarl&lt;2.0,&gt;=1.12.0-&gt;aiohttp-&gt;pyvespa) (0.2.0)\nRequirement already satisfied: sniffio&gt;=1.1 in /opt/anaconda3/envs/vespa-env/lib/python3.12/site-packages (from anyio-&gt;httpx[http2]-&gt;pyvespa) (1.3.1)\n</pre> <p>Import all the required packages in this notebook.</p> In\u00a0[3]: Copied! <pre>import os\nimport hashlib\nimport json\n\nfrom vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    FieldSet,\n    SecondPhaseRanking,\n    Function,\n)\n\nfrom vespa.deployment import VespaCloud\nfrom vespa.io import VespaResponse, VespaQueryResponse\n\nfrom twelvelabs import TwelveLabs\nfrom twelvelabs.models.embed import EmbeddingsTask\n\nimport pandas as pd\n\nfrom datetime import datetime\n</pre> import os import hashlib import json  from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     FieldSet,     SecondPhaseRanking,     Function, )  from vespa.deployment import VespaCloud from vespa.io import VespaResponse, VespaQueryResponse  from twelvelabs import TwelveLabs from twelvelabs.models.embed import EmbeddingsTask  import pandas as pd  from datetime import datetime In\u00a0[8]: Copied! <pre>TL_API_KEY = os.getenv(\"TL_API_KEY\") or input(\"Enter your TL_API key: \")\n</pre> TL_API_KEY = os.getenv(\"TL_API_KEY\") or input(\"Enter your TL_API key: \") In\u00a0[\u00a0]: Copied! <pre># Replace with your tenant name from the Vespa Cloud Console\ntenant_name = \"vespa-team\"\n# Replace with your application name (does not need to exist yet)\napplication = \"videosearch\"\n</pre> # Replace with your tenant name from the Vespa Cloud Console tenant_name = \"vespa-team\" # Replace with your application name (does not need to exist yet) application = \"videosearch\" In\u00a0[10]: Copied! <pre>VIDEO_URLs = [\n    \"https://archive.org/download/the-end-blue-sky-studios/The%20End%281080P_60FPS%29.ia.mp4\",\n    \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\",\n    \"https://archive.org/download/The_Worm_in_the_Apple_Animation_Test/AnimationTest.mov\",\n]\n</pre> VIDEO_URLs = [     \"https://archive.org/download/the-end-blue-sky-studios/The%20End%281080P_60FPS%29.ia.mp4\",     \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\",     \"https://archive.org/download/The_Worm_in_the_Apple_Animation_Test/AnimationTest.mov\", ] <p>In order to generate text on the videos, the prerequisite is to upload the videos and index them. Let's first create an index below:</p> In\u00a0[11]: Copied! <pre># Spin-up session\nclient = TwelveLabs(api_key=TL_API_KEY)\n\n# Generating Index Name\ntimestamp = int(datetime.now().timestamp())\nindex_name = \"Vespa_\" + str(timestamp)\n\n# Create Index\nprint(\"Creating Index:\" + index_name)\nindex = client.index.create(\n    name=index_name,\n    models=[\n        {\n            \"name\": \"pegasus1.2\",\n            \"options\": [\"visual\", \"audio\"],\n        }\n    ],\n    addons=[\"thumbnail\"],  # Optional\n)\nprint(f\"Created index: id={index.id} name={index.name} models={index.models}\")\n</pre> # Spin-up session client = TwelveLabs(api_key=TL_API_KEY)  # Generating Index Name timestamp = int(datetime.now().timestamp()) index_name = \"Vespa_\" + str(timestamp)  # Create Index print(\"Creating Index:\" + index_name) index = client.index.create(     name=index_name,     models=[         {             \"name\": \"pegasus1.2\",             \"options\": [\"visual\", \"audio\"],         }     ],     addons=[\"thumbnail\"],  # Optional ) print(f\"Created index: id={index.id} name={index.name} models={index.models}\") <pre>Creating Index:Vespa_1752595622\nCreated index: id=68767ca6e01b53f51c3f2ac5 name=Vespa_1752595622 models=root=[Model(name='pegasus1.2', options=['visual', 'audio'], addons=None, finetuned=False)]\n</pre> <p>We can now upload the videos:</p> In\u00a0[12]: Copied! <pre># Capturing index id for upload\nindex_id = index.id\n\ndef on_task_update(task: EmbeddingsTask):\n    print(f\"  Status={task.status}\")\n\n\nfor video_url in VIDEO_URLs:\n    # Create a video indexing task\n    task = client.task.create(index_id=index_id, url=video_url)\n    print(f\"Task created successfully! Task ID: {task.id}\")\n    status = task.wait_for_done(sleep_interval=10, callback=on_task_update)\n    print(f\"Indexing done: {status}\")\n    if task.status != \"ready\":\n        raise RuntimeError(f\"Indexing failed with status {task.status}\")\n    print(\n        f\"Uploaded {video_url}. The unique identifer of your video is {task.video_id}.\"\n    )\n</pre> # Capturing index id for upload index_id = index.id  def on_task_update(task: EmbeddingsTask):     print(f\"  Status={task.status}\")   for video_url in VIDEO_URLs:     # Create a video indexing task     task = client.task.create(index_id=index_id, url=video_url)     print(f\"Task created successfully! Task ID: {task.id}\")     status = task.wait_for_done(sleep_interval=10, callback=on_task_update)     print(f\"Indexing done: {status}\")     if task.status != \"ready\":         raise RuntimeError(f\"Indexing failed with status {task.status}\")     print(         f\"Uploaded {video_url}. The unique identifer of your video is {task.video_id}.\"     ) <pre>Task created successfully! Task ID: 68767caa47c93cd3ab1e4b05\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=pending\n  Status=ready\nIndexing done: Task(id='68767caa47c93cd3ab1e4b05', created_at='2025-07-15T16:07:08.998Z', updated_at='2025-07-15T16:07:08.998Z', index_id='68767ca6e01b53f51c3f2ac5', video_id='68767caa47c93cd3ab1e4b05', status='ready', system_metadata={'filename': 'The End(1080P_60FPS).ia.mp4', 'duration': 34.667392, 'width': 1920, 'height': 1080}, hls=TaskHLS(video_url='', thumbnail_urls=[], status='PROCESSING', updated_at='2025-07-15T16:07:08.998Z'))\nUploaded https://archive.org/download/the-end-blue-sky-studios/The%20End%281080P_60FPS%29.ia.mp4. The unique identifer of your video is 68767caa47c93cd3ab1e4b05.\nTask created successfully! Task ID: 68767ce06c4253f85f0820d0\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=indexing\n  Status=ready\nIndexing done: Task(id='68767ce06c4253f85f0820d0', created_at='2025-07-15T16:08:01.059Z', updated_at='2025-07-15T16:08:01.059Z', index_id='68767ca6e01b53f51c3f2ac5', video_id='68767ce06c4253f85f0820d0', status='ready', system_metadata={'filename': 'twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4', 'duration': 1448.88, 'width': 640, 'height': 480}, hls=TaskHLS(video_url='', thumbnail_urls=[], status='PROCESSING', updated_at='2025-07-15T16:08:01.059Z'))\nUploaded https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4. The unique identifer of your video is 68767ce06c4253f85f0820d0.\nTask created successfully! Task ID: 68767d7a03f1a1f6cd14797d\n  Status=pending\n  Status=indexing\n  Status=ready\nIndexing done: Task(id='68767d7a03f1a1f6cd14797d', created_at='2025-07-15T16:10:37.601Z', updated_at='2025-07-15T16:10:37.601Z', index_id='68767ca6e01b53f51c3f2ac5', video_id='68767d7a03f1a1f6cd14797d', status='ready', system_metadata={'filename': 'AnimationTest.mov', 'duration': 24.45679, 'width': 720, 'height': 405}, hls=TaskHLS(video_url='', thumbnail_urls=[], status='PROCESSING', updated_at='2025-07-15T16:10:37.601Z'))\nUploaded https://archive.org/download/The_Worm_in_the_Apple_Animation_Test/AnimationTest.mov. The unique identifer of your video is 68767d7a03f1a1f6cd14797d.\n</pre> <p>Now that the videos have been uploaded, we can generate the keywords, and summaries on the videos below. You will notice on the output that the video uploaded last is the one that is processed first in this stage. This matters since we store other attributes on the videos on arrays (eg URLs, Titles).</p> In\u00a0[13]: Copied! <pre>import textwrap\nclient = TwelveLabs(api_key=TL_API_KEY)\n\n\nsummaries = []\nkeywords_array = []\n\n# Get all videos in an Index\nvideos = client.index.video.list(index_id)\nfor video in videos:\n    print(f\"Generating text for {video.id}\")\n\n    res = client.summarize(\n        video_id=video.id,\n        type=\"summary\",\n        prompt=\"Generate an abstract of the video serving as metadata on the video, up to five sentences.\",\n    )\n    \n    wrapped = textwrap.wrap(res.summary, width=110)\n    print(\"Summary:\")\n    print(\"\\n\".join(wrapped))\n    summaries.append(res.summary)\n\n    keywords = client.analyze(\n        video_id=video.id,\n        prompt=\"Based on this video, I want to generate five keywords for SEO (Search Engine Optimization). Provide just the keywords as a comma delimited list without any additional text.\",\n    )\n    print(f\"Open-ended Text: {keywords.data}\")\n    keywords_array.append(keywords.data)\n</pre> import textwrap client = TwelveLabs(api_key=TL_API_KEY)   summaries = [] keywords_array = []  # Get all videos in an Index videos = client.index.video.list(index_id) for video in videos:     print(f\"Generating text for {video.id}\")      res = client.summarize(         video_id=video.id,         type=\"summary\",         prompt=\"Generate an abstract of the video serving as metadata on the video, up to five sentences.\",     )          wrapped = textwrap.wrap(res.summary, width=110)     print(\"Summary:\")     print(\"\\n\".join(wrapped))     summaries.append(res.summary)      keywords = client.analyze(         video_id=video.id,         prompt=\"Based on this video, I want to generate five keywords for SEO (Search Engine Optimization). Provide just the keywords as a comma delimited list without any additional text.\",     )     print(f\"Open-ended Text: {keywords.data}\")     keywords_array.append(keywords.data) <pre>Generating text for 68767d7a03f1a1f6cd14797d\nSummary:\nThe video titled \"The Worm in the Apple Animation Test\" showcases a whimsical scene where a segmented worm\nemerges from a red apple, positioned on the left side of the frame, and moves across a green field under a\ncloudy sky. As the worm progresses, its segments detach one by one, leaving the head connected to the last\nsegment, with the detached parts scattered around the base of the hill where the apple rests. The camera zooms\nout to reveal more of the grassy terrain and then focuses closely on the worm's face, which exhibits a range\nof expressions from surprise to anger, enhancing the animated narrative. The worm's journey ends as it crawls\noff-screen, leaving behind a visually engaging and animated sequence. The video is accompanied by a\nrepetitive, light-hearted musical score that adds to the playful tone of the animation.\nOpen-ended Text: worm, apple, animation, test, victor lyuboslavsky\nGenerating text for 68767ce06c4253f85f0820d0\nSummary:\nThe video is an animated adaptation of \"Twas The Night Before Christmas,\" featuring a blend of human and mouse\ncharacters. It begins with a snowy night scene and transitions to a clockmaker's workshop, where the\nclockmaker, Joshua Trundle, and his family face challenges after a critical letter to Santa is written by\nAlbert, Trundle's son. The story unfolds with the town's efforts to reconcile with Santa through a special\nclock designed to play a welcoming song on Christmas Eve, but complications arise when the clock malfunctions.\nDespite the setbacks, the family and community work together to fix the clock and restore belief in Santa,\nculminating in his magical arrival, bringing joy and gifts to all. The video concludes with a heartfelt\nmessage about the power of belief and the importance of making amends.\nOpen-ended Text: snowy village, clock tower, Santa Claus, mechanical gears, Christmas chimes\nGenerating text for 68767caa47c93cd3ab1e4b05\nSummary:\nThe video captures a serene snowy landscape with pine trees under a cloudy sky, where a squirrel emerges from\nbehind a rock formation carrying an acorn. Upon noticing another acorn in the foreground, the squirrel appears\nmomentarily surprised, as indicated by its vocalization \"Oh...\". It then drops one acorn and begins to nibble\non the other, eventually discarding fragments of it before leaping away. The scene concludes with the\nsquirrel's departure, leaving behind the remnants of the acorn, as darkness gradually engulfs the snowy\nsetting.\nOpen-ended Text: squirrel, acorn, winter, snow, forest\n</pre> <p>We need to store the titles of the videos as an additional attribute.</p> In\u00a0[14]: Copied! <pre># Creating array with titles\ntitles = [\n    \"The Worm in the Apple Animation Test\",\n    \"Twas the night before Christmas\",\n    \"The END (Blue Sky Studios)\",\n]\n</pre> # Creating array with titles titles = [     \"The Worm in the Apple Animation Test\",     \"Twas the night before Christmas\",     \"The END (Blue Sky Studios)\", ] In\u00a0[15]: Copied! <pre>client = TwelveLabs(api_key=TL_API_KEY)\n\n# Initialize an array to store the task IDs as strings\ntask_ids = []\n\nfor url in VIDEO_URLs:\n    task = client.embed.task.create(model_name=\"Marengo-retrieval-2.7\", video_url=url)\n    print(\n        f\"Created task: id={task.id} model_name={task.model_name} status={task.status}\"\n    )\n    # Append the task ID to the array\n    task_ids.append(str(task.id))\n    status = task.wait_for_done(sleep_interval=10, callback=on_task_update)\n    print(f\"Embedding done: {status}\")\n    if task.status != \"ready\":\n        raise RuntimeError(f\"Embedding failed with status {task.status}\")\n</pre> client = TwelveLabs(api_key=TL_API_KEY)  # Initialize an array to store the task IDs as strings task_ids = []  for url in VIDEO_URLs:     task = client.embed.task.create(model_name=\"Marengo-retrieval-2.7\", video_url=url)     print(         f\"Created task: id={task.id} model_name={task.model_name} status={task.status}\"     )     # Append the task ID to the array     task_ids.append(str(task.id))     status = task.wait_for_done(sleep_interval=10, callback=on_task_update)     print(f\"Embedding done: {status}\")     if task.status != \"ready\":         raise RuntimeError(f\"Embedding failed with status {task.status}\") <pre>Created task: id=6876856e4fc16ea9b2fdb823 model_name=Marengo-retrieval-2.7 status=processing\n  Status=processing\n  Status=processing\n  Status=ready\nEmbedding done: ready\nCreated task: id=68768593de7e2a0235058cc6 model_name=Marengo-retrieval-2.7 status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=processing\n  Status=ready\nEmbedding done: ready\nCreated task: id=6876860547c93cd3ab1e4cd7 model_name=Marengo-retrieval-2.7 status=processing\n  Status=processing\n  Status=ready\nEmbedding done: ready\n</pre> In\u00a0[16]: Copied! <pre># Spin-up session\nclient = TwelveLabs(api_key=TL_API_KEY)\n\n# Initialize an array to store the task objects directly\ntasks = []\n\nfor task_id in task_ids:\n    # Retrieve the task\n    task = client.embed.task.retrieve(task_id)\n    tasks.append(task)\n\n    # Print task details\n    print(f\"Task ID: {task.id}\")\n    print(f\"Status: {task.status}\")\n</pre> # Spin-up session client = TwelveLabs(api_key=TL_API_KEY)  # Initialize an array to store the task objects directly tasks = []  for task_id in task_ids:     # Retrieve the task     task = client.embed.task.retrieve(task_id)     tasks.append(task)      # Print task details     print(f\"Task ID: {task.id}\")     print(f\"Status: {task.status}\") <pre>Task ID: 6876856e4fc16ea9b2fdb823\nStatus: ready\nTask ID: 68768593de7e2a0235058cc6\nStatus: ready\nTask ID: 6876860547c93cd3ab1e4cd7\nStatus: ready\n</pre> <p>We can now review the output structure of the first segment for each one of these videos. This output will help us define the schema to store the embeddings in Vespa in the second part of this notebook.</p> <p>From looking at this output, the video has been embedded into chunks of 6 seconds each (default configurable value in the Embed API). Each embedding has a float vector of dimension 1024.</p> <p>The number of segments generated vary per video, based on the length of the videos ranging from 37 to 242 segments.</p> In\u00a0[17]: Copied! <pre>for task in tasks:\n    print(task.id)\n    # Display data types of each field\n    for key, value in task.video_embedding.segments[0]:\n        if isinstance(value, list):\n            print(\n                f\"{key}: list of size {len(value)} (truncated to 5 items): {value[:5]} \"\n            )\n        else:\n            print(f\"{key}: {type(value).__name__} : {value}\")\n    print(f\"Total Number of segments: {len(task.video_embedding.segments)}\")\n</pre> for task in tasks:     print(task.id)     # Display data types of each field     for key, value in task.video_embedding.segments[0]:         if isinstance(value, list):             print(                 f\"{key}: list of size {len(value)} (truncated to 5 items): {value[:5]} \"             )         else:             print(f\"{key}: {type(value).__name__} : {value}\")     print(f\"Total Number of segments: {len(task.video_embedding.segments)}\") <pre>6876856e4fc16ea9b2fdb823\nstart_offset_sec: float : 0.0\nend_offset_sec: float : 6.0\nembedding_scope: str : clip\nembedding_option: str : visual-text\nembeddings_float: list of size 1024 (truncated to 5 items): [0.0227238, -0.002079417, 0.01519275, -0.009030234, -0.00162781] \nTotal Number of segments: 12\n68768593de7e2a0235058cc6\nstart_offset_sec: float : 0.0\nend_offset_sec: float : 6.0\nembedding_scope: str : clip\nembedding_option: str : visual-text\nembeddings_float: list of size 1024 (truncated to 5 items): [0.024328815, -0.0035867887, 0.016065866, 0.02501548, 0.007778642] \nTotal Number of segments: 484\n6876860547c93cd3ab1e4cd7\nstart_offset_sec: float : 0.0\nend_offset_sec: float : 6.0\nembedding_scope: str : clip\nembedding_option: str : visual-text\nembeddings_float: list of size 1024 (truncated to 5 items): [0.05419811, -0.0018933096, 0.008044507, -0.01940344, 0.013152712] \nTotal Number of segments: 8\n</pre> In\u00a0[18]: Copied! <pre>videos_schema = Schema(\n    name=\"videos\",\n    document=Document(\n        fields=[\n            Field(name=\"video_url\", type=\"string\", indexing=[\"summary\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"keywords\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"video_summary\",\n                type=\"string\",\n                indexing=[\"index\", \"summary\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"embedding_scope\", type=\"string\", indexing=[\"attribute\", \"summary\"]\n            ),\n            Field(\n                name=\"start_offset_sec\",\n                type=\"array&lt;float&gt;\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n            Field(\n                name=\"end_offset_sec\",\n                type=\"array&lt;float&gt;\",\n                indexing=[\"attribute\", \"summary\"],\n            ),\n            Field(\n                name=\"embeddings\",\n                type=\"tensor&lt;float&gt;(p{},x[1024])\",\n                indexing=[\"index\", \"attribute\"],\n                ann=HNSW(distance_metric=\"angular\"),\n            ),\n        ]\n    ),\n)\n\nfieldsets = (\n    [\n        FieldSet(\n            name=\"default\",\n            fields=[\"title\", \"keywords\", \"video_summary\"],\n        ),\n    ],\n)\n\nmapfunctions = [\n    Function(\n        name=\"similarities\",\n        expression=\"\"\"\n                      sum(\n                          query(q) * attribute(embeddings), x\n                          )\n                      \"\"\",\n    ),\n    Function(\n        name=\"bm25_score\",\n        expression=\"bm25(title) + bm25(keywords) + bm25(video_summary)\",\n    ),\n]\n\nsemantic_rankprofile = RankProfile(\n    name=\"hybrid\",\n    inputs=[(\"query(q)\", \"tensor&lt;float&gt;(x[1024])\")],\n    first_phase=\"bm25_score\",\n    second_phase=SecondPhaseRanking(\n        expression=\"closeness(field, embeddings)\", rerank_count=10\n    ),\n    match_features=[\"closest(embeddings)\"],\n    summary_features=[\"similarities\"],\n    functions=mapfunctions,\n)\n\nvideos_schema.add_rank_profile(semantic_rankprofile)\n</pre> videos_schema = Schema(     name=\"videos\",     document=Document(         fields=[             Field(name=\"video_url\", type=\"string\", indexing=[\"summary\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"keywords\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"video_summary\",                 type=\"string\",                 indexing=[\"index\", \"summary\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"embedding_scope\", type=\"string\", indexing=[\"attribute\", \"summary\"]             ),             Field(                 name=\"start_offset_sec\",                 type=\"array\",                 indexing=[\"attribute\", \"summary\"],             ),             Field(                 name=\"end_offset_sec\",                 type=\"array\",                 indexing=[\"attribute\", \"summary\"],             ),             Field(                 name=\"embeddings\",                 type=\"tensor(p{},x[1024])\",                 indexing=[\"index\", \"attribute\"],                 ann=HNSW(distance_metric=\"angular\"),             ),         ]     ), )  fieldsets = (     [         FieldSet(             name=\"default\",             fields=[\"title\", \"keywords\", \"video_summary\"],         ),     ], )  mapfunctions = [     Function(         name=\"similarities\",         expression=\"\"\"                       sum(                           query(q) * attribute(embeddings), x                           )                       \"\"\",     ),     Function(         name=\"bm25_score\",         expression=\"bm25(title) + bm25(keywords) + bm25(video_summary)\",     ), ]  semantic_rankprofile = RankProfile(     name=\"hybrid\",     inputs=[(\"query(q)\", \"tensor(x[1024])\")],     first_phase=\"bm25_score\",     second_phase=SecondPhaseRanking(         expression=\"closeness(field, embeddings)\", rerank_count=10     ),     match_features=[\"closest(embeddings)\"],     summary_features=[\"similarities\"],     functions=mapfunctions, )  videos_schema.add_rank_profile(semantic_rankprofile) <p>We can now create the package based on the previous schema</p> In\u00a0[19]: Copied! <pre># Create the Vespa application package\npackage = ApplicationPackage(name=application, schema=[videos_schema])\n</pre> # Create the Vespa application package package = ApplicationPackage(name=application, schema=[videos_schema]) In\u00a0[20]: Copied! <pre>vespa_cloud = VespaCloud(\n    tenant=tenant_name,\n    application=application,\n    application_package=package,\n    key_content=os.getenv(\"VESPA_TEAM_API_KEY\", None),\n)\n</pre> vespa_cloud = VespaCloud(     tenant=tenant_name,     application=application,     application_package=package,     key_content=os.getenv(\"VESPA_TEAM_API_KEY\", None), ) <pre>Setting application...\nRunning: vespa config set application vespa-presales.videosearch.default\nSetting target cloud...\nRunning: vespa config set target cloud\n\nNo api-key found for control plane access. Using access token.\nChecking for access token in auth.json...\nAccess token expired. Please re-authenticate.\nYour Device Confirmation code is: MJKL-VTBW\nAutomatically open confirmation page in your default browser? [Y/n] \nOpened link in your browser: https://login.console.vespa-cloud.com/activate?user_code=MJKL-VTBW\nWaiting for login to complete in browser ... done;1m\u28fd\nSuccess: Logged in\n auth.json created at /Users/zohar/.vespa/auth.json\nSuccessfully obtained access token for control plane access.\n</pre> In\u00a0[21]: Copied! <pre>app = vespa_cloud.deploy()\n</pre> app = vespa_cloud.deploy() <pre>Deployment started in run 19 of dev-aws-us-east-1c for vespa-presales.videosearch. This may take a few minutes the first time.\nINFO    [16:48:18]  Deploying platform version 8.547.15 and application dev build 11 for dev-aws-us-east-1c of default ...\nINFO    [16:48:18]  Using CA signed certificate version 3\nINFO    [16:48:18]  Using 1 nodes in container cluster 'videosearch_container'\nINFO    [16:48:21]  Session 7523 for tenant 'vespa-presales' prepared and activated.\nINFO    [16:48:21]  ######## Details for all nodes ########\nINFO    [16:48:21]  h121570a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [16:48:21]  --- platform vespa/cloud-tenant-rhel8:8.547.15\nINFO    [16:48:21]  --- container on port 4080 has config generation 7522, wanted is 7523\nINFO    [16:48:21]  --- metricsproxy-container on port 19092 has config generation 7522, wanted is 7523\nINFO    [16:48:21]  h119160h.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [16:48:21]  --- platform vespa/cloud-tenant-rhel8:8.547.15\nINFO    [16:48:21]  --- container-clustercontroller on port 19050 has config generation 7522, wanted is 7523\nINFO    [16:48:21]  --- metricsproxy-container on port 19092 has config generation 7523, wanted is 7523\nINFO    [16:48:21]  h117409h.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [16:48:21]  --- platform vespa/cloud-tenant-rhel8:8.547.15\nINFO    [16:48:21]  --- logserver-container on port 4080 has config generation 7523, wanted is 7523\nINFO    [16:48:21]  --- metricsproxy-container on port 19092 has config generation 7522, wanted is 7523\nINFO    [16:48:21]  h121486b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\nINFO    [16:48:21]  --- platform vespa/cloud-tenant-rhel8:8.547.15\nINFO    [16:48:21]  --- storagenode on port 19102 has config generation 7522, wanted is 7523\nINFO    [16:48:21]  --- searchnode on port 19107 has config generation 7523, wanted is 7523\nINFO    [16:48:21]  --- distributor on port 19111 has config generation 7523, wanted is 7523\nINFO    [16:48:21]  --- metricsproxy-container on port 19092 has config generation 7523, wanted is 7523\nINFO    [16:48:29]  Found endpoints:\nINFO    [16:48:29]  - dev.aws-us-east-1c\nINFO    [16:48:29]   |-- https://d4ed0f5e.ee8b6819.z.vespa-app.cloud/ (cluster 'videosearch_container')\nINFO    [16:48:30]  Deployment of new application revision complete!\nOnly region: aws-us-east-1c available in dev environment.\nFound mtls endpoint for videosearch_container\nURL: https://d4ed0f5e.ee8b6819.z.vespa-app.cloud/\nApplication is up!\n</pre> In\u00a0[22]: Copied! <pre># Initialize a list to store Vespa feed documents\nvespa_feed = []\n\n# Need to reverse VIDEO_URLS as keywords/summaries generated in reverse order\nVIDEO_URLs.reverse()\n\n# Iterate through each task and corresponding metadata\nfor i, task in enumerate(tasks):\n    video_url = VIDEO_URLs[i]\n    title = titles[i]\n    keywords = keywords_array[i]\n    summary = summaries[i]\n\n    start_offsets = []  # Reset for each video\n    end_offsets = []  # Reset for each video\n    embeddings = {}  # Reset for each video\n\n    # Iterate through the video embedding segments\n    for index, segment in enumerate(task.video_embedding.segments):\n        # Append start and end offsets as floats\n        start_offsets.append(float(segment.start_offset_sec))\n        end_offsets.append(float(segment.end_offset_sec))\n\n        # Add embedding to a multi-dimensional dictionary with index as the key\n        embeddings[str(index)] = list(map(float, segment.embeddings_float))\n\n    # Create Vespa document for each task\n    for segment in task.video_embedding.segments:\n        start_offset_sec = segment.start_offset_sec\n        end_offset_sec = segment.end_offset_sec\n        embedding = list(map(float, segment.embeddings_float))\n\n        # Create a unique ID by hashing the URL and segment index\n        id_hash = hashlib.md5(f\"{video_url}_{index}\".encode()).hexdigest()\n\n        document = {\n            \"id\": id_hash,\n            \"fields\": {\n                \"video_url\": video_url,\n                \"title\": title,\n                \"keywords\": keywords,\n                \"video_summary\": summary,\n                \"embedding_scope\": segment.embedding_scope,\n                \"start_offset_sec\": start_offsets,\n                \"end_offset_sec\": end_offsets,\n                \"embeddings\": embeddings,\n            },\n        }\n    vespa_feed.append(document)\n</pre> # Initialize a list to store Vespa feed documents vespa_feed = []  # Need to reverse VIDEO_URLS as keywords/summaries generated in reverse order VIDEO_URLs.reverse()  # Iterate through each task and corresponding metadata for i, task in enumerate(tasks):     video_url = VIDEO_URLs[i]     title = titles[i]     keywords = keywords_array[i]     summary = summaries[i]      start_offsets = []  # Reset for each video     end_offsets = []  # Reset for each video     embeddings = {}  # Reset for each video      # Iterate through the video embedding segments     for index, segment in enumerate(task.video_embedding.segments):         # Append start and end offsets as floats         start_offsets.append(float(segment.start_offset_sec))         end_offsets.append(float(segment.end_offset_sec))          # Add embedding to a multi-dimensional dictionary with index as the key         embeddings[str(index)] = list(map(float, segment.embeddings_float))      # Create Vespa document for each task     for segment in task.video_embedding.segments:         start_offset_sec = segment.start_offset_sec         end_offset_sec = segment.end_offset_sec         embedding = list(map(float, segment.embeddings_float))          # Create a unique ID by hashing the URL and segment index         id_hash = hashlib.md5(f\"{video_url}_{index}\".encode()).hexdigest()          document = {             \"id\": id_hash,             \"fields\": {                 \"video_url\": video_url,                 \"title\": title,                 \"keywords\": keywords,                 \"video_summary\": summary,                 \"embedding_scope\": segment.embedding_scope,                 \"start_offset_sec\": start_offsets,                 \"end_offset_sec\": end_offsets,                 \"embeddings\": embeddings,             },         }     vespa_feed.append(document) <p>We can quickly validate the number of the number of documents created (one for each video), and visually check the first record.</p> In\u00a0[23]: Copied! <pre># Print Vespa feed size and an example\nprint(f\"Total documents created: {len(vespa_feed)}\")\n</pre> # Print Vespa feed size and an example print(f\"Total documents created: {len(vespa_feed)}\") <pre>Total documents created: 3\n</pre> In\u00a0[24]: Copied! <pre># The positional index of the document\ni = 0\n\n# Iterate through the first 3 embeddings in vespa_feed\nfor i in range(\n    min(3, len(vespa_feed))\n):  # Ensure we don't exceed the length of vespa_feed\n    # Limit the embedding to the first 3 keys and first 5 values for each key\n    embedding = vespa_feed[i][\"fields\"][\"embeddings\"]\n    embedding_sample = {key: values[:3] for key, values in list(embedding.items())[:3]}\n\n# Beautify and print the first document with only the first 5 embedding values\npretty_json = json.dumps(\n    {\n        \"id\": vespa_feed[i][\"id\"],\n        \"fields\": {\n            \"video_url\": vespa_feed[i][\"fields\"][\"video_url\"],\n            \"title\": vespa_feed[i][\"fields\"][\"title\"],\n            \"keywords\": vespa_feed[i][\"fields\"][\"keywords\"],\n            \"video_summary\": vespa_feed[i][\"fields\"][\"video_summary\"],\n            \"embedding_scope\": vespa_feed[i][\"fields\"][\"embedding_scope\"],\n            \"start_offset_sec\": vespa_feed[i][\"fields\"][\"start_offset_sec\"][:3],\n            \"end_offset_sec\": vespa_feed[i][\"fields\"][\"end_offset_sec\"][:3],\n            \"embedding\": embedding_sample,\n        },\n    },\n    indent=4,\n)\n\nprint(pretty_json)\n</pre> # The positional index of the document i = 0  # Iterate through the first 3 embeddings in vespa_feed for i in range(     min(3, len(vespa_feed)) ):  # Ensure we don't exceed the length of vespa_feed     # Limit the embedding to the first 3 keys and first 5 values for each key     embedding = vespa_feed[i][\"fields\"][\"embeddings\"]     embedding_sample = {key: values[:3] for key, values in list(embedding.items())[:3]}  # Beautify and print the first document with only the first 5 embedding values pretty_json = json.dumps(     {         \"id\": vespa_feed[i][\"id\"],         \"fields\": {             \"video_url\": vespa_feed[i][\"fields\"][\"video_url\"],             \"title\": vespa_feed[i][\"fields\"][\"title\"],             \"keywords\": vespa_feed[i][\"fields\"][\"keywords\"],             \"video_summary\": vespa_feed[i][\"fields\"][\"video_summary\"],             \"embedding_scope\": vespa_feed[i][\"fields\"][\"embedding_scope\"],             \"start_offset_sec\": vespa_feed[i][\"fields\"][\"start_offset_sec\"][:3],             \"end_offset_sec\": vespa_feed[i][\"fields\"][\"end_offset_sec\"][:3],             \"embedding\": embedding_sample,         },     },     indent=4, )  print(pretty_json) <pre>{\n    \"id\": \"93d8476bee530eb39a2122f586d0d13a\",\n    \"fields\": {\n        \"video_url\": \"https://archive.org/download/the-end-blue-sky-studios/The%20End%281080P_60FPS%29.ia.mp4\",\n        \"title\": \"The END (Blue Sky Studios)\",\n        \"keywords\": \"squirrel, acorn, winter, snow, forest\",\n        \"video_summary\": \"The video captures a serene snowy landscape with pine trees under a cloudy sky, where a squirrel emerges from behind a rock formation carrying an acorn. Upon noticing another acorn in the foreground, the squirrel appears momentarily surprised, as indicated by its vocalization \\\"Oh...\\\". It then drops one acorn and begins to nibble on the other, eventually discarding fragments of it before leaping away. The scene concludes with the squirrel's departure, leaving behind the remnants of the acorn, as darkness gradually engulfs the snowy setting.\",\n        \"embedding_scope\": \"clip\",\n        \"start_offset_sec\": [\n            0.0,\n            6.0,\n            12.0\n        ],\n        \"end_offset_sec\": [\n            6.0,\n            12.0,\n            18.0\n        ],\n        \"embedding\": {\n            \"0\": [\n                0.05419811,\n                -0.0018933096,\n                0.008044507\n            ],\n            \"1\": [\n                0.016035125,\n                -0.015930071,\n                0.022429857\n            ],\n            \"2\": [\n                0.014023403,\n                -0.012773005,\n                0.019988379\n            ]\n        }\n    }\n}\n</pre> <p>Now we can feed to Vespa using <code>feed_iterable</code> which accepts any <code>Iterable</code> and an optional callback function where we can check the outcome of each operation.</p> In\u00a0[25]: Copied! <pre>def callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\n# Feed data into Vespa synchronously\napp.feed_iterable(vespa_feed, schema=\"videos\", callback=callback)\n</pre> def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   # Feed data into Vespa synchronously app.feed_iterable(vespa_feed, schema=\"videos\", callback=callback) In\u00a0[28]: Copied! <pre>client = TwelveLabs(api_key=TL_API_KEY)\nuser_query = \"Santa Claus on his sleigh\"\n\nres = client.embed.create(\n    model_name=\"Marengo-retrieval-2.7\",\n    text=user_query,\n)\n\nprint(\"Created a text embedding\")\nprint(f\" Model: {res.model_name}\")\nif res.text_embedding is not None and res.text_embedding.segments is not None:\n    q_embedding = res.text_embedding.segments[0].embeddings_float\n    print(f\" Embedding Dimension: {len(q_embedding)}\")\n    print(f\" Sample 5 values from array: {q_embedding[:5]}\")\n</pre> client = TwelveLabs(api_key=TL_API_KEY) user_query = \"Santa Claus on his sleigh\"  res = client.embed.create(     model_name=\"Marengo-retrieval-2.7\",     text=user_query, )  print(\"Created a text embedding\") print(f\" Model: {res.model_name}\") if res.text_embedding is not None and res.text_embedding.segments is not None:     q_embedding = res.text_embedding.segments[0].embeddings_float     print(f\" Embedding Dimension: {len(q_embedding)}\")     print(f\" Sample 5 values from array: {q_embedding[:5]}\") <pre>Created a text embedding\n Model: Marengo-retrieval-2.7\n Embedding Dimension: 1024\n Sample 5 values from array: [-0.018066406, -0.0065307617, 0.05859375, -0.033447266, -0.02368164]\n</pre> <p>The following uses dense vector representations of the query embedding obtained previously and document and matching is performed and accelerated by Vespa's support for approximate nearest neighbor search.</p> <p>The output is limited to the top 1 hit, as we only have a sample of 3 videos. The top hit returned was based on a hybrid ranking based on a bm25 ranking based on a lexical search on the text, keywords and summary of the video, performed as a first phase, and similarity search on the embeddings.</p> <p>We can see as part of the <code>match-features</code>, the segment 212 in the video was the one providing the highest match.</p> <p>We also calculate the similarities as part of the <code>summary-features</code> for the rest of the segments so we can look for top N segments within a video, optionally.</p> In\u00a0[29]: Copied! <pre>with app.syncio(connections=1) as session:\n    response: VespaQueryResponse = session.query(\n        yql=\"select * from videos where userQuery() OR ({targetHits:100}nearestNeighbor(embeddings,q))\",\n        query=user_query,\n        ranking=\"hybrid\",\n        hits=1,\n        body={\"input.query(q)\": q_embedding},\n    )\n    assert response.is_successful()\n\nhit = response.hits[0]\n\n# Extract metadata\ndoc_id = hit.get(\"id\")\nrelevance = hit.get(\"relevance\")\nsource = hit.get(\"source\")\nfields = hit.get(\"fields\", {})\n\n# Extract the embedding match cell index (first key in matchfeatures)\nmatch_cells = fields.get(\"matchfeatures\", {}).get(\"closest(embeddings)\", {}).get(\"cells\", {})\nif not match_cells:\n    raise ValueError(\"No cells found in matchfeatures.closest(embeddings)\")\n\n# Get the first (and only) cell key and value\ncell_index, cell_value = next(iter(match_cells.items()))\ncell_index = int(cell_index)  # Convert key from string to int\n\n# Extract aligned fields using the index\nstart_offset = fields.get(\"start_offset_sec\", [])[cell_index]\nend_offset = fields.get(\"end_offset_sec\", [])[cell_index]\nsimilarity = fields.get(\"summaryfeatures\", {}).get(\"similarities\", {}).get(\"cells\", {}).get(str(cell_index))\n\n# Print full info\nprint(\"Document Metadata:\")\nprint(f\"documentid: {doc_id}\")\nprint(f\"Relevance: {relevance}\")\nprint(f\"Source: {source}\")\nprint(f\"Match Features: {fields.get('matchfeatures', 'N/A')}\")\nprint()\n\nprint(f\"Title: {fields.get('title', 'N/A')}\")\nprint(f\"Keywords: {fields.get('keywords', 'N/A')}\")\nprint(f\"Video URL: {fields.get('video_url', 'N/A')}\")\nprint(f\"Video Summary: {fields.get('video_summary', 'N/A')}\")\nprint(f\"Embedding Scope: {fields.get('embedding_scope', 'N/A')}\")\nprint()\n\n# Print details for the matched cell\nprint(f\"Details for cell {cell_index}:\")\nprint(f\"Start offset: {start_offset} sec\")\nprint(f\"End offset: {end_offset} sec\")\nprint(f\"Similarity score: {similarity}\")\nprint(f\"Match feature score: {cell_value}\")\n</pre> with app.syncio(connections=1) as session:     response: VespaQueryResponse = session.query(         yql=\"select * from videos where userQuery() OR ({targetHits:100}nearestNeighbor(embeddings,q))\",         query=user_query,         ranking=\"hybrid\",         hits=1,         body={\"input.query(q)\": q_embedding},     )     assert response.is_successful()  hit = response.hits[0]  # Extract metadata doc_id = hit.get(\"id\") relevance = hit.get(\"relevance\") source = hit.get(\"source\") fields = hit.get(\"fields\", {})  # Extract the embedding match cell index (first key in matchfeatures) match_cells = fields.get(\"matchfeatures\", {}).get(\"closest(embeddings)\", {}).get(\"cells\", {}) if not match_cells:     raise ValueError(\"No cells found in matchfeatures.closest(embeddings)\")  # Get the first (and only) cell key and value cell_index, cell_value = next(iter(match_cells.items())) cell_index = int(cell_index)  # Convert key from string to int  # Extract aligned fields using the index start_offset = fields.get(\"start_offset_sec\", [])[cell_index] end_offset = fields.get(\"end_offset_sec\", [])[cell_index] similarity = fields.get(\"summaryfeatures\", {}).get(\"similarities\", {}).get(\"cells\", {}).get(str(cell_index))  # Print full info print(\"Document Metadata:\") print(f\"documentid: {doc_id}\") print(f\"Relevance: {relevance}\") print(f\"Source: {source}\") print(f\"Match Features: {fields.get('matchfeatures', 'N/A')}\") print()  print(f\"Title: {fields.get('title', 'N/A')}\") print(f\"Keywords: {fields.get('keywords', 'N/A')}\") print(f\"Video URL: {fields.get('video_url', 'N/A')}\") print(f\"Video Summary: {fields.get('video_summary', 'N/A')}\") print(f\"Embedding Scope: {fields.get('embedding_scope', 'N/A')}\") print()  # Print details for the matched cell print(f\"Details for cell {cell_index}:\") print(f\"Start offset: {start_offset} sec\") print(f\"End offset: {end_offset} sec\") print(f\"Similarity score: {similarity}\") print(f\"Match feature score: {cell_value}\")   <pre>Document Metadata:\ndocumentid: id:videos:videos::d4175516790d7e55a79eb7f190495a92\nRelevance: 0.47162757625475055\nSource: videosearch_content\nMatch Features: {'closest(embeddings)': {'type': 'tensor&lt;float&gt;(p{})', 'cells': {'212': 1.0}}}\n\nTitle: Twas the night before Christmas\nKeywords: snowy village, clock tower, Santa Claus, mechanical gears, Christmas chimes\nVideo URL: https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\nVideo Summary: The video is an animated adaptation of \"Twas The Night Before Christmas,\" featuring a blend of human and mouse characters. It begins with a snowy night scene and transitions to a clockmaker's workshop, where the clockmaker, Joshua Trundle, and his family face challenges after a critical letter to Santa is written by Albert, Trundle's son. The story unfolds with the town's efforts to reconcile with Santa through a special clock designed to play a welcoming song on Christmas Eve, but complications arise when the clock malfunctions. Despite the setbacks, the family and community work together to fix the clock and restore belief in Santa, culminating in his magical arrival, bringing joy and gifts to all. The video concludes with a heartfelt message about the power of belief and the importance of making amends.\nEmbedding Scope: clip\n\nDetails for cell 212:\nStart offset: 1272.0 sec\nEnd offset: 1278.0 sec\nSimilarity score: 0.43537065386772156\nMatch feature score: 1.0\n</pre> <p>You should see output similar to this:</p> <pre><code>Document\ndocumentid: id:videos:videos::d4175516790d7e55a79eb7f190495a92\nRelevance: 0.47162757625475055\nSource: videosearch_content\nMatch Features: {'closest(embeddings)': {'type': 'tensor&lt;float&gt;(p{})', 'cells': {'212': 1.0}}}\n\nTitle: Twas the night before Christmas\nKeywords: snowy village, clock tower, Santa Claus, mechanical gears, Christmas chimes\nVideo URL: https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\nVideo Summary: The video is an animated adaptation of \"Twas The Night Before Christmas,\" featuring a blend of human and mouse characters. It begins with a snowy night scene and transitions to a clockmaker's workshop, where the clockmaker, Joshua Trundle, and his family face challenges after a critical letter to Santa is written by Albert, Trundle's son. The story unfolds with the town's efforts to reconcile with Santa through a special clock designed to play a welcoming song on Christmas Eve, but complications arise when the clock malfunctions. Despite the setbacks, the family and community work together to fix the clock and restore belief in Santa, culminating in his magical arrival, bringing joy and gifts to all. The video concludes with a heartfelt message about the power of belief and the importance of making amends.\nEmbedding Scope: clip\n\nDetails for cell 212:\nStart offset: 1272.0 sec\nEnd offset: 1278.0 sec\nSimilarity score: 0.43537065386772156\nMatch feature score: 1.0```\n</code></pre> <p>In order to process the results above in a more consumable format and sort out the top N segments based on similarities, we can do this more conveniently in a pandas dataframe below:</p> In\u00a0[37]: Copied! <pre>def get_top_n_similarity_matches(data, N=5):\n    \"\"\"\n    Function to extract the top N similarity scores and their corresponding start and end offsets.\n\n    Args:\n    - data (dict): Input JSON-like structure containing similarities and offsets.\n    - N (int): The number of top similarity scores to return.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with the top N similarity scores and their corresponding offsets.\n    \"\"\"\n    # Extract relevant fields\n    similarities = data[\"fields\"][\"summaryfeatures\"][\"similarities\"][\"cells\"]\n    start_offset_sec = data[\"fields\"][\"start_offset_sec\"]\n    end_offset_sec = data[\"fields\"][\"end_offset_sec\"]\n\n    # Convert similarity scores to a list of tuples (index, similarity_score) and sort by similarity score\n    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n\n    # Extract top N similarity scores\n    top_n_similarities = sorted_similarities[:N]\n\n    # Prepare results\n    results = []\n    for index_str, score in top_n_similarities:\n        index = int(index_str)\n        if index &lt; len(start_offset_sec):\n            result = {\n                \"index\": index,\n                \"similarity_score\": score,\n                \"start_offset_sec\": start_offset_sec[index],\n                \"end_offset_sec\": end_offset_sec[index],\n            }\n        else:\n            result = {\n                \"index\": index,\n                \"similarity_score\": score,\n                \"start_offset_sec\": None,\n                \"end_offset_sec\": None,\n            }\n        results.append(result)\n\n    # Convert results to a DataFrame\n    df = pd.DataFrame(results)\n    return df\n</pre> def get_top_n_similarity_matches(data, N=5):     \"\"\"     Function to extract the top N similarity scores and their corresponding start and end offsets.      Args:     - data (dict): Input JSON-like structure containing similarities and offsets.     - N (int): The number of top similarity scores to return.      Returns:     - pd.DataFrame: A DataFrame with the top N similarity scores and their corresponding offsets.     \"\"\"     # Extract relevant fields     similarities = data[\"fields\"][\"summaryfeatures\"][\"similarities\"][\"cells\"]     start_offset_sec = data[\"fields\"][\"start_offset_sec\"]     end_offset_sec = data[\"fields\"][\"end_offset_sec\"]      # Convert similarity scores to a list of tuples (index, similarity_score) and sort by similarity score     sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)      # Extract top N similarity scores     top_n_similarities = sorted_similarities[:N]      # Prepare results     results = []     for index_str, score in top_n_similarities:         index = int(index_str)         if index &lt; len(start_offset_sec):             result = {                 \"index\": index,                 \"similarity_score\": score,                 \"start_offset_sec\": start_offset_sec[index],                 \"end_offset_sec\": end_offset_sec[index],             }         else:             result = {                 \"index\": index,                 \"similarity_score\": score,                 \"start_offset_sec\": None,                 \"end_offset_sec\": None,             }         results.append(result)      # Convert results to a DataFrame     df = pd.DataFrame(results)     return df In\u00a0[38]: Copied! <pre>df_result = get_top_n_similarity_matches(response.hits[0], N=10)\ndf_result\n</pre> df_result = get_top_n_similarity_matches(response.hits[0], N=10) df_result Out[38]: index similarity_score start_offset_sec end_offset_sec 0 212 0.435371 1272.0 1278.0 1 230 0.418007 1380.0 1386.0 2 210 0.411242 1260.0 1266.0 3 211 0.409344 1266.0 1272.0 4 208 0.408644 1248.0 1254.0 5 231 0.406000 1386.0 1392.0 6 209 0.404767 1254.0 1260.0 7 229 0.403729 1374.0 1380.0 8 203 0.403292 1218.0 1224.0 9 207 0.391671 1242.0 1248.0 In\u00a0[40]: Copied! <pre>def concatenate_contiguous_segments(df):\n    \"\"\"\n    Function to concatenate contiguous segments based on their start and end offsets.\n    Converts the concatenated segments to MM:SS format.\n\n    Args:\n    - df (pd.DataFrame): DataFrame with columns 'start_offset_sec' and 'end_offset_sec'.\n\n    Returns:\n    - List of tuples with concatenated segments in MM:SS format as (start_time, end_time).\n    \"\"\"\n    if df.empty:\n        return []\n\n    # Sort by start_offset_sec for ordered processing\n    df = df.sort_values(by=\"start_offset_sec\").reset_index(drop=True)\n\n    # Initialize the list to hold concatenated segments\n    concatenated_segments = []\n\n    # Initialize the first segment\n    start = df.iloc[0][\"start_offset_sec\"]\n    end = df.iloc[0][\"end_offset_sec\"]\n\n    for i in range(1, len(df)):\n        current_start = df.iloc[i][\"start_offset_sec\"]\n        current_end = df.iloc[i][\"end_offset_sec\"]\n\n        # Check if the current segment is contiguous with the previous one\n        if current_start &lt;= end:\n            # Extend the segment if it is contiguous\n            end = max(end, current_end)\n        else:\n            # Add the previous segment to the result list in MM:SS format\n            concatenated_segments.append(\n                (convert_seconds_to_mmss(start - 3), convert_seconds_to_mmss(end + 3))\n            )\n            # Start a new segment\n            start = current_start\n            end = current_end\n\n    # Add the final segment\n    concatenated_segments.append(\n        (convert_seconds_to_mmss(start - 3), convert_seconds_to_mmss(end + 3))\n    )\n\n    return concatenated_segments\n\n\ndef convert_seconds_to_mmss(seconds):\n    \"\"\"\n    Converts seconds to MM:SS format.\n\n    Args:\n    - seconds (float): Time in seconds.\n\n    Returns:\n    - str: Time in MM:SS format.\n    \"\"\"\n    minutes = int(seconds // 60)\n    seconds = int(seconds % 60)\n    return f\"{minutes:02}:{seconds:02}\"\n</pre> def concatenate_contiguous_segments(df):     \"\"\"     Function to concatenate contiguous segments based on their start and end offsets.     Converts the concatenated segments to MM:SS format.      Args:     - df (pd.DataFrame): DataFrame with columns 'start_offset_sec' and 'end_offset_sec'.      Returns:     - List of tuples with concatenated segments in MM:SS format as (start_time, end_time).     \"\"\"     if df.empty:         return []      # Sort by start_offset_sec for ordered processing     df = df.sort_values(by=\"start_offset_sec\").reset_index(drop=True)      # Initialize the list to hold concatenated segments     concatenated_segments = []      # Initialize the first segment     start = df.iloc[0][\"start_offset_sec\"]     end = df.iloc[0][\"end_offset_sec\"]      for i in range(1, len(df)):         current_start = df.iloc[i][\"start_offset_sec\"]         current_end = df.iloc[i][\"end_offset_sec\"]          # Check if the current segment is contiguous with the previous one         if current_start &lt;= end:             # Extend the segment if it is contiguous             end = max(end, current_end)         else:             # Add the previous segment to the result list in MM:SS format             concatenated_segments.append(                 (convert_seconds_to_mmss(start - 3), convert_seconds_to_mmss(end + 3))             )             # Start a new segment             start = current_start             end = current_end      # Add the final segment     concatenated_segments.append(         (convert_seconds_to_mmss(start - 3), convert_seconds_to_mmss(end + 3))     )      return concatenated_segments   def convert_seconds_to_mmss(seconds):     \"\"\"     Converts seconds to MM:SS format.      Args:     - seconds (float): Time in seconds.      Returns:     - str: Time in MM:SS format.     \"\"\"     minutes = int(seconds // 60)     seconds = int(seconds % 60)     return f\"{minutes:02}:{seconds:02}\" In\u00a0[41]: Copied! <pre>segments = concatenate_contiguous_segments(df_result)\nsegments\n</pre> segments = concatenate_contiguous_segments(df_result) segments Out[41]: <pre>[('20:15', '20:27'), ('20:39', '21:21'), ('22:51', '23:15')]</pre> <p>We can now spin-up the player and review the segments of interest. Video player is set to start in the middle of the first segment.</p> In\u00a0[42]: Copied! <pre>from IPython.display import HTML\n\nvideo_url = \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\"\n\nvideo_player = f\"\"\"\n&lt;video id=\"myVideo\" width=\"640\" height=\"480\" controls&gt;\n  &lt;source src=\"{video_url}\" type=\"video/mp4\"&gt;\n  Your browser does not support the video tag.\n&lt;/video&gt;\n\n\"\"\"\n\nHTML(video_player)\n</pre> from IPython.display import HTML  video_url = \"https://ia601401.us.archive.org/1/items/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net/twas-the-night-before-christmas-1974-full-movie-freedownloadvideo.net.mp4\"  video_player = f\"\"\"     Your browser does not support the video tag.   \"\"\"  HTML(video_player) Out[42]:    Your browser does not support the video tag.  In\u00a0[35]: Copied! <pre>vespa_cloud.delete()\n</pre> vespa_cloud.delete() <pre>Deactivated vespa-presales.videosearch in dev.aws-us-east-1c\nDeleted instance vespa-presales.videosearch.default\n</pre> <p>The following will delete the index created earlier where videos where uploaded:</p> In\u00a0[36]: Copied! <pre># Creating a client\nclient = TwelveLabs(api_key=TL_API_KEY)\n\nclient.index.delete(index_id)\n</pre> # Creating a client client = TwelveLabs(api_key=TL_API_KEY)  client.index.delete(index_id)"},{"location":"examples/video_search_twelvelabs_cloud.html#video-search-and-retrieval-with-vespa-and-twelvelabs","title":"Video Search and Retrieval with Vespa and TwelveLabs\u00b6","text":"<p>In the following notebook, we will demonstrate how to leverage TwelveLabs <code>Marengo-retrieval-2.7</code> a SOTA multimodal embedding model to demonstrate a use case of video embeddings storage and semantic search retrieval using Vespa.ai.</p> <p>The steps we will take in this notebook are:</p> <ol> <li>Setup and configuration</li> <li>Generate Attributes and Embeddings for 3 sample videos using the TwelveLabs python SDK.</li> <li>Deploy the Vespa application to Vespa Cloud and Feed the Data</li> <li>Perform a semantic search with hybrid multi-phase ranking on the videos</li> <li>Review the results</li> <li>Cleanup</li> </ol> <p>All the steps that are needed to provision the Vespa application, including feeding the data, can be done by running this notebook. We have tried to make it easy for others to run this notebook, to create your own Video semantic search application using TwelveLabs models with Vespa.</p> <p></p>"},{"location":"examples/video_search_twelvelabs_cloud.html#1-setup-and-configuration","title":"1. Setup and Configuration\u00b6","text":"<p>For reference, this is the Python version used for this notebook.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#11-install-libraries","title":"1.1 Install libraries\u00b6","text":"<p>Install the required Python dependencies from TwelveLabs python SDK and pyvespa python API.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#12-get-a-twelvelabs-api-key","title":"1.2 Get a TwelveLabs API key\u00b6","text":"<p>Sign-up for TwelveLabs.</p> <p>After logging in, navigate to your profile and get your API key. Copy it and paste it below.</p> <p>The Free plan includes indexing of 600 mins of videos, which should be sufficient to explore the capabilities of the API.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#13-sign-up-for-a-vespa-trial-account","title":"1.3 Sign-up for a Vespa Trial Account\u00b6","text":"<p>Pre-requisite:</p> <ul> <li>Spin-up a Vespa Cloud Trial account.</li> <li>Login to the account you just created and create a tenant at console.vespa-cloud.com.</li> <li>Save the tenant name.</li> </ul>"},{"location":"examples/video_search_twelvelabs_cloud.html#14-setup-the-tenant-name-and-the-application-name","title":"1.4 Setup the tenant name and the application name\u00b6","text":"<ul> <li>Paste below the name of the tenant name.</li> <li>Give your application a name. Note that the name cannot have <code>-</code> or <code>_</code>.</li> </ul>"},{"location":"examples/video_search_twelvelabs_cloud.html#2-generate-attributes-and-embeddings-for-sample-videos-using-twelvelabs-embedding-api","title":"2. Generate Attributes and Embeddings for sample videos using TwelveLabs Embedding API\u00b6","text":""},{"location":"examples/video_search_twelvelabs_cloud.html#21-generate-attributes-on-the-videos","title":"2.1 Generate attributes on the videos\u00b6","text":"<p>In this section, we will leverage the Pegasus 1.2 generative model to generate some attributes about our videos to store as part of the searchable information in Vespa. Attributes we want to store as part of the videos include:</p> <ul> <li>Keywords</li> <li>Summaries</li> </ul> <p>For video samples, we are selecting the 3 videos in the array below from the Internet Archive.</p> <p>You can customize this code with the urls of your choice. Note that there are certain restrictions such as the resolution of the videos.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#22-generate-embeddings","title":"2.2 Generate Embeddings\u00b6","text":"<p>The following code leverages the Embed API to create an asynchronous embedding task to embed the sample videos.</p> <p>Twelve Labs video embeddings capture all the subtle cues and interactions between different modalities, including the visual expressions, body language, spoken words, and the overall context of the video, encapsulating the essence of all these modalities and their interrelations over time.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#23-retrieve-embeddings","title":"2.3 Retrieve Embeddings\u00b6","text":"<p>Once the embedding task is completed, we can retrieve the results of the embedding task based on the task_ids.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#3-deploy-a-vespa-application","title":"3. Deploy a Vespa Application\u00b6","text":"<p>At this point, we are ready to deploy a Vespa Application. We have generated the attributes we needed on each video, as well as the embeddings.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#31-create-an-application-package","title":"3.1 Create an Application Package\u00b6","text":"<p>The application package has all the Vespa configuration files - create one from scratch:</p> <p>The Vespa schema deployed as part of the package is called <code>videos</code>. All the fields are matching the output of the Twelvelabs Embed API above. Refer to the Vespa documentation for more information on the schema specification.</p> <p>We can first define the schema using pyvespa</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#32-deploy-the-application-package","title":"3.2 Deploy the Application Package\u00b6","text":"<p>The app is now defined and ready to deploy to Vespa Cloud.</p> <p>Deploy <code>package</code> to Vespa Cloud, by creating an instance of VespaCloud:</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#33-feed-the-vespa-application","title":"3.3 Feed the Vespa Application\u00b6","text":"<p>The <code>vespa_feed</code> feed format for <code>pyvespa</code> expects a dict with the keys <code>id</code> and <code>fields</code>:</p> <p><code>{\u00a0\"id\": \"vespa-document-id\", \"fields\": {\"vespa_field\": \"vespa-field-value\"}}</code></p> <p>For the id, we will use a md5 hash of the video url.</p> <p>The video embedding output segments are added to the <code>fields</code> in <code>vespa_feed</code>.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#4-performing-search-on-the-videos","title":"4. Performing search on the videos\u00b6","text":""},{"location":"examples/video_search_twelvelabs_cloud.html#41-performing-a-hybrid-search-on-the-video","title":"4.1 Performing a hybrid search on the video\u00b6","text":"<p>As an example query, we will retrieve all the chunks which shows Santa Claus on his sleigh. The first step is to generate a text embedding for <code>Santa Claus on his sleigh</code> using the <code>Marengo-retrieval-2.7</code> model.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#5-review-results-optional","title":"5. Review results (Optional)\u00b6","text":"<p>We can review the results by spinning up a video player in the notebook and check the segments identified and judge by ourselves.</p> <p>But, first we need to obtain the contiguous segments, add 3 seconds overlap in the consolidated segments and convert to MM:SS so we can quickly find the segments to watch in the player. Let's write a function that takes the response as an input and provides the consolidated segments to view in the player.</p>"},{"location":"examples/video_search_twelvelabs_cloud.html#6-clean-up","title":"6. Clean-up\u00b6","text":"<p>The following will delete the application and data from the dev environment.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html","title":"Visual pdf rag with vespa colpali cloud","text":"In\u00a0[\u00a0]: Copied! <pre>!python --version\n</pre> !python --version <p>Install dependencies:</p> <p>Note that the python pdf2image package requires poppler-utils, see other installation options here.</p> In\u00a0[\u00a0]: Copied! <pre>!sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y\n</pre> !sudo apt-get update &amp;&amp; sudo apt-get install poppler-utils -y <p>Now install the required python packages:</p> In\u00a0[\u00a0]: Copied! <pre>!pip3 install colpali-engine==0.3.10 pdf2image pypdf==5.0.1 pyvespa&gt;=0.50.0 vespacli numpy==1.26.4 pillow==10.4.0 google-generativeai==0.8.3 transformers python-dotenv\n</pre> !pip3 install colpali-engine==0.3.10 pdf2image pypdf==5.0.1 pyvespa&gt;=0.50.0 vespacli numpy==1.26.4 pillow==10.4.0 google-generativeai==0.8.3 transformers python-dotenv In\u00a0[\u00a0]: Copied! <pre>import os\nimport json\nfrom typing import Tuple\nimport hashlib\nimport numpy as np\n\n# Vespa\nfrom vespa.package import (\n    ApplicationPackage,\n    Field,\n    Schema,\n    Document,\n    HNSW,\n    RankProfile,\n    Function,\n    FieldSet,\n    SecondPhaseRanking,\n    Summary,\n    DocumentSummary,\n)\nfrom vespa.deployment import VespaCloud\nfrom vespa.application import Vespa\nfrom vespa.io import VespaResponse\n\n# Google Generative AI for Google Gemini interaction\nimport google.generativeai as genai\n\n# Torch and other ML libraries\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom pdf2image import convert_from_path\nfrom pypdf import PdfReader\n\n# ColPali model and processor\nfrom colpali_engine.models import ColPali, ColPaliProcessor\nfrom colpali_engine.utils.torch_utils import get_torch_device\n\n# Load environment variables\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Avoid warning from huggingface tokenizers\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n</pre> import os import json from typing import Tuple import hashlib import numpy as np  # Vespa from vespa.package import (     ApplicationPackage,     Field,     Schema,     Document,     HNSW,     RankProfile,     Function,     FieldSet,     SecondPhaseRanking,     Summary,     DocumentSummary, ) from vespa.deployment import VespaCloud from vespa.application import Vespa from vespa.io import VespaResponse  # Google Generative AI for Google Gemini interaction import google.generativeai as genai  # Torch and other ML libraries import torch from torch.utils.data import DataLoader from tqdm import tqdm from pdf2image import convert_from_path from pypdf import PdfReader  # ColPali model and processor from colpali_engine.models import ColPali, ColPaliProcessor from colpali_engine.utils.torch_utils import get_torch_device  # Load environment variables from dotenv import load_dotenv  load_dotenv()  # Avoid warning from huggingface tokenizers os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" In\u00a0[\u00a0]: Copied! <pre>VESPA_TENANT_NAME = \"vespa-team\"  # Replace with your tenant name\n</pre> VESPA_TENANT_NAME = \"vespa-team\"  # Replace with your tenant name <p>Here, set your desired application name. (Will be created in later steps) Note that you can not have hyphen <code>-</code> or underscore <code>_</code> in the application name.</p> In\u00a0[\u00a0]: Copied! <pre>VESPA_APPLICATION_NAME = \"colpalidemodev\"\nVESPA_SCHEMA_NAME = \"pdf_page\"\n</pre> VESPA_APPLICATION_NAME = \"colpalidemodev\" VESPA_SCHEMA_NAME = \"pdf_page\" <p>Next, you can to create a token. This is an optional authentication method (the default is mTLS), and will be used for feeding data, and querying the application. For details, see Authenticating to Vespa Cloud. For now, we will use a single token with both read and write permissions. For production, we recommend separate tokens for feeding and querying, (the former with write permission, and the latter with read permission). The tokens can be created from the Vespa Cloud console in the 'Account' -&gt; 'Tokens' section. Please make sure to save the both the token id and it's value somwhere safe - you'll need it when you're going to connect to your app.</p> In\u00a0[\u00a0]: Copied! <pre># Replace this with the id of your token\nVESPA_TOKEN_ID = \"pyvespa_integration\"  # This needs to match the token_id that you created in the Vespa Cloud Console\n</pre> # Replace this with the id of your token VESPA_TOKEN_ID = \"pyvespa_integration\"  # This needs to match the token_id that you created in the Vespa Cloud Console <p>We also need to set the value of the write token to be able to feed data to the Vespa application (value of VESPA_TOKEN_ID_WRITE). Please run the cell below to set the variable.</p> In\u00a0[\u00a0]: Copied! <pre>VESPA_CLOUD_SECRET_TOKEN = os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\") or input(\n    \"Enter Vespa cloud secret token: \"\n)\n</pre> VESPA_CLOUD_SECRET_TOKEN = os.getenv(\"VESPA_CLOUD_SECRET_TOKEN\") or input(     \"Enter Vespa cloud secret token: \" ) <p>We will use Google's Gemini API to create sample queries for our images. Create a Gemini API key from here. Once you have the key, please run the cell below. You can also use other VLM's to create these queries.</p> In\u00a0[\u00a0]: Copied! <pre>GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or input(\n    \"Enter Google Generative AI API key: \"\n)\n# Configure Google Generative AI\ngenai.configure(api_key=GOOGLE_API_KEY)\n</pre> GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or input(     \"Enter Google Generative AI API key: \" ) # Configure Google Generative AI genai.configure(api_key=GOOGLE_API_KEY) In\u00a0[\u00a0]: Copied! <pre>MODEL_NAME = \"vidore/colpali-v1.2\"\n\n# Set device for Torch\ndevice = get_torch_device(\"auto\")\nprint(f\"Using device: {device}\")\n\n# Load the ColPali model and processor\nmodel = ColPali.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float32,\n    device_map=device,\n).eval()\n\nprocessor = ColPaliProcessor.from_pretrained(MODEL_NAME)\n</pre> MODEL_NAME = \"vidore/colpali-v1.2\"  # Set device for Torch device = get_torch_device(\"auto\") print(f\"Using device: {device}\")  # Load the ColPali model and processor model = ColPali.from_pretrained(     MODEL_NAME,     torch_dtype=torch.float32,     device_map=device, ).eval()  processor = ColPaliProcessor.from_pretrained(MODEL_NAME) <p>As we can see, a lot of the information is in the form of tables, charts and numbers. These are not easily extractable using pdf-readers or OCR tools.</p> In\u00a0[\u00a0]: Copied! <pre>import requests\n\npdfs = [\n    {\n        \"url\": \"https://drive.google.com/uc?export=download&amp;id=1nDO0KN_BjyFu42xFAfhJagOeeaJ8fhki\",\n        \"path\": \"pdfs/gpfg-half-year-report-2024.pdf\",\n        \"year\": \"2024\",\n    },\n    {\n        \"url\": \"https://drive.google.com/uc?export=download&amp;id=1Saw_wM8RI6Zej5qkWDDpeM-3tyOQQTwR\",\n        \"path\": \"pdfs/gpfg-annual-report_2023.pdf\",\n        \"year\": \"2023\",\n    },\n]\n</pre> import requests  pdfs = [     {         \"url\": \"https://drive.google.com/uc?export=download&amp;id=1nDO0KN_BjyFu42xFAfhJagOeeaJ8fhki\",         \"path\": \"pdfs/gpfg-half-year-report-2024.pdf\",         \"year\": \"2024\",     },     {         \"url\": \"https://drive.google.com/uc?export=download&amp;id=1Saw_wM8RI6Zej5qkWDDpeM-3tyOQQTwR\",         \"path\": \"pdfs/gpfg-annual-report_2023.pdf\",         \"year\": \"2023\",     }, ] In\u00a0[\u00a0]: Copied! <pre>PDFS_DIR = \"pdfs\"\nos.makedirs(PDFS_DIR, exist_ok=True)\n\n\ndef download_pdf(url: str, path: str):\n    r = requests.get(url, stream=True)\n    with open(path, \"wb\") as f:\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:\n                f.write(chunk)\n    return path\n\n\n# Download the pdfs\nfor pdf in pdfs:\n    download_pdf(pdf[\"url\"], pdf[\"path\"])\n</pre> PDFS_DIR = \"pdfs\" os.makedirs(PDFS_DIR, exist_ok=True)   def download_pdf(url: str, path: str):     r = requests.get(url, stream=True)     with open(path, \"wb\") as f:         for chunk in r.iter_content(chunk_size=1024):             if chunk:                 f.write(chunk)     return path   # Download the pdfs for pdf in pdfs:     download_pdf(pdf[\"url\"], pdf[\"path\"]) In\u00a0[\u00a0]: Copied! <pre>def get_pdf_images(pdf_path):\n    reader = PdfReader(pdf_path)\n    page_texts = []\n    for page_number in range(len(reader.pages)):\n        page = reader.pages[page_number]\n        text = page.extract_text()\n        page_texts.append(text)\n    # Convert to PIL images\n    images = convert_from_path(pdf_path)\n    assert len(images) == len(page_texts)\n    return images, page_texts\n\n\npdf_folder = \"pdfs\"\npdf_pages = []\nfor pdf in tqdm(pdfs):\n    pdf_file = pdf[\"path\"]\n    title = os.path.splitext(os.path.basename(pdf_file))[0]\n    images, texts = get_pdf_images(pdf_file)\n    for page_no, (image, text) in enumerate(zip(images, texts)):\n        pdf_pages.append(\n            {\n                \"title\": title,\n                \"year\": pdf[\"year\"],\n                \"url\": pdf[\"url\"],\n                \"path\": pdf_file,\n                \"image\": image,\n                \"text\": text,\n                \"page_no\": page_no,\n            }\n        )\n</pre> def get_pdf_images(pdf_path):     reader = PdfReader(pdf_path)     page_texts = []     for page_number in range(len(reader.pages)):         page = reader.pages[page_number]         text = page.extract_text()         page_texts.append(text)     # Convert to PIL images     images = convert_from_path(pdf_path)     assert len(images) == len(page_texts)     return images, page_texts   pdf_folder = \"pdfs\" pdf_pages = [] for pdf in tqdm(pdfs):     pdf_file = pdf[\"path\"]     title = os.path.splitext(os.path.basename(pdf_file))[0]     images, texts = get_pdf_images(pdf_file)     for page_no, (image, text) in enumerate(zip(images, texts)):         pdf_pages.append(             {                 \"title\": title,                 \"year\": pdf[\"year\"],                 \"url\": pdf[\"url\"],                 \"path\": pdf_file,                 \"image\": image,                 \"text\": text,                 \"page_no\": page_no,             }         ) In\u00a0[\u00a0]: Copied! <pre>len(pdf_pages)\n</pre> len(pdf_pages) In\u00a0[\u00a0]: Copied! <pre>MAX_PAGES = 10  # Set to None to use all pages\npdf_pages = pdf_pages[:MAX_PAGES] if MAX_PAGES else pdf_pages\n</pre> MAX_PAGES = 10  # Set to None to use all pages pdf_pages = pdf_pages[:MAX_PAGES] if MAX_PAGES else pdf_pages <p>We now have 176 pages, which will be the entity we define as one document in Vespa.</p> <p>Let us look at the extracted text from the pages displayed above.</p> In\u00a0[\u00a0]: Copied! <pre>pdf_pages[8][\"image\"]\n</pre> pdf_pages[8][\"image\"] In\u00a0[\u00a0]: Copied! <pre>print(pdf_pages[8][\"text\"])\n</pre> print(pdf_pages[8][\"text\"]) In\u00a0[\u00a0]: Copied! <pre># print(pdf_pages[95][\"text\"])\n</pre> # print(pdf_pages[95][\"text\"]) <p>As we can see, the extracted text fails to capture the visual information we see in the image, and it would be difficult for an LLM to correctly answer questions such as 'Price development in Technology sector from April 2023?' based on the text alone.</p> In\u00a0[\u00a0]: Copied! <pre>from pydantic import BaseModel\n\n\nclass GeneratedQueries(BaseModel):\n    broad_topical_question: str\n    broad_topical_query: str\n    specific_detail_question: str\n    specific_detail_query: str\n    visual_element_question: str\n    visual_element_query: str\n\n\ndef get_retrieval_prompt() -&gt; Tuple[str, GeneratedQueries]:\n    prompt = (\n        prompt\n    ) = \"\"\"You are an investor, stock analyst and financial expert. You will be presented an image of a document page from a report published by the Norwegian Government Pension Fund Global (GPFG). The report may be annual or quarterly reports, or policy reports, on topics such as responsible investment, risk etc.\nYour task is to generate retrieval queries and questions that you would use to retrieve this document (or ask based on this document) in a large corpus.\nPlease generate 3 different types of retrieval queries and questions.\nA retrieval query is a keyword based query, made up of 2-5 words, that you would type into a search engine to find this document.\nA question is a natural language question that you would ask, for which the document contains the answer.\nThe queries should be of the following types:\n1. A broad topical query: This should cover the main subject of the document.\n2. A specific detail query: This should cover a specific detail or aspect of the document.\n3. A visual element query: This should cover a visual element of the document, such as a chart, graph, or image.\n\nImportant guidelines:\n- Ensure the queries are relevant for retrieval tasks, not just describing the page content.\n- Use a fact-based natural language style for the questions.\n- Frame the queries as if someone is searching for this document in a large corpus.\n- Make the queries diverse and representative of different search strategies.\n\nFormat your response as a JSON object with the structure of the following example:\n{\n    \"broad_topical_question\": \"What was the Responsible Investment Policy in 2019?\",\n    \"broad_topical_query\": \"responsible investment policy 2019\",\n    \"specific_detail_question\": \"What is the percentage of investments in renewable energy?\",\n    \"specific_detail_query\": \"renewable energy investments percentage\",\n    \"visual_element_question\": \"What is the trend of total holding value over time?\",\n    \"visual_element_query\": \"total holding value trend\"\n}\n\nIf there are no relevant visual elements, provide an empty string for the visual element question and query.\nHere is the document image to analyze:\nGenerate the queries based on this image and provide the response in the specified JSON format.\nOnly return JSON. Don't return any extra explanation text. \"\"\"\n\n    return prompt, GeneratedQueries\n\n\nprompt_text, pydantic_model = get_retrieval_prompt()\n</pre> from pydantic import BaseModel   class GeneratedQueries(BaseModel):     broad_topical_question: str     broad_topical_query: str     specific_detail_question: str     specific_detail_query: str     visual_element_question: str     visual_element_query: str   def get_retrieval_prompt() -&gt; Tuple[str, GeneratedQueries]:     prompt = (         prompt     ) = \"\"\"You are an investor, stock analyst and financial expert. You will be presented an image of a document page from a report published by the Norwegian Government Pension Fund Global (GPFG). The report may be annual or quarterly reports, or policy reports, on topics such as responsible investment, risk etc. Your task is to generate retrieval queries and questions that you would use to retrieve this document (or ask based on this document) in a large corpus. Please generate 3 different types of retrieval queries and questions. A retrieval query is a keyword based query, made up of 2-5 words, that you would type into a search engine to find this document. A question is a natural language question that you would ask, for which the document contains the answer. The queries should be of the following types: 1. A broad topical query: This should cover the main subject of the document. 2. A specific detail query: This should cover a specific detail or aspect of the document. 3. A visual element query: This should cover a visual element of the document, such as a chart, graph, or image.  Important guidelines: - Ensure the queries are relevant for retrieval tasks, not just describing the page content. - Use a fact-based natural language style for the questions. - Frame the queries as if someone is searching for this document in a large corpus. - Make the queries diverse and representative of different search strategies.  Format your response as a JSON object with the structure of the following example: {     \"broad_topical_question\": \"What was the Responsible Investment Policy in 2019?\",     \"broad_topical_query\": \"responsible investment policy 2019\",     \"specific_detail_question\": \"What is the percentage of investments in renewable energy?\",     \"specific_detail_query\": \"renewable energy investments percentage\",     \"visual_element_question\": \"What is the trend of total holding value over time?\",     \"visual_element_query\": \"total holding value trend\" }  If there are no relevant visual elements, provide an empty string for the visual element question and query. Here is the document image to analyze: Generate the queries based on this image and provide the response in the specified JSON format. Only return JSON. Don't return any extra explanation text. \"\"\"      return prompt, GeneratedQueries   prompt_text, pydantic_model = get_retrieval_prompt() In\u00a0[\u00a0]: Copied! <pre>gemini_model = genai.GenerativeModel(\"gemini-flash-lite-latest\")\n\n\ndef generate_queries(image, prompt_text, pydantic_model):\n    try:\n        response = gemini_model.generate_content(\n            [image, \"\\n\\n\", prompt_text],\n            generation_config=genai.GenerationConfig(\n                response_mime_type=\"application/json\",\n                response_schema=pydantic_model,\n            ),\n        )\n        queries = json.loads(response.text)\n    except Exception as _e:\n        print(_e)\n        queries = {\n            \"broad_topical_question\": \"\",\n            \"broad_topical_query\": \"\",\n            \"specific_detail_question\": \"\",\n            \"specific_detail_query\": \"\",\n            \"visual_element_question\": \"\",\n            \"visual_element_query\": \"\",\n        }\n    return queries\n</pre> gemini_model = genai.GenerativeModel(\"gemini-flash-lite-latest\")   def generate_queries(image, prompt_text, pydantic_model):     try:         response = gemini_model.generate_content(             [image, \"\\n\\n\", prompt_text],             generation_config=genai.GenerationConfig(                 response_mime_type=\"application/json\",                 response_schema=pydantic_model,             ),         )         queries = json.loads(response.text)     except Exception as _e:         print(_e)         queries = {             \"broad_topical_question\": \"\",             \"broad_topical_query\": \"\",             \"specific_detail_question\": \"\",             \"specific_detail_query\": \"\",             \"visual_element_question\": \"\",             \"visual_element_query\": \"\",         }     return queries In\u00a0[\u00a0]: Copied! <pre>for pdf in tqdm(pdf_pages):\n    image = pdf.get(\"image\")\n    pdf[\"queries\"] = generate_queries(image, prompt_text, pydantic_model)\n</pre> for pdf in tqdm(pdf_pages):     image = pdf.get(\"image\")     pdf[\"queries\"] = generate_queries(image, prompt_text, pydantic_model) <p>Let's take a look at the queries and questions generated for the page displayed above.</p> In\u00a0[\u00a0]: Copied! <pre>pdf_pages[8][\"queries\"]\n</pre> pdf_pages[8][\"queries\"] In\u00a0[\u00a0]: Copied! <pre>def generate_embeddings(images, model, processor, batch_size=1) -&gt; np.ndarray:\n    \"\"\"\n    Generate embeddings for a list of images.\n    Move to CPU only once per batch.\n\n    Args:\n        images (List[PIL.Image]): List of PIL images.\n        model (nn.Module): The model to generate embeddings.\n        processor: The processor to preprocess images.\n        batch_size (int, optional): Batch size for processing. Defaults to 64.\n\n    Returns:\n        np.ndarray: Embeddings for the images, shape\n                    (len(images), processor.max_patch_length (1030 for ColPali), model.config.hidden_size (Patch embedding dimension - 128 for ColPali)).\n    \"\"\"\n\n    def collate_fn(batch):\n        # Batch is a list of images\n        return processor.process_images(batch)  # Should return a dict of tensors\n\n    dataloader = DataLoader(\n        images,\n        shuffle=False,\n        collate_fn=collate_fn,\n    )\n\n    embeddings_list = []\n    for batch in tqdm(dataloader):\n        with torch.no_grad():\n            batch = {k: v.to(model.device) for k, v in batch.items()}\n            embeddings_batch = model(**batch)\n            # Convert tensor to numpy array and append to list\n            embeddings_list.extend(\n                [t.cpu().numpy() for t in torch.unbind(embeddings_batch)]\n            )\n\n    # Stack all embeddings into a single numpy array\n    all_embeddings = np.stack(embeddings_list, axis=0)\n    return all_embeddings\n</pre> def generate_embeddings(images, model, processor, batch_size=1) -&gt; np.ndarray:     \"\"\"     Generate embeddings for a list of images.     Move to CPU only once per batch.      Args:         images (List[PIL.Image]): List of PIL images.         model (nn.Module): The model to generate embeddings.         processor: The processor to preprocess images.         batch_size (int, optional): Batch size for processing. Defaults to 64.      Returns:         np.ndarray: Embeddings for the images, shape                     (len(images), processor.max_patch_length (1030 for ColPali), model.config.hidden_size (Patch embedding dimension - 128 for ColPali)).     \"\"\"      def collate_fn(batch):         # Batch is a list of images         return processor.process_images(batch)  # Should return a dict of tensors      dataloader = DataLoader(         images,         shuffle=False,         collate_fn=collate_fn,     )      embeddings_list = []     for batch in tqdm(dataloader):         with torch.no_grad():             batch = {k: v.to(model.device) for k, v in batch.items()}             embeddings_batch = model(**batch)             # Convert tensor to numpy array and append to list             embeddings_list.extend(                 [t.cpu().numpy() for t in torch.unbind(embeddings_batch)]             )      # Stack all embeddings into a single numpy array     all_embeddings = np.stack(embeddings_list, axis=0)     return all_embeddings In\u00a0[\u00a0]: Copied! <pre># Generate embeddings for all images\nimages = [pdf[\"image\"] for pdf in pdf_pages]\nembeddings = generate_embeddings(images, model, processor)\n</pre> # Generate embeddings for all images images = [pdf[\"image\"] for pdf in pdf_pages] embeddings = generate_embeddings(images, model, processor) <p>Now, we have one embedding vector of dimension 128 for each patch of each image (1024 patches + some special tokens).</p> In\u00a0[\u00a0]: Copied! <pre>embeddings.shape\n</pre> embeddings.shape In\u00a0[\u00a0]: Copied! <pre>assert len(pdf_pages) == embeddings.shape[0]\nassert embeddings.shape[1] &gt; 1028  # Number of patches (including special tokens)\nassert embeddings.shape[2] == 128  # Embedding dimension per patch\n</pre> assert len(pdf_pages) == embeddings.shape[0] assert embeddings.shape[1] &gt; 1028  # Number of patches (including special tokens) assert embeddings.shape[2] == 128  # Embedding dimension per patch In\u00a0[\u00a0]: Copied! <pre>def float_to_binary_embedding(float_query_embedding: dict) -&gt; dict:\n    \"\"\"Utility function to convert float query embeddings to binary query embeddings.\"\"\"\n    binary_query_embeddings = {}\n    for k, v in float_query_embedding.items():\n        binary_vector = (\n            np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()\n        )\n        binary_query_embeddings[k] = binary_vector\n    return binary_query_embeddings\n</pre> def float_to_binary_embedding(float_query_embedding: dict) -&gt; dict:     \"\"\"Utility function to convert float query embeddings to binary query embeddings.\"\"\"     binary_query_embeddings = {}     for k, v in float_query_embedding.items():         binary_vector = (             np.packbits(np.where(np.array(v) &gt; 0, 1, 0)).astype(np.int8).tolist()         )         binary_query_embeddings[k] = binary_vector     return binary_query_embeddings <p>We also need a couple of image processing helper functions. These are borrowed from vidore-benchmark repo.</p> In\u00a0[\u00a0]: Copied! <pre>import base64\nimport io\nfrom pathlib import Path\nfrom typing import Union\nfrom PIL import Image\n\n\ndef scale_image(image: Image.Image, new_height: int = 1024) -&gt; Image.Image:\n    \"\"\"\n    Scale an image to a new height while maintaining the aspect ratio.\n    \"\"\"\n    # Calculate the scaling factor\n    width, height = image.size\n    aspect_ratio = width / height\n    new_width = int(new_height * aspect_ratio)\n\n    # Resize the image\n    scaled_image = image.resize((new_width, new_height))\n    return scaled_image\n\n\ndef get_base64_image(img: Union[str, Image.Image], add_url_prefix: bool = True) -&gt; str:\n    \"\"\"\n    Convert an image (from a filepath or a PIL.Image object) to a JPEG-base64 string.\n    \"\"\"\n    if isinstance(img, str):\n        img = Image.open(img)\n    elif isinstance(img, Image.Image):\n        pass\n    else:\n        raise ValueError(\"`img` must be a path to an image or a PIL Image object.\")\n\n    buffered = io.BytesIO()\n    img.save(buffered, format=\"jpeg\")\n    b64_data = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    return f\"data:image/jpeg;base64,{b64_data}\" if add_url_prefix else b64_data\n</pre> import base64 import io from pathlib import Path from typing import Union from PIL import Image   def scale_image(image: Image.Image, new_height: int = 1024) -&gt; Image.Image:     \"\"\"     Scale an image to a new height while maintaining the aspect ratio.     \"\"\"     # Calculate the scaling factor     width, height = image.size     aspect_ratio = width / height     new_width = int(new_height * aspect_ratio)      # Resize the image     scaled_image = image.resize((new_width, new_height))     return scaled_image   def get_base64_image(img: Union[str, Image.Image], add_url_prefix: bool = True) -&gt; str:     \"\"\"     Convert an image (from a filepath or a PIL.Image object) to a JPEG-base64 string.     \"\"\"     if isinstance(img, str):         img = Image.open(img)     elif isinstance(img, Image.Image):         pass     else:         raise ValueError(\"`img` must be a path to an image or a PIL Image object.\")      buffered = io.BytesIO()     img.save(buffered, format=\"jpeg\")     b64_data = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")     return f\"data:image/jpeg;base64,{b64_data}\" if add_url_prefix else b64_data <p>Note that we also store a scaled down (blurred) version of the image in Vespa. The purpose of this is to return this fast on first results to the frontend, to provide a snappy user experience, and then load the full resolution image async in the background.</p> In\u00a0[\u00a0]: Copied! <pre>vespa_feed = []\nfor pdf, embedding in zip(pdf_pages, embeddings):\n    url = pdf[\"url\"]\n    year = pdf[\"year\"]\n    title = pdf[\"title\"]\n    image = pdf[\"image\"]\n    text = pdf.get(\"text\", \"\")\n    page_no = pdf[\"page_no\"]\n    query_dict = pdf[\"queries\"]\n    questions = [v for k, v in query_dict.items() if \"question\" in k and v]\n    queries = [v for k, v in query_dict.items() if \"query\" in k and v]\n    base_64_image = get_base64_image(\n        scale_image(image, 32), add_url_prefix=False\n    )  # Scaled down image to return fast on search (~1kb)\n    base_64_full_image = get_base64_image(image, add_url_prefix=False)\n    embedding_dict = {k: v for k, v in enumerate(embedding)}\n    binary_embedding = float_to_binary_embedding(embedding_dict)\n    # id_hash should be md5 hash of url and page_number\n    id_hash = hashlib.md5(f\"{url}_{page_no}\".encode()).hexdigest()\n    page = {\n        \"id\": id_hash,\n        \"fields\": {\n            \"id\": id_hash,\n            \"url\": url,\n            \"title\": title,\n            \"year\": year,\n            \"page_number\": page_no,\n            \"blur_image\": base_64_image,\n            \"full_image\": base_64_full_image,\n            \"text\": text,\n            \"embedding\": binary_embedding,\n            \"queries\": queries,\n            \"questions\": questions,\n        },\n    }\n    vespa_feed.append(page)\n</pre> vespa_feed = [] for pdf, embedding in zip(pdf_pages, embeddings):     url = pdf[\"url\"]     year = pdf[\"year\"]     title = pdf[\"title\"]     image = pdf[\"image\"]     text = pdf.get(\"text\", \"\")     page_no = pdf[\"page_no\"]     query_dict = pdf[\"queries\"]     questions = [v for k, v in query_dict.items() if \"question\" in k and v]     queries = [v for k, v in query_dict.items() if \"query\" in k and v]     base_64_image = get_base64_image(         scale_image(image, 32), add_url_prefix=False     )  # Scaled down image to return fast on search (~1kb)     base_64_full_image = get_base64_image(image, add_url_prefix=False)     embedding_dict = {k: v for k, v in enumerate(embedding)}     binary_embedding = float_to_binary_embedding(embedding_dict)     # id_hash should be md5 hash of url and page_number     id_hash = hashlib.md5(f\"{url}_{page_no}\".encode()).hexdigest()     page = {         \"id\": id_hash,         \"fields\": {             \"id\": id_hash,             \"url\": url,             \"title\": title,             \"year\": year,             \"page_number\": page_no,             \"blur_image\": base_64_image,             \"full_image\": base_64_full_image,             \"text\": text,             \"embedding\": binary_embedding,             \"queries\": queries,             \"questions\": questions,         },     }     vespa_feed.append(page) In\u00a0[\u00a0]: Copied! <pre># os.makedirs(\"output\", exist_ok=True)\n# with open(\"output/vespa_feed.jsonl\", \"w\") as f:\n#     vespa_feed_to_save = []\n#     for page in vespa_feed:\n#         document_id = page[\"id\"]\n#         put_id = f\"id:{VESPA_APPLICATION_NAME}:{VESPA_SCHEMA_NAME}::{document_id}\"\n#         vespa_feed_to_save.append({\"put\": put_id, \"fields\": page[\"fields\"]})\n#     json.dump(vespa_feed_to_save, f)\n</pre> # os.makedirs(\"output\", exist_ok=True) # with open(\"output/vespa_feed.jsonl\", \"w\") as f: #     vespa_feed_to_save = [] #     for page in vespa_feed: #         document_id = page[\"id\"] #         put_id = f\"id:{VESPA_APPLICATION_NAME}:{VESPA_SCHEMA_NAME}::{document_id}\" #         vespa_feed_to_save.append({\"put\": put_id, \"fields\": page[\"fields\"]}) #     json.dump(vespa_feed_to_save, f) In\u00a0[\u00a0]: Copied! <pre>colpali_schema = Schema(\n    name=VESPA_SCHEMA_NAME,\n    document=Document(\n        fields=[\n            Field(\n                name=\"id\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"word\"],\n            ),\n            Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),\n            Field(name=\"year\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(\n                name=\"title\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),\n            Field(name=\"blur_image\", type=\"raw\", indexing=[\"summary\"]),\n            Field(name=\"full_image\", type=\"raw\", indexing=[\"summary\"]),\n            Field(\n                name=\"text\",\n                type=\"string\",\n                indexing=[\"summary\", \"index\"],\n                match=[\"text\"],\n                index=\"enable-bm25\",\n            ),\n            Field(\n                name=\"embedding\",\n                type=\"tensor&lt;int8&gt;(patch{}, v[16])\",\n                indexing=[\n                    \"attribute\",\n                    \"index\",\n                ],\n                ann=HNSW(\n                    distance_metric=\"hamming\",\n                    max_links_per_node=32,\n                    neighbors_to_explore_at_insert=400,\n                ),\n            ),\n            Field(\n                name=\"questions\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\", \"attribute\"],\n                summary=Summary(fields=[\"matched-elements-only\"]),\n            ),\n            Field(\n                name=\"queries\",\n                type=\"array&lt;string&gt;\",\n                indexing=[\"summary\", \"attribute\"],\n                summary=Summary(fields=[\"matched-elements-only\"]),\n            ),\n        ]\n    ),\n    fieldsets=[\n        FieldSet(\n            name=\"default\",\n            fields=[\"title\", \"text\"],\n        ),\n    ],\n    document_summaries=[\n        DocumentSummary(\n            name=\"default\",\n            summary_fields=[\n                Summary(\n                    name=\"text\",\n                    fields=[(\"bolding\", \"on\")],\n                ),\n                Summary(\n                    name=\"snippet\",\n                    fields=[(\"source\", \"text\"), \"dynamic\"],\n                ),\n            ],\n            from_disk=True,\n        ),\n        DocumentSummary(\n            name=\"suggestions\",\n            summary_fields=[\n                Summary(name=\"questions\"),\n            ],\n            from_disk=True,\n        ),\n    ],\n)\n\n# Define similarity functions used in all rank profiles\nmapfunctions = [\n    Function(\n        name=\"similarities\",  # computes similarity scores between each query token and image patch\n        expression=\"\"\"\n                sum(\n                    query(qt) * unpack_bits(attribute(embedding)), v\n                )\n            \"\"\",\n    ),\n    Function(\n        name=\"normalized\",  # normalizes the similarity scores to [-1, 1]\n        expression=\"\"\"\n                (similarities - reduce(similarities, min)) / (reduce((similarities - reduce(similarities, min)), max)) * 2 - 1\n            \"\"\",\n    ),\n    Function(\n        name=\"quantized\",  # quantizes the normalized similarity scores to signed 8-bit integers [-128, 127]\n        expression=\"\"\"\n                cell_cast(normalized * 127.999, int8)\n            \"\"\",\n    ),\n]\n\n# Define the 'bm25' rank profile\nbm25 = RankProfile(\n    name=\"bm25\",\n    inputs=[(\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\")],\n    first_phase=\"bm25(title) + bm25(text)\",\n    functions=mapfunctions,\n)\n\n\n# A function to create an inherited rank profile which also returns quantized similarity scores\ndef with_quantized_similarity(rank_profile: RankProfile) -&gt; RankProfile:\n    return RankProfile(\n        name=f\"{rank_profile.name}_sim\",\n        first_phase=rank_profile.first_phase,\n        inherits=rank_profile.name,\n        summary_features=[\"quantized\"],\n    )\n\n\ncolpali_schema.add_rank_profile(bm25)\ncolpali_schema.add_rank_profile(with_quantized_similarity(bm25))\n\n\n# Update the 'colpali' rank profile\ninput_query_tensors = []\nMAX_QUERY_TERMS = 64\nfor i in range(MAX_QUERY_TERMS):\n    input_query_tensors.append((f\"query(rq{i})\", \"tensor&lt;int8&gt;(v[16])\"))\n\ninput_query_tensors.extend(\n    [\n        (\"query(qt)\", \"tensor&lt;float&gt;(querytoken{}, v[128])\"),\n        (\"query(qtb)\", \"tensor&lt;int8&gt;(querytoken{}, v[16])\"),\n    ]\n)\n\ncolpali = RankProfile(\n    name=\"colpali\",\n    inputs=input_query_tensors,\n    first_phase=\"max_sim_binary\",\n    second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),\n    functions=mapfunctions\n    + [\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim_binary\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        1 / (1 + sum(\n                            hamming(query(qtb), attribute(embedding)), v)\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n    ],\n)\ncolpali_schema.add_rank_profile(colpali)\ncolpali_schema.add_rank_profile(with_quantized_similarity(colpali))\n\n# Update the 'hybrid' rank profile\nhybrid = RankProfile(\n    name=\"hybrid\",\n    inputs=input_query_tensors,\n    first_phase=\"max_sim_binary\",\n    second_phase=SecondPhaseRanking(\n        expression=\"max_sim + 2 * (bm25(text) + bm25(title))\", rerank_count=10\n    ),\n    functions=mapfunctions\n    + [\n        Function(\n            name=\"max_sim\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        sum(\n                            query(qt) * unpack_bits(attribute(embedding)), v\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n        Function(\n            name=\"max_sim_binary\",\n            expression=\"\"\"\n                sum(\n                    reduce(\n                        1 / (1 + sum(\n                            hamming(query(qtb), attribute(embedding)), v)\n                        ),\n                        max, patch\n                    ),\n                    querytoken\n                )\n            \"\"\",\n        ),\n    ],\n)\ncolpali_schema.add_rank_profile(hybrid)\ncolpali_schema.add_rank_profile(with_quantized_similarity(hybrid))\n</pre> colpali_schema = Schema(     name=VESPA_SCHEMA_NAME,     document=Document(         fields=[             Field(                 name=\"id\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"word\"],             ),             Field(name=\"url\", type=\"string\", indexing=[\"summary\", \"index\"]),             Field(name=\"year\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(                 name=\"title\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(name=\"page_number\", type=\"int\", indexing=[\"summary\", \"attribute\"]),             Field(name=\"blur_image\", type=\"raw\", indexing=[\"summary\"]),             Field(name=\"full_image\", type=\"raw\", indexing=[\"summary\"]),             Field(                 name=\"text\",                 type=\"string\",                 indexing=[\"summary\", \"index\"],                 match=[\"text\"],                 index=\"enable-bm25\",             ),             Field(                 name=\"embedding\",                 type=\"tensor(patch{}, v[16])\",                 indexing=[                     \"attribute\",                     \"index\",                 ],                 ann=HNSW(                     distance_metric=\"hamming\",                     max_links_per_node=32,                     neighbors_to_explore_at_insert=400,                 ),             ),             Field(                 name=\"questions\",                 type=\"array\",                 indexing=[\"summary\", \"attribute\"],                 summary=Summary(fields=[\"matched-elements-only\"]),             ),             Field(                 name=\"queries\",                 type=\"array\",                 indexing=[\"summary\", \"attribute\"],                 summary=Summary(fields=[\"matched-elements-only\"]),             ),         ]     ),     fieldsets=[         FieldSet(             name=\"default\",             fields=[\"title\", \"text\"],         ),     ],     document_summaries=[         DocumentSummary(             name=\"default\",             summary_fields=[                 Summary(                     name=\"text\",                     fields=[(\"bolding\", \"on\")],                 ),                 Summary(                     name=\"snippet\",                     fields=[(\"source\", \"text\"), \"dynamic\"],                 ),             ],             from_disk=True,         ),         DocumentSummary(             name=\"suggestions\",             summary_fields=[                 Summary(name=\"questions\"),             ],             from_disk=True,         ),     ], )  # Define similarity functions used in all rank profiles mapfunctions = [     Function(         name=\"similarities\",  # computes similarity scores between each query token and image patch         expression=\"\"\"                 sum(                     query(qt) * unpack_bits(attribute(embedding)), v                 )             \"\"\",     ),     Function(         name=\"normalized\",  # normalizes the similarity scores to [-1, 1]         expression=\"\"\"                 (similarities - reduce(similarities, min)) / (reduce((similarities - reduce(similarities, min)), max)) * 2 - 1             \"\"\",     ),     Function(         name=\"quantized\",  # quantizes the normalized similarity scores to signed 8-bit integers [-128, 127]         expression=\"\"\"                 cell_cast(normalized * 127.999, int8)             \"\"\",     ), ]  # Define the 'bm25' rank profile bm25 = RankProfile(     name=\"bm25\",     inputs=[(\"query(qt)\", \"tensor(querytoken{}, v[128])\")],     first_phase=\"bm25(title) + bm25(text)\",     functions=mapfunctions, )   # A function to create an inherited rank profile which also returns quantized similarity scores def with_quantized_similarity(rank_profile: RankProfile) -&gt; RankProfile:     return RankProfile(         name=f\"{rank_profile.name}_sim\",         first_phase=rank_profile.first_phase,         inherits=rank_profile.name,         summary_features=[\"quantized\"],     )   colpali_schema.add_rank_profile(bm25) colpali_schema.add_rank_profile(with_quantized_similarity(bm25))   # Update the 'colpali' rank profile input_query_tensors = [] MAX_QUERY_TERMS = 64 for i in range(MAX_QUERY_TERMS):     input_query_tensors.append((f\"query(rq{i})\", \"tensor(v[16])\"))  input_query_tensors.extend(     [         (\"query(qt)\", \"tensor(querytoken{}, v[128])\"),         (\"query(qtb)\", \"tensor(querytoken{}, v[16])\"),     ] )  colpali = RankProfile(     name=\"colpali\",     inputs=input_query_tensors,     first_phase=\"max_sim_binary\",     second_phase=SecondPhaseRanking(expression=\"max_sim\", rerank_count=10),     functions=mapfunctions     + [         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim_binary\",             expression=\"\"\"                 sum(                     reduce(                         1 / (1 + sum(                             hamming(query(qtb), attribute(embedding)), v)                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),     ], ) colpali_schema.add_rank_profile(colpali) colpali_schema.add_rank_profile(with_quantized_similarity(colpali))  # Update the 'hybrid' rank profile hybrid = RankProfile(     name=\"hybrid\",     inputs=input_query_tensors,     first_phase=\"max_sim_binary\",     second_phase=SecondPhaseRanking(         expression=\"max_sim + 2 * (bm25(text) + bm25(title))\", rerank_count=10     ),     functions=mapfunctions     + [         Function(             name=\"max_sim\",             expression=\"\"\"                 sum(                     reduce(                         sum(                             query(qt) * unpack_bits(attribute(embedding)), v                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),         Function(             name=\"max_sim_binary\",             expression=\"\"\"                 sum(                     reduce(                         1 / (1 + sum(                             hamming(query(qtb), attribute(embedding)), v)                         ),                         max, patch                     ),                     querytoken                 )             \"\"\",         ),     ], ) colpali_schema.add_rank_profile(hybrid) colpali_schema.add_rank_profile(with_quantized_similarity(hybrid)) In\u00a0[\u00a0]: Copied! <pre>from vespa.configuration.services import (\n    services,\n    container,\n    search,\n    document_api,\n    document_processing,\n    clients,\n    client,\n    config,\n    content,\n    redundancy,\n    documents,\n    node,\n    certificate,\n    token,\n    document,\n    nodes,\n)\nfrom vespa.configuration.vt import vt\nfrom vespa.package import ServicesConfiguration\n\nservice_config = ServicesConfiguration(\n    application_name=VESPA_APPLICATION_NAME,\n    services_config=services(\n        container(\n            search(),\n            document_api(),\n            document_processing(),\n            clients(\n                client(\n                    certificate(file=\"security/clients.pem\"),\n                    id=\"mtls\",\n                    permissions=\"read,write\",\n                ),\n                client(\n                    token(id=f\"{VESPA_TOKEN_ID}\"),\n                    id=\"token_write\",\n                    permissions=\"read,write\",\n                ),\n            ),\n            config(\n                vt(\"tag\")(\n                    vt(\"bold\")(\n                        vt(\"open\", \"&lt;strong&gt;\"),\n                        vt(\"close\", \"&lt;/strong&gt;\"),\n                    ),\n                    vt(\"separator\", \"...\"),\n                ),\n                name=\"container.qr-searchers\",\n            ),\n            id=f\"{VESPA_APPLICATION_NAME}_container\",\n            version=\"1.0\",\n        ),\n        content(\n            redundancy(\"1\"),\n            documents(document(type=\"pdf_page\", mode=\"index\")),\n            nodes(node(distribution_key=\"0\", hostalias=\"node1\")),\n            config(\n                vt(\"max_matches\", \"2\", replace_underscores=False),\n                vt(\"length\", \"1000\"),\n                vt(\"surround_max\", \"500\", replace_underscores=False),\n                vt(\"min_length\", \"300\", replace_underscores=False),\n                name=\"vespa.config.search.summary.juniperrc\",\n            ),\n            id=f\"{VESPA_APPLICATION_NAME}_content\",\n            version=\"1.0\",\n        ),\n        version=\"1.0\",\n    ),\n)\n</pre> from vespa.configuration.services import (     services,     container,     search,     document_api,     document_processing,     clients,     client,     config,     content,     redundancy,     documents,     node,     certificate,     token,     document,     nodes, ) from vespa.configuration.vt import vt from vespa.package import ServicesConfiguration  service_config = ServicesConfiguration(     application_name=VESPA_APPLICATION_NAME,     services_config=services(         container(             search(),             document_api(),             document_processing(),             clients(                 client(                     certificate(file=\"security/clients.pem\"),                     id=\"mtls\",                     permissions=\"read,write\",                 ),                 client(                     token(id=f\"{VESPA_TOKEN_ID}\"),                     id=\"token_write\",                     permissions=\"read,write\",                 ),             ),             config(                 vt(\"tag\")(                     vt(\"bold\")(                         vt(\"open\", \"\"),                         vt(\"close\", \"\"),                     ),                     vt(\"separator\", \"...\"),                 ),                 name=\"container.qr-searchers\",             ),             id=f\"{VESPA_APPLICATION_NAME}_container\",             version=\"1.0\",         ),         content(             redundancy(\"1\"),             documents(document(type=\"pdf_page\", mode=\"index\")),             nodes(node(distribution_key=\"0\", hostalias=\"node1\")),             config(                 vt(\"max_matches\", \"2\", replace_underscores=False),                 vt(\"length\", \"1000\"),                 vt(\"surround_max\", \"500\", replace_underscores=False),                 vt(\"min_length\", \"300\", replace_underscores=False),                 name=\"vespa.config.search.summary.juniperrc\",             ),             id=f\"{VESPA_APPLICATION_NAME}_content\",             version=\"1.0\",         ),         version=\"1.0\",     ), ) In\u00a0[\u00a0]: Copied! <pre># Create the Vespa application package\nvespa_application_package = ApplicationPackage(\n    name=VESPA_APPLICATION_NAME,\n    schema=[colpali_schema],\n    services_config=service_config,\n)\n</pre> # Create the Vespa application package vespa_application_package = ApplicationPackage(     name=VESPA_APPLICATION_NAME,     schema=[colpali_schema],     services_config=service_config, ) In\u00a0[\u00a0]: Copied! <pre># This is only needed for CI.\nVESPA_TEAM_API_KEY = os.getenv(\"VESPA_TEAM_API_KEY\", None)\n</pre> # This is only needed for CI. VESPA_TEAM_API_KEY = os.getenv(\"VESPA_TEAM_API_KEY\", None) In\u00a0[\u00a0]: Copied! <pre>vespa_cloud = VespaCloud(\n    tenant=VESPA_TENANT_NAME,\n    application=VESPA_APPLICATION_NAME,\n    key_content=VESPA_TEAM_API_KEY,\n    application_package=vespa_application_package,\n)\n\n# Deploy the application\nvespa_cloud.deploy()\n\n# Output the endpoint URL\nendpoint_url = vespa_cloud.get_token_endpoint()\nprint(f\"Application deployed. Token endpoint URL: {endpoint_url}\")\n</pre> vespa_cloud = VespaCloud(     tenant=VESPA_TENANT_NAME,     application=VESPA_APPLICATION_NAME,     key_content=VESPA_TEAM_API_KEY,     application_package=vespa_application_package, )  # Deploy the application vespa_cloud.deploy()  # Output the endpoint URL endpoint_url = vespa_cloud.get_token_endpoint() print(f\"Application deployed. Token endpoint URL: {endpoint_url}\") <p>Make sure to take note of the token endpoint_url. You need to put this in your <code>.env</code> file for your web application - <code>VESPA_APP_TOKEN_URL=https://abcd.vespa-app.cloud</code> - to access the Vespa application from your web application.</p> In\u00a0[\u00a0]: Copied! <pre># Instantiate Vespa connection using token\napp = Vespa(url=endpoint_url, vespa_cloud_secret_token=VESPA_CLOUD_SECRET_TOKEN)\napp.get_application_status()\n</pre> # Instantiate Vespa connection using token app = Vespa(url=endpoint_url, vespa_cloud_secret_token=VESPA_CLOUD_SECRET_TOKEN) app.get_application_status() <p>Now, let us feed the data to Vespa. If you have a large dataset, you could also do this async, with <code>feed_async_iterable()</code>, see Feeding Vespa cloud for a detailed comparison.</p> In\u00a0[\u00a0]: Copied! <pre>def callback(response: VespaResponse, id: str):\n    if not response.is_successful():\n        print(\n            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n        )\n\n\n# Feed data into Vespa synchronously\napp.feed_iterable(vespa_feed, schema=VESPA_SCHEMA_NAME, callback=callback)\n</pre> def callback(response: VespaResponse, id: str):     if not response.is_successful():         print(             f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"         )   # Feed data into Vespa synchronously app.feed_iterable(vespa_feed, schema=VESPA_SCHEMA_NAME, callback=callback) <p>For now, we will just run a query with the default rank profile. We will need a utility function to generate embeddings for the query, and pass this to Vespa to use for calculating MaxSim. In the web application, we also provide function to generate binary embeddings, allowing the user to choose different rank profiles at query time.</p> In\u00a0[\u00a0]: Copied! <pre>query = \"Price development in Technology sector from April 2023?\"\n</pre> query = \"Price development in Technology sector from April 2023?\" In\u00a0[\u00a0]: Copied! <pre>def get_q_embs_vespa_format(query: str):\n    inputs = processor.process_queries([query]).to(model.device)\n    with torch.no_grad():\n        embeddings_query = model(**inputs)\n        q_embs = embeddings_query.to(\"cpu\")[0]  # Extract the single embedding\n    return {idx: emb.tolist() for idx, emb in enumerate(q_embs)}\n</pre> def get_q_embs_vespa_format(query: str):     inputs = processor.process_queries([query]).to(model.device)     with torch.no_grad():         embeddings_query = model(**inputs)         q_embs = embeddings_query.to(\"cpu\")[0]  # Extract the single embedding     return {idx: emb.tolist() for idx, emb in enumerate(q_embs)} In\u00a0[\u00a0]: Copied! <pre>q_emb = get_q_embs_vespa_format(query)\n</pre> q_emb = get_q_embs_vespa_format(query) In\u00a0[\u00a0]: Copied! <pre>with app.syncio() as sess:\n    response = sess.query(\n        body={\n            \"yql\": (\n                f\"select id, url, title, year, full_image, quantized  from {VESPA_SCHEMA_NAME} where userQuery();\"\n            ),\n            \"ranking\": \"default\",\n            \"query\": query,\n            \"timeout\": \"10s\",\n            \"hits\": 3,\n            \"input.query(qt)\": q_emb,\n            \"presentation.timing\": True,\n        }\n    )\n</pre> with app.syncio() as sess:     response = sess.query(         body={             \"yql\": (                 f\"select id, url, title, year, full_image, quantized  from {VESPA_SCHEMA_NAME} where userQuery();\"             ),             \"ranking\": \"default\",             \"query\": query,             \"timeout\": \"10s\",             \"hits\": 3,             \"input.query(qt)\": q_emb,             \"presentation.timing\": True,         }     ) In\u00a0[\u00a0]: Copied! <pre>assert len(response.json[\"root\"][\"children\"]) == 3\n</pre> assert len(response.json[\"root\"][\"children\"]) == 3 <p>Great. You have now deployed the Vespa application and fed the data to it, and made sure you are able to query it using the vespa endpoint and a token.</p> In\u00a0[\u00a0]: Copied! <pre>key_path = Path(\n    f\"~/.vespa/{VESPA_TENANT_NAME}.{VESPA_APPLICATION_NAME}.default/data-plane-private-key.pem\"\n).expanduser()\ncert_path = Path(\n    f\"~/.vespa/{VESPA_TENANT_NAME}.{VESPA_APPLICATION_NAME}.default/data-plane-public-cert.pem\"\n).expanduser()\n\nassert key_path.exists(), cert_path.exists()\n</pre> key_path = Path(     f\"~/.vespa/{VESPA_TENANT_NAME}.{VESPA_APPLICATION_NAME}.default/data-plane-private-key.pem\" ).expanduser() cert_path = Path(     f\"~/.vespa/{VESPA_TENANT_NAME}.{VESPA_APPLICATION_NAME}.default/data-plane-public-cert.pem\" ).expanduser()  assert key_path.exists(), cert_path.exists() In\u00a0[\u00a0]: Copied! <pre>!git clone --depth 1 --filter=blob:none --sparse https://github.com/vespa-engine/sample-apps.git src &amp;&amp; cd src &amp;&amp; git sparse-checkout set visual-retrieval-colpali\n</pre> !git clone --depth 1 --filter=blob:none --sparse https://github.com/vespa-engine/sample-apps.git src &amp;&amp; cd src &amp;&amp; git sparse-checkout set visual-retrieval-colpali <p>Now, you have the code for the webapp in your <code>src/visual-retrieval-colpali</code>-directory</p> In\u00a0[\u00a0]: Copied! <pre>os.listdir(\"src/visual-retrieval-colpali\")\n</pre> os.listdir(\"src/visual-retrieval-colpali\") In\u00a0[\u00a0]: Copied! <pre># rename src/visual-retrieval-colpali/.env.example\nos.rename(\n    \"src/visual-retrieval-colpali/.env.example\", dst=\"src/visual-retrieval-colpali/.env\"\n)\n</pre> # rename src/visual-retrieval-colpali/.env.example os.rename(     \"src/visual-retrieval-colpali/.env.example\", dst=\"src/visual-retrieval-colpali/.env\" ) <p>And you're ready to spin up your web app locally, and deploy to huggingface spaces if you want. Navigate to <code>src/visual-retrieval-colpali/</code> directory and follow the instructions in the <code>README.md</code> to continue. \ud83d\ude80</p> In\u00a0[\u00a0]: Copied! <pre>if os.getenv(\"CI\", \"false\") == \"true\":\n    vespa_cloud.delete()\n</pre> if os.getenv(\"CI\", \"false\") == \"true\":     vespa_cloud.delete()"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#visual-pdf-rag-with-vespa-colpali-demo-application","title":"Visual PDF RAG with Vespa - ColPali demo application\u00b6","text":"<p>We created an end-to-end demo application for visual retrieval of PDF pages using Vespa, including a frontend web application. To see the live demo, visit https://vespa-engine-colpali-vespa-visual-retrieval.hf.space/.</p> <p>The main goal of the demo is to make it easy for you to create your own PDF Enterprise Search application using Vespa. To deploy a full demo, you need two main components:</p> <ol> <li>A Vespa application that lets you index and search PDF pages using ColPali embeddings.</li> <li>A live web application that lets you interact with the Vespa application.</li> </ol> <p></p> <p>After running this notebook, you will have set up a Vespa application, and indexed some PDF pages. You can then test that you are able to query the Vespa application, and you will be ready to deploy the web application including the frontend.</p> <p>Some of the features we want to highlight in this demo are:</p> <ul> <li>Visual retrieval of PDF pages using ColPali embeddings</li> <li>Explainability by displaying similarity maps over the patches in the PDF pages for each query token.</li> <li>Extracting queries and questions from the PDF pages using <code>gemini-1.5-8b</code> model.</li> <li>Type-ahead search suggestions based on the extracted queries and questions.</li> <li>Comparison of different retrieval and ranking strategies (BM25, ColPali MaxSim, and a combination of both).</li> <li>AI-generated responses to the query based on the top ranked PDF pages. Also using the <code>gemini-1.5-8b</code> model.</li> </ul> <p>We also wanted to give a notion of which latency one can expect using Vespa for this use case. Event though your users might not state this explicitly, we consider it important to provide a snappy user experience.</p> <p>In this notebook, we will prepare the Vespa backend application for our visual retrieval demo. We will use ColPali as the model to extract patch vectors from images of pdf pages. At query time, we use MaxSim to retrieve and/or (based on the configuration) rank the page results.</p> <p></p> <p></p> <p>The steps we will take in this notebook are:</p> <ol> <li>Setup and configuration</li> <li>Download PDFs</li> <li>Convert PDFs to images</li> <li>Generate queries and questions</li> <li>Generate ColPali embeddings</li> <li>Prepare the Vespa application package</li> <li>Deploy the Vespa application to Vespa Cloud</li> <li>Feed the data to the Vespa application</li> <li>Test a query to the Vespa application</li> </ol> <p>All the steps that are needed to provision the Vespa application, including feeding the data, can be done by running this notebook. We have tried to make it easy for others to run this notebook, to create your own PDF Enterprise Search application using Vespa.</p> <p>If you want to run this notebook in Colab, you can do so by clicking the button below:</p> <p></p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#1-setup-and-configuration","title":"1. Setup and Configuration\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#create-a-free-trial-in-vespa-cloud","title":"Create a free trial in Vespa Cloud\u00b6","text":"<p>Create a tenant from here. The trial includes $300 credit. Take note of your tenant name, and input it below.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#loading-the-colpali-model-from-huggingface","title":"Loading the ColPali model from huggingface \ud83e\udd17\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#2-download-pdfs","title":"2. Download PDFs\u00b6","text":"<p>We are going to use public reports from the Norwegian Government Pension Fund Global (also known as the Oil Fund). The fund puts transparency at the forefront and publishes reports on its investments, holdings, and returns, as well as its strategy and governance.</p> <p>These reports are the ones we are going to use for this showcase. Here are some sample images:</p> <p> </p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#downloading-the-pdfs","title":"Downloading the PDFs\u00b6","text":"<p>We create a function to download the PDFs from the web to the provided directory.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#3-convert-pdfs-to-images","title":"3. Convert PDFs to Images\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#4-generate-queries","title":"4. Generate Queries\u00b6","text":"<p>In this step, we want to generate queries for each page image. These will be useful for 2 reasons:</p> <ol> <li>We can use these queries as typeahead suggestions in the search bar.</li> <li>We could potentially use the queries to generate an evaluation dataset. See Improving Retrieval with LLM-as-a-judge for a deeper dive into this topic. This will not be within the scope of this notebook though.</li> </ol> <p>The prompt for generating queries is adapted from this wonderful blog post by Daniel van Strien.</p> <p>We have modified the prompt to also generate keword based queries, in addition to the question based queries.</p> <p>We will use the Gemini API to generate these queries, with <code>gemini-flash-lite-latest</code> as the model.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#5-generate-embeddings","title":"5. Generate embeddings\u00b6","text":"<p>Now that we have the queries, we can use the ColPali model to generate embeddings for each page image.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#6-prepare-data-on-vespa-format","title":"6. Prepare Data on Vespa Format\u00b6","text":"<p>Now, that we have all the data we need, all that remains is to make sure it is in the right format for Vespa.</p> <p>We now convert the embeddings to Vespa JSON format so we can store (and index) them in Vespa. Details in Vespa JSON feed format doc.</p> <p>We use binary quantization (BQ) of the page level ColPali vector embeddings to reduce their size by 32x.</p> <p>Read more about binarization of multi-vector representations in the colbert blog post.</p> <p>The binarization step maps 128 dimensional floats to 128 bits, or 16 bytes per vector. Reducing the size by 32x. On the DocVQA benchmark, binarization results in only a small drop in ranking accuracy.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#optional-saving-the-feed-file","title":"[Optional] Saving the feed file\u00b6","text":"<p>If you have a large dataset, you can optionally save the file, and feed it using the Vespa CLI, which is more performant than the pyvespa client. See Feeding to Vespa Cloud for more details. Uncomment the cell below if you want to save the feed file.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#7-prepare-vespa-application","title":"7. Prepare Vespa Application\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#configuring-the-application-package","title":"Configuring the application package\u00b6","text":"<p>PyVespa helps us build the Vespa application package. A Vespa application package consists of configuration files, schemas, models, and code (plugins).</p> <p>Here are some of the key components of this application package:</p> <ol> <li>We store images (and a scaled down version of the image) as a <code>raw</code> field.</li> <li>We store the binarized ColPali embeddings as a <code>tensor&lt;int8&gt;</code> field.</li> <li>We store the queries and questions as a <code>array&lt;str&gt;</code> field.</li> <li>We define 3 different ranking profiles:<ul> <li><code>default</code> Uses BM25 for first phase ranking and MaxSim for second phase ranking.</li> <li><code>bm25</code> Uses <code>bm25(title) + bm25(text)</code> (first phase only) for ranking.</li> <li><code>retrieval-and-rerank</code> Uses <code>nearestneighbor</code> of the query embedding over the document embeddings for retrieval, <code>binary_max_sim</code> for first phase ranking, and <code>max_sim</code> of the query-embeddings as float for second phase ranking. Vespa's phased ranking allows us to use different ranking strategies for retrieval and reranking, to choose attractive trade-offs between latency, cost, and accuracy.</li> </ul> </li> <li>We also calculate dot product between the query and each document, so that it can be returned with the results, to generate the similarity maps, which show which patches of the image is most similar to the query token embeddings.</li> </ol> <p>First, we define a Vespa schema with the fields we want to store and their type.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#configuring-the-servicesxml","title":"Configuring the <code>services.xml</code>\u00b6","text":"<p>services.xml is the primary configuration file for a Vespa application, with a plethora of options to configure the application.</p> <p>Since <code>pyvespa</code> version <code>0.50.0</code>, these configuration options are also available in <code>pyvespa</code>. See Pyvespa - Advanced configuration for more details. (Note that configurating this is optional, and pyvespa will use basic defaults for you if you opt out).</p> <p>We will use the advanced configuration to configure up dynamic snippets. This allows us to highlight matched terms in the search results and generate a <code>snippet</code> to display, rather than the full text of the document.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#8-deploy-vespa-application","title":"8. Deploy Vespa Application\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#9-feed-data-to-vespa","title":"9. Feed Data to Vespa\u00b6","text":"<p>We will need the <code>enpdoint_url</code> and <code>colpalidemo_write</code> token to feed the data to the Vespa application.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#10-test-a-query-to-the-vespa-application","title":"10. Test a query to the Vespa application\u00b6","text":""},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#saving-the-generated-keycert-files","title":"Saving the generated key/cert files\u00b6","text":"<p>A key and cert file is generated for you as an alternative to using tokens for authentication. We advise you to save these files in a secure location, in case you want to use them for authentication in the future.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#11-deploying-your-web-app","title":"11. Deploying your web app\u00b6","text":"<p>To deploy a frontend to let users interact with the Vespa application. you can clone the sample app from sample-apps repo. It includes instructions for running and connecting your web application to your vespa app.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#setting-environment-variables-for-your-web-app","title":"Setting environment variables for your web app\u00b6","text":"<p>Now, you need to set the following variables in the <code>src/.env.example</code>-file:</p> <pre>VESPA_APP_TOKEN_URL=https://abcde.z.vespa-app.cloud # Your token endpoint url you got after deploying your Vespa app.\nVESPA_CLOUD_SECRET_TOKEN=vespa_cloud_xxxxxxxx # The value of the token that your created in this notebook. \nGEMINI_API_KEY=your_api_key # The same as GOOGLE_API_KEY in this notebook\nHF_TOKEN=hf_xxxx # If you want to deploy your web app to huggingface spaces - https://huggingface.co/settings/tokens\n</pre> <p>After, that, rename your file to .env.</p>"},{"location":"examples/visual_pdf_rag_with_vespa_colpali_cloud.html#cleanup","title":"Cleanup\u00b6","text":"<p>As this notebook runs in CI, we will delete the Vespa application after running the notebook. DO NOT run the cell below unless you are sure you want to delete the Vespa application.</p>"}]}