{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "<picture>\n",
    "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://assets.vespa.ai/logos/Vespa-logo-green-RGB.svg\">\n",
    "  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://assets.vespa.ai/logos/Vespa-logo-dark-RGB.svg\">\n",
    "  <img alt=\"#Vespa\" width=\"200\" src=\"https://assets.vespa.ai/logos/Vespa-logo-dark-RGB.svg\" style=\"margin-bottom: 25px;\">\n",
    "</picture>\n",
    "\n",
    "# Scalable Asymmetric Retrieval with Voyage AI Embeddings in Vespa\n",
    "\n",
    "The [Voyage 4 model family](https://blog.voyageai.com/2026/01/15/voyage-4/) offers state-of-the-art embedding quality\n",
    "across a range of model sizes. Vespa recently added an integration to allow for seamless embedding through Voyage's API.\n",
    "This notebook demonstrates an **asymmetric retrieval** pattern, combining both this API-based integration, and a Vespa-local Open Source model:\n",
    "\n",
    "- **Indexing**: Use `voyage-4-large` (API-based, highest quality) to embed documents once via Vespa's\n",
    "  [voyage-ai-embedder](https://docs.vespa.ai/en/reference/rag/embedding.html#voyageai-embedder).\n",
    "- **Querying**: Use `voyage-4-nano` (open-source, runs locally on the Vespa container) via\n",
    "  [hugging-face-embedder](https://docs.vespa.ai/en/reference/rag/embedding.html#hugging-face-embedder) for zero-cost, low-latency queries.\n",
    "\n",
    "We combine [binary embeddings](https://docs.vespa.ai/en/rag/binarizing-vectors.html) for fast first-phase retrieval with **float reranking** for accuracy.\n",
    "\n",
    "Relevant resources:\n",
    "- [Vespa embedding documentation](https://docs.vespa.ai/en/rag/embedding.html)\n",
    "- [Embedding Tradeoffs, Quantified](https://blog.vespa.ai/embedding-tradeoffs-quantified/) — benchmarks of voyage-4-nano-int8 and other models on Vespa\n",
    "- [Nearest Neighbor Search](https://docs.vespa.ai/en/querying/nearest-neighbor-search.html)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/voyage-ai-embeddings-cloud.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-deps",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U pyvespa vespacli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cost-analysis",
   "metadata": {},
   "source": [
    "## Why Asymmetric Retrieval?\n",
    "\n",
    "Embedding documents and queries with the same API-based model works well, but at high query volumes\n",
    "the cost of embedding every query adds up. Asymmetric retrieval eliminates this cost entirely.\n",
    "\n",
    "### The asymmetric insight\n",
    "\n",
    "- **Documents are embedded once** at indexing time. Use the best model (`voyage-4-large`) for maximum quality.\n",
    "- **Queries happen on every search**. Use a fast, local model (`voyage-4-nano`) with zero API cost and no rate limits.\n",
    "\n",
    "### Example: 10K QPS\n",
    "\n",
    "At 10,000 queries/sec with ~30-token queries, that's ~18M tokens per minute. Even at $0.02 per 1M tokens,\n",
    "this adds up to **~$31K/month** in embedding costs alone. Running `voyage-4-nano` locally on the Vespa\n",
    "container reduces this to **$0/month** with single-digit ms latency — the model runs as part of the\n",
    "serving infrastructure you're already paying for.\n",
    "\n",
    "The `voyage-4-nano` model from the same Voyage 4 family produces embeddings in the same vector space\n",
    "as `voyage-4-large`, making cross-model similarity meaningful.\n",
    "\n",
    "### voyage-4-nano-int8 Quality Benchmarks\n",
    "\n",
    "From [Embedding Tradeoffs, Quantified](https://blog.vespa.ai/embedding-tradeoffs-quantified/),\n",
    "benchmarked on an AWS c7g.2xlarge instance. The model is 332 MB and supports a 32,768 token context\n",
    "with an embedding latency of 12.6-15.0 ms, running on CPU. It also supports\n",
    "[Matryoshka Representation Learning (MRL)](https://arxiv.org/abs/2205.13147) for flexible dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gi074jjciie",
   "metadata": {},
   "source": [
    "![Asymmetric Retrieval Architecture](../_static/asymmetric-embeddings.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-intro",
   "metadata": {},
   "source": [
    "## Define the Vespa Schema\n",
    "\n",
    "We define a [Vespa schema](https://docs.vespa.ai/en/schemas.html) with two document fields (`id`, `text`) and two synthetic embedding fields\n",
    "computed at indexing time by the `voyage-4-large` embedder:\n",
    "\n",
    "- `embedding_float`: Half-precision (bfloat16) embeddings (2048 dimensions) for accurate reranking.\n",
    "  Uses [`paged` attribute](https://docs.vespa.ai/en/content/attributes.html#paged-attributes) to keep data on disk, reducing memory cost.\n",
    "- `embedding_binary`: [Binary (int8) embeddings](https://docs.vespa.ai/en/rag/binarizing-vectors.html) (2048/8 = 256 bytes) for fast [hamming-distance](https://docs.vespa.ai/en/reference/schema-reference.html#distance-metric) retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "schema-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import Schema, Document, Field\n",
    "\n",
    "SCHEMA_NAME = \"doc\"\n",
    "FEED_MODEL_ID = \"voyage-4-large\"\n",
    "QUERY_MODEL_ID = \"voyage-4-nano-int8\"\n",
    "\n",
    "schema = Schema(\n",
    "    name=SCHEMA_NAME,\n",
    "    document=Document(\n",
    "        fields=[\n",
    "            Field(name=\"id\", type=\"string\", indexing=[\"summary\", \"attribute\"]),\n",
    "            Field(name=\"text\", type=\"string\", indexing=[\"index\", \"summary\"]),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Synthetic fields: computed from 'text' at indexing time using the voyage-4-large embedder.\n",
    "# These are not part of the document type, so is_document_field=False.\n",
    "schema.add_fields(\n",
    "    Field(\n",
    "        name=\"embedding_float\",\n",
    "        type=\"tensor<bfloat16>(x[2048])\",\n",
    "        indexing=[\"input text\", f\"embed {FEED_MODEL_ID}\", \"attribute\"],\n",
    "        attribute=[\"distance-metric: prenormalized-angular\", \"paged\"],\n",
    "        is_document_field=False,\n",
    "    )\n",
    ")\n",
    "schema.add_fields(\n",
    "    Field(\n",
    "        name=\"embedding_binary\",\n",
    "        type=\"tensor<int8>(x[256])\",  # 2048 bits / 8 = 256 bytes\n",
    "        indexing=[\"input text\", f\"embed {FEED_MODEL_ID}\", \"attribute\"],\n",
    "        attribute=[\"distance-metric: hamming\"],\n",
    "        is_document_field=False,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rank-profile-intro",
   "metadata": {},
   "source": [
    "## Rank Profile: Binary Retrieval with Float Reranking\n",
    "\n",
    "This [rank profile](https://docs.vespa.ai/en/ranking.html) implements a two-phase strategy:\n",
    "\n",
    "1. **First phase**: Hamming distance on binary embeddings. This is extremely fast and scans\n",
    "   many candidates cheaply.\n",
    "2. **Second phase**: Cosine closeness on full float embeddings. This is more accurate and\n",
    "   applied only to the top candidates from phase one.\n",
    "\n",
    "The query inputs (`q_float`, `q_bin`) are produced by the local `voyage-4-nano` model at query time.\n",
    "The [`rerank_count`](https://docs.vespa.ai/en/reference/schema-reference.html#rerank-count) controls how many first-phase candidates are rescored in the second phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "rank-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile, Function, SecondPhaseRanking\n",
    "\n",
    "RERANK_COUNT = 2000\n",
    "\n",
    "schema.add_rank_profile(\n",
    "    RankProfile(\n",
    "        name=\"binary-with-rerank\",\n",
    "        inputs=[\n",
    "            (\"query(q_float)\", \"tensor<float>(x[2048])\"),\n",
    "            (\"query(q_bin)\", \"tensor<int8>(x[256])\"),\n",
    "        ],\n",
    "        functions=[\n",
    "            Function(\n",
    "                name=\"binary_closeness\",\n",
    "                expression=\"1 - (distance(field, embedding_binary) / 2048)\",\n",
    "            ),\n",
    "            Function(\n",
    "                name=\"float_closeness\",\n",
    "                expression=\"reduce(query(q_float) * attribute(embedding_float), sum, x)\",\n",
    "            ),\n",
    "        ],\n",
    "        first_phase=\"binary_closeness\",\n",
    "        second_phase=SecondPhaseRanking(\n",
    "            expression=\"float_closeness\", rerank_count=RERANK_COUNT\n",
    "        ),\n",
    "        summary_features=[\n",
    "            \"binary_closeness\",\n",
    "            \"float_closeness\",\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7s16frihbha",
   "metadata": {},
   "source": [
    "### Why `paged` for float embeddings?\n",
    "\n",
    "The `embedding_float` field uses the [`paged` attribute](https://docs.vespa.ai/en/content/attributes.html#paged-attributes),\n",
    "which lets Vespa page attribute data from memory to disk. This is critical for keeping memory costs manageable.\n",
    "\n",
    "**Napkin math** — memory per document at 2048 dimensions:\n",
    "\n",
    "| Representation | Type | Bytes/vector | 1M docs | 10M docs | 100M docs |\n",
    "|---|---|---|---|---|---|\n",
    "| `embedding_float` | `float` (32-bit) | 8,192 B | ~7.6 GB | ~76 GB | ~763 GB |\n",
    "| `embedding_binary` | `int8` (1-bit packed) | 256 B | ~0.24 GB | ~2.4 GB | ~24 GB |\n",
    "\n",
    "The float embeddings are **32x larger** than the binary ones. Without `paged`, all float vectors must fit in memory.\n",
    "At 100M documents that's ~763 GB of RAM just for one field. With `paged`, the OS kernel manages what's in memory\n",
    "based on access patterns — only the vectors actually touched during reranking need to be resident.\n",
    "\n",
    "This works well here because **float vectors are only accessed during second-phase reranking**, not during\n",
    "first-phase retrieval. The first phase uses only the compact binary embeddings (always in memory), and\n",
    "the second phase touches at most `rerank-count` float vectors per query per content node.\n",
    "\n",
    "> **Important**: Do not combine `paged` with [HNSW indexing](https://docs.vespa.ai/en/approximate-nn-hnsw.html),\n",
    "> as HNSW requires random access across the full graph during search, which would cause excessive disk I/O.\n",
    "> Here we use `paged` safely because `embedding_float` has no HNSW index — it's accessed only via direct\n",
    "> attribute lookups during reranking.\n",
    "\n",
    "### Why `rerank-count` matters with `paged`\n",
    "\n",
    "The [`rerank-count`](https://docs.vespa.ai/en/ranking/phased-ranking.html) parameter (set to 2000 above) controls\n",
    "how many first-phase candidates are re-scored in the second phase **per content node**. This is the knob that\n",
    "bounds cost:\n",
    "\n",
    "- **Too low** (e.g., 50): Fast, but the cheap binary first-phase may miss relevant documents that float\n",
    "  reranking would have rescued. Recall suffers.\n",
    "- **Too high** (e.g., 50,000): More float vectors paged in from disk per query, increasing latency and disk I/O.\n",
    "  The quality gains diminish quickly — most relevant documents are already in the top few thousand candidates.\n",
    "- **2000**: A reasonable default that balances recall, latency, and disk I/O. At 2000 candidates x 8,192 bytes\n",
    "  per vector = ~16 MB of float data accessed per query per node — easily serviceable from the OS page cache\n",
    "  for any reasonable query rate.\n",
    "\n",
    "The combination of `paged` + bounded `rerank-count` is what makes this architecture work: you get the storage\n",
    "efficiency of keeping float vectors on disk, with the performance guarantee that each query only touches a\n",
    "small, predictable number of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "services-intro",
   "metadata": {},
   "source": "## Services Configuration\n\nWe configure two [embedder components](https://docs.vespa.ai/en/rag/embedding.html):\n\n1. **`voyage-4-large`** ([voyage-ai-embedder](https://docs.vespa.ai/en/reference/rag/embedding.html#voyageai-embedder)):\n   Calls the Voyage AI API. Used at document indexing time to produce high-quality embeddings.\n   Requires an API key stored in Vespa Cloud's [secret store](https://cloud.vespa.ai/en/security/secret-store).\n2. **`voyage-4-nano-int8`** ([hugging-face-embedder](https://docs.vespa.ai/en/reference/rag/embedding.html#hugging-face-embedder)):\n   Runs locally on the Vespa container as an ONNX model. Used at query time for zero-cost, low-latency embedding.\n   No API key needed.\n\n### Batching for throughput\n\nThe `voyage-ai-embedder` supports\n[dynamic batching](https://docs.vespa.ai/en/reference/rag/embedding.html#voyageai-embedder) of concurrent\nembedding requests into single API calls. We configure `max-size: 20` (up to 20 documents per batch) and\n`max-delay: 20ms` (maximum wait time before sending a partial batch). This significantly reduces\nthe number of API calls during bulk feeding. Combined with the increased `document-processing` threadpool\n(1000 threads), this allows high-throughput parallel embedding at index time.\n\n### Quantization\n\nThe `voyage-ai-embedder` also supports server-side `quantization` (with values `auto`, `float`, `int8`, or `binary`).\nWhen set to `auto` (the default), Vespa infers the appropriate quantization from the destination tensor's\ncell type and dimensions — so our `tensor<bfloat16>` float field and `tensor<int8>` binary field are\nhandled automatically. For this notebook we rely on `auto` quantization, which gives us full-precision\nbfloat16 embeddings paged to disk for accurate reranking, and compact binary embeddings in memory for\nfast retrieval.\n\nThe `document-processing` threadpool is increased to allow parallel embedding during feeding.\n\nThe `ServicesConfiguration` class below uses pyvespa's type-safe Python API for generating\n[`services.xml`](https://docs.vespa.ai/en/reference/services.html).\nFor a deeper dive into all the configuration options available, see the\n[advanced configuration](../advanced-configuration.ipynb) notebook."
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "services-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ServicesConfiguration\n",
    "from vespa.configuration.services import (\n",
    "    services,\n",
    "    batching,\n",
    "    container,\n",
    "    content,\n",
    "    search,\n",
    "    document_api,\n",
    "    document_processing,\n",
    "    component,\n",
    "    components,\n",
    "    model,\n",
    "    api_key_secret_ref,\n",
    "    dimensions,\n",
    "    documents,\n",
    "    document,\n",
    "    nodes,\n",
    "    node,\n",
    "    secrets,\n",
    "    threadpool,\n",
    "    threads,\n",
    "    redundancy,\n",
    "    transformer_model,\n",
    "    tokenizer_model,\n",
    "    pooling_strategy,\n",
    "    normalize,\n",
    "    prepend,\n",
    "    max_tokens,\n",
    "    query,\n",
    ")\n",
    "from vespa.configuration.vt import vt\n",
    "\n",
    "APPLICATION_NAME = \"voyageai\"\n",
    "\n",
    "# Replace with your Vespa Cloud secret store vault and secret name\n",
    "SECRET_STORE_VAULT_NAME = \"pyvespa-testvault\"\n",
    "VOYAGE_SECRET_NAME = \"voyage_api_key\"\n",
    "\n",
    "services_config = ServicesConfiguration(\n",
    "    application_name=APPLICATION_NAME,\n",
    "    services_config=services(\n",
    "        container(id=f\"{APPLICATION_NAME}_container\", version=\"1.0\")(\n",
    "            secrets(\n",
    "                vt(\n",
    "                    tag=\"apiKey\",\n",
    "                    vault=SECRET_STORE_VAULT_NAME,\n",
    "                    name=VOYAGE_SECRET_NAME,\n",
    "                )\n",
    "            ),\n",
    "            search(),\n",
    "            document_api(),\n",
    "            document_processing(threadpool(threads(\"1000\"))),\n",
    "            components(\n",
    "                # Local model for query-time embedding (zero API cost)\n",
    "                component(id=\"voyage-4-nano-int8\", type_=\"hugging-face-embedder\")(\n",
    "                    transformer_model(model_id=\"voyage-4-nano-int8\"),\n",
    "                    tokenizer_model(model_id=\"voyage-4-nano-vocab\"),\n",
    "                    max_tokens(\"32768\"),\n",
    "                    pooling_strategy(\"mean\"),\n",
    "                    normalize(\"true\"),\n",
    "                    prepend(\n",
    "                        query(\n",
    "                            \"Represent the query for retrieving supporting documents: \"\n",
    "                        )\n",
    "                    ),\n",
    "                ),\n",
    "                # API-based model for index-time embedding (highest quality)\n",
    "                component(id=\"voyage-4-large\", type_=\"voyage-ai-embedder\")(\n",
    "                    model(\"voyage-4-large\"),\n",
    "                    api_key_secret_ref(\"apiKey\"),\n",
    "                    dimensions(\"2048\"),\n",
    "                    batching(max_size=\"20\", max_delay=\"20ms\"),\n",
    "                ),\n",
    "            ),\n",
    "            nodes(count=\"1\", required=\"true\"),\n",
    "        ),\n",
    "        content(id=f\"{APPLICATION_NAME}_content\", version=\"1.0\")(\n",
    "            redundancy(\"1\"),\n",
    "            documents(document(type_=\"doc\", mode=\"index\")),\n",
    "            nodes(node(distribution_key=\"0\", hostalias=\"node1\")),\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "app-package-intro",
   "metadata": {},
   "source": [
    "## Create and Deploy the Application Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "app-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage\n",
    "\n",
    "app_package = ApplicationPackage(\n",
    "    name=APPLICATION_NAME,\n",
    "    schema=[schema],\n",
    "    services_config=services_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deploy-intro",
   "metadata": {},
   "source": [
    "Deploy to [Vespa Cloud](https://cloud.vespa.ai/en/).\n",
    "Create a tenant at [console.vespa-cloud.com](https://console.vespa-cloud.com/) if you don't have one.\n",
    "\n",
    "Before deploying, you need to configure a secret in the Vespa Cloud secret store with your\n",
    "Voyage AI API key. See [Vespa Cloud secret store](https://cloud.vespa.ai/en/security/secret-store)\n",
    "for instructions.\n",
    "\n",
    "> Deployments to dev expire after 14 days of inactivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "deploy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting application...\n",
      "Running: vespa config set application vespa-team.voyageai.default\n",
      "Setting target cloud...\n",
      "Running: vespa config set target cloud\n",
      "\n",
      "No api-key found for control plane access. Using access token.\n",
      "Checking for access token in auth.json...\n",
      "Access token expired. Please re-authenticate.\n",
      "Your Device Confirmation code is: RWHK-VXWW\n",
      "Automatically open confirmation page in your default browser? [Y/n] y\n",
      "Opened link in your browser:\n",
      "\t https://login.console.vespa-cloud.com/activate?user_code=RWHK-VXWW\n",
      "Waiting for login to complete in browser ... done;1m⣻\u001b[0;22m\n",
      "\u001b[32mSuccess:\u001b[0m Logged in\n",
      " auth.json created at /Users/thomas/.vespa/auth.json\n",
      "Successfully obtained access token for control plane access.\n",
      "Deployment started in run 9 of dev-aws-us-east-1c for vespa-team.voyageai. This may take a few minutes the first time.\n",
      "INFO    [13:26:31]  Deploying platform version 8.649.29 and application dev build 9 for dev-aws-us-east-1c of default ...\n",
      "INFO    [13:26:31]  Using CA signed certificate version 1\n",
      "INFO    [13:26:37]  Session 404822 for tenant 'vespa-team' prepared and activated.\n",
      "INFO    [13:27:04]  ######## Details for all nodes ########\n",
      "INFO    [13:27:04]  h136163a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [13:27:04]  --- platform vespa/cloud-tenant-rhel8:8.649.29\n",
      "INFO    [13:27:04]  --- logserver-container on port 4080 has not started \n",
      "INFO    [13:27:04]  --- metricsproxy-container on port 19092 has not started \n",
      "INFO    [13:27:04]  h136163b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [13:27:04]  --- platform vespa/cloud-tenant-rhel8:8.649.29\n",
      "INFO    [13:27:04]  --- container-clustercontroller on port 19050 has not started \n",
      "INFO    [13:27:04]  --- metricsproxy-container on port 19092 has not started \n",
      "INFO    [13:27:04]  h136175a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [13:27:04]  --- platform vespa/cloud-tenant-rhel8:8.649.29\n",
      "INFO    [13:27:04]  --- container on port 4080 has not started \n",
      "INFO    [13:27:04]  --- metricsproxy-container on port 19092 has not started \n",
      "INFO    [13:27:04]  h136066a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [13:27:04]  --- platform vespa/cloud-tenant-rhel8:8.649.29\n",
      "INFO    [13:27:04]  --- storagenode on port 19102 has not started \n",
      "INFO    [13:27:04]  --- searchnode on port 19107 has not started \n",
      "INFO    [13:27:04]  --- distributor on port 19111 has not started \n",
      "INFO    [13:27:04]  --- metricsproxy-container on port 19092 has not started \n",
      "INFO    [13:28:01]  Waiting for convergence of 10 services across 4 nodes\n",
      "INFO    [13:28:01]  1 nodes booting\n",
      "INFO    [13:28:01]  1 application services still deploying\n",
      "DEBUG   [13:28:01]  h136175a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "DEBUG   [13:28:01]  --- platform vespa/cloud-tenant-rhel8:8.649.29\n",
      "DEBUG   [13:28:01]  --- container on port 4080 has not started \n",
      "DEBUG   [13:28:01]  --- metricsproxy-container on port 19092 has config generation 404822, wanted is 404822\n",
      "INFO    [13:28:43]  Found endpoints:\n",
      "INFO    [13:28:43]  - dev.aws-us-east-1c\n",
      "INFO    [13:28:43]   |-- https://ca603d84.b347094a.z.vespa-app.cloud/ (cluster 'voyageai_container')\n",
      "INFO    [13:28:43]  Deployment complete!\n",
      "Only region: aws-us-east-1c available in dev environment.\n",
      "Found mtls endpoint for voyageai_container\n",
      "URL: https://ca603d84.b347094a.z.vespa-app.cloud/\n",
      "Application is up!\n"
     ]
    }
   ],
   "source": [
    "from vespa.deployment import VespaCloud\n",
    "from vespa.application import Vespa\n",
    "import os\n",
    "\n",
    "tenant_name = \"vespa-team\"  # Replace with your tenant name\n",
    "\n",
    "key = os.getenv(\"VESPA_TEAM_API_KEY\", None)\n",
    "if key is not None:\n",
    "    key = key.replace(r\"\\n\", \"\\n\")\n",
    "\n",
    "vespa_cloud = VespaCloud(\n",
    "    tenant=tenant_name,\n",
    "    application=APPLICATION_NAME,\n",
    "    # key_content=key,\n",
    "    application_package=app_package,\n",
    ")\n",
    "app: Vespa = vespa_cloud.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feed-intro",
   "metadata": {},
   "source": [
    "## Feed Sample Documents\n",
    "\n",
    "We feed a few sample passages. At indexing time, Vespa calls the `voyage-4-large` API\n",
    "to generate both the float and binary embedding representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "feed-documents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fed document 1\n",
      "Fed document 2\n",
      "Fed document 3\n",
      "Fed document 4\n",
      "Fed document 5\n"
     ]
    }
   ],
   "source": [
    "from vespa.io import VespaResponse\n",
    "\n",
    "sample_docs = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"fields\": {\n",
    "            \"id\": \"1\",\n",
    "            \"text\": \"Retrieval-augmented generation (RAG) combines a retrieval system with a generative language model. The retriever finds relevant passages from a corpus, and the generator uses them as context to produce accurate, grounded answers.\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"fields\": {\n",
    "            \"id\": \"2\",\n",
    "            \"text\": \"Binary quantization reduces embedding storage by representing each dimension as a single bit. While this loses precision, combining binary retrieval with float reranking recovers most of the accuracy at a fraction of the memory cost.\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"fields\": {\n",
    "            \"id\": \"3\",\n",
    "            \"text\": \"Vespa is a fully featured search engine and vector database. It supports real-time indexing, structured and unstructured data, and advanced ranking with multiple retrieval and ranking phases.\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"4\",\n",
    "        \"fields\": {\n",
    "            \"id\": \"4\",\n",
    "            \"text\": \"Asymmetric retrieval uses different models for documents and queries. Documents are embedded once with an expensive, high-quality model, while queries use a smaller, faster model to keep latency low and costs down.\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"5\",\n",
    "        \"fields\": {\n",
    "            \"id\": \"5\",\n",
    "            \"text\": \"The Voyage 4 embedding model family includes voyage-4-large for maximum quality, voyage-4-lite for a balance of cost and quality, and voyage-4-nano as a small open-source model suitable for local deployment.\",\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "for doc in sample_docs:\n",
    "    response: VespaResponse = app.feed_data_point(\n",
    "        schema=SCHEMA_NAME, data_id=doc[\"id\"], fields=doc[\"fields\"]\n",
    "    )\n",
    "    assert (\n",
    "        response.is_successful()\n",
    "    ), f\"Failed to feed doc {doc['id']}: {response.get_json()}\"\n",
    "    print(f\"Fed document {doc['id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-intro",
   "metadata": {},
   "source": [
    "## Query with Binary Retrieval and Float Reranking\n",
    "\n",
    "At query time, Vespa uses the local `voyage-4-nano` model to embed the query text.\n",
    "The [`embed()` function](https://docs.vespa.ai/en/rag/embedding.html#embedding-a-query-text) in the query invokes the local embedder, producing both\n",
    "float and binary query representations.\n",
    "\n",
    "The retrieval pipeline:\n",
    "1. [`nearestNeighbor`](https://docs.vespa.ai/en/querying/nearest-neighbor-search.html) on `embedding_binary` scans candidates using fast hamming distance.\n",
    "2. First-phase ranking scores by `binary_closeness`.\n",
    "3. Second-phase reranking scores the top candidates by `float_closeness`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "query",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"fields\": {\n",
      "    \"documentid\": \"id:doc:doc::4\",\n",
      "    \"id\": \"4\",\n",
      "    \"sddocname\": \"doc\",\n",
      "    \"summaryfeatures\": {\n",
      "      \"binary_closeness\": 0.63623046875,\n",
      "      \"float_closeness\": 0.5481828630707257,\n",
      "      \"vespa.summaryFeatures.cached\": 0.0\n",
      "    },\n",
      "    \"text\": \"Asymmetric retrieval uses different models for documents and queries. Documents are embedded once with an expensive, high-quality model, while queries use a smaller, faster model to keep latency low and costs down.\"\n",
      "  },\n",
      "  \"id\": \"id:doc:doc::4\",\n",
      "  \"relevance\": 0.5481828630707257,\n",
      "  \"source\": \"voyageai_content\"\n",
      "}\n",
      "{\n",
      "  \"fields\": {\n",
      "    \"documentid\": \"id:doc:doc::2\",\n",
      "    \"id\": \"2\",\n",
      "    \"sddocname\": \"doc\",\n",
      "    \"summaryfeatures\": {\n",
      "      \"binary_closeness\": 0.607421875,\n",
      "      \"float_closeness\": 0.44831951343722665,\n",
      "      \"vespa.summaryFeatures.cached\": 0.0\n",
      "    },\n",
      "    \"text\": \"Binary quantization reduces embedding storage by representing each dimension as a single bit. While this loses precision, combining binary retrieval with float reranking recovers most of the accuracy at a fraction of the memory cost.\"\n",
      "  },\n",
      "  \"id\": \"id:doc:doc::2\",\n",
      "  \"relevance\": 0.44831951343722665,\n",
      "  \"source\": \"voyageai_content\"\n",
      "}\n",
      "{\n",
      "  \"fields\": {\n",
      "    \"documentid\": \"id:doc:doc::1\",\n",
      "    \"id\": \"1\",\n",
      "    \"sddocname\": \"doc\",\n",
      "    \"summaryfeatures\": {\n",
      "      \"binary_closeness\": 0.58837890625,\n",
      "      \"float_closeness\": 0.34075708348829004,\n",
      "      \"vespa.summaryFeatures.cached\": 0.0\n",
      "    },\n",
      "    \"text\": \"Retrieval-augmented generation (RAG) combines a retrieval system with a generative language model. The retriever finds relevant passages from a corpus, and the generator uses them as context to produce accurate, grounded answers.\"\n",
      "  },\n",
      "  \"id\": \"id:doc:doc::1\",\n",
      "  \"relevance\": 0.34075708348829004,\n",
      "  \"source\": \"voyageai_content\"\n",
      "}\n",
      "{\n",
      "  \"fields\": {\n",
      "    \"documentid\": \"id:doc:doc::5\",\n",
      "    \"id\": \"5\",\n",
      "    \"sddocname\": \"doc\",\n",
      "    \"summaryfeatures\": {\n",
      "      \"binary_closeness\": 0.58154296875,\n",
      "      \"float_closeness\": 0.31555799518827143,\n",
      "      \"vespa.summaryFeatures.cached\": 0.0\n",
      "    },\n",
      "    \"text\": \"The Voyage 4 embedding model family includes voyage-4-large for maximum quality, voyage-4-lite for a balance of cost and quality, and voyage-4-nano as a small open-source model suitable for local deployment.\"\n",
      "  },\n",
      "  \"id\": \"id:doc:doc::5\",\n",
      "  \"relevance\": 0.31555799518827143,\n",
      "  \"source\": \"voyageai_content\"\n",
      "}\n",
      "{\n",
      "  \"fields\": {\n",
      "    \"documentid\": \"id:doc:doc::3\",\n",
      "    \"id\": \"3\",\n",
      "    \"sddocname\": \"doc\",\n",
      "    \"summaryfeatures\": {\n",
      "      \"binary_closeness\": 0.5703125,\n",
      "      \"float_closeness\": 0.29142659282264916,\n",
      "      \"vespa.summaryFeatures.cached\": 0.0\n",
      "    },\n",
      "    \"text\": \"Vespa is a fully featured search engine and vector database. It supports real-time indexing, structured and unstructured data, and advanced ranking with multiple retrieval and ranking phases.\"\n",
      "  },\n",
      "  \"id\": \"id:doc:doc::3\",\n",
      "  \"relevance\": 0.29142659282264916,\n",
      "  \"source\": \"voyageai_content\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from vespa.io import VespaQueryResponse\n",
    "import vespa.querybuilder as qb\n",
    "import json\n",
    "\n",
    "query_text = \"How does asymmetric embedding retrieval work?\"\n",
    "\n",
    "response: VespaQueryResponse = app.query(\n",
    "    yql=str(\n",
    "        qb.select(\"*\")\n",
    "        .from_(SCHEMA_NAME)\n",
    "        .where(\n",
    "            qb.nearestNeighbor(\n",
    "                field=\"embedding_binary\",\n",
    "                query_vector=\"q_bin\",\n",
    "                annotations={\"targetHits\": 100},\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    "    ranking=\"binary-with-rerank\",\n",
    "    body={\n",
    "        \"input.query(q_bin)\": f'embed({QUERY_MODEL_ID}, \"{query_text}\")',\n",
    "        \"input.query(q_float)\": f'embed({QUERY_MODEL_ID}, \"{query_text}\")',\n",
    "        \"hits\": 5,\n",
    "        \"presentation.timing\": \"true\",\n",
    "    },\n",
    ")\n",
    "assert response.is_successful()\n",
    "\n",
    "for hit in response.hits:\n",
    "    print(json.dumps(hit, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-explanation",
   "metadata": {},
   "source": [
    "The `summaryfeatures` in each hit show both scoring phases:\n",
    "\n",
    "- `binary_closeness`: The first-phase hamming-based score (fast, approximate).\n",
    "- `float_closeness`: The second-phase dot-product score between the query and document float embeddings. Since both embeddings are unit-normalized (`prenormalized-angular`), the dot product equals cosine similarity.\n",
    "\n",
    "The final `relevance` score is the second-phase float closeness for reranked candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8dd40fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'querytime': 0.032,\n",
       " 'searchtime': 0.051000000000000004,\n",
       " 'summaryfetchtime': 0.015}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json[\"timing\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-intro",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleanup",
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_cloud.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyvespa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}