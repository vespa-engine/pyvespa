{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "b3ae8a2b",
            "metadata": {},
            "source": [
                "<picture>\n",
                "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://assets.vespa.ai/logos/Vespa-logo-green-RGB.svg\">\n",
                "  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://assets.vespa.ai/logos/Vespa-logo-dark-RGB.svg\">\n",
                "  <img alt=\"#Vespa\" width=\"200\" src=\"https://assets.vespa.ai/logos/Vespa-logo-green-RGB.svg\" style=\"margin-bottom: 25px;\">\n",
                "</picture>\n",
                "\n",
                "# BGE-M3 - The Mother of all embedding models\n",
                "\n",
                "BAAI released BGE-M3 on January 30th, a new member of the BGE model series.\n",
                "\n",
                "> M3 stands for Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec (colbert) retrieval).\n",
                "\n",
                "This notebook demonstrates how to use the [BGE-M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3) embeddings and\n",
                "represent all three embedding representations in Vespa! Vespa is the only scalable serving engine that can handle all M3 representations.\n",
                "\n",
                "This code is inspired by the README from the model hub [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3).\n",
                "\n",
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/mother-of-all-embedding-models-cloud.ipynb)\n",
                "\n",
                "Let's get started! First, install dependencies:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4ffa3cbe",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip3 install -U pyvespa FlagEmbedding vespacli"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "33c8d886",
            "metadata": {},
            "source": [
                "### Explore the multiple representations of M3\n",
                "\n",
                "When encoding text, we can ask for the representations we want\n",
                "\n",
                "- Sparse vectors with weights for the token IDs (from the multilingual tokenization process)\n",
                "- Dense (DPR) regular text embeddings\n",
                "- Multi-Dense (ColBERT) - contextualized multi-token vectors\n",
                "\n",
                "Let us dive into it - To use this model on the CPU we set `use_fp16` to False, for GPU inference, it is recommended to use `use_fp16=True` for accelerated inference.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4776f0c7",
            "metadata": {},
            "outputs": [],
            "source": [
                "from FlagEmbedding import BGEM3FlagModel\n",
                "\n",
                "model = BGEM3FlagModel(\"BAAI/bge-m3\", use_fp16=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6f97c414",
            "metadata": {},
            "source": [
                "## A demo passage\n",
                "\n",
                "Let us encode a simple passage\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "06045105",
            "metadata": {},
            "outputs": [],
            "source": [
                "passage = [\n",
                "    \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n",
                "]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cdc307bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "passage_embeddings = model.encode(\n",
                "    passage, return_dense=True, return_sparse=True, return_colbert_vecs=True\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "e5aae8ce",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "dict_keys(['dense_vecs', 'lexical_weights', 'colbert_vecs'])"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "passage_embeddings.keys()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "da356d25",
            "metadata": {},
            "source": [
                "## Defining the Vespa application\n",
                "\n",
                "[PyVespa](https://pyvespa.readthedocs.io/en/latest/) helps us build the [Vespa application package](https://docs.vespa.ai/en/application-packages.html).\n",
                "A Vespa application package consists of configuration files, schemas, models, and code (plugins).\n",
                "\n",
                "First, we define a [Vespa schema](https://docs.vespa.ai/en/schemas.html) with the fields we want to store and their type. We\n",
                "use Vespa [tensors](https://docs.vespa.ai/en/tensor-user-guide.html) to represent the three different M3 representations.\n",
                "\n",
                "- We use a mapped tensor denoted by `t{}` to represent the sparse lexical representation\n",
                "- We use an indexed tensor denoted by `x[1024]` to represent the dense single vector representation of 1024 dimensions\n",
                "- For the colbert_rep (multi-vector), we use a mixed tensor that combines a mapped and an indexed dimension. This mixed tensor allows us to represent variable lengths.\n",
                "\n",
                "We use `bfloat16` tensor cell type, saving 50% storage compared to `float`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "0dca2378",
            "metadata": {},
            "outputs": [],
            "source": [
                "from vespa.package import Schema, Document, Field, FieldSet\n",
                "\n",
                "m_schema = Schema(\n",
                "    name=\"m\",\n",
                "    document=Document(\n",
                "        fields=[\n",
                "            Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n",
                "            Field(\n",
                "                name=\"text\",\n",
                "                type=\"string\",\n",
                "                indexing=[\"summary\", \"index\"],\n",
                "                index=\"enable-bm25\",\n",
                "            ),\n",
                "            Field(\n",
                "                name=\"lexical_rep\",\n",
                "                type=\"tensor<bfloat16>(t{})\",\n",
                "                indexing=[\"summary\", \"attribute\"],\n",
                "            ),\n",
                "            Field(\n",
                "                name=\"dense_rep\",\n",
                "                type=\"tensor<bfloat16>(x[1024])\",\n",
                "                indexing=[\"summary\", \"attribute\"],\n",
                "                attribute=[\"distance-metric: angular\"],\n",
                "            ),\n",
                "            Field(\n",
                "                name=\"colbert_rep\",\n",
                "                type=\"tensor<bfloat16>(t{}, x[1024])\",\n",
                "                indexing=[\"summary\", \"attribute\"],\n",
                "            ),\n",
                "        ],\n",
                "    ),\n",
                "    fieldsets=[FieldSet(name=\"default\", fields=[\"text\"])],\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2834fe25",
            "metadata": {},
            "source": [
                "The above defines our `m` schema with the original text and the three different representations\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "66c5da1d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from vespa.package import ApplicationPackage\n",
                "\n",
                "vespa_app_name = \"m\"\n",
                "vespa_application_package = ApplicationPackage(name=vespa_app_name, schema=[m_schema])"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7fe3d7bd",
            "metadata": {},
            "source": [
                "In the last step, we configure [ranking](https://docs.vespa.ai/en/ranking.html) by adding `rank-profile`'s to the schema.\n",
                "\n",
                "We define three functions that implement the three different scoring functions for the different representations\n",
                "\n",
                "- dense (dense cosine similarity)\n",
                "- sparse (sparse dot product)\n",
                "- max_sim (The colbert max sim operation)\n",
                "\n",
                "Then, we combine these three scoring functions using a linear combination with weights, as suggested\n",
                "by the authors [here](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#compute-score-for-text-pairs).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "a8ce5624",
            "metadata": {},
            "outputs": [],
            "source": [
                "from vespa.package import RankProfile, Function, FirstPhaseRanking\n",
                "\n",
                "\n",
                "semantic = RankProfile(\n",
                "    name=\"m3hybrid\",\n",
                "    inputs=[\n",
                "        (\"query(q_dense)\", \"tensor<bfloat16>(x[1024])\"),\n",
                "        (\"query(q_lexical)\", \"tensor<bfloat16>(t{})\"),\n",
                "        (\"query(q_colbert)\", \"tensor<bfloat16>(qt{}, x[1024])\"),\n",
                "        (\"query(q_len_colbert)\", \"float\"),\n",
                "    ],\n",
                "    functions=[\n",
                "        Function(\n",
                "            name=\"dense\",\n",
                "            expression=\"cosine_similarity(query(q_dense), attribute(dense_rep),x)\",\n",
                "        ),\n",
                "        Function(\n",
                "            name=\"lexical\", expression=\"sum(query(q_lexical) * attribute(lexical_rep))\"\n",
                "        ),\n",
                "        Function(\n",
                "            name=\"max_sim\",\n",
                "            expression=\"sum(reduce(sum(query(q_colbert) * attribute(colbert_rep) , x),max, t),qt)/query(q_len_colbert)\",\n",
                "        ),\n",
                "    ],\n",
                "    first_phase=FirstPhaseRanking(\n",
                "        expression=\"0.4*dense + 0.2*lexical +  0.4*max_sim\", rank_score_drop_limit=0.0\n",
                "    ),\n",
                "    match_features=[\"dense\", \"lexical\", \"max_sim\", \"bm25(text)\"],\n",
                ")\n",
                "m_schema.add_rank_profile(semantic)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ce78268c",
            "metadata": {},
            "source": [
                "The `m3hybrid` rank-profile above defines the query input embedding type and a similarities function that\n",
                "uses a Vespa [tensor compute function](https://docs.vespa.ai/en/reference/ranking-expressions.html#tensor-functions) that calculates\n",
                "the M3 similarities for dense, lexical, and the max_sim for the colbert representations.\n",
                "\n",
                "The profile only defines a single ranking phase, using a linear combination of multiple features using the suggested weighting.\n",
                "\n",
                "Using [match-features](https://docs.vespa.ai/en/reference/schema-reference.html#match-features), Vespa\n",
                "returns selected features along with the hit in the SERP (result page). We also include BM25. We can view BM25 as the fourth dimension.\n",
                "Especially for long-context retrieval, it can be helpful compared to the neural representations.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "846545f9",
            "metadata": {},
            "source": [
                "## Deploy the application to Vespa Cloud\n",
                "\n",
                "With the configured application, we can deploy it to [Vespa Cloud](https://cloud.vespa.ai/en/).\n",
                "\n",
                "To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:\n",
                "\n",
                "Create a tenant at [console.vespa-cloud.com](https://console.vespa-cloud.com/) (unless you already have one).\n",
                "This step requires a Google or GitHub account, and will start your [free trial](https://cloud.vespa.ai/en/free-trial).\n",
                "\n",
                "Make note of the tenant name, it is used in the next steps.\n",
                "\n",
                "> Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "b5fddf9f",
            "metadata": {},
            "outputs": [],
            "source": [
                "from vespa.deployment import VespaCloud\n",
                "import os\n",
                "\n",
                "# Replace with your tenant name from the Vespa Cloud Console\n",
                "tenant_name = \"vespa-team\"\n",
                "\n",
                "# Key is only used for CI/CD. Can be removed if logging in interactively\n",
                "key = os.getenv(\"VESPA_TEAM_API_KEY\", None)\n",
                "if key is not None:\n",
                "    key = key.replace(r\"\\n\", \"\\n\")  # To parse key correctly\n",
                "\n",
                "vespa_cloud = VespaCloud(\n",
                "    tenant=tenant_name,\n",
                "    application=vespa_app_name,\n",
                "    key_content=key,  # Key is only used for CI/CD. Can be removed if logging in interactively\n",
                "    application_package=vespa_application_package,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fa9baa5a",
            "metadata": {},
            "source": [
                "Now deploy the app to Vespa Cloud dev zone.\n",
                "\n",
                "The first deployment typically takes 2 minutes until the endpoint is up.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "fe954dc4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Deployment started in run 1 of dev-aws-us-east-1c for samples.m. This may take a few minutes the first time.\n",
                        "INFO    [22:13:09]  Deploying platform version 8.299.14 and application dev build 1 for dev-aws-us-east-1c of default ...\n",
                        "INFO    [22:13:10]  Using CA signed certificate version 0\n",
                        "INFO    [22:13:10]  Using 1 nodes in container cluster 'm_container'\n",
                        "INFO    [22:13:14]  Session 939 for tenant 'samples' prepared and activated.\n",
                        "INFO    [22:13:17]  ######## Details for all nodes ########\n",
                        "INFO    [22:13:31]  h88976d.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\n",
                        "INFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 <-- :\n",
                        "INFO    [22:13:31]  --- container-clustercontroller on port 19050 has not started \n",
                        "INFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \n",
                        "INFO    [22:13:31]  h89388b.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\n",
                        "INFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 <-- :\n",
                        "INFO    [22:13:31]  --- storagenode on port 19102 has not started \n",
                        "INFO    [22:13:31]  --- searchnode on port 19107 has not started \n",
                        "INFO    [22:13:31]  --- distributor on port 19111 has not started \n",
                        "INFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \n",
                        "INFO    [22:13:31]  h90001a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\n",
                        "INFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 <-- :\n",
                        "INFO    [22:13:31]  --- logserver-container on port 4080 has not started \n",
                        "INFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \n",
                        "INFO    [22:13:31]  h90550a.dev.aws-us-east-1c.vespa-external.aws.oath.cloud: expected to be UP\n",
                        "INFO    [22:13:31]  --- platform vespa/cloud-tenant-rhel8:8.299.14 <-- :\n",
                        "INFO    [22:13:31]  --- container on port 4080 has not started \n",
                        "INFO    [22:13:31]  --- metricsproxy-container on port 19092 has not started \n",
                        "INFO    [22:14:31]  Found endpoints:\n",
                        "INFO    [22:14:31]  - dev.aws-us-east-1c\n",
                        "INFO    [22:14:31]   |-- https://d29bf3e7.f064e220.z.vespa-app.cloud/ (cluster 'm_container')\n",
                        "INFO    [22:14:32]  Installation succeeded!\n",
                        "Using mTLS (key,cert) Authentication against endpoint https://d29bf3e7.f064e220.z.vespa-app.cloud//ApplicationStatus\n",
                        "Application is up!\n",
                        "Finished deployment.\n"
                    ]
                }
            ],
            "source": [
                "from vespa.application import Vespa\n",
                "\n",
                "app: Vespa = vespa_cloud.deploy()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b7d54bd1",
            "metadata": {},
            "source": [
                "## Feed the M3 representations\n",
                "\n",
                "We convert the three different representations to Vespa feed format\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "1674c66e",
            "metadata": {},
            "outputs": [],
            "source": [
                "vespa_fields = {\n",
                "    \"text\": passage[0],\n",
                "    \"lexical_rep\": {\n",
                "        key: float(value)\n",
                "        for key, value in passage_embeddings[\"lexical_weights\"][0].items()\n",
                "    },\n",
                "    \"dense_rep\": passage_embeddings[\"dense_vecs\"][0].tolist(),\n",
                "    \"colbert_rep\": {\n",
                "        index: passage_embeddings[\"colbert_vecs\"][0][index].tolist()\n",
                "        for index in range(passage_embeddings[\"colbert_vecs\"][0].shape[0])\n",
                "    },\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "e0d36138",
            "metadata": {},
            "outputs": [],
            "source": [
                "from vespa.io import VespaResponse\n",
                "\n",
                "response: VespaResponse = app.feed_data_point(\n",
                "    schema=\"m\", data_id=0, fields=vespa_fields\n",
                ")\n",
                "assert response.is_successful()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "20b007ec",
            "metadata": {},
            "source": [
                "### Querying data\n",
                "\n",
                "Now, we can also query our data.\n",
                "\n",
                "Read more about querying Vespa in:\n",
                "\n",
                "- [Vespa Query API](https://docs.vespa.ai/en/query-api.html)\n",
                "- [Vespa Query API reference](https://docs.vespa.ai/en/reference/query-api-reference.html)\n",
                "- [Vespa Query Language API (YQL)](https://docs.vespa.ai/en/query-language.html)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "810b99d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "query = [\"What is BGE M3?\"]\n",
                "query_embeddings = model.encode(\n",
                "    query, return_dense=True, return_sparse=True, return_colbert_vecs=True\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3533ecc3",
            "metadata": {},
            "source": [
                "The M3 colbert scoring function needs the query length to normalize the score to the range 0 to 1. This helps when combining\n",
                "the score with the other scoring functions.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "4d1c75ff",
            "metadata": {},
            "outputs": [],
            "source": [
                "query_length = query_embeddings[\"colbert_vecs\"][0].shape[0]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "4dc09c4f",
            "metadata": {},
            "outputs": [],
            "source": [
                "query_fields = {\n",
                "    \"input.query(q_lexical)\": {\n",
                "        key: float(value)\n",
                "        for key, value in query_embeddings[\"lexical_weights\"][0].items()\n",
                "    },\n",
                "    \"input.query(q_dense)\": query_embeddings[\"dense_vecs\"][0].tolist(),\n",
                "    \"input.query(q_colbert)\": str(\n",
                "        {\n",
                "            index: query_embeddings[\"colbert_vecs\"][0][index].tolist()\n",
                "            for index in range(query_embeddings[\"colbert_vecs\"][0].shape[0])\n",
                "        }\n",
                "    ),\n",
                "    \"input.query(q_len_colbert)\": query_length,\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "b9349fb4",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "{\n",
                        "  \"id\": \"index:m_content/0/cfcd2084234135f700f08abf\",\n",
                        "  \"relevance\": 0.5993361056332731,\n",
                        "  \"source\": \"m_content\",\n",
                        "  \"fields\": {\n",
                        "    \"matchfeatures\": {\n",
                        "      \"bm25(text)\": 0.8630462173553426,\n",
                        "      \"dense\": 0.6258970723760484,\n",
                        "      \"lexical\": 0.1941967010498047,\n",
                        "      \"max_sim\": 0.7753448411822319\n",
                        "    },\n",
                        "    \"text\": \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n",
                        "  }\n",
                        "}\n"
                    ]
                }
            ],
            "source": [
                "from vespa.io import VespaQueryResponse\n",
                "import json\n",
                "\n",
                "response: VespaQueryResponse = app.query(\n",
                "    yql=\"select id, text from m where userQuery() or ({targetHits:10}nearestNeighbor(dense_rep,q_dense))\",\n",
                "    ranking=\"m3hybrid\",\n",
                "    query=query[0],\n",
                "    body={**query_fields},\n",
                ")\n",
                "assert response.is_successful()\n",
                "print(json.dumps(response.hits[0], indent=2))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4d3ca1da",
            "metadata": {},
            "source": [
                "Notice the `matchfeatures` that returns the configured match-features from the rank-profile. We can\n",
                "use these to compare the torch model scoring with the computations specified in Vespa.\n",
                "\n",
                "Now, we can compare the Vespa computed scores with the model torch code and they line up perfectly\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "06f7b1c7",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.19554455392062664"
                        ]
                    },
                    "execution_count": 22,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.compute_lexical_matching_score(\n",
                "    passage_embeddings[\"lexical_weights\"][0], query_embeddings[\"lexical_weights\"][0]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "id": "c9af5c2f",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "0.6259037"
                        ]
                    },
                    "execution_count": 23,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "query_embeddings[\"dense_vecs\"][0] @ passage_embeddings[\"dense_vecs\"][0].T"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "id": "1bcf591c",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "tensor(0.7797)"
                        ]
                    },
                    "execution_count": 24,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model.colbert_score(\n",
                "    query_embeddings[\"colbert_vecs\"][0], passage_embeddings[\"colbert_vecs\"][0]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "341dd861",
            "metadata": {},
            "source": [
                "### That is it!\n",
                "\n",
                "That is how easy it is to represent the brand new M3 FlagEmbedding representations in Vespa! Read more in the\n",
                "[M3 technical report](https://github.com/FlagOpen/FlagEmbedding/blob/master/FlagEmbedding/BGE_M3/BGE_M3.pdf).\n",
                "\n",
                "We can go ahead and delete the Vespa cloud instance we deployed by:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af4b9ead",
            "metadata": {},
            "outputs": [],
            "source": [
                "vespa_cloud.delete()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.5"
        },
        "vscode": {
            "interpreter": {
                "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}