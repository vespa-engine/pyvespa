{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ae8a2b",
   "metadata": {},
   "source": [
    "<picture>\n",
    "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://vespa.ai/assets/vespa-ai-logo-heather.svg\">\n",
    "  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://vespa.ai/assets/vespa-ai-logo-rock.svg\">\n",
    "  <img alt=\"#Vespa\" width=\"200\" src=\"https://vespa.ai/assets/vespa-ai-logo-rock.svg\" style=\"margin-bottom: 25px;\">\n",
    "</picture>\n",
    "\n",
    "\n",
    "# BGE-M3 - The Mother of all embedding models\n",
    "\n",
    "BAAI released BGE-M3 on January 30th, a new member of the BGE model series. \n",
    "\n",
    "> M3 stands for Multi-linguality (100+ languages), Multi-granularities (input length up to 8192), Multi-Functionality (unification of dense, lexical, multi-vec (colbert) retrieval).\n",
    "\n",
    "This notebook demonstrates how to use [BGE_M3](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3) embeddings and \n",
    "represent all three representations in Vespa! The only vector database that can handle all three representations!\n",
    "\n",
    "This code is inspired by the README from the model hub [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3).\n",
    "\n",
    "\n",
    "Let's get started! First, install dependencies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U pyvespa FlagEmbedding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8d886",
   "metadata": {},
   "source": [
    "### Explore the multiple representations of M3\n",
    "When encoding text, we can ask for the representations we want\n",
    "\n",
    "- Sparse (SPLADE) vectors \n",
    "- Dense (DPR) regular text embeddings \n",
    "- Multi-Dense (ColBERT) - contextualized multi-token vectors \n",
    "\n",
    "Let us dive int - To use this model on CPU, we set `use_fp16` to False, for GPU inference, it is recommended to use `use_fp16=True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4776f0c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0a9f2613914d83b8b5c019d87de662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading existing colbert_linear and sparse_linear---------\n"
     ]
    }
   ],
   "source": [
    "from FlagEmbedding import BGEM3FlagModel\n",
    "\n",
    "model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97c414",
   "metadata": {},
   "source": [
    "## A demo passage \n",
    "\n",
    "Let us encode a simple passage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "06045105",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "passage = [\"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc307bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "passage_embeddings = model.encode(passage, return_dense=True, return_sparse=True, return_colbert_vecs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da356d25",
   "metadata": {},
   "source": [
    "## Defining the Vespa application\n",
    "[PyVespa](https://pyvespa.readthedocs.io/en/latest/) helps us build the [Vespa application package](https://docs.vespa.ai/en/application-packages.html). \n",
    "A Vespa application package consists of configuration files, schemas, models, and code (plugins).   \n",
    "\n",
    "First, we define a [Vespa schema](https://docs.vespa.ai/en/schemas.html) with the fields we want to store and their type. We\n",
    "use Vespa [tensors](https://docs.vespa.ai/en/tensor-user-guide.html) to represent the 3 different M3 representations. \n",
    "\n",
    "- We use a mapped tensor denoted by `t{}` to represent the sparse lexical representation \n",
    "- We use an indexed tensor denoted by `x[1024]` to represent the dense single vector representation of 1024 dimensions\n",
    "- For the colbert_rep (multi vector), we use a mixed tensor that combines a mapped and an indexed dimension. \n",
    "\n",
    "To save resource footprint, we use `bfloat16` tensor cell type, this saves 50% storage compared to `float`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0dca2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import Schema, Document, Field, FieldSet\n",
    "m_schema = Schema(\n",
    "            name=\"m\",\n",
    "            document=Document(\n",
    "                fields=[\n",
    "                    Field(name=\"id\", type=\"string\", indexing=[\"summary\"]),\n",
    "                    Field(name=\"text\", type=\"string\", indexing=[\"summary\", \"index\"]),\n",
    "                    Field(name=\"lexical_rep\", type=\"tensor<bfloat16>(t{})\", indexing=[\"summary\", \"attribute\"]),\n",
    "                    Field(name=\"dense_rep\", type=\"tensor<bfloat16>(x[1024])\", indexing=[\"summary\", \"attribute\"], attribute=[\"distance-metric: angular\"]),\n",
    "                    Field(name=\"colbert_rep\", type=\"tensor<bfloat16>(t{}, x[1024])\", indexing=[\"summary\", \"attribute\"])\n",
    "                ],\n",
    "            ),\n",
    "            fieldsets=[\n",
    "                FieldSet(name = \"default\", fields = [\"text\"])\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2834fe25",
   "metadata": {},
   "source": [
    "The above defines our `m` schema with the original text and the three different representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66c5da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import ApplicationPackage\n",
    "\n",
    "vespa_app_name = \"m\"\n",
    "vespa_application_package = ApplicationPackage(\n",
    "        name=vespa_app_name,\n",
    "        schema=[m_schema]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe3d7bd",
   "metadata": {},
   "source": [
    "In the last step, we configure [ranking](https://docs.vespa.ai/en/ranking.html) by adding `rank-profile`'s to the schema. \n",
    "\n",
    "\n",
    "We define three functions that implements the 3 different scoring functions for the different representations\n",
    "\n",
    "- dense (dense cosine similarity)\n",
    "- sparse (sparse dot product)\n",
    "- max_sim (The colbert max sim operation)\n",
    "\n",
    "Then we combine these three scoring functions using a linear combination with weights as suggested\n",
    "[here](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3#compute-score-for-text-pairs). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a8ce5624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import RankProfile, Function,  FirstPhaseRanking\n",
    "\n",
    "\n",
    "semantic = RankProfile(\n",
    "    name=\"m3hybrid\", \n",
    "    inputs=[\n",
    "        (\"query(q_dense)\", \"tensor<bfloat16>(x[1024])\"), \n",
    "        (\"query(q_lexical)\", \"tensor<bfloat16>(t{})\"), \n",
    "        (\"query(q_colbert)\", \"tensor<bfloat16>(qt{}, x[1024])\"),\n",
    "        (\"query(q_len_colbert)\", \"float\"),\n",
    "    ],\n",
    "    functions=[\n",
    "        Function(\n",
    "            name=\"dense\",\n",
    "            expression=\"cosine_similarity(query(q_dense), attribute(dense_rep),x)\"\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"lexical\",\n",
    "            expression=\"sum(query(q_lexical) * attribute(lexical_rep))\"\n",
    "        ),\n",
    "        Function(\n",
    "            name=\"max_sim\",\n",
    "            expression=\"sum(reduce(sum(query(q_colbert) * attribute(colbert_rep) , x),max, t),qt)/query(q_len_colbert)\"\n",
    "        )\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(\n",
    "        expression=\"0.4*dense + 0.2*lexical +  0.4*max_sim\",\n",
    "        rank_score_drop_limit=0.0\n",
    "    ),\n",
    "    match_features=[\"dense\", \"lexical\", \"max_sim\"]\n",
    ")\n",
    "m_schema.add_rank_profile(semantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce78268c",
   "metadata": {},
   "source": [
    "The `m3hybrid` rank-profile above defines the query input embedding type and a similarities function that\n",
    "uses a Vespa [tensor compute function](https://docs.vespa.ai/en/reference/ranking-expressions.html#tensor-functions) that calculates\n",
    "the M3 similarities for dense, lexical and max_sim for colbert. \n",
    "\n",
    "The profile only defines a single ranking phase, using a linear combination of multiple features using the suggested weighting. \n",
    "\n",
    "Using [match-features](https://docs.vespa.ai/en/reference/schema-reference.html#match-features), Vespa\n",
    "returns selected features along with the hit in the SERP (result page)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846545f9",
   "metadata": {},
   "source": [
    "## Deploy the application to Vespa Cloud\n",
    "\n",
    "With the configured application, we can deploy it to [Vespa Cloud](https://cloud.vespa.ai/en/). \n",
    "It is also possible to deploy the app using docker; see the [Hybrid Search - Quickstart](https://pyvespa.readthedocs.io/en/latest/getting-started-pyvespa.html) guide for\n",
    "an example of deploying it to a local docker container. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16179d9b",
   "metadata": {},
   "source": [
    "Install the Vespa CLI using [homebrew](https://brew.sh/) - or download a binary from GitHub as demonstrated below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343981ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install vespa-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863d0700",
   "metadata": {},
   "source": [
    "Alternatively, if running in Colab, download the Vespa CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5670bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "res = requests.get(url=\"https://api.github.com/repos/vespa-engine/vespa/releases/latest\").json()\n",
    "os.environ[\"VERSION\"] = res[\"tag_name\"].replace(\"v\", \"\")\n",
    "!curl -fsSL https://github.com/vespa-engine/vespa/releases/download/v${VERSION}/vespa-cli_${VERSION}_linux_amd64.tar.gz | tar -zxf -\n",
    "!ln -sf /content/vespa-cli_${VERSION}_linux_amd64/bin/vespa /bin/vespa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff00727",
   "metadata": {},
   "source": [
    "To deploy the application to Vespa Cloud we need to create a tenant in the Vespa Cloud:\n",
    "\n",
    "Create a tenant at [console.vespa-cloud.com](https://console.vespa-cloud.com/) (unless you already have one). \n",
    "This step requires a Google or GitHub account, and will start your [free trial](https://cloud.vespa.ai/en/free-trial). \n",
    "Make note of the tenant name, it is used in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9f9a1c",
   "metadata": {},
   "source": [
    "### Configure Vespa Cloud date-plane security\n",
    "\n",
    "Create Vespa Cloud data-plane mTLS cert/key-pair. The mutual certificate pair is used to talk to your Vespa cloud endpoints. See [Vespa Cloud Security Guide](https://cloud.vespa.ai/en/security/guide) for details.\n",
    "\n",
    "We save the paths to the credentials for later data-plane access without using pyvespa APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a766d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TENANT_NAME\"] = \"vespa-team\" # Replace with your tenant name\n",
    "\n",
    "vespa_cli_command = f'vespa config set application {os.environ[\"TENANT_NAME\"]}.{vespa_app_name}'\n",
    "\n",
    "!vespa config set target cloud\n",
    "!{vespa_cli_command}\n",
    "!vespa auth cert -N "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228381b",
   "metadata": {},
   "source": [
    "Validate that we have the expected data-plane credential files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1f0b97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "\n",
    "cert_path = Path.home() / \".vespa\" / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-public-cert.pem\"\n",
    "key_path = Path.home() / \".vespa\" / f\"{os.environ['TENANT_NAME']}.{vespa_app_name}.default/data-plane-private-key.pem\"\n",
    "\n",
    "if not exists(cert_path) or not exists(key_path):\n",
    "    print(\"ERROR: set the correct paths to security credentials. Correct paths above and rerun until you do not see this error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ce80e0",
   "metadata": {},
   "source": [
    "Note that the subsequent Vespa Cloud deploy call below will add `data-plane-public-cert.pem` to the application before deploying it to Vespa Cloud, so that\n",
    "you have access to both the private key and the public certificate. At the same time, Vespa Cloud only knows the public certificate. \n",
    "\n",
    "### Configure Vespa Cloud control-plane security \n",
    "\n",
    "Authenticate to generate a tenant level control plane API key for deploying the applications to Vespa Cloud, and save the path to it. \n",
    "\n",
    "The generated tenant api key must be added in the Vespa Console before attemting to deploy the application. \n",
    "\n",
    "```\n",
    "To use this key in Vespa Cloud click 'Add custom key' at\n",
    "https://console.vespa-cloud.com/tenant/TENANT_NAME/account/keys\n",
    "and paste the entire public key including the BEGIN and END lines.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf8731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!vespa auth api-key\n",
    "\n",
    "from pathlib import Path\n",
    "api_key_path = Path.home() / \".vespa\" / f\"{os.environ['TENANT_NAME']}.api-key.pem\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db1010",
   "metadata": {},
   "source": [
    "### Deploy to Vespa Cloud\n",
    "\n",
    "Now that we have data-plane and control-plane credentials ready, we can deploy our application to Vespa Cloud! \n",
    "\n",
    "`PyVespa` supports deploying apps to the [development zone](https://cloud.vespa.ai/en/reference/environments#dev-and-perf).\n",
    "\n",
    ">Note: Deployments to dev and perf expire after 7 days of inactivity, i.e., 7 days after running deploy. This applies to all plans, not only the Free Trial. Use the Vespa Console to extend the expiry period, or redeploy the application to add 7 more days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b5fddf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.deployment import VespaCloud\n",
    "\n",
    "def read_secret():\n",
    "    \"\"\"Read the API key from the environment variable. This is \n",
    "    only used for CI/CD purposes.\"\"\"\n",
    "    t = os.getenv(\"VESPA_TEAM_API_KEY\")\n",
    "    if t:\n",
    "        return t.replace(r\"\\n\", \"\\n\")\n",
    "    else:\n",
    "        return t\n",
    "\n",
    "vespa_cloud = VespaCloud(\n",
    "    tenant=os.environ[\"TENANT_NAME\"],\n",
    "    application=vespa_app_name,\n",
    "    key_content=read_secret() if read_secret() else None,\n",
    "    key_location=api_key_path,\n",
    "    application_package=vespa_application_package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9baa5a",
   "metadata": {},
   "source": [
    "Now deploy the app to Vespa Cloud dev zone. \n",
    "\n",
    "The first deployment typically takes 2 minutes until the endpoint is up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe954dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.application import Vespa\n",
    "app:Vespa = vespa_cloud.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d54bd1",
   "metadata": {},
   "source": [
    "# Feed the M3 representations\n",
    "\n",
    "We convert the three different representations to Vespa feed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1674c66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_fields = {\n",
    "    \"text\": passage[0],\n",
    "    \"lexical_rep\": {key: float(value) for key, value in passage_embeddings['lexical_weights'][0].items()},\n",
    "    \"dense_rep\":passage_embeddings['dense_vecs'][0].tolist(),\n",
    "    \"colbert_rep\":  {index: passage_embeddings['colbert_vecs'][0][index].tolist() for index in range(passage_embeddings['colbert_vecs'][0].shape[0])}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d36138",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.feed_data_point(schema='m', data_id=0, fields=vespa_fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b007ec",
   "metadata": {},
   "source": [
    "### Querying data\n",
    "\n",
    "Now, we can also query our data. \n",
    "\n",
    "Read more about querying Vespa in:\n",
    "\n",
    "- [Vespa Query API](https://docs.vespa.ai/en/query-api.html)\n",
    "- [Vespa Query API reference](https://docs.vespa.ai/en/reference/query-api-reference.html)\n",
    "- [Vespa Query Language API (YQL)](https://docs.vespa.ai/en/query-language.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810b99d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query  = [\"What is BGE M3?\"]\n",
    "query_embeddings = model.encode(query, return_dense=True, return_sparse=True, return_colbert_vecs=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533ecc3",
   "metadata": {},
   "source": [
    "The M3 colbert scoring function needs the query length to normalize the score to the range 0 to 1. This helps when combining\n",
    "the score with the other scoring functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4d1c75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_length = query_embeddings['colbert_vecs'][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4dc09c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_fields = {\n",
    "    \"input.query(q_lexical)\": {key: float(value) for key, value in query_embeddings['lexical_weights'][0].items()},\n",
    "    \"input.query(q_dense)\": query_embeddings['dense_vecs'][0].tolist(),\n",
    "    \"input.query(q_colbert)\":  str({index: query_embeddings['colbert_vecs'][0][index].tolist() for index in range(query_embeddings['colbert_vecs'][0].shape[0])}),\n",
    "    \"input.query(q_len_colbert)\": query_length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b9349fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": \"index:m_content/0/cfcd2084234135f700f08abf\",\n",
      "  \"relevance\": 0.5993392548000422,\n",
      "  \"source\": \"m_content\",\n",
      "  \"fields\": {\n",
      "    \"matchfeatures\": {\n",
      "      \"dense\": 0.6259025238542771,\n",
      "      \"lexical\": 0.1941967010498047,\n",
      "      \"max_sim\": 0.7753472626209259\n",
      "    },\n",
      "    \"text\": \"BGE M3 is an embedding model supporting dense retrieval, lexical matching and multi-vector interaction.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from vespa.io import VespaQueryResponse\n",
    "import json\n",
    "\n",
    "response:VespaQueryResponse = app.query(\n",
    "    yql=\"select id, text from m where ({targetHits:10}nearestNeighbor(dense_rep,q_dense))\",\n",
    "    ranking=\"m3hybrid\",\n",
    "    body={\n",
    "        **query_fields\n",
    "    }\n",
    ")\n",
    "assert(response.is_successful())\n",
    "print(json.dumps(response.hits[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3ca1da",
   "metadata": {},
   "source": [
    "Notice the `matchfeatures` that returns the configured match-features from the rank-profile. We can \n",
    "use these to compare the torch model scoring with the computations specified in Vespa. \n",
    "\n",
    "Now, we can compare the Vespa computed scores with the model torch code and they line up perfectly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "06f7b1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19554464519023895"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_lexical_matching_score(passage_embeddings['lexical_weights'][0], query_embeddings['lexical_weights'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c9af5c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6259037"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embeddings['dense_vecs'][0] @ passage_embeddings['dense_vecs'][0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1bcf591c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7797)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.colbert_score(query_embeddings['colbert_vecs'][0],passage_embeddings['colbert_vecs'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341dd861",
   "metadata": {},
   "source": [
    "That's it! That is how easy it is to represent M3 FlagEmbedding in Vespa, and use all three different representations that M3 FlagEmbedding offers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
