{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be90cc12",
   "metadata": {
    "id": "be90cc12"
   },
   "source": [
    "<picture>\n",
    "  <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://assets.vespa.ai/logos/Vespa-logo-green-RGB.svg\">\n",
    "  <source media=\"(prefers-color-scheme: light)\" srcset=\"https://assets.vespa.ai/logos/Vespa-logo-dark-RGB.svg\">\n",
    "  <img alt=\"#Vespa\" width=\"200\" src=\"https://assets.vespa.ai/logos/Vespa-logo-dark-RGB.svg\" style=\"margin-bottom: 25px;\">\n",
    "</picture>\n",
    "\n",
    "# RAG Blueprint tutorial\n",
    "\n",
    "Many of our users use Vespa to power large scale RAG Applications.\n",
    "\n",
    "This blueprint aims to exemplify many of the best practices we have learned while supporting these users.\n",
    "\n",
    "While many RAG tutorials exist, this blueprint provides a customizable template that:\n",
    "\n",
    "* Can [(auto)scale](https://docs.vespa.ai/en/cloud/autoscaling.html) with your data size and/or query load.\n",
    "* Is fast and [production grade](https://docs.vespa.ai/en/cloud/production-deployment.html).\n",
    "* Enables you to build RAG applications with state-of-the-art quality.\n",
    "\n",
    "This tutorial will show how we can develop a _high-quality_ RAG application with an evaluation-driven mindset, while being a resource you can revisit for making informed choices for your own use case.\n",
    "\n",
    "We will guide you through the following steps:\n",
    "\n",
    "1.  [Installing dependencies](#installing-dependencies)\n",
    "2.  [Cloning the RAG Blueprint](#cloning-the-rag-blueprint)\n",
    "3.  [Inspecting the RAG Blueprint](#inspecting-the-rag-blueprint)\n",
    "4.  [Deploying to Vespa Cloud](#deploying-to-vespa-cloud)\n",
    "5.  [Our use case](#our-use-case)\n",
    "6.  [Data modeling](#data-modeling)\n",
    "7.  [Structuring your Vespa application](#structuring-your-vespa-application)\n",
    "8.  [Configuring match-phase (retrieval)](#configuring-match-phase-retrieval)\n",
    "9.  [First-phase ranking](#first-phase-ranking)\n",
    "10. [Second-phase ranking](#second-phase-ranking)\n",
    "11. [(Optional) Global-phase reranking](#optional-global-phase-reranking)\n",
    "\n",
    "All the accompanying code can be found in our [sample app](https://github.com/vespa-engine/sample-apps/tree/master/rag-blueprint) repo, but we will also clone the repo and run the code in this notebook. \n",
    "\n",
    "Some of the python scripts from the sample app will be adapted and shown inline in this notebook instead of running them separately.\n",
    "\n",
    "Each step will contain reasoning behind the choices and design of the blueprint, as well as pointers for customizing to your own application.\n",
    "\n",
    "This is not a **'Deploy RAG in 5 minutes'** tutorial (although you _can_ technically do that by just running the notebook). This focus is more about providing you with the insights and tools for you to apply it to your own use case. Therefore we suggest taking your time to look at the code in the sample app, and run the described steps.\"\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vespa-engine/pyvespa/blob/master/docs/sphinx/source/examples/rag-blueprint-vespa-cloud.ipynb)\n",
    "\n",
    "\n",
    "Here is an overview of the retrieval and ranking pipeline we will build in this tutorial:\n",
    "\n",
    "<img\n",
    "  src=\"../_static/rag-blueprint-overview.svg\"\n",
    "  alt=\"RAG Blueprint overview\"\n",
    "  style=\"\n",
    "    width: 100%;\n",
    "    height: auto;        /* keeps aspect ratio */\n",
    "    object-fit: contain; /* show whole drawing */\n",
    "    max-height: 90vh;    /* never taller than viewport */\n",
    "    max-width: 600px;    /* cap on large screens        */\n",
    "  \">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4030ed",
   "metadata": {},
   "source": [
    "## Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fe72e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: 0.58.0 not found\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyvespa>=0.58.0 vespacli scikit-learn lightgbm pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a05ac",
   "metadata": {},
   "source": [
    "## Cloning the RAG Blueprint\n",
    "\n",
    "Although you _could_ define all components of the application with python code only from pyvespa, this would go against our advise on  or the [Advanced Configuration](https://vespa-engine.github.io/pyvespa/advanced-configuration.html) notebook for a guide if you want to do that.\n",
    "\n",
    "Here, we will use pyvespa to deploy an application package from the existing files.\n",
    "Let us start by cloning the RAG Blueprint application from the [Vespa sample-apps repository](https://github.com/vespa-engine/sample-apps/tree/master/rag-blueprint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039939b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'src' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Clone the RAG Blueprint sample application\n",
    "!git clone --depth 1 --filter=blob:none --sparse https://github.com/vespa-engine/sample-apps.git src && cd src && git sparse-checkout set rag-blueprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e66bb",
   "metadata": {
    "id": "d08e66bb"
   },
   "source": [
    "## Inspecting the RAG Blueprint\n",
    "\n",
    "First, let's examine the structure of the RAG Blueprint application we just cloned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ede02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def tree(\n",
    "    root: str | Path = \".\", *, show_hidden: bool = False, max_depth: int | None = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Return a Unix‐style 'tree' listing for *root*.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str | Path\n",
    "        Directory to walk (default: \".\")\n",
    "    show_hidden : bool\n",
    "        Include dotfiles and dot-dirs? (default: False)\n",
    "    max_depth : int | None\n",
    "        Limit recursion depth; None = no limit.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        A newline-joined string identical to `tree` output.\n",
    "    \"\"\"\n",
    "    root_path = Path(root).resolve()\n",
    "    lines = [root_path.as_posix()]\n",
    "\n",
    "    def _walk(current: Path, prefix: str = \"\", depth: int = 0) -> None:\n",
    "        if max_depth is not None and depth >= max_depth:\n",
    "            return\n",
    "\n",
    "        entries = sorted(\n",
    "            (e for e in current.iterdir() if show_hidden or not e.name.startswith(\".\")),\n",
    "            key=lambda p: (not p.is_dir(), p.name.lower()),\n",
    "        )\n",
    "        last = len(entries) - 1\n",
    "\n",
    "        for idx, entry in enumerate(entries):\n",
    "            connector = \"└── \" if idx == last else \"├── \"\n",
    "            lines.append(f\"{prefix}{connector}{entry.name}\")\n",
    "            if entry.is_dir():\n",
    "                extension = \"    \" if idx == last else \"│   \"\n",
    "                _walk(entry, prefix + extension, depth + 1)\n",
    "\n",
    "    _walk(root_path)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b58f243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thomas/Repos/pyvespa/docs/sphinx/source/examples/src/rag-blueprint\n",
      "├── app\n",
      "│   ├── models\n",
      "│   │   └── lightgbm_model.json\n",
      "│   ├── schemas\n",
      "│   │   ├── doc\n",
      "│   │   │   ├── base-features.profile\n",
      "│   │   │   ├── collect-second-phase.profile\n",
      "│   │   │   ├── collect-training-data.profile\n",
      "│   │   │   ├── learned-linear.profile\n",
      "│   │   │   ├── match-only.profile\n",
      "│   │   │   └── second-with-gbdt.profile\n",
      "│   │   └── doc.sd\n",
      "│   ├── search\n",
      "│   │   └── query-profiles\n",
      "│   │       ├── deepresearch-with-gbdt.xml\n",
      "│   │       ├── deepresearch.xml\n",
      "│   │       ├── hybrid-with-gbdt.xml\n",
      "│   │       ├── hybrid.xml\n",
      "│   │       ├── rag-with-gbdt.xml\n",
      "│   │       └── rag.xml\n",
      "│   ├── security\n",
      "│   │   └── clients.pem\n",
      "│   └── services.xml\n",
      "├── dataset\n",
      "│   ├── docs.jsonl\n",
      "│   ├── queries.json\n",
      "│   └── test_queries.json\n",
      "├── eval\n",
      "│   ├── output\n",
      "│   │   ├── Vespa-training-data_match_first_phase_20250623_133241.csv\n",
      "│   │   ├── Vespa-training-data_match_first_phase_20250623_133241_logreg_coefficients.txt\n",
      "│   │   ├── Vespa-training-data_match_rank_second_phase_20250623_135819.csv\n",
      "│   │   └── Vespa-training-data_match_rank_second_phase_20250623_135819_feature_importance.csv\n",
      "│   ├── collect_pyvespa.py\n",
      "│   ├── evaluate_match_phase.py\n",
      "│   ├── evaluate_ranking.py\n",
      "│   ├── pyproject.toml\n",
      "│   ├── README.md\n",
      "│   ├── resp.json\n",
      "│   ├── train_lightgbm.py\n",
      "│   └── train_logistic_regression.py\n",
      "├── deploy-locally.md\n",
      "├── generation.md\n",
      "├── query-profiles.md\n",
      "├── README.md\n",
      "└── relevance.md\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the RAG Blueprint application structure\n",
    "print(tree(\"src/rag-blueprint\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cf261",
   "metadata": {},
   "source": [
    "We can see that the RAG Blueprint includes a complete application package with:\n",
    "- `schemas/doc.sd` - The document schema with chunking and embeddings\n",
    "- `schemas/doc/*.profile` - Ranking profiles for collecting training data, first-phase ranking, and second-phase ranking\n",
    "- `services.xml` - Services configuration with embedder and LLM integration  \n",
    "- `search/query-profiles/*.xml` - Pre-configured query profiles for different use cases\n",
    "- `models/` - Pre-trained ranking models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba05ffd",
   "metadata": {},
   "source": [
    "## Deploying to Vespa Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9a48f",
   "metadata": {
    "id": "2ad9a48f"
   },
   "source": [
    "### Create a free trial\n",
    "\n",
    "Create a tenant from [here](https://vespa.ai/free-trial/).\n",
    "The trial includes $300 credit.\n",
    "Take note of your tenant name, and input it below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca1acca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.deployment import VespaCloud\n",
    "from vespa.application import Vespa\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38c0ceaf",
   "metadata": {
    "id": "38c0ceaf"
   },
   "outputs": [],
   "source": [
    "VESPA_TENANT_NAME = \"vespa-team\"  # Replace with your tenant name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d79b10",
   "metadata": {
    "id": "87d79b10"
   },
   "source": [
    "Here, set your desired application name. (Will be created in later steps)\n",
    "Note that you can not have hyphen `-` or underscore `_` in the application name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab7001fd",
   "metadata": {
    "id": "ab7001fd"
   },
   "outputs": [],
   "source": [
    "VESPA_APPLICATION_NAME = \"rag-blueprint\"  # No hyphens or underscores allowed\n",
    "VESPA_SCHEMA_NAME = \"doc\"  # RAG Blueprint uses 'doc' schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a249c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_root = Path(\"src/rag-blueprint\")\n",
    "application_root = repo_root / \"app\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875de5b0",
   "metadata": {
    "id": "875de5b0"
   },
   "source": [
    "Note, you could also enable a token endpoint, for easier connection after deployment, see [Authenticating to Vespa Cloud](https://vespa-engine.github.io/pyvespa/authenticating-to-vespa-cloud.html) for details. We will stick to the default MTLS key/cert authentication for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddc7d09",
   "metadata": {},
   "source": [
    "### Adding secret to Vespa Cloud Secret Store\n",
    "\n",
    "In order to use the LLM integration, you need to add your OpenAI API key to the Vespa Cloud [Secret Store](https://docs.vespa.ai/en/cloud/security/secret-store.html#).\n",
    "\n",
    "Then, we can reference this secret in our `services.xml` file, so that Vespa can use it to access the OpenAI API.\n",
    "Below we have added a vault called `sample-apps` and a secret named `openai-dev` that contains the OpenAI API key.\n",
    "\n",
    "![Adding secret to Vespa Cloud Secret Store](../_static/secret-store.png)\n",
    "\n",
    "We also need to assign permissions for our application to access this secret, but this can not be done until the application is deployed.\n",
    "\n",
    "```xml\n",
    "        <!-- Uncomment this to use secret from Vespa Cloud Secret Store -->\n",
    "        <secrets>\n",
    "            <openai-api-key vault=\"sample-apps\" name=\"openai-dev\" />\n",
    "        </secrets>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf552a",
   "metadata": {},
   "source": [
    "Let us first take a look at the original `services.xml` file, which contains the configuration for the Vespa application services, including the LLM integration and embedder.\n",
    "\n",
    "!!! note\n",
    "    It is also possible to define the services.xml-configuration in python code, see [Advanced Configuration](https://vespa-engine.github.io/pyvespa/advanced-configuration.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5561fa6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<services version=\"1.0\" xmlns:deploy=\"vespa\" xmlns:preprocess=\"properties\"\n",
       "    minimum-required-vespa-version=\"8.519.55\">\n",
       "\n",
       "    <container id=\"default\" version=\"1.0\">\n",
       "        <document-processing />\n",
       "        <document-api />\n",
       "        <!-- Uncomment this to use secret from Vespa Cloud Secret Store -->\n",
       "        <secrets>\n",
       "            <openai-api-key vault=\"sample-apps\" name=\"openai-dev\" />\n",
       "        </secrets>\n",
       "        <!-- Setup the client to OpenAI -->\n",
       "        <component id=\"openai\" class=\"ai.vespa.llm.clients.OpenAI\">\n",
       "            <config name=\"ai.vespa.llm.clients.llm-client\">\n",
       "                <!-- Uncomment this to use secret from Vespa Cloud Secret Store -->\n",
       "                <apiKeySecretName>openai-api-key</apiKeySecretName>\n",
       "            </config>\n",
       "        </component>\n",
       "\n",
       "        <component id=\"nomicmb\" type=\"hugging-face-embedder\">\n",
       "            <transformer-model\n",
       "                url=\"https://data.vespa-cloud.com/onnx_models/nomic-ai-modernbert-embed-base/model.onnx\" />\n",
       "            <transformer-token-type-ids />\n",
       "            <tokenizer-model\n",
       "                url=\"https://data.vespa-cloud.com/onnx_models/nomic-ai-modernbert-embed-base/tokenizer.json\" />\n",
       "            <transformer-output>token_embeddings</transformer-output>\n",
       "            <max-tokens>8192</max-tokens>\n",
       "            <prepend>\n",
       "                <query>search_query:</query>\n",
       "                <document>search_document:</document>\n",
       "            </prepend>\n",
       "        </component>\n",
       "        <search>\n",
       "            <chain id=\"openai\" inherits=\"vespa\">\n",
       "                <searcher id=\"ai.vespa.search.llm.RAGSearcher\">\n",
       "                    <config name=\"ai.vespa.search.llm.llm-searcher\">\n",
       "                        <providerId>openai</providerId>\n",
       "                    </config>\n",
       "                </searcher>\n",
       "            </chain>\n",
       "        </search>\n",
       "        <nodes>\n",
       "            <node hostalias=\"node1\" />\n",
       "        </nodes>\n",
       "    </container>\n",
       "\n",
       "    <!-- See https://docs.vespa.ai/en/reference/services-content.html -->\n",
       "    <content id=\"content\" version=\"1.0\">\n",
       "        <min-redundancy>2</min-redundancy>\n",
       "        <documents>\n",
       "            <document type=\"doc\" mode=\"index\" />\n",
       "        </documents>\n",
       "        <nodes>\n",
       "            <node hostalias=\"node1\" distribution-key=\"0\" />\n",
       "        </nodes>\n",
       "    </content>\n",
       "\n",
       "</services>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def display_md(text: str, tag: str = \"txt\"):\n",
    "    text = text.rstrip()\n",
    "    md = f\"\"\"```{tag}\n",
    "{text}\n",
    "```\"\"\"\n",
    "    display(Markdown(md))\n",
    "\n",
    "\n",
    "services_content = (application_root / \"services.xml\").read_text()\n",
    "display_md(services_content, \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a9d5fc",
   "metadata": {
    "id": "d2a9d5fc"
   },
   "source": [
    "## Deploy the application to Vespa Cloud\n",
    "\n",
    "Now let's deploy the RAG Blueprint application to Vespa Cloud:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f1147e",
   "metadata": {
    "id": "e2f1147e"
   },
   "outputs": [],
   "source": [
    "# This is only needed for CI.\n",
    "VESPA_TEAM_API_KEY = os.getenv(\"VESPA_TEAM_API_KEY\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cd98149",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cd98149",
    "outputId": "99b6ff7f-c84d-44f9-b4f2-29c9be994538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting application...\n",
      "Running: vespa config set application vespa-team.rag-blueprint.default\n",
      "Setting target cloud...\n",
      "Running: vespa config set target cloud\n",
      "\n",
      "Api-key found for control plane access. Using api-key.\n"
     ]
    }
   ],
   "source": [
    "vespa_cloud = VespaCloud(\n",
    "    tenant=VESPA_TENANT_NAME,\n",
    "    application=VESPA_APPLICATION_NAME,\n",
    "    key_content=VESPA_TEAM_API_KEY,\n",
    "    application_root=application_root,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962aca9",
   "metadata": {},
   "source": [
    "Now, we will deploy the application to Vespa Cloud. This will take a few minutes, so feel free to skip ahead to the next section while waiting for the deployment to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c88a84ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment started in run 85 of dev-aws-us-east-1c for vespa-team.rag-blueprint. This may take a few minutes the first time.\n",
      "INFO    [09:40:36]  Deploying platform version 8.586.25 and application dev build 85 for dev-aws-us-east-1c of default ...\n",
      "INFO    [09:40:36]  Using CA signed certificate version 5\n",
      "INFO    [09:40:43]  Session 379704 for tenant 'vespa-team' prepared and activated.\n",
      "INFO    [09:40:43]  ######## Details for all nodes ########\n",
      "INFO    [09:40:43]  h125699b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:40:43]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:40:43]  --- storagenode on port 19102 has config generation 379704, wanted is 379704\n",
      "INFO    [09:40:43]  --- searchnode on port 19107 has config generation 379704, wanted is 379704\n",
      "INFO    [09:40:43]  --- distributor on port 19111 has config generation 379699, wanted is 379704\n",
      "INFO    [09:40:43]  --- metricsproxy-container on port 19092 has config generation 379704, wanted is 379704\n",
      "INFO    [09:40:43]  h125755a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:40:43]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:40:43]  --- container on port 4080 has config generation 379699, wanted is 379704\n",
      "INFO    [09:40:43]  --- metricsproxy-container on port 19092 has config generation 379704, wanted is 379704\n",
      "INFO    [09:40:43]  h97530b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:40:43]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:40:43]  --- logserver-container on port 4080 has config generation 379704, wanted is 379704\n",
      "INFO    [09:40:43]  --- metricsproxy-container on port 19092 has config generation 379704, wanted is 379704\n",
      "INFO    [09:40:43]  h119190c.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:40:43]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:40:43]  --- container-clustercontroller on port 19050 has config generation 379699, wanted is 379704\n",
      "INFO    [09:40:43]  --- metricsproxy-container on port 19092 has config generation 379699, wanted is 379704\n",
      "INFO    [09:40:51]  Found endpoints:\n",
      "INFO    [09:40:51]  - dev.aws-us-east-1c\n",
      "INFO    [09:40:51]   |-- https://fe5fe13c.fe19121d.z.vespa-app.cloud/ (cluster 'default')\n",
      "INFO    [09:40:51]  Deployment of new application revision complete!\n",
      "Only region: aws-us-east-1c available in dev environment.\n",
      "Found mtls endpoint for default\n",
      "URL: https://fe5fe13c.fe19121d.z.vespa-app.cloud/\n",
      "Application is up!\n"
     ]
    }
   ],
   "source": [
    "# Deploy the application\n",
    "app: Vespa = vespa_cloud.deploy(disk_folder=application_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7cfac9",
   "metadata": {},
   "source": [
    "## Uncomment secret reference and redeploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4358c1",
   "metadata": {},
   "source": [
    "Now is the time to assign permissions for this application (in this case `ragblueprintdemo`) to access the secret.\n",
    "\n",
    "![Assigning permissions in Vespa Cloud](../_static/add-secret-permission.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b1f92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<services version=\"1.0\" xmlns:deploy=\"vespa\" xmlns:preprocess=\"properties\"\n",
       "    minimum-required-vespa-version=\"8.519.55\">\n",
       "\n",
       "    <container id=\"default\" version=\"1.0\">\n",
       "        <document-processing />\n",
       "        <document-api />\n",
       "        <!-- Uncomment this to use secret from Vespa Cloud Secret Store -->\n",
       "        <secrets>\n",
       "            <openai-api-key vault=\"sample-apps\" name=\"openai-dev\" />\n",
       "        </secrets>\n",
       "        <!-- Setup the client to OpenAI -->\n",
       "        <component id=\"openai\" class=\"ai.vespa.llm.clients.OpenAI\">\n",
       "            <config name=\"ai.vespa.llm.clients.llm-client\">\n",
       "                <!-- Uncomment this to use secret from Vespa Cloud Secret Store -->\n",
       "                <apiKeySecretName>openai-api-key</apiKeySecretName>\n",
       "            </config>\n",
       "        </component>\n",
       "\n",
       "        <component id=\"nomicmb\" type=\"hugging-face-embedder\">\n",
       "            <transformer-model\n",
       "                url=\"https://data.vespa-cloud.com/onnx_models/nomic-ai-modernbert-embed-base/model.onnx\" />\n",
       "            <transformer-token-type-ids />\n",
       "            <tokenizer-model\n",
       "                url=\"https://data.vespa-cloud.com/onnx_models/nomic-ai-modernbert-embed-base/tokenizer.json\" />\n",
       "            <transformer-output>token_embeddings</transformer-output>\n",
       "            <max-tokens>8192</max-tokens>\n",
       "            <prepend>\n",
       "                <query>search_query:</query>\n",
       "                <document>search_document:</document>\n",
       "            </prepend>\n",
       "        </component>\n",
       "        <search>\n",
       "            <chain id=\"openai\" inherits=\"vespa\">\n",
       "                <searcher id=\"ai.vespa.search.llm.RAGSearcher\">\n",
       "                    <config name=\"ai.vespa.search.llm.llm-searcher\">\n",
       "                        <providerId>openai</providerId>\n",
       "                    </config>\n",
       "                </searcher>\n",
       "            </chain>\n",
       "        </search>\n",
       "        <nodes>\n",
       "            <node hostalias=\"node1\" />\n",
       "        </nodes>\n",
       "    </container>\n",
       "\n",
       "    <!-- See https://docs.vespa.ai/en/reference/services-content.html -->\n",
       "    <content id=\"content\" version=\"1.0\">\n",
       "        <min-redundancy>2</min-redundancy>\n",
       "        <documents>\n",
       "            <document type=\"doc\" mode=\"index\" />\n",
       "        </documents>\n",
       "        <nodes>\n",
       "            <node hostalias=\"node1\" distribution-key=\"0\" />\n",
       "        </nodes>\n",
       "    </content>\n",
       "\n",
       "</services>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def uncomment_secrets(xml_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Uncomments specific secret-related blocks in an XML string.\n",
    "\n",
    "    This function finds XML blocks that are commented out and contain either:\n",
    "    - <secrets>...</secrets> tags\n",
    "    - <apiKeySecretName>...</apiKeySecretName> tags\n",
    "\n",
    "    Args:\n",
    "        xml_content: A string containing the XML data with commented sections.\n",
    "\n",
    "    Returns:\n",
    "        The XML string with secret sections uncommented.\n",
    "\n",
    "    Example:\n",
    "        Input:  \"<!-- <secrets><key>value</key></secrets> -->\"\n",
    "        Output: \"<secrets><key>value</key></secrets>\"\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    # Pattern to find commented-out <secrets> blocks\n",
    "    # Matches: <!-- <secrets>content</secrets> -->\n",
    "    secrets_pattern = re.compile(r\"<!--\\s*(<secrets>.*?</secrets>)\\s*-->\", re.DOTALL)\n",
    "\n",
    "    # Pattern to find commented-out <apiKeySecretName> blocks\n",
    "    # Matches: <!-- <apiKeySecretName>content</apiKeySecretName> -->\n",
    "    api_key_pattern = re.compile(\n",
    "        r\"<!--\\s*(<apiKeySecretName>.*?</apiKeySecretName>)\\s*-->\", re.DOTALL\n",
    "    )\n",
    "\n",
    "    # Uncomment the blocks by replacing with just the XML content\n",
    "    uncommented_content = secrets_pattern.sub(r\"\\1\", xml_content)\n",
    "    uncommented_content = api_key_pattern.sub(r\"\\1\", uncommented_content)\n",
    "\n",
    "    return uncommented_content\n",
    "\n",
    "\n",
    "uncommented_services_content = uncomment_secrets(services_content)\n",
    "display_md(uncommented_services_content, \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7524cf6",
   "metadata": {},
   "source": [
    "Let us write the uncommented `services.xml` file to the application package directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b157f0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2398"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(application_root / \"services.xml\").write_text(uncommented_services_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9b0a1",
   "metadata": {},
   "source": [
    "Now, we can redeploy the application to Vespa Cloud with the secret reference included in the `services.xml` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9082ca03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment started in run 86 of dev-aws-us-east-1c for vespa-team.rag-blueprint. This may take a few minutes the first time.\n",
      "INFO    [09:40:56]  Deploying platform version 8.586.25 and application dev build 86 for dev-aws-us-east-1c of default ...\n",
      "INFO    [09:40:56]  Using CA signed certificate version 5\n",
      "INFO    [09:41:01]  Session 379705 for tenant 'vespa-team' prepared and activated.\n",
      "INFO    [09:41:01]  ######## Details for all nodes ########\n",
      "INFO    [09:41:01]  h125699b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:41:01]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:41:01]  --- storagenode on port 19102 has config generation 379704, wanted is 379705\n",
      "INFO    [09:41:01]  --- searchnode on port 19107 has config generation 379705, wanted is 379705\n",
      "INFO    [09:41:01]  --- distributor on port 19111 has config generation 379704, wanted is 379705\n",
      "INFO    [09:41:01]  --- metricsproxy-container on port 19092 has config generation 379705, wanted is 379705\n",
      "INFO    [09:41:01]  h125755a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:41:01]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:41:01]  --- container on port 4080 has config generation 379705, wanted is 379705\n",
      "INFO    [09:41:01]  --- metricsproxy-container on port 19092 has config generation 379705, wanted is 379705\n",
      "INFO    [09:41:01]  h97530b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:41:01]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:41:01]  --- logserver-container on port 4080 has config generation 379705, wanted is 379705\n",
      "INFO    [09:41:01]  --- metricsproxy-container on port 19092 has config generation 379705, wanted is 379705\n",
      "INFO    [09:41:01]  h119190c.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:41:01]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:41:01]  --- container-clustercontroller on port 19050 has config generation 379705, wanted is 379705\n",
      "INFO    [09:41:01]  --- metricsproxy-container on port 19092 has config generation 379705, wanted is 379705\n",
      "INFO    [09:41:08]  Found endpoints:\n",
      "INFO    [09:41:08]  - dev.aws-us-east-1c\n",
      "INFO    [09:41:08]   |-- https://fe5fe13c.fe19121d.z.vespa-app.cloud/ (cluster 'default')\n",
      "INFO    [09:41:08]  Deployment of new application revision complete!\n",
      "Only region: aws-us-east-1c available in dev environment.\n",
      "Found mtls endpoint for default\n",
      "URL: https://fe5fe13c.fe19121d.z.vespa-app.cloud/\n",
      "Application is up!\n"
     ]
    }
   ],
   "source": [
    "app: Vespa = vespa_cloud.deploy(disk_folder=application_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaa6acc",
   "metadata": {
    "id": "bcaa6acc"
   },
   "source": [
    "## Feed Sample Data\n",
    "\n",
    "The RAG Blueprint comes with sample data. Let's download and feed it to test our deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e97868b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e97868b",
    "outputId": "227dd121-a7b7-42db-a9a9-c892bbaf73e6"
   },
   "outputs": [],
   "source": [
    "doc_file = repo_root / \"dataset\" / \"docs.jsonl\"\n",
    "with open(doc_file, \"r\") as f:\n",
    "    docs = [json.loads(line) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "987c4c94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'put': 'id:doc:doc::1',\n",
       "  'fields': {'created_timestamp': 1675209600,\n",
       "   'modified_timestamp': 1675296000,\n",
       "   'text': '# SynapseCore Module: Custom Attention Implementation\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass CustomAttention(nn.Module):\\n    def __init__(self, hidden_dim):\\n        super(CustomAttention, self).__init__()\\n        self.hidden_dim = hidden_dim\\n        self.query_layer = nn.Linear(hidden_dim, hidden_dim)\\n        self.key_layer = nn.Linear(hidden_dim, hidden_dim)\\n        self.value_layer = nn.Linear(hidden_dim, hidden_dim)\\n        # More layers and logic here\\n\\n    def forward(self, query_input, key_input, value_input, mask=None):\\n        # Q, K, V projections\\n        Q = self.query_layer(query_input)\\n        K = self.key_layer(key_input)\\n        V = self.value_layer(value_input)\\n\\n        # Scaled Dot-Product Attention\\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.hidden_dim ** 0.5)\\n        if mask is not None:\\n            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\\n        \\n        attention_probs = F.softmax(attention_scores, dim=-1)\\n        context_vector = torch.matmul(attention_probs, V)\\n        return context_vector, attention_probs\\n\\n# Example Usage:\\n# attention_module = CustomAttention(hidden_dim=512)\\n# output, probs = attention_module(q_tensor, k_tensor, v_tensor)\\n```\\n\\n## Design Notes:\\n- Optimized for speed with batched operations.\\n- Includes optional masking for variable sequence lengths.\\n## <MORE_TEXT:HERE>',\n",
       "   'favorite': True,\n",
       "   'last_opened_timestamp': 1717308000,\n",
       "   'open_count': 25,\n",
       "   'title': 'custom_attention_impl.py.md',\n",
       "   'id': '1'}},\n",
       " {'put': 'id:doc:doc::2',\n",
       "  'fields': {'created_timestamp': 1709251200,\n",
       "   'modified_timestamp': 1709254800,\n",
       "   'text': \"# YC Workshop Notes: Scaling B2B Sales (W25)\\nDate: 2025-03-01\\nSpeaker: [YC Partner Name]\\n\\n## Key Takeaways:\\n1.  **ICP Definition is Crucial:** Don't try to sell to everyone. Narrow down your Ideal Customer Profile.\\n    -   Characteristics: Industry, company size, pain points, decision-maker roles.\\n2.  **Outbound Strategy:**\\n    -   Personalized outreach > Mass emails.\\n    -   Tools mentioned: Apollo.io, Outreach.io.\\n    -   Metrics: Open rates, reply rates, meeting booked rates.\\n3.  **Sales Process Stages:**\\n    -   Prospecting -> Qualification -> Demo -> Proposal -> Negotiation -> Close.\\n    -   Define clear entry/exit criteria for each stage.\\n4.  **Value Proposition:** Clearly articulate how you solve the customer's pain and deliver ROI.\\n5.  **Early Hires:** First sales hire should be a 'hunter-farmer' hybrid if possible, or a strong individual contributor.\\n\\n## Action Items for SynapseFlow:\\n-   [ ] Refine ICP based on beta user feedback.\\n-   [ ] Experiment with a small, targeted outbound campaign for 2 specific verticals.\\n-   [ ] Draft initial sales playbook outline.\\n## <MORE_TEXT:HERE>\",\n",
       "   'favorite': True,\n",
       "   'last_opened_timestamp': 1717000000,\n",
       "   'open_count': 12,\n",
       "   'title': 'yc_b2b_sales_workshop_notes.md',\n",
       "   'id': '2'}}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b06814f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fields': {'created_timestamp': 1675209600,\n",
       "   'modified_timestamp': 1675296000,\n",
       "   'text': '# SynapseCore Module: Custom Attention Implementation\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nclass CustomAttention(nn.Module):\\n    def __init__(self, hidden_dim):\\n        super(CustomAttention, self).__init__()\\n        self.hidden_dim = hidden_dim\\n        self.query_layer = nn.Linear(hidden_dim, hidden_dim)\\n        self.key_layer = nn.Linear(hidden_dim, hidden_dim)\\n        self.value_layer = nn.Linear(hidden_dim, hidden_dim)\\n        # More layers and logic here\\n\\n    def forward(self, query_input, key_input, value_input, mask=None):\\n        # Q, K, V projections\\n        Q = self.query_layer(query_input)\\n        K = self.key_layer(key_input)\\n        V = self.value_layer(value_input)\\n\\n        # Scaled Dot-Product Attention\\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.hidden_dim ** 0.5)\\n        if mask is not None:\\n            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\\n        \\n        attention_probs = F.softmax(attention_scores, dim=-1)\\n        context_vector = torch.matmul(attention_probs, V)\\n        return context_vector, attention_probs\\n\\n# Example Usage:\\n# attention_module = CustomAttention(hidden_dim=512)\\n# output, probs = attention_module(q_tensor, k_tensor, v_tensor)\\n```\\n\\n## Design Notes:\\n- Optimized for speed with batched operations.\\n- Includes optional masking for variable sequence lengths.\\n## <MORE_TEXT:HERE>',\n",
       "   'favorite': True,\n",
       "   'last_opened_timestamp': 1717308000,\n",
       "   'open_count': 25,\n",
       "   'title': 'custom_attention_impl.py.md',\n",
       "   'id': '1'},\n",
       "  'id': '1'},\n",
       " {'fields': {'created_timestamp': 1709251200,\n",
       "   'modified_timestamp': 1709254800,\n",
       "   'text': \"# YC Workshop Notes: Scaling B2B Sales (W25)\\nDate: 2025-03-01\\nSpeaker: [YC Partner Name]\\n\\n## Key Takeaways:\\n1.  **ICP Definition is Crucial:** Don't try to sell to everyone. Narrow down your Ideal Customer Profile.\\n    -   Characteristics: Industry, company size, pain points, decision-maker roles.\\n2.  **Outbound Strategy:**\\n    -   Personalized outreach > Mass emails.\\n    -   Tools mentioned: Apollo.io, Outreach.io.\\n    -   Metrics: Open rates, reply rates, meeting booked rates.\\n3.  **Sales Process Stages:**\\n    -   Prospecting -> Qualification -> Demo -> Proposal -> Negotiation -> Close.\\n    -   Define clear entry/exit criteria for each stage.\\n4.  **Value Proposition:** Clearly articulate how you solve the customer's pain and deliver ROI.\\n5.  **Early Hires:** First sales hire should be a 'hunter-farmer' hybrid if possible, or a strong individual contributor.\\n\\n## Action Items for SynapseFlow:\\n-   [ ] Refine ICP based on beta user feedback.\\n-   [ ] Experiment with a small, targeted outbound campaign for 2 specific verticals.\\n-   [ ] Draft initial sales playbook outline.\\n## <MORE_TEXT:HERE>\",\n",
       "   'favorite': True,\n",
       "   'last_opened_timestamp': 1717000000,\n",
       "   'open_count': 12,\n",
       "   'title': 'yc_b2b_sales_workshop_notes.md',\n",
       "   'id': '2'},\n",
       "  'id': '2'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vespa_feed = []\n",
    "for doc in docs:\n",
    "    vespa_doc = doc.copy()\n",
    "    vespa_doc[\"id\"] = doc[\"fields\"][\"id\"]\n",
    "    vespa_doc.pop(\"put\")\n",
    "    vespa_feed.append(vespa_doc)\n",
    "vespa_feed[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caae9d21",
   "metadata": {},
   "source": [
    "Now, let us feed the data to Vespa. \n",
    "If you have a large dataset, you could also do this async, with `feed_async_iterable()`, see [Feeding Vespa cloud](https://vespa-engine.github.io/pyvespa/examples/feed_performance_cloud.html) for a detailed comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3d650aa",
   "metadata": {
    "id": "d3d650aa"
   },
   "outputs": [],
   "source": [
    "from vespa.io import VespaResponse\n",
    "\n",
    "\n",
    "def callback(response: VespaResponse, id: str):\n",
    "    if not response.is_successful():\n",
    "        print(\n",
    "            f\"Failed to feed document {id} with status code {response.status_code}: Reason {response.get_json()}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Feed data into Vespa synchronously\n",
    "app.feed_iterable(vespa_feed, schema=VESPA_SCHEMA_NAME, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b40258",
   "metadata": {
    "id": "b6b40258"
   },
   "source": [
    "## Test a query to the Vespa application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d50eff1",
   "metadata": {},
   "source": [
    "Let us test some queries to see if the application is working as expected.\n",
    "We will use one of the pre-configured query profiles, which we will explain in more detail later.\n",
    "For now, let us just see that we can get some results back from the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "293c672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'root': {'id': 'toplevel',\n",
       "  'relevance': 1.0,\n",
       "  'fields': {'totalCount': 100},\n",
       "  'coverage': {'coverage': 100,\n",
       "   'documents': 100,\n",
       "   'full': True,\n",
       "   'nodes': 1,\n",
       "   'results': 1,\n",
       "   'resultsFull': 1},\n",
       "  'children': [{'id': 'index:content/0/e369853debf684767dff1f16',\n",
       "    'relevance': 1.7111883427143333,\n",
       "    'source': 'content',\n",
       "    'fields': {'sddocname': 'doc',\n",
       "     'chunks_top3': ['# YC Application Draft Snippets - SynapseFlow (Late 2024)\\n\\n**Q: Describe what your company does in 50 characters or less.**\\n- AI model deployment made easy for developers.\\n- Effortless MLOps for startups.\\n- Deploy ML models in minutes, not weeks.\\n\\n**Q: What is your company going to make?**\\nSynapseFlow is building a PaaS solution that radically simplifies the deployment, management, and scaling of machine learning models. We provide a developer-first API and intuitive UI that abstracts away the complexities of MLOps infrastructure (Kubernetes, model servers, monitoring), allowing data scientists and developers ',\n",
       "      \"to focus on building models, not wrestling with ops. Our vision is to be the Heroku for AI.\\n\\n**Q: Why did you pick this idea to work on?**\\nAs an AI engineer, I've experienced firsthand the immense friction and time wasted in operationalizing ML models. Existing solutions are often too complex for smaller teams (e.g., full SageMaker/Vertex AI) or lack the flexibility needed for custom model development. We believe there's a huge unmet need for a simple, powerful, and affordable MLOps platform.\\n\\n## <MORE_TEXT:HERE> (More Q&A drafts, team background notes)\"],\n",
       "     'summaryfeatures': {'top_3_chunk_sim_scores': {'type': 'tensor<float>(chunk{})',\n",
       "       'cells': {'0': 0.36166757345199585, '1': 0.21831661462783813}},\n",
       "      'vespa.summaryFeatures.cached': 0.0}}},\n",
       "   {'id': 'index:content/0/98f13708aca18c358d9d52d0',\n",
       "    'relevance': 1.309791587164871,\n",
       "    'source': 'content',\n",
       "    'fields': {'sddocname': 'doc',\n",
       "     'chunks_top3': [\"# Ideas for SynapseFlow Blog Post - 'Demystifying MLOps'\\n\\n**Target Audience:** Developers, data scientists new to MLOps, product managers.\\n**Goal:** Explain what MLOps is, why it's important, and how SynapseFlow helps.\\n\\n## Outline:\\n1.  **Introduction: The AI/ML Development Lifecycle is More Than Just Model Training**\\n    * Analogy: Building a model is like writing code; MLOps is like DevOps for ML.\\n2.  **What is MLOps? (The Core Pillars)**\\n    * Data Management (Versioning, Lineage, Quality)\\n    * Experiment Tracking & Model Versioning\\n    * CI/CD for ML (Continuous Integration, Continuous Delivery, Continuous Training)\\n    * Model Deployment & Serving\\n    * Monitoring & Observability (Performance, Drift, Data Quality)\\n    * Governance & Reproducibility\\n3.  **Why is MLOps Hard? (The Challenges)\",\n",
       "      \"**\\n    * Complexity of the ML lifecycle.\\n    * Bridging the gap between data science and engineering.\\n    * Tooling fragmentation.\\n    * Need for specialized skills.\\n4.  **How SynapseFlow Addresses These Challenges (Subtle Product Weave-in)**\\n    * Focus on ease of deployment (our current strength).\\n    * Streamlined workflow from experiment to production (our vision).\\n    * (Mention specific features that align with MLOps pillars without being overly salesy).\\n5.  **Getting Started with MLOps - Practical Tips**\\n    * Start simple, iterate.\\n    * Focus on automation early.\\n    * Choose tools that fit your team's scale and expertise.\\n6.  **Conclusion: MLOps is an Enabler for Realizing AI Value**\\n\\n## <MORE_TEXT:HERE> (Draft paragraphs, links to reference articles, potential graphics ideas)\"],\n",
       "     'summaryfeatures': {'top_3_chunk_sim_scores': {'type': 'tensor<float>(chunk{})',\n",
       "       'cells': {'0': 0.3064674735069275, '1': 0.29259079694747925}},\n",
       "      'vespa.summaryFeatures.cached': 0.0}}}]}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is SynapseFlows strategy\"\n",
    "body = {\n",
    "    \"query\": query,\n",
    "    \"queryProfile\": \"hybrid\",\n",
    "    \"hits\": 2,\n",
    "}\n",
    "with app.syncio() as sess:\n",
    "    response = sess.query(body)\n",
    "response.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b9f9e",
   "metadata": {},
   "source": [
    "And by changing to the `rag` query profile, and adding the `streaming=True` parameter, we can stream the results from the LLM as server-sent events (SSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ea33fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SynapseFlow's strategy revolves around simplifying the deployment, management, and scaling of machine learning (ML) models through a developer-first platform-as-a-service (PaaS) solution. The key elements of their strategy include:\n",
      "\n",
      "1. **Developer-Focused Solution:** SynapseFlow aims to provide a user-friendly API and intuitive user interface that abstracts the complexities associated with MLOps infrastructure (such as Kubernetes and model servers). This allows developers and data scientists to focus primarily on building models rather than dealing with operational challenges.\n",
      "\n",
      "2. **Addressing Market Gaps:** The founders identified a significant pain point in the existing MLOps landscape, particularly for smaller teams. Many current solutions are either too complex or not flexible enough for custom model development. SynapseFlow targets this unmet need for a straightforward, powerful, and cost-effective platform.\n",
      "\n",
      "3. **Vision of Simplified MLOps:** By positioning itself as \"the Heroku for AI,\" SynapseFlow aims to offer an all-in-one solution that streamlines the workflow from experimentation to production, thus enhancing efficiency and speed in ML project deployment.\n",
      "\n",
      "4. **Education and Support:** Their strategy also includes educational initiatives, as outlined in their blog post ideas targeting developers and product managers new to MLOps. By demystifying MLOps and discussing its challenges and the way SynapseFlow addresses them, they plan to enhance user understanding and adoption of their platform.\n",
      "\n",
      "5. **Continuous Improvement:** SynapseFlow emphasizes a relentless focus on ease of deployment and improving automation capabilities, suggesting an iterative approach to platform development that responds to user feedback and evolving industry needs.\n",
      "\n",
      "Overall, SynapseFlow's strategy is centered on providing user-friendly solutions that reduce operational complexity, enabling faster deployment of machine learning models and supporting teams in successfully realizing the value of AI."
     ]
    }
   ],
   "source": [
    "query = \"What is SynapseFlows strategy\"\n",
    "body = {\n",
    "    \"query\": query,\n",
    "    \"queryProfile\": \"rag\",\n",
    "    \"hits\": 2,\n",
    "}\n",
    "resp_string = \"\"  # Adding a string variable to use for asserting the response in CI.\n",
    "with app.syncio() as sess:\n",
    "    stream_resp = sess.query(\n",
    "        body,\n",
    "        streaming=True,\n",
    "    )\n",
    "    for line in stream_resp:\n",
    "        if line.startswith(\"data: \"):\n",
    "            event = json.loads(line[6:])\n",
    "            token = event.get(\"token\", \"\")\n",
    "            resp_string += token\n",
    "            print(token, end=\"\")\n",
    "assert len(resp_string) > 10, \"Response string should be longer than 10 characters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136651f5",
   "metadata": {},
   "source": [
    "Great, we got some results. The quality is not very good yet, but we will show how to improve it in the next steps.\n",
    "\n",
    "But first, let us explain the use case we are trying to solve with this RAG application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9ba199",
   "metadata": {},
   "source": [
    "## Our use case\n",
    "\n",
    "The sample use case is a document search application, for a user who wants to get answers and insights quickly from a document collection containing company documents, notes, learning material, training logs.\n",
    "To make the blueprint more realistic, we required a dataset with more structured fields than are commonly found in public datasets. Therefore, we used a Large Language Model (LLM) to generate a custom one.\n",
    "\n",
    "It is a toy example, with only 100 documents, but we think it will illustrate the necessary concepts.\n",
    "You can also feel confident that the blueprint will provide a starting point that can scale as you want, with minimal changes.\n",
    "\n",
    "Below you can see a sample document from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d96ac083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'put': 'id:doc:doc::11',\n",
       " 'fields': {'created_timestamp': 1698796800,\n",
       "  'modified_timestamp': 1698796800,\n",
       "  'text': \"# Journal Entry - 2024-11-01\\n\\nFeeling the YC pressure cooker, but in a good way. The pace is insane. It reminds me of peaking for a powerlifting meet – everything has to be precise, every session counts, and you're constantly pushing your limits.\\n\\nThinking about **periodization** in lifting – how you structure macrocycles, mesocycles, and microcycles. Can this apply to startup sprints? We have our big YC Demo Day goal (macro), then maybe 2-week sprints are mesocycles, and daily tasks are microcycles. Need to ensure we're not just redlining constantly but building in phases of intense work, focused development, and even 'deload' (strategic rest/refinement) to avoid burnout and make sustainable progress.\\n\\n**RPE (Rate of Perceived Exertion)** is another concept. In the gym, it helps auto-regulate training based on how you feel. For the startup, maybe we need an RPE check for the team? Are we pushing too hard on a feature that's yielding low returns (high RPE, low ROI)? Can we adjust the 'load' (scope) or 'reps' (iterations) based on team capacity and feedback?\\n\\nIt's interesting how the discipline and structured thinking from strength training can offer mental models for tackling the chaos of a startup. Both require consistency, grit, and a willingness to fail and learn.\\n\\n## <MORE_TEXT:HERE> (More reflections on YC, specific project challenges)\",\n",
       "  'favorite': False,\n",
       "  'last_opened_timestamp': 1700000000,\n",
       "  'open_count': 5,\n",
       "  'title': 'journal_2024_11_01_yc_and_lifting.md',\n",
       "  'id': '11'}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "docs_file = repo_root / \"dataset\" / \"docs.jsonl\"\n",
    "\n",
    "with open(docs_file) as f:\n",
    "    docs = [json.loads(line) for line in f]\n",
    "\n",
    "docs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79613c",
   "metadata": {},
   "source": [
    "In order to evaluate the quality of the RAG application, we also need a set of representative queries, with annotated relevant documents.\n",
    "Crucially, you need a set of representative queries that thoroughly cover your expected use case. More is better, but _some_ eval is always better than none.\n",
    "\n",
    "We used `gemini-2.5-pro` to create our queries and relevant document labels. Please check out our [blog post](https://blog.vespa.ai/improving-retrieval-with-llm-as-a-judge/) to learn more about using LLM-as-a-judge.\n",
    "\n",
    "We decided to generate some queries that need several documents to provide a good answer, and some that only need one document.\n",
    "\n",
    "If these queries are representative of the use case, we will show that they can be a great starting point for creating an (initial) ranking expression that can be used for retrieving and ranking candidate documents. But, it can (and should) also be improved, for example by collecting user interaction data, human labeling and/ or using an LLM to generate relevance feedback following the initial ranking expression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a35c274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': 'alex_q_11',\n",
       " 'query_text': \"Where's that journal entry where I compared YC to powerlifting?\",\n",
       " 'category': 'Navigational - Personal',\n",
       " 'description': 'Finding a specific personal reflection in his journal.',\n",
       " 'relevant_document_ids': ['11', '58', '100']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_file = repo_root / \"dataset\" / \"queries.json\"\n",
    "\n",
    "with open(queries_file) as f:\n",
    "    queries = json.load(f)\n",
    "\n",
    "queries[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76aebf",
   "metadata": {},
   "source": [
    "## Data modeling\n",
    "\n",
    "Here is the schema that we will use for our sample application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d087e686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```txt\n",
       "# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n",
       "schema doc {\n",
       "\n",
       "    document doc {\n",
       "\n",
       "        field id type string {\n",
       "            indexing: summary | attribute\n",
       "        }\n",
       "\n",
       "        field title type string {\n",
       "            indexing: index | summary\n",
       "            index: enable-bm25\n",
       "        }\n",
       "\n",
       "        field text type string {\n",
       "            \n",
       "        }\n",
       "\n",
       "        field created_timestamp type long {\n",
       "            indexing: attribute | summary\n",
       "        }\n",
       "        field modified_timestamp type long {\n",
       "            indexing: attribute | summary\n",
       "        }\n",
       "        \n",
       "        field last_opened_timestamp type long {\n",
       "            indexing: attribute | summary\n",
       "        }\n",
       "        field open_count type int {\n",
       "            indexing: attribute | summary\n",
       "        }\n",
       "        field favorite type bool {\n",
       "            indexing: attribute | summary\n",
       "        }\n",
       "\n",
       "    }\n",
       "\n",
       "    field title_embedding type tensor<int8>(x[96]) {\n",
       "        indexing: input title | embed | pack_bits | attribute | index\n",
       "        attribute {\n",
       "            distance-metric: hamming\n",
       "        }\n",
       "    }\n",
       "\n",
       "    field chunks type array<string> {\n",
       "        indexing: input text | chunk fixed-length 1024 | summary | index\n",
       "        index: enable-bm25\n",
       "    }\n",
       "\n",
       "    field chunk_embeddings type tensor<int8>(chunk{}, x[96]) {\n",
       "        indexing: input text | chunk fixed-length 1024 | embed | pack_bits | attribute | index\n",
       "        attribute {\n",
       "            distance-metric: hamming\n",
       "        }\n",
       "    }\n",
       "\n",
       "    fieldset default {\n",
       "        fields: title, chunks\n",
       "    }\n",
       "\n",
       "    document-summary no-chunks {\n",
       "        summary id {}\n",
       "        summary title {}\n",
       "        summary created_timestamp {}\n",
       "        summary modified_timestamp {}\n",
       "        summary last_opened_timestamp {}\n",
       "        summary open_count {}\n",
       "        summary favorite {}\n",
       "        summary chunks {}\n",
       "    }\n",
       "\n",
       "    document-summary top_3_chunks {\n",
       "        from-disk\n",
       "        summary chunks_top3 {\n",
       "            source: chunks\n",
       "            select-elements-by: top_3_chunk_sim_scores #this needs to be added a summary-feature to the rank-profile\n",
       "        }\n",
       "    }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "schema_file = repo_root / \"app\" / \"schemas\" / \"doc.sd\"\n",
    "schema_content = schema_file.read_text()\n",
    "\n",
    "display_md(schema_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec995a",
   "metadata": {},
   "source": [
    "Keep reading for an explanation and reasoning behind the choices in the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4998a7ef",
   "metadata": {},
   "source": [
    "### Picking your searchable unit\n",
    "\n",
    "When building a RAG application, your first key decision is choosing the \"searchable unit.\" This is the basic block of information your system will search through and return as context to the LLM. For instance, if you have millions of documents, some hundreds of pages long, what should be your searchable unit?\n",
    "\n",
    "Consider these points when selecting your searchable unit:\n",
    "\n",
    "* **Too fine-grained (e.g., individual sentences or very small paragraphs):**\n",
    "  * Leads to duplication of context and metadata across many small units.\n",
    "  * May result in units lacking sufficient context for the LLM to make good selections or generate relevant responses.\n",
    "  * Increases overhead for managing many small document units.\n",
    "* **Too coarse-grained (e.g., very long chapters or entire large documents):**\n",
    "  * Can cause performance issues due to the size of the units being processed.\n",
    "  * May lead to some large documents appearing relevant to too many queries, reducing precision.\n",
    "  * If you embed the whole document, a too large context will lead to reduced retrieval quality.\n",
    "\n",
    "We recommend erring on the side of using slightly larger units.\n",
    "\n",
    "* LLMs are increasingly capable of handling larger contexts.\n",
    "* In Vespa, you can index larger units, while avoiding data duplication and performance issues, by returning only the most relevant parts.\n",
    "\n",
    "With Vespa, it is now possible to return only the top k most relevant chunks of a document, and include and combine both document-level and chunk-level features in ranking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93cf5e9",
   "metadata": {},
   "source": [
    "### Chunk selection\n",
    "\n",
    "Assume you have chosen a document as your searchable unit.\n",
    "Your documents may then contain text index fields of highly variable lengths. Consider for example a corpus of web pages. Some might be very long, while the average is well within the recommended size. See [scaling retrieval size](https://docs.vespa.ai/en/performance/sizing-search.html#scaling-retrieval-size) for more details.\n",
    "\n",
    "While we recommend implementing guards against too long documents in your feeding pipeline, you still probably do not want to return every chunk of the top k documents to an LLM for RAG.\n",
    "\n",
    "In Vespa, we now have a solution for this problem. Below, we show how you can score both documents as well as individual chunks, and use that score to select the best chunks to be returned in a summary, instead of returning all chunks belonging to the top k ranked documents. \n",
    "\n",
    "Compute closeness per chunk in a ranking function; use `elementwise(bm25(chunks), i, double)` for a per-chunk text signal. See [rank feature reference](https://docs.vespa.ai/en/reference/rank-features.html#elementwise-bm25)\n",
    "\n",
    "This allows you to pick a large document as the searchable unit, while still addressing the potential drawbacks many encounter as follows:\n",
    "\n",
    "* Pick your (larger) document as your searchable unit.\n",
    "* Chunk the text-fields automatically on indexing.\n",
    "* Embed each chunk (enabled through Vespa's multivector support)\n",
    "* Calculate chunk-level features (e.g. bm25 and embedding similarity) and document-level features. Combine as you want.\n",
    "* Limit the actual chunks that are returned to the ones that are actually relevant context for the LLM.\n",
    "\n",
    "This allows you to index larger units, while avoiding data duplication and performance issues, by returning only the most relevant parts.\n",
    "\n",
    "Vespa also supports automatic [chunking](https://docs.vespa.ai/en/reference/indexing-language-reference.html#converters) in the [indexing language](https://docs.vespa.ai/en/indexing.html).\n",
    "\n",
    "Here are the parts of the schema, which defines the searchable unit as a document with a text field, and automatically chunks it into smaller parts of 1024 characters, which each are embedded and indexed separately:\n",
    "\n",
    "```txt\n",
    "field chunks type array<string> {\n",
    "    indexing: input text | chunk fixed-length 1024 | summary | index\n",
    "    index: enable-bm25\n",
    "}\n",
    "\n",
    "field chunk_embeddings type tensor<int8>(chunk{}, x[96]) {\n",
    "    indexing: input text | chunk fixed-length 1024 | embed | pack_bits | attribute | index\n",
    "    attribute {\n",
    "        distance-metric: hamming\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "In Vespa, we can specify which chunks to be returned with a summary feature, see [docs](https://docs.vespa.ai/en/reference/schema-reference.html#select-elements-by) for details. For this blueprint, we will return the top 3 chunks based on the similarity score of the chunk embeddings, which is calculated in the ranking phase. Note that this feature could be any chunk-level summary feature defined in your rank-profile.\n",
    "\n",
    "Here is how the summary feature is calculated in the rank-profile:\n",
    "\n",
    "```txt\n",
    "# This function unpack the bits of each dimenrion of the mapped chunk_embeddings attribute tensor\n",
    "function chunk_emb_vecs() {\n",
    "    expression: unpack_bits(attribute(chunk_embeddings))\n",
    "}\n",
    "\n",
    "# This function calculate the dot product between the query embedding vector and the chunk embeddings (both are now float) over the x dimension\n",
    "function chunk_dot_prod() {\n",
    "    expression: reduce(query(float_embedding) * chunk_emb_vecs(), sum, x)\n",
    "}\n",
    "\n",
    "# This function calculate the L2 normalized length of an input tensor\n",
    "function vector_norms(t) {\n",
    "    expression: sqrt(sum(pow(t, 2), x))\n",
    "}\n",
    "\n",
    "# Here we calculate cosine similarity by dividing the dot product by the product of the L2 normalized query embedding and document embeddings\n",
    "function chunk_sim_scores() {\n",
    "    expression: chunk_dot_prod() / (vector_norms(chunk_emb_vecs()) * vector_norms(query(float_embedding)))\n",
    "}\n",
    "\n",
    "function top_3_chunk_text_scores() {\n",
    "    expression: top(3, chunk_text_scores())\n",
    "}\n",
    "\n",
    "function top_3_chunk_sim_scores() {\n",
    "        expression: top(3, chunk_sim_scores())\n",
    "    }\n",
    "\n",
    "summary-features {\n",
    "        top_3_chunk_sim_scores\n",
    "    }\n",
    "```\n",
    "\n",
    "The ranking expression may seem a bit complex, as we chose to embed each chunk independently, store the embeddings in a binarized format, and then unpack them to calculate similarity based on their float representations. For single dimension dense vector similarity between same-precision embeddings, this can be simplified significantly using the [closeness](https://docs.vespa.ai/en/reference/rank-features.html#closeness(name)) convenience function.\n",
    "\n",
    "Note that we want to use the float-representation of the query-embedding, and thus also need to convert the binary embedding of the chunks to float. After that, we can calculate the similarity score between the query embedding and the chunk embeddings using cosine similarity (the dot product, and then normalize it by the norms of the embeddings).\n",
    "\n",
    "See [ranking expressions](https://docs.vespa.ai/en/reference/ranking-expressions.html#non-primitive-functions) for more details on the `top`-function, and other functions available for ranking expressions.\n",
    "\n",
    "Now, we can use this summary feature in our document summary to return the top 3 chunks of the document, which will be used as context for the LLM. Note that we can also define a document summary that returns all chunks, which might be useful for another use case, such as deep research.\n",
    "\n",
    "```txt\n",
    "document-summary top_3_chunks {\n",
    "      from-disk\n",
    "      summary chunks_top3 {\n",
    "          source: chunks\n",
    "          select-elements-by: top_3_chunk_sim_scores #this needs to be added a summary-feature to the rank-profile\n",
    "      }\n",
    "  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8148d3",
   "metadata": {},
   "source": [
    "### Use multiple text fields, consider multiple embeddings\n",
    "\n",
    "We recommend indexing different textual content as separate indexes.\n",
    "These can be searched together, using [field-sets](https://docs.vespa.ai/en/reference/schema-reference.html#fieldset)\n",
    "\n",
    "In our schema, this is exemplified by the sections below, which define the `title` and `chunks` fields as separate indexed text fields.\n",
    "\n",
    "```txt\n",
    "...\n",
    "field title type title {\n",
    "    indexing: index | summary\n",
    "    index: enable-bm25\n",
    "}\n",
    "field chunks type array<string> {\n",
    "    indexing: input text | chunk fixed-length 1024 | summary | index\n",
    "    index: enable-bm25\n",
    "}\n",
    "```\n",
    "\n",
    "Whether you should have separate embedding fields, depends on whether the added memory usage is justified by the quality improvement you could get from the additional embedding field.\n",
    "\n",
    "We choose to index both a `title_embedding` and a `chunk_embeddings` field for this blueprint, as we aim to minimize cost by embedding the binary vectors.\n",
    "\n",
    "```txt\n",
    "field title_embedding type tensor<int8>(title{}, x[96]) {\n",
    "    indexing: input text | embed | pack_bits | attribute | index\n",
    "    attribute {\n",
    "        distance-metric: hamming\n",
    "    }\n",
    "}\n",
    "field chunk_embeddings type tensor<int8>(chunk{}, x[96]) {\n",
    "    indexing: input text | chunk fixed-length 1024 | embed | pack_bits | attribute | index\n",
    "    attribute {\n",
    "        distance-metric: hamming\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Indexing several embedding fields may not be worth the cost for you. Evaluate whether the cost-quality trade-off is worth it for your application.\n",
    "\n",
    "If you have different vector space representations of your document (e.g images), indexing them separately is likely worth it, as they are likely to provide signals that are complementary to the text-based embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae9275d",
   "metadata": {},
   "source": [
    "### Model Metadata and Signals Using Structured Fields\n",
    "\n",
    "We recommend modeling metadata and signals as structured fields in your schema.\n",
    "Below are some general recommendations, as well as the implementation in our blueprint schema.\n",
    "\n",
    "**Metadata** — knowledge about your data:\n",
    "\n",
    "* Authors, publish time, source, links, category, price, …\n",
    "* Usage: filters, ranking, grouping/aggregation\n",
    "* Index only metadata that are strong filters\n",
    "\n",
    "In our blueprint schema, we include these metadata fields to demonstrate these concepts:\n",
    "\n",
    "* `id` - document identifier \n",
    "* `title` - document name/filename for display and text matching\n",
    "* `created_timestamp`, `modified_timestamp` - temporal metadata for filtering and ranking by recency\n",
    "\n",
    "**Signals** — observations about your data:\n",
    "\n",
    "* Popularity, quality, spam probability, click_probability, …\n",
    "* Usage: ranking\n",
    "* Often updated separately via partial updates\n",
    "* Multiple teams can add their own signals independently\n",
    "\n",
    "In our blueprint schema, we include several of these signals:\n",
    "\n",
    "* `last_opened_timestamp` - user engagement signal for personalization\n",
    "* `open_count` - popularity signal indicating document importance\n",
    "* `favorite` - explicit user preference signal, can be used for boosting relevant content\n",
    "\n",
    "These fields are configured as `attribute | summary` to enable efficient filtering, sorting, and grouping operations while being returned in search results. The timestamp fields allow for temporal filtering (e.g., \"recent documents\") and recency-based ranking, while usage signals like `open_count` and `favorite` can boost frequently accessed or explicitly marked important documents.\n",
    "\n",
    "Consider [parent-child](https://docs.vespa.ai/en/parent-child.html) relationships for low-cardinality metadata.\n",
    "Most large scale RAG application schemas contain at least a hundred structured fields.\n",
    "\n",
    "## LLM-generation with OpenAI-client\n",
    "\n",
    "Vespa supports both Local LLMs, and any OpenAI-compatible API for LLM generation. For details, see [LLMs in Vespa](https://docs.vespa.ai/en/llms-in-vespa.html)\n",
    "\n",
    "The recommended way to provide an API key is by using the [secret store](https://docs.vespa.ai/en/cloud/security/secret-store.html) in Vespa Cloud.\n",
    "\n",
    "To enable this, you need to create a vault (if you don't have one already) and a secret through the [Vespa Cloud console](https://cloud.vespa.ai/). If your vault is named `sample-apps` and contains a secret with the name `openai-api-key`, you would use the following configuration in your `services.xml` to set up the OpenAI client to use that secret:\n",
    "\n",
    "```xml\n",
    "  <secrets>\n",
    "      <openai-api-key vault=\"sample-apps\" name=\"openai-dev\" />\n",
    "  </secrets>\n",
    "  <!-- Setup the client to OpenAI -->\n",
    "  <component id=\"openai\" class=\"ai.vespa.llm.clients.OpenAI\">\n",
    "      <config name=\"ai.vespa.llm.clients.llm-client\">\n",
    "          <apiKeySecretRef>openai-api-key</apiKeySecretRef>\n",
    "      </config>\n",
    "  </component>\n",
    "```\n",
    "\n",
    "Alternatively, for local deployments, you can set the `X-LLM-API-KEY` header in your query to use the OpenAI client for generation.\n",
    "\n",
    "To test generation using the OpenAI client, post a query that runs the `openai` search chain, with `format=sse`. (Use `format=json` for a streaming json response including both the search hits and the LLM-generated tokens.)\n",
    "\n",
    "```bash\n",
    "vespa query \\\n",
    "    --timeout 60 \\\n",
    "    --header=\"X-LLM-API-KEY:<your-api-key>\" \\\n",
    "    yql='select *\n",
    "    from doc\n",
    "    where userInput(@query) or\n",
    "    ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n",
    "    ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))' \\\n",
    "    query=\"Summarize the key architectural decisions documented for SynapseFlow's v0.2 release.\" \\\n",
    "    searchChain=openai \\\n",
    "    format=sse \\\n",
    "    hits=5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfc836",
   "metadata": {},
   "source": [
    "## Structuring your vespa application\n",
    "\n",
    "This section provides recommendations for structuring your Vespa application package. See also the [application package docs](https://docs.vespa.ai/en/application-packages.html) for more details on the application package structure.\n",
    "Note that this is not mandatory, and it might be simpler to start without query profiles and rank profiles, but as you scale out your application, it will be beneficial to have a well-structured application package.\n",
    "\n",
    "Consider the following structure for our application package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "019b9ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/thomas/Repos/pyvespa/docs/sphinx/source/examples/src/rag-blueprint\n",
      "├── app\n",
      "│   ├── models\n",
      "│   │   └── lightgbm_model.json\n",
      "│   ├── schemas\n",
      "│   │   ├── doc\n",
      "│   │   │   ├── base-features.profile\n",
      "│   │   │   ├── collect-second-phase.profile\n",
      "│   │   │   ├── collect-training-data.profile\n",
      "│   │   │   ├── learned-linear.profile\n",
      "│   │   │   ├── match-only.profile\n",
      "│   │   │   └── second-with-gbdt.profile\n",
      "│   │   └── doc.sd\n",
      "│   ├── search\n",
      "│   │   └── query-profiles\n",
      "│   │       ├── deepresearch-with-gbdt.xml\n",
      "│   │       ├── deepresearch.xml\n",
      "│   │       ├── hybrid-with-gbdt.xml\n",
      "│   │       ├── hybrid.xml\n",
      "│   │       ├── rag-with-gbdt.xml\n",
      "│   │       └── rag.xml\n",
      "│   ├── security\n",
      "│   │   └── clients.pem\n",
      "│   └── services.xml\n",
      "├── dataset\n",
      "│   ├── docs.jsonl\n",
      "│   ├── queries.json\n",
      "│   └── test_queries.json\n",
      "├── eval\n",
      "│   ├── output\n",
      "│   │   ├── Vespa-training-data_match_first_phase_20250623_133241.csv\n",
      "│   │   ├── Vespa-training-data_match_first_phase_20250623_133241_logreg_coefficients.txt\n",
      "│   │   ├── Vespa-training-data_match_rank_second_phase_20250623_135819.csv\n",
      "│   │   └── Vespa-training-data_match_rank_second_phase_20250623_135819_feature_importance.csv\n",
      "│   ├── collect_pyvespa.py\n",
      "│   ├── evaluate_match_phase.py\n",
      "│   ├── evaluate_ranking.py\n",
      "│   ├── pyproject.toml\n",
      "│   ├── README.md\n",
      "│   ├── resp.json\n",
      "│   ├── train_lightgbm.py\n",
      "│   └── train_logistic_regression.py\n",
      "├── deploy-locally.md\n",
      "├── generation.md\n",
      "├── query-profiles.md\n",
      "├── README.md\n",
      "└── relevance.md\n"
     ]
    }
   ],
   "source": [
    "# Let's explore the RAG Blueprint application structure\n",
    "print(tree(\"src/rag-blueprint\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48912892",
   "metadata": {},
   "source": [
    "You can see that we have separated the [query profiles](https://docs.vespa.ai/en/query-profiles.html), and [rank profiles](https://docs.vespa.ai/en/ranking.html#rank-profiles) into their own directories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab774c0f",
   "metadata": {},
   "source": [
    "### Manage queries in query profiles\n",
    "\n",
    "Query profiles let you maintain collections of query parameters in one file.\n",
    "Clients choose a query profile → the profile sets everything else.\n",
    "This lets us change behavior for a use case without involving clients.\n",
    "\n",
    "Let us take a closer look at 3 of the query profiles in our sample application.\n",
    "\n",
    "1. `hybrid`\n",
    "2. `rag`\n",
    "3. `deepresearch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945d538",
   "metadata": {},
   "source": [
    "### **_hybrid_** query profile\n",
    "\n",
    "This query profile will be the one used by clients for traditional search, where the user is presented a limited number of hits.\n",
    "Our other query profiles will inherit this one (but may override some fields)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de0bb719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<!--\n",
       "match_avg_top_3_chunk_sim_scores   : 13.383840\n",
       "match_avg_top_3_chunk_text_scores  : 0.203145\n",
       "match_bm25(chunks)                 : 0.159914\n",
       "match_bm25(title)                  : 0.191867\n",
       "match_max_chunk_sim_scores         : 10.067169\n",
       "match_max_chunk_text_scores        : 0.153392\n",
       "Intercept                          : -7.798639\n",
       "-->\n",
       "<query-profile id=\"hybrid\">\n",
       "    <field name=\"schema\">doc</field>\n",
       "    <field name=\"ranking.features.query(embedding)\">embed(@query)</field>\n",
       "    <field name=\"ranking.features.query(float_embedding)\">embed(@query)</field>\n",
       "    <field name=\"ranking.features.query(intercept)\">-7.798639</field>\n",
       "    <field name=\"ranking.features.query(avg_top_3_chunk_sim_scores_param)\">13.383840</field>\n",
       "    <field name=\"ranking.features.query(avg_top_3_chunk_text_scores_param)\">0.203145</field>\n",
       "    <field name=\"ranking.features.query(bm25_chunks_param)\">0.159914</field>\n",
       "    <field name=\"ranking.features.query(bm25_title_param)\">0.191867</field>\n",
       "    <field name=\"ranking.features.query(max_chunk_sim_scores_param)\">10.067169</field>\n",
       "    <field name=\"ranking.features.query(max_chunk_text_scores_param)\">0.153392</field>\n",
       "    <field name=\"yql\">\n",
       "        select *\n",
       "        from %{schema}\n",
       "        where userInput(@query) or\n",
       "        ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n",
       "        ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))\n",
       "    </field>\n",
       "    <field name=\"hits\">10</field>\n",
       "    <field name=\"ranking.profile\">learned-linear</field>\n",
       "    <field name=\"presentation.summary\">top_3_chunks</field>\n",
       "</query-profile>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qp_dir = repo_root / \"app\" / \"search\" / \"query-profiles\"\n",
    "hybrid_qp = (qp_dir / \"hybrid.xml\").read_text()\n",
    "\n",
    "display_md(hybrid_qp, tag=\"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb81813",
   "metadata": {},
   "source": [
    "### **_rag_** query profile\n",
    "\n",
    "This will be the query profile where the `openai` searchChain will be added, to generate a response based on the retrieved context. \n",
    "Here, we set some configuration that are specific to this use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b52c16b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<query-profile id=\"rag\" inherits=\"hybrid\">\n",
       "  <field name=\"hits\">50</field>\n",
       "  <field name=\"searchChain\">openai</field>\n",
       "  <field name=\"presentation.format\">sse</field>\n",
       "</query-profile>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_blueprint_qp = (qp_dir / \"rag.xml\").read_text()\n",
    "display_md(rag_blueprint_qp, tag=\"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b6fcc7",
   "metadata": {},
   "source": [
    "### **_deepresearch_** query profile\n",
    "\n",
    "Again, we will inherit from the `hybrid` query profile, but override with a `targetHits` value of 10 000 (original was 100) that prioritizes recall over latency.\n",
    "We will also increase number of hits to be returned, and increase the timeout to 5 seconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc317420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<query-profile id=\"deepresearch\" inherits=\"hybrid\">\n",
       "  <field name=\"yql\">\n",
       "    select *\n",
       "    from %{schema}\n",
       "    where userInput(@query) or\n",
       "    ({label:\"title_label\", targetHits:10000}nearestNeighbor(title_embedding, embedding)) or\n",
       "    ({label:\"chunks_label\", targetHits:10000}nearestNeighbor(chunk_embeddings, embedding))\n",
       "  </field>\n",
       "  <field name=\"hits\">100</field>\n",
       "  <field name=\"timeout\">5s</field>\n",
       "</query-profile>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "deep_qp = (qp_dir / \"deepresearch.xml\").read_text()\n",
    "display_md(deep_qp, tag=\"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e117b",
   "metadata": {},
   "source": [
    "\n",
    "We will leave out the LLM-generation for this one, and let an LLM agent on the client side be responsible for using this API call as a tool, and to determine whether enough relevant context to answer has been retrieved.\n",
    "Note that the `targetHits` parameter set here does not really makes sense until your dataset reach a certain scale.\n",
    "\n",
    "As we add more rank-profiles, we can also inherit the existing query profiles, only to override the `ranking.profile` field to use a different rank profile. This is what we have done for the `rag-with-gbdt` and `deepresearch-with-gbdt` query profiles, which will use the `second-with-gbdt` rank profile instead of the `learned-linear` rank profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b5cfb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<query-profile id=\"rag-with-gbdt\" inherits=\"hybrid-with-gbdt\">\n",
       "  <field name=\"hits\">50</field>\n",
       "  <field name=\"searchChain\">openai</field>\n",
       "  <field name=\"presentation.format\">sse</field>\n",
       "</query-profile>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_gbdt_qp = (qp_dir / \"rag-with-gbdt.xml\").read_text()\n",
    "display_md(rag_gbdt_qp, tag=\"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99bb83",
   "metadata": {},
   "source": [
    "### Separating out rank profiles\n",
    "\n",
    "To build a great RAG application, assume you’ll need many ranking models. This will allow you to bucket-test alternatives continuously and to serve different use cases, including data collection for different phases, and the rank profiles to be used in production.\n",
    "\n",
    "Separate common functions/setup into parent rank profiles and use `.profile` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023621a",
   "metadata": {},
   "source": [
    "## Phased ranking in Vespa\n",
    "\n",
    "Before we move on, it might be useful to recap Vespa´s [phased ranking](https://docs.vespa.ai/en/phased-ranking.html) approach.\n",
    "\n",
    "Below is a schematic overview of how to think about retrieval and ranking for this RAG blueprint. Since we are developing this as a tutorial using a small toy dataset, the application can be deployed in a single machine, using a single docker container, where only one container node and one container node will run. This is obviously not the case for most real-world RAG applications, so this is cruical to have in mind as you want to scale your application.\n",
    "\n",
    "<img src=\"https://docs.vespa.ai/assets/img/phased-ranking.png\">\n",
    "\n",
    "It is worth noting that parameters such as `targetHits` (for the match phase) and `rerank-count` (for first and second phase) are applied **per content node**. Also note that the stateless container nodes can also be [scaled independently](https://docs.vespa.ai/en/performance/sizing-search.html) to handle increased query load.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee68edb",
   "metadata": {},
   "source": [
    "## Configuring match-phase (retrieval)\n",
    "\n",
    "This section will contain important considerations for the retrieval-phase of a RAG application in Vespa.\n",
    "\n",
    "The goal of the retrieval phase is to retrieve candidate documents efficiently, and maximize recall, without exposing too many documents to ranking.\n",
    "\n",
    "### Choosing a Retrieval Strategy: Vector, Text, or Hybrid?\n",
    "\n",
    "As you could see from the schema, we create and index both a text representation and a vector representation for each chunk of the document. This will allow us to use both text-based features and semantic features for both recall and ranking.\n",
    "\n",
    "The text and vector representation complement each other well:\n",
    "\n",
    "* **Text-only** → misses recall of semantically similar content\n",
    "* **Vector-only** → misses recall of specific content not well understood by the embedding models\n",
    "\n",
    "Our recommendation is to default to hybrid retrieval:\n",
    "\n",
    "```sql\n",
    "select *\n",
    "        from doc\n",
    "        where userInput(@query) or\n",
    "        ({label:\"title_label\", targetHits:1000}nearestNeighbor(title_embedding, embedding)) or\n",
    "        ({label:\"chunks_label\", targetHits:1000}nearestNeighbor(chunk_embeddings, embedding))\n",
    "```\n",
    "\n",
    "In generic domains, or if you have fine-tuned an embedding model for your specific data, you might consider a vector-only approach:\n",
    "\n",
    "```sql\n",
    "select *\n",
    "        from doc\n",
    "        where rank({targetHits:10000}nearestNeighbor(embeddings_field, query_embedding, userInput(@query)))\n",
    "```\n",
    "\n",
    "Notice that only the first argument of the [rank](https://docs.vespa.ai/en/reference/query-language-reference.html#rank)-operator will be used to determine if a document is a match, while all arguments are used for calculating rank features. This mean we can do vector only for matching, but still use text-based features such as `bm25` and `nativeRank` for ranking.\n",
    "Note that if you do this, it makes sense to increase the number of `targetHits` for the `nearestNeighbor`-operator.\n",
    "\n",
    "For our sample application, we add three different retrieval operators (that are combined with `OR`), one with `weakAnd` for text matching, and two `nearestNeighbor` operators for vector matching, one for the title and one for the chunks. This will allow us to retrieve both relevant documents based on text and vector similarity, while also allowing us to return the most relevant chunks of the documents.\n",
    "\n",
    "```sql\n",
    "select *\n",
    "        from doc\n",
    "        where userInput(@query) or\n",
    "        ({targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n",
    "        ({targetHits:100}nearestNeighbor(chunk_embeddings, embedding))\n",
    "```\n",
    "\n",
    "### Choosing your embedding model (and strategy)\n",
    "\n",
    "Choice of embedding model will be a trade-off between inference time (both indexing and query time), memory usage (embedding dimensions) and quality. There are many good open-source models available, and we recommend checking out the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard), and look at the `Retrieval`-column to gauge performance, while also considering the memory usage, vector dimensions, and context length of the model.\n",
    "\n",
    "See [model hub](https://docs.vespa.ai/en/cloud/model-hub.html) for a list of provided models ready to use with Vespa. See also [Huggingface Embedder](https://docs.vespa.ai/en/embedding.html#huggingface-embedder) for details on using other models (exported as ONNX) with Vespa.\n",
    "\n",
    "In addition to dense vector representation, Vespa supports sparse embeddings (token weights) and multi-vector (ColBERT-style) embeddings.\n",
    "See our [example notebook](https://vespa-engine.github.io/pyvespa//examples/mother-of-all-embedding-models-cloud.html#bge-m3-the-mother-of-all-embedding-models) of using the bge-m3 model, which supports both, with Vespa.\n",
    "\n",
    "Vespa also supports [Matryoshka embeddings](https://blog.vespa.ai/combining-matryoshka-with-binary-quantization-using-embedder/), which can be a great way of reducing inference cost for retrieval phases, by using a subset of the embedding dimensions, while using more dimensions for increased precision in the later ranking phases.\n",
    "\n",
    "For domain-specific applications or less popular languages, you may want to consider finetuning a model on your own data.\n",
    "\n",
    "### Consider binary vectors for recall\n",
    "\n",
    "Another decision to make is which precision you will use for your embeddings.\n",
    "See [binarization docs](https://docs.vespa.ai/en/binarizing-vectors.html) for an introduction to binarization in Vespa.\n",
    "\n",
    "For most cases, binary vectors (in Vespa, packed into `int8`-representation) will provide an attractive tradeoff, especially for recall during match-phase.\n",
    "Consider these factors to determine whether this holds true for your application:\n",
    "\n",
    "* Reduces memory-vector cost by 5 – 30 ×\n",
    "* Reduces query and indexing cost by 30 ×\n",
    "* Often reduces quality by only a few percentage points\n",
    "\n",
    "```txt\n",
    "field binary_chunk_embeddings type tensor<int8>(chunk{}, x) {\n",
    "  indexing: input text | chunk fixed-length 1024 | embed | pack_bits | attribute | index \n",
    "  attribute { distance-metric: hamming }\n",
    "}\n",
    "```\n",
    "\n",
    "If you need higher precision vector similarity, you should use bfloat16 precision, and consider paging these vectors to disk to avoid large memory cost. Note that this means that when accessing this field in ranking, they will also need to be read from disk, so you need to restrict the number of hits that accesses this field to avoid performance issues.\n",
    "\n",
    "```txt\n",
    "field chunk_embeddings type tensor<bfloat16>(chunk{}, x) {\n",
    "  indexing: input text | chunk fixed-length 1024 | embed | attribute \n",
    "  attribute: paged\n",
    "}\n",
    "```\n",
    "\n",
    "For example, if you want to calculate `closeness` for a paged embedding vector in first-phase, consider configuring your retrieval operators (typically `weakAnd` and/or `nearestNeighbor`, optionally combined with filters) so that not too many hits are matched. Another option is to enable match-phase limiting, see [match-phase docs](https://docs.vespa.ai/en/reference/schema-reference.html#match-phase). In essence, you restrict the number of matches by specifying an attribute field.\n",
    "\n",
    "### Consider float-binary for ranking\n",
    "\n",
    "In our blueprint, we choose to index binary vectors of the documents. This does not prevent us from using the float-representation of the query embedding though.\n",
    "\n",
    "By unpacking the binary document chunk embeddings to their float representations (using [`unpack_bits`](https://docs.vespa.ai/en/reference/ranking-expressions.html#unpack-bits)), we can calculate the similarity between query and document with slightly higher precision using a `float-binary` dot product, instead of hamming distance (`binary-binary`)\n",
    "\n",
    "Below, you can see how we can do this:\n",
    "\n",
    "```txt\n",
    "rank-profile collect-training-data {\n",
    " \n",
    "        inputs {\n",
    "            query(embedding) tensor<int8>(x[96])\n",
    "            query(float_embedding) tensor<float>(x[768])\n",
    "        }\n",
    "        \n",
    "        function chunk_emb_vecs() {\n",
    "            expression: unpack_bits(attribute(chunk_embeddings))\n",
    "        }\n",
    "\n",
    "        function chunk_dot_prod() {\n",
    "            expression: reduce(query(float_embedding) * chunk_emb_vecs(), sum, x)\n",
    "        }\n",
    "\n",
    "        function vector_norms(t) {\n",
    "            expression: sqrt(sum(pow(t, 2), x))\n",
    "        }\n",
    "        function chunk_sim_scores() {\n",
    "            expression: chunk_dot_prod() / (vector_norms(chunk_emb_vecs()) * vector_norms(query(float_embedding)))\n",
    "        }\n",
    "\n",
    "        function top_3_chunk_text_scores() {\n",
    "            expression: top(3, chunk_text_scores())\n",
    "        }\n",
    "\n",
    "        function top_3_chunk_sim_scores() {\n",
    "            expression: top(3, chunk_sim_scores())\n",
    "        }\n",
    "}\n",
    "```\n",
    "\n",
    "### Use complex linguistics/recall only for precision\n",
    "\n",
    "Vespa gives you extensive control over [linguistics](https://docs.vespa.ai/en/linguistics.html).\n",
    "You can decide [match mode](https://docs.vespa.ai/en/reference/schema-reference.html#match), stemming, normalization, or control derived tokens.\n",
    "\n",
    "It is also possible to use more specific operators than [weakAnd](https://docs.vespa.ai/en/reference/query-language-reference.html#weakand) to match only close occurrences ([near](https://docs.vespa.ai/en/reference/query-language-reference.html#near)/ [onear](https://docs.vespa.ai/en/reference/query-language-reference.html#near)), multiple alternatives ([equiv](https://docs.vespa.ai/en/query-rewriting.html#equiv)), weight items, set connectivity, and apply [query-rewrite](https://docs.vespa.ai/en/query-rewriting.html) rules.\n",
    "\n",
    "**Don’t use this to increase recall — improve your embedding model instead.**\n",
    "\n",
    "Consider using it to improve precision when needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bde2f6",
   "metadata": {},
   "source": [
    "### Evaluating recall of the retrieval phase\n",
    "\n",
    "To know whether your retrieval phase is working well, you need to measure recall, number of total matches and the reported time spent.\n",
    "\n",
    "We can use [`VespaMatchEvaluator`](https://vespa-engine.github.io/pyvespa/api/vespa/evaluation.html#vespa.evaluation.VespaMatchEvaluator) from the pyvespa client library to do this.\n",
    "\n",
    "For this sample application, we set up an evaluation script that compares three different retrieval strategies, let us call them \"retrieval arms\":\n",
    "\n",
    "1. **Semantic-only**: Uses only vector similarity through `nearestNeighbor` operators.\n",
    "2. **WeakAnd-only**: Uses only text-based matching with `userQuery()`.\n",
    "3. **Hybrid**: Combines both approaches with OR logic.\n",
    "\n",
    "Note that this is only generic suggestion for and that you are of course free to include both [filter clauses](https://docs.vespa.ai/en/reference/query-language-reference.html#where), [grouping](https://docs.vespa.ai/en/grouping), [predicates](https://docs.vespa.ai/en/predicate-fields.html), [geosearch](https://docs.vespa.ai/en/geo-search) etc. to support your specific use cases.\n",
    "\n",
    "It is recommended to use a ranking profile that does not use any first-phase ranking, to run the match-phase evaluation faster.\n",
    "\n",
    "The evaluation will output metrics like:\n",
    "\n",
    "* Recall (percentage of relevant documents matched)\n",
    "* Total number of matches per query\n",
    "* Query latency statistics\n",
    "* Per-query detailed results (when `write_verbose=True`) to identify \"offending\" queries with regards to recall or performance.\n",
    "\n",
    "This will be valuable input for tuning each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60a07c",
   "metadata": {},
   "source": [
    "\n",
    "Run the cells below to evaluate all three retrieval strategies on your dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f568865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_to_query = {query[\"query_id\"]: query[\"query_text\"] for query in queries}\n",
    "relevant_docs = {\n",
    "    query[\"query_id\"]: set(query[\"relevant_document_ids\"])\n",
    "    for query in queries\n",
    "    if \"relevant_document_ids\" in query\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb2dc952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating semantic...\n",
      "Evaluating weakand...\n",
      "Evaluating hybrid...\n"
     ]
    }
   ],
   "source": [
    "from vespa.evaluation import VespaMatchEvaluator\n",
    "from vespa.application import Vespa\n",
    "import vespa.querybuilder as qb\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def match_weakand_query_fn(query_text: str, top_k: int) -> dict:\n",
    "    return {\n",
    "        \"yql\": str(\n",
    "            qb.select(\"*\").from_(VESPA_SCHEMA_NAME).where(qb.userQuery(query_text))\n",
    "        ),\n",
    "        \"query\": query_text,\n",
    "        \"ranking\": \"match-only\",\n",
    "        \"input.query(embedding)\": f\"embed({query_text})\",\n",
    "        \"presentation.summary\": \"no-chunks\",\n",
    "    }\n",
    "\n",
    "\n",
    "def match_hybrid_query_fn(query_text: str, top_k: int) -> dict:\n",
    "    return {\n",
    "        \"yql\": str(\n",
    "            qb.select(\"*\")\n",
    "            .from_(VESPA_SCHEMA_NAME)\n",
    "            .where(\n",
    "                qb.nearestNeighbor(\n",
    "                    field=\"title_embedding\",\n",
    "                    query_vector=\"embedding\",\n",
    "                    annotations={\"targetHits\": 100},\n",
    "                )\n",
    "                | qb.nearestNeighbor(\n",
    "                    field=\"chunk_embeddings\",\n",
    "                    query_vector=\"embedding\",\n",
    "                    annotations={\"targetHits\": 100},\n",
    "                )\n",
    "                | qb.userQuery(\n",
    "                    query_text,\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        \"query\": query_text,\n",
    "        \"ranking\": \"match-only\",\n",
    "        \"input.query(embedding)\": f\"embed({query_text})\",\n",
    "        \"presentation.summary\": \"no-chunks\",\n",
    "    }\n",
    "\n",
    "\n",
    "def match_semantic_query_fn(query_text: str, top_k: int) -> dict:\n",
    "    return {\n",
    "        \"yql\": str(\n",
    "            qb.select(\"*\")\n",
    "            .from_(VESPA_SCHEMA_NAME)\n",
    "            .where(\n",
    "                qb.nearestNeighbor(\n",
    "                    field=\"title_embedding\",\n",
    "                    query_vector=\"embedding\",\n",
    "                    annotations={\"targetHits\": 100},\n",
    "                )\n",
    "                | qb.nearestNeighbor(\n",
    "                    field=\"chunk_embeddings\",\n",
    "                    query_vector=\"embedding\",\n",
    "                    annotations={\"targetHits\": 100},\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        \"query\": query_text,\n",
    "        \"ranking\": \"match-only\",\n",
    "        \"input.query(embedding)\": f\"embed({query_text})\",\n",
    "        \"presentation.summary\": \"no-chunks\",\n",
    "    }\n",
    "\n",
    "\n",
    "match_results = {}\n",
    "for evaluator_name, query_fn in [\n",
    "    (\"semantic\", match_semantic_query_fn),\n",
    "    (\"weakand\", match_weakand_query_fn),\n",
    "    (\"hybrid\", match_hybrid_query_fn),\n",
    "]:\n",
    "    print(f\"Evaluating {evaluator_name}...\")\n",
    "\n",
    "    match_evaluator = VespaMatchEvaluator(\n",
    "        queries=ids_to_query,\n",
    "        relevant_docs=relevant_docs,\n",
    "        vespa_query_fn=query_fn,\n",
    "        app=app,\n",
    "        name=\"test-run\",\n",
    "        id_field=\"id\",\n",
    "        write_csv=False,\n",
    "        write_verbose=False,  # optionally write verbose metrics to CSV\n",
    "    )\n",
    "\n",
    "    results = match_evaluator()\n",
    "    match_results[evaluator_name] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5924fd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>semantic</th>\n",
       "      <th>weakand</th>\n",
       "      <th>hybrid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>match_recall</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_recall_per_query</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_relevant_docs</th>\n",
       "      <td>51.00000</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>51.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_matched_relevant</th>\n",
       "      <td>51.00000</td>\n",
       "      <td>51.0000</td>\n",
       "      <td>51.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_matched_per_query</th>\n",
       "      <td>100.00000</td>\n",
       "      <td>88.7500</td>\n",
       "      <td>100.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_queries</th>\n",
       "      <td>20.00000</td>\n",
       "      <td>20.0000</td>\n",
       "      <td>20.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_avg</th>\n",
       "      <td>0.06275</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>0.04395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q50</th>\n",
       "      <td>0.03200</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>0.03750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q90</th>\n",
       "      <td>0.06400</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>0.08500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q95</th>\n",
       "      <td>0.10055</td>\n",
       "      <td>0.0703</td>\n",
       "      <td>0.08800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         semantic  weakand     hybrid\n",
       "match_recall              1.00000   1.0000    1.00000\n",
       "avg_recall_per_query      1.00000   1.0000    1.00000\n",
       "total_relevant_docs      51.00000  51.0000   51.00000\n",
       "total_matched_relevant   51.00000  51.0000   51.00000\n",
       "avg_matched_per_query   100.00000  88.7500  100.00000\n",
       "total_queries            20.00000  20.0000   20.00000\n",
       "searchtime_avg            0.06275   0.0330    0.04395\n",
       "searchtime_q50            0.03200   0.0290    0.03750\n",
       "searchtime_q90            0.06400   0.0511    0.08500\n",
       "searchtime_q95            0.10055   0.0703    0.08800"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(match_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074f9f7",
   "metadata": {},
   "source": [
    "### Tuning the retrieval phase\n",
    "\n",
    "We can see that all queries match all relevant documents, which is expected, since we use `targetHits:100` in the `nearestNeighbor` operator, and this is also the default for `weakAnd`(and `userQuery`). By setting `targetHits` lower, we can see that recall will drop.\n",
    "\n",
    "In general, you have these options if you want to increase recall:\n",
    "\n",
    "1. Increase `targetHits` in your retrieval operators (e.g., `nearestNeighbor`, `weakAnd`).\n",
    "2. Improve your embedding model (use a better model or finetune it on your data).\n",
    "3. You can also consider tuning HNSW parameters, see [docs on HNSW](https://docs.vespa.ai/en/approximate-nn-hnsw.html#using-vespas-approximate-nearest-neighbor-search).\n",
    "\n",
    "Conversely, if you want to reduce the latency of one of your retrieval 'arms' at the cost of a small trade-off in recall, you can:\n",
    "\n",
    "1. Tune `weakAnd` parameters. This has potential to 3x your performance for the `weakAnd`-parameter of your query, see [blog post](https://blog.vespa.ai/tripling-the-query-performance-of-lexical-search/).\n",
    "\n",
    "Below are some empirically found default parameters that work well for most use cases:\n",
    "\n",
    "```txt\n",
    "rank-profile optimized inherits baseline {\n",
    "    filter-threshold: 0.05\n",
    "    weakand {\n",
    "      stopword-limit: 0.6\n",
    "      adjust-target: 0.01\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "See the [reference](https://docs.vespa.ai/en/reference/schema-reference.html#weakand) for more details on the `weakAnd` parameters.\n",
    "These can also be set as query parameters.\n",
    "\n",
    "1. As already [mentioned](#consider-binary-vectors-for-recall), consider binary vectors for your embeddings.\n",
    "2. Consider using an embedding model with less dimensions, or using only a subset of the dimensions (e.g., using [Matryoshka embeddings](https://blog.vespa.ai/combining-matryoshka-with-binary-quantization-using-embedder/)).\n",
    "\n",
    "## First-phase ranking\n",
    "\n",
    "For the first-phase ranking, we must use a computationally cheap function, as it is applied to all documents matched in the retrieval phase. For many applications, this can amount to millions of candidate documents.\n",
    "\n",
    "Common options include (learned) linear combination of features including text similarity features, vector closeness, and metadata.\n",
    "It could also be a heuristic handwritten function.\n",
    "\n",
    "Text features should include [nativeRank](https://docs.vespa.ai/en/reference/nativerank.html#nativerank) or [bm25](https://docs.vespa.ai/en/reference/bm25.html#ranking-function) — not [fieldMatch](https://docs.vespa.ai/en/reference/rank-features.html#field-match-features-normalized) (it is too expensive).\n",
    "\n",
    "Considerations for deciding whether to choose `bm25` or `nativeRank`:\n",
    "\n",
    "* **bm25**: cheapest, strong significance, no proximity, not normalized.\n",
    "* **nativeRank**: 2 – 3 × costlier, truncated significance, includes proximity, normalized.\n",
    "\n",
    "For this blueprint, we opted for using `bm25` for first phase, but you could evaluate and compare to see whether the additional cost of using `nativeRank` is justified by increased quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b491868",
   "metadata": {},
   "source": [
    "### Collecting training data for first-phase ranking\n",
    "\n",
    "The features we will use for first-phase ranking are not normalized (ie. they have values in different ranges). This means we can't just weight them equally and expect that to be a good proxy for relevance.\n",
    "\n",
    "Below we will show how we can find (learn) optimal weights (coefficients) for each feature, so that we can combine them into a ranking-expression on the format:\n",
    "\n",
    "```python\n",
    "a * bm25(title) + b * bm25(chunks) + c * max_chunk_sim_scores() + d * max_chunk_text_scores() + e * avg_top_3_chunk_sim_scores() + f * avg_top_3_chunk_text_scores()\n",
    "```\n",
    "\n",
    "The first thing we need to is to collect training data.\n",
    "We do this using the [VespaFeatureCollector](https://vespa-engine.github.io/pyvespa/api/vespa/evaluation.html#vespa.evaluation.VespaFeatureCollector) from the pyvespa library.\n",
    "\n",
    "These are the features we will include:\n",
    "\n",
    "```txt\n",
    "rank-profile collect-training-data {\n",
    "        match-features {\n",
    "            bm25(title)\n",
    "            bm25(chunks)\n",
    "            max_chunk_sim_scores\n",
    "            max_chunk_text_scores\n",
    "            avg_top_3_chunk_sim_scores\n",
    "            avg_top_3_chunk_text_scores\n",
    "\n",
    "        }\n",
    "\n",
    "        # Since we need both binary embeddings (for match-phase) and float embeddings (for ranking) we define it as two inputs.\n",
    "        inputs {\n",
    "            query(embedding) tensor<int8>(x[96])\n",
    "            query(float_embedding) tensor<float>(x[768])\n",
    "        }\n",
    "\n",
    "        rank chunks {\n",
    "            element-gap: 0 # Fixed length chunking should not cause any positional gap between elements\n",
    "        }\n",
    "        function chunk_text_scores() {\n",
    "            expression: elementwise(bm25(chunks),chunk,float)\n",
    "        }\n",
    "\n",
    "        function chunk_emb_vecs() {\n",
    "            expression: unpack_bits(attribute(chunk_embeddings))\n",
    "        }\n",
    "\n",
    "        function chunk_dot_prod() {\n",
    "            expression: reduce(query(float_embedding) * chunk_emb_vecs(), sum, x)\n",
    "        }\n",
    "\n",
    "        function vector_norms(t) {\n",
    "            expression: sqrt(sum(pow(t, 2), x))\n",
    "        }\n",
    "        function chunk_sim_scores() {\n",
    "            expression: chunk_dot_prod() / (vector_norms(chunk_emb_vecs()) * vector_norms(query(float_embedding)))\n",
    "        }\n",
    "\n",
    "        function top_3_chunk_text_scores() {\n",
    "            expression: top(3, chunk_text_scores())\n",
    "        }\n",
    "\n",
    "        function top_3_chunk_sim_scores() {\n",
    "            expression: top(3, chunk_sim_scores())\n",
    "        }\n",
    "\n",
    "        function avg_top_3_chunk_text_scores() {\n",
    "            expression: reduce(top_3_chunk_text_scores(), avg, chunk)\n",
    "        }\n",
    "        function avg_top_3_chunk_sim_scores() {\n",
    "            expression: reduce(top_3_chunk_sim_scores(), avg, chunk)\n",
    "        }\n",
    "        \n",
    "        function max_chunk_text_scores() {\n",
    "            expression: reduce(chunk_text_scores(), max, chunk)\n",
    "        }\n",
    "\n",
    "        function max_chunk_sim_scores() {\n",
    "            expression: reduce(chunk_sim_scores(), max, chunk)\n",
    "        }\n",
    "\n",
    "        first-phase {\n",
    "            expression {\n",
    "                # Not used in this profile\n",
    "                bm25(title) + \n",
    "                bm25(chunks) +\n",
    "                max_chunk_sim_scores() +\n",
    "                max_chunk_text_scores()\n",
    "            }\n",
    "        }\n",
    "\n",
    "        second-phase {\n",
    "            expression: random\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "\n",
    "As you can see, we rely on the `bm25` and different vector similarity features (both document-level and chunk-level) for the first-phase ranking.\n",
    "These are relatively cheap to calculate, and will likely provide good enough ranking signals for the first-phase ranking.\n",
    "\n",
    "Running the command below will save a .csv-file with the collected features, which can be used to train a ranking model for the first-phase ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd18fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.application import Vespa\n",
    "from vespa.evaluation import VespaFeatureCollector\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def feature_collection_second_phase_query_fn(\n",
    "    query_text: str, top_k: int = 10, query_id: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert plain text into a JSON body for Vespa query with 'feature-collection' rank profile.\n",
    "    Includes both semantic similarity and BM25 matching with match features.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"yql\": str(\n",
    "            qb.select(\"*\")\n",
    "            .from_(\"doc\")\n",
    "            .where(\n",
    "                (\n",
    "                    qb.nearestNeighbor(\n",
    "                        field=\"title_embedding\",\n",
    "                        query_vector=\"embedding\",\n",
    "                        annotations={\n",
    "                            \"targetHits\": 100,\n",
    "                            \"label\": \"title_label\",\n",
    "                        },\n",
    "                    )\n",
    "                    | qb.nearestNeighbor(\n",
    "                        field=\"chunk_embeddings\",\n",
    "                        query_vector=\"embedding\",\n",
    "                        annotations={\n",
    "                            \"targetHits\": 100,\n",
    "                            \"label\": \"chunk_label\",\n",
    "                        },\n",
    "                    )\n",
    "                    | qb.userQuery(\n",
    "                        query_text,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        \"query\": query_text,\n",
    "        \"ranking\": \"collect-second-phase\",\n",
    "        \"input.query(embedding)\": f\"embed({query_text})\",\n",
    "        \"input.query(float_embedding)\": f\"embed({query_text})\",\n",
    "        \"hits\": top_k,\n",
    "        \"timeout\": \"10s\",\n",
    "        \"presentation.summary\": \"no-chunks\",\n",
    "        \"presentation.timing\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def feature_collection_first_phase_query_fn(\n",
    "    query_text: str, top_k: int = 10, query_id: str = None\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Convert plain text into a JSON body for Vespa query with 'feature-collection' rank profile.\n",
    "    Includes both semantic similarity and BM25 matching with match features.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"yql\": str(\n",
    "            qb.select(\"*\")\n",
    "            .from_(\"doc\")\n",
    "            .where(\n",
    "                (\n",
    "                    qb.nearestNeighbor(\n",
    "                        field=\"title_embedding\",\n",
    "                        query_vector=\"embedding\",\n",
    "                        annotations={\n",
    "                            \"targetHits\": 100,\n",
    "                            \"label\": \"title_label\",\n",
    "                        },\n",
    "                    )\n",
    "                    | qb.nearestNeighbor(\n",
    "                        field=\"chunk_embeddings\",\n",
    "                        query_vector=\"embedding\",\n",
    "                        annotations={\n",
    "                            \"targetHits\": 100,\n",
    "                            \"label\": \"chunk_label\",\n",
    "                        },\n",
    "                    )\n",
    "                    | qb.userQuery(\n",
    "                        query_text,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        \"query\": query_text,\n",
    "        \"ranking\": \"collect-training-data\",\n",
    "        \"input.query(embedding)\": f\"embed({query_text})\",\n",
    "        \"input.query(float_embedding)\": f\"embed({query_text})\",\n",
    "        \"hits\": top_k,\n",
    "        \"timeout\": \"10s\",\n",
    "        \"presentation.summary\": \"no-chunks\",\n",
    "        \"presentation.timing\": True,\n",
    "    }\n",
    "\n",
    "\n",
    "def generate_collector_name(\n",
    "    collect_matchfeatures: bool,\n",
    "    collect_rankfeatures: bool,\n",
    "    collect_summaryfeatures: bool,\n",
    "    second_phase: bool,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a collector name based on feature collection settings and phase.\n",
    "\n",
    "    Args:\n",
    "        collect_matchfeatures: Whether match features are being collected\n",
    "        collect_rankfeatures: Whether rank features are being collected\n",
    "        collect_summaryfeatures: Whether summary features are being collected\n",
    "        second_phase: Whether using second phase (True) or first phase (False)\n",
    "\n",
    "    Returns:\n",
    "        Generated collector name string\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    if collect_matchfeatures:\n",
    "        features.append(\"match\")\n",
    "    if collect_rankfeatures:\n",
    "        features.append(\"rank\")\n",
    "    if collect_summaryfeatures:\n",
    "        features.append(\"summary\")\n",
    "\n",
    "    features_str = \"_\".join(features) if features else \"nofeatures\"\n",
    "    phase_str = \"second_phase\" if second_phase else \"first_phase\"\n",
    "    return f\"{features_str}_{phase_str}\"\n",
    "\n",
    "\n",
    "feature_collector = VespaFeatureCollector(\n",
    "    queries=ids_to_query,\n",
    "    relevant_docs=relevant_docs,\n",
    "    vespa_query_fn=feature_collection_first_phase_query_fn,\n",
    "    app=app,\n",
    "    name=\"first-phase\",\n",
    "    id_field=\"id\",\n",
    "    collect_matchfeatures=True,\n",
    "    collect_summaryfeatures=False,\n",
    "    collect_rankfeatures=False,\n",
    "    write_csv=False,\n",
    "    random_hits_strategy=\"ratio\",\n",
    "    random_hits_value=1,\n",
    ")\n",
    "results = feature_collector.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81414294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relevance_label</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>match_avg_top_3_chunk_sim_scores</th>\n",
       "      <th>match_avg_top_3_chunk_text_scores</th>\n",
       "      <th>match_bm25(chunks)</th>\n",
       "      <th>match_bm25(title)</th>\n",
       "      <th>match_max_chunk_sim_scores</th>\n",
       "      <th>match_max_chunk_text_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.734995</td>\n",
       "      <td>0.358027</td>\n",
       "      <td>15.100841</td>\n",
       "      <td>23.010389</td>\n",
       "      <td>4.333828</td>\n",
       "      <td>0.391143</td>\n",
       "      <td>20.582403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.262686</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>12.327676</td>\n",
       "      <td>18.611592</td>\n",
       "      <td>2.453409</td>\n",
       "      <td>0.258905</td>\n",
       "      <td>15.644889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.060615</td>\n",
       "      <td>0.248329</td>\n",
       "      <td>8.444725</td>\n",
       "      <td>7.717984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.268457</td>\n",
       "      <td>8.444725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.994799</td>\n",
       "      <td>0.238926</td>\n",
       "      <td>3.608304</td>\n",
       "      <td>4.940433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262717</td>\n",
       "      <td>4.063323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.986948</td>\n",
       "      <td>0.265199</td>\n",
       "      <td>3.424351</td>\n",
       "      <td>3.615531</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.265199</td>\n",
       "      <td>3.424351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>alex_q_19</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.958641</td>\n",
       "      <td>0.210284</td>\n",
       "      <td>1.256423</td>\n",
       "      <td>2.238139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229001</td>\n",
       "      <td>1.967774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>alex_q_20</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.656100</td>\n",
       "      <td>0.337411</td>\n",
       "      <td>8.959117</td>\n",
       "      <td>12.534452</td>\n",
       "      <td>9.865092</td>\n",
       "      <td>0.402615</td>\n",
       "      <td>12.799867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>alex_q_20</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.306241</td>\n",
       "      <td>0.227978</td>\n",
       "      <td>8.462585</td>\n",
       "      <td>13.478890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.239757</td>\n",
       "      <td>13.353056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>alex_q_20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999038</td>\n",
       "      <td>0.200672</td>\n",
       "      <td>0.942418</td>\n",
       "      <td>0.871042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.206993</td>\n",
       "      <td>0.942418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>alex_q_20</td>\n",
       "      <td>45</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.964807</td>\n",
       "      <td>0.151361</td>\n",
       "      <td>2.288041</td>\n",
       "      <td>2.695306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151361</td>\n",
       "      <td>2.288041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_id doc_id  relevance_label  relevance_score  \\\n",
       "0    alex_q_01      1              1.0         0.734995   \n",
       "1    alex_q_01     82              1.0         0.262686   \n",
       "2    alex_q_01     50              1.0         0.060615   \n",
       "3    alex_q_01     64              0.0         0.994799   \n",
       "4    alex_q_01     21              0.0         0.986948   \n",
       "..         ...    ...              ...              ...   \n",
       "97   alex_q_19      4              0.0         0.958641   \n",
       "98   alex_q_20     20              1.0         0.656100   \n",
       "99   alex_q_20     35              1.0         0.306241   \n",
       "100  alex_q_20      2              0.0         0.999038   \n",
       "101  alex_q_20     45              0.0         0.964807   \n",
       "\n",
       "     match_avg_top_3_chunk_sim_scores  match_avg_top_3_chunk_text_scores  \\\n",
       "0                            0.358027                          15.100841   \n",
       "1                            0.225300                          12.327676   \n",
       "2                            0.248329                           8.444725   \n",
       "3                            0.238926                           3.608304   \n",
       "4                            0.265199                           3.424351   \n",
       "..                                ...                                ...   \n",
       "97                           0.210284                           1.256423   \n",
       "98                           0.337411                           8.959117   \n",
       "99                           0.227978                           8.462585   \n",
       "100                          0.200672                           0.942418   \n",
       "101                          0.151361                           2.288041   \n",
       "\n",
       "     match_bm25(chunks)  match_bm25(title)  match_max_chunk_sim_scores  \\\n",
       "0             23.010389           4.333828                    0.391143   \n",
       "1             18.611592           2.453409                    0.258905   \n",
       "2              7.717984           0.000000                    0.268457   \n",
       "3              4.940433           0.000000                    0.262717   \n",
       "4              3.615531           0.000000                    0.265199   \n",
       "..                  ...                ...                         ...   \n",
       "97             2.238139           0.000000                    0.229001   \n",
       "98            12.534452           9.865092                    0.402615   \n",
       "99            13.478890           0.000000                    0.239757   \n",
       "100            0.871042           0.000000                    0.206993   \n",
       "101            2.695306           0.000000                    0.151361   \n",
       "\n",
       "     match_max_chunk_text_scores  \n",
       "0                      20.582403  \n",
       "1                      15.644889  \n",
       "2                       8.444725  \n",
       "3                       4.063323  \n",
       "4                       3.424351  \n",
       "..                           ...  \n",
       "97                      1.967774  \n",
       "98                     12.799867  \n",
       "99                     13.353056  \n",
       "100                     0.942418  \n",
       "101                     2.288041  \n",
       "\n",
       "[102 rows x 10 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df = pd.DataFrame(results[\"results\"])\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed92829a",
   "metadata": {},
   "source": [
    "Note that the `relevance_score` in this table is just the random expression we used in the `second-phase` of the `collect-training-data` rank profile, and will be dropped before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9c17d0",
   "metadata": {},
   "source": [
    "### Training a first-phase ranking model\n",
    "\n",
    "As you recall, a first-phase ranking expression must be cheap to evaluate.\n",
    "This most often means a heuristic handwritten combination of match features, or a linear model trained on match features.\n",
    "\n",
    "We will demonstrate how to train a simple Logistic Regression model to predict relevance based on the collected match features.\n",
    "The full training script can be found in the [sample-apps repository](https://github.com/vespa-engine/sample-apps/blob/master/rag-blueprint/eval/train_logistic_regression.py). \n",
    "\n",
    "Some \"gotchas\" to be aware of:\n",
    "\n",
    "* We sample an equal number of relevant and random documents for each query, to avoid class imbalance.\n",
    "* We make sure that we drop `query_id` and `doc_id` columns before training.\n",
    "* We apply standard scaling to the features before training the model. We apply the inverse transform to the model coefficients after training, so that we can use them in Vespa.\n",
    "* We do 5-fold stratified cross-validation to evaluate the model performance, ensuring that each fold has a balanced number of relevant and random documents.\n",
    "* We also make sure to have an unseen set of test queries to evaluate the model on, to avoid overfitting.\n",
    "\n",
    "Run the cell below to train the model and get the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5b4d51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cross-Validation Results ---\n",
      "       Metric     Mean  Std Dev\n",
      "     Accuracy 0.659524 0.115234\n",
      "    Precision 0.623102 0.085545\n",
      "       Recall 1.000000 0.000000\n",
      "     F1-Score 0.764337 0.065585\n",
      "     Log Loss 0.639436 0.014668\n",
      "      ROC AUC 0.974949 0.019901\n",
      "Avg Precision 0.979207 0.018465\n",
      "\n",
      "========================================\n",
      "\n",
      "--- Model Coefficients ---\n",
      "                          Feature  Coefficient (Standardized)  Coefficient (Original)\n",
      " match_avg_top_3_chunk_sim_scores                    0.034383                0.421609\n",
      "match_avg_top_3_chunk_text_scores                    0.031768                0.006793\n",
      "               match_bm25(chunks)                    0.031909                0.004862\n",
      "                match_bm25(title)                    0.021095                0.008671\n",
      "       match_max_chunk_sim_scores                    0.034131                0.352846\n",
      "      match_max_chunk_text_scores                    0.032141                0.005228\n",
      "                        Intercept                    0.158401               -0.143366\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "\n",
    "def get_coefficients_info(model, features, intercept, scaler):\n",
    "    \"\"\"\n",
    "    Returns the model coefficients as a dictionary that accounts for standardization.\n",
    "    The transformation allows the model to be expressed in terms of the original, unscaled features.\n",
    "    \"\"\"\n",
    "    # For standardized features, the transformation is z = (x - mean) / std.\n",
    "    # The original expression 'coef * z + intercept' becomes:\n",
    "    # (coef / std) * x + (intercept - coef * mean / std)\n",
    "    transformed_coefs = model.coef_[0] / scaler.scale_\n",
    "    transformed_intercept = intercept - np.sum(\n",
    "        model.coef_[0] * scaler.mean_ / scaler.scale_\n",
    "    )\n",
    "\n",
    "    # Create a mathematical expression for the model using original (unscaled) features\n",
    "    expression_parts = [f\"{transformed_intercept:.6f}\"]\n",
    "    for feature, coef in zip(features, transformed_coefs):\n",
    "        expression_parts.append(f\"{coef:+.6f}*{feature}\")\n",
    "    expression = \"\".join(expression_parts)\n",
    "\n",
    "    # Return a dictionary containing scaling parameters and coefficient information\n",
    "    return {\n",
    "        \"expression\": expression,\n",
    "        \"feature_means\": dict(zip(features, scaler.mean_)),\n",
    "        \"feature_stds\": dict(zip(features, scaler.scale_)),\n",
    "        \"original_coefficients\": dict(zip(features, model.coef_[0])),\n",
    "        \"original_intercept\": float(intercept),\n",
    "        \"transformed_coefficients\": dict(zip(features, transformed_coefs)),\n",
    "        \"transformed_intercept\": float(transformed_intercept),\n",
    "    }\n",
    "\n",
    "\n",
    "def perform_cross_validation(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Loads data, applies standardization, and performs 5-fold stratified cross-validation.\n",
    "\n",
    "    Args:\n",
    "        df: A pandas DataFrame with features and a 'relevance_label' target column.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing two pandas DataFrames:\n",
    "        - cv_results_df: The mean and standard deviation of evaluation metrics.\n",
    "        - coef_df: The model coefficients for both scaled and unscaled features.\n",
    "    \"\"\"\n",
    "    # Define and drop irrelevant columns\n",
    "    columns_to_drop = [\"doc_id\", \"query_id\", \"relevance_score\"]\n",
    "    # Drop only the columns that exist in the DataFrame\n",
    "    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "    df[\"relevance_label\"] = df[\"relevance_label\"].astype(int)\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    X = df.drop(columns=[\"relevance_label\"])\n",
    "    features = X.columns.tolist()\n",
    "    y = df[\"relevance_label\"]\n",
    "\n",
    "    # Initialize StandardScaler, model, and cross-validator\n",
    "    scaler = StandardScaler()\n",
    "    N_SPLITS = 5\n",
    "    skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "    model = LogisticRegression(C=0.001, tol=1e-2, random_state=42)\n",
    "\n",
    "    # Lists to store metrics for each fold\n",
    "    metrics = {\n",
    "        \"Accuracy\": [],\n",
    "        \"Precision\": [],\n",
    "        \"Recall\": [],\n",
    "        \"F1-Score\": [],\n",
    "        \"Log Loss\": [],\n",
    "        \"ROC AUC\": [],\n",
    "        \"Avg Precision\": [],\n",
    "    }\n",
    "\n",
    "    # Perform 5-Fold Stratified Cross-Validation\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Fit scaler on training data and transform both sets\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Train the model and make predictions\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "        # Calculate and store metrics for the fold\n",
    "        metrics[\"Accuracy\"].append(accuracy_score(y_test, y_pred))\n",
    "        metrics[\"Precision\"].append(precision_score(y_test, y_pred, zero_division=0))\n",
    "        metrics[\"Recall\"].append(recall_score(y_test, y_pred, zero_division=0))\n",
    "        metrics[\"F1-Score\"].append(f1_score(y_test, y_pred, zero_division=0))\n",
    "        metrics[\"Log Loss\"].append(log_loss(y_test, y_pred_proba))\n",
    "        metrics[\"ROC AUC\"].append(roc_auc_score(y_test, y_pred_proba))\n",
    "        metrics[\"Avg Precision\"].append(average_precision_score(y_test, y_pred_proba))\n",
    "\n",
    "    # --- Prepare Results DataFrames ---\n",
    "\n",
    "    # Create DataFrame for cross-validation results\n",
    "    cv_results = {\n",
    "        \"Metric\": list(metrics.keys()),\n",
    "        \"Mean\": [np.mean(v) for v in metrics.values()],\n",
    "        \"Std Dev\": [np.std(v) for v in metrics.values()],\n",
    "    }\n",
    "    cv_results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "    # Retrain on full standardized data to get final coefficients\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    model.fit(X_scaled, y)\n",
    "\n",
    "    # Get transformed coefficients for original (unscaled) features\n",
    "    coef_info = get_coefficients_info(model, features, model.intercept_[0], scaler)\n",
    "\n",
    "    # Create DataFrame for coefficients\n",
    "    coef_data = {\n",
    "        \"Feature\": features + [\"Intercept\"],\n",
    "        \"Coefficient (Standardized)\": np.append(model.coef_[0], model.intercept_[0]),\n",
    "        \"Coefficient (Original)\": np.append(\n",
    "            list(coef_info[\"transformed_coefficients\"].values()),\n",
    "            coef_info[\"transformed_intercept\"],\n",
    "        ),\n",
    "    }\n",
    "    coef_df = pd.DataFrame(coef_data)\n",
    "\n",
    "    return cv_results_df, coef_df\n",
    "\n",
    "\n",
    "# Perform cross-validation and get the results\n",
    "cv_results_df, coefficients_df = perform_cross_validation(feature_df)\n",
    "\n",
    "# Print the results\n",
    "print(\"--- Cross-Validation Results ---\")\n",
    "print(cv_results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\" * 40 + \"\\n\")\n",
    "print(\"--- Model Coefficients ---\")\n",
    "print(coefficients_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2fe8840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient (Standardized)</th>\n",
       "      <th>Coefficient (Original)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>match_avg_top_3_chunk_sim_scores</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.421609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>match_avg_top_3_chunk_text_scores</td>\n",
       "      <td>0.031768</td>\n",
       "      <td>0.006793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>match_bm25(chunks)</td>\n",
       "      <td>0.031909</td>\n",
       "      <td>0.004862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>match_bm25(title)</td>\n",
       "      <td>0.021095</td>\n",
       "      <td>0.008671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>match_max_chunk_sim_scores</td>\n",
       "      <td>0.034131</td>\n",
       "      <td>0.352846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>match_max_chunk_text_scores</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>0.005228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>0.158401</td>\n",
       "      <td>-0.143366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Feature  Coefficient (Standardized)  \\\n",
       "0   match_avg_top_3_chunk_sim_scores                    0.034383   \n",
       "1  match_avg_top_3_chunk_text_scores                    0.031768   \n",
       "2                 match_bm25(chunks)                    0.031909   \n",
       "3                  match_bm25(title)                    0.021095   \n",
       "4         match_max_chunk_sim_scores                    0.034131   \n",
       "5        match_max_chunk_text_scores                    0.032141   \n",
       "6                          Intercept                    0.158401   \n",
       "\n",
       "   Coefficient (Original)  \n",
       "0                0.421609  \n",
       "1                0.006793  \n",
       "2                0.004862  \n",
       "3                0.008671  \n",
       "4                0.352846  \n",
       "5                0.005228  \n",
       "6               -0.143366  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af7bc26",
   "metadata": {},
   "source": [
    "Which seems quite good. With such a small dataset however, it is easy to overfit. Let us evaluate on the unseen test queries to see how well the model generalizes.\n",
    "\n",
    "First, we need to add the learned coefficients as inputs to a new rank profile in our schema, so that we can use them in Vespa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f1bd661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```txt\n",
       "rank-profile learned-linear inherits base-features {\n",
       "        match-features: \n",
       "        inputs {\n",
       "            query(embedding) tensor<int8>(x[96])\n",
       "            query(float_embedding) tensor<float>(x[768])\n",
       "            query(intercept) double\n",
       "            query(avg_top_3_chunk_sim_scores_param) double\n",
       "            query(avg_top_3_chunk_text_scores_param) double\n",
       "            query(bm25_chunks_param) double\n",
       "            query(bm25_title_param) double\n",
       "            query(max_chunk_sim_scores_param) double\n",
       "            query(max_chunk_text_scores_param) double\n",
       "        }\n",
       "        first-phase {\n",
       "            expression {\n",
       "                query(intercept) + \n",
       "                query(avg_top_3_chunk_sim_scores_param) * avg_top_3_chunk_sim_scores() +\n",
       "                query(avg_top_3_chunk_text_scores_param) * avg_top_3_chunk_text_scores() +\n",
       "                query(bm25_title_param) * bm25(title) + \n",
       "                query(bm25_chunks_param) * bm25(chunks) +\n",
       "                query(max_chunk_sim_scores_param) * max_chunk_sim_scores() +\n",
       "                query(max_chunk_text_scores_param) * max_chunk_text_scores()\n",
       "            }\n",
       "        }\n",
       "        summary-features {\n",
       "            top_3_chunk_sim_scores\n",
       "        }\n",
       "        \n",
       "    }\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learned_linear_rp = (\n",
    "    repo_root / \"app\" / \"schemas\" / \"doc\" / \"learned-linear.profile\"\n",
    ").read_text()\n",
    "display_md(learned_linear_rp, tag=\"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55216537",
   "metadata": {},
   "source": [
    "To allow for changing the parameters without redeploying the application, we will also add the values of the coefficients as query parameters to a new query profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73d429a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<!--\n",
       "match_avg_top_3_chunk_sim_scores   : 13.383840\n",
       "match_avg_top_3_chunk_text_scores  : 0.203145\n",
       "match_bm25(chunks)                 : 0.159914\n",
       "match_bm25(title)                  : 0.191867\n",
       "match_max_chunk_sim_scores         : 10.067169\n",
       "match_max_chunk_text_scores        : 0.153392\n",
       "Intercept                          : -7.798639\n",
       "-->\n",
       "<query-profile id=\"hybrid\">\n",
       "    <field name=\"schema\">doc</field>\n",
       "    <field name=\"ranking.features.query(embedding)\">embed(@query)</field>\n",
       "    <field name=\"ranking.features.query(float_embedding)\">embed(@query)</field>\n",
       "    <field name=\"ranking.features.query(intercept)\">-7.798639</field>\n",
       "    <field name=\"ranking.features.query(avg_top_3_chunk_sim_scores_param)\">13.383840</field>\n",
       "    <field name=\"ranking.features.query(avg_top_3_chunk_text_scores_param)\">0.203145</field>\n",
       "    <field name=\"ranking.features.query(bm25_chunks_param)\">0.159914</field>\n",
       "    <field name=\"ranking.features.query(bm25_title_param)\">0.191867</field>\n",
       "    <field name=\"ranking.features.query(max_chunk_sim_scores_param)\">10.067169</field>\n",
       "    <field name=\"ranking.features.query(max_chunk_text_scores_param)\">0.153392</field>\n",
       "    <field name=\"yql\">\n",
       "        select *\n",
       "        from %{schema}\n",
       "        where userInput(@query) or\n",
       "        ({label:\"title_label\", targetHits:100}nearestNeighbor(title_embedding, embedding)) or\n",
       "        ({label:\"chunks_label\", targetHits:100}nearestNeighbor(chunk_embeddings, embedding))\n",
       "    </field>\n",
       "    <field name=\"hits\">10</field>\n",
       "    <field name=\"ranking.profile\">learned-linear</field>\n",
       "    <field name=\"presentation.summary\">top_3_chunks</field>\n",
       "</query-profile>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_md(hybrid_qp, tag=\"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74258899",
   "metadata": {},
   "source": [
    "### Evaluating first-phase ranking\n",
    "\n",
    "Now we are ready to evaluate our first-phase ranking function.\n",
    "We can use the [VespaEvaluator](https://vespa-engine.github.io/pyvespa/evaluating-vespa-application-cloud.html#vespaevaluator) to evaluate the first-phase ranking function on the unseen test queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25b9562c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries_file = repo_root / \"dataset\" / \"test_queries.json\"\n",
    "\n",
    "with open(test_queries_file) as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "test_ids_to_query = {query[\"query_id\"]: query[\"query_text\"] for query in test_queries}\n",
    "test_relevant_docs = {\n",
    "    query[\"query_id\"]: set(query[\"relevant_document_ids\"])\n",
    "    for query in test_queries\n",
    "    if \"relevant_document_ids\" in query\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9a71a9",
   "metadata": {},
   "source": [
    "We need to parse the coefficients into the required format for input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f19e308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Coefficient (Standardized)</th>\n",
       "      <th>Coefficient (Original)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>match_avg_top_3_chunk_sim_scores</td>\n",
       "      <td>0.034383</td>\n",
       "      <td>0.421609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>match_avg_top_3_chunk_text_scores</td>\n",
       "      <td>0.031768</td>\n",
       "      <td>0.006793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>match_bm25(chunks)</td>\n",
       "      <td>0.031909</td>\n",
       "      <td>0.004862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>match_bm25(title)</td>\n",
       "      <td>0.021095</td>\n",
       "      <td>0.008671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>match_max_chunk_sim_scores</td>\n",
       "      <td>0.034131</td>\n",
       "      <td>0.352846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>match_max_chunk_text_scores</td>\n",
       "      <td>0.032141</td>\n",
       "      <td>0.005228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>0.158401</td>\n",
       "      <td>-0.143366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Feature  Coefficient (Standardized)  \\\n",
       "0   match_avg_top_3_chunk_sim_scores                    0.034383   \n",
       "1  match_avg_top_3_chunk_text_scores                    0.031768   \n",
       "2                 match_bm25(chunks)                    0.031909   \n",
       "3                  match_bm25(title)                    0.021095   \n",
       "4         match_max_chunk_sim_scores                    0.034131   \n",
       "5        match_max_chunk_text_scores                    0.032141   \n",
       "6                          Intercept                    0.158401   \n",
       "\n",
       "   Coefficient (Original)  \n",
       "0                0.421609  \n",
       "1                0.006793  \n",
       "2                0.004862  \n",
       "3                0.008671  \n",
       "4                0.352846  \n",
       "5                0.005228  \n",
       "6               -0.143366  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b35964e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Feature': {0: 'match_avg_top_3_chunk_sim_scores',\n",
       "  1: 'match_avg_top_3_chunk_text_scores',\n",
       "  2: 'match_bm25(chunks)',\n",
       "  3: 'match_bm25(title)',\n",
       "  4: 'match_max_chunk_sim_scores',\n",
       "  5: 'match_max_chunk_text_scores',\n",
       "  6: 'Intercept'},\n",
       " 'Coefficient (Standardized)': {0: 0.03438259396169029,\n",
       "  1: 0.031767760839597856,\n",
       "  2: 0.03190853104175455,\n",
       "  3: 0.021094809721098663,\n",
       "  4: 0.03413143203194206,\n",
       "  5: 0.0321408033796812,\n",
       "  6: 0.1584007329169953},\n",
       " 'Coefficient (Original)': {0: 0.421609061801165,\n",
       "  1: 0.0067931485936015825,\n",
       "  2: 0.004861617295220699,\n",
       "  3: 0.008671224628375315,\n",
       "  4: 0.3528463496849927,\n",
       "  5: 0.005227988942349101,\n",
       "  6: -0.14336597939520906}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_dict = coefficients_df.to_dict()\n",
    "coef_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "770c1f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input.query(avg_top_3_chunk_sim_scores_param)': 0.421609061801165,\n",
       " 'input.query(avg_top_3_chunk_text_scores_param)': 0.0067931485936015825,\n",
       " 'input.query(bm25_chunks_param)': 0.004861617295220699,\n",
       " 'input.query(bm25_title_param)': 0.008671224628375315,\n",
       " 'input.query(max_chunk_sim_scores_param)': 0.3528463496849927,\n",
       " 'input.query(max_chunk_text_scores_param)': 0.005227988942349101,\n",
       " 'input.query(intercept)': -0.14336597939520906}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_key(feature):\n",
    "    \"\"\"Formats the feature string into the desired key format.\"\"\"\n",
    "    if feature == \"Intercept\":\n",
    "        return \"input.query(intercept)\"\n",
    "    name = feature.removeprefix(\"match_\").replace(\"(\", \"_\").replace(\")\", \"\")\n",
    "    return f\"input.query({name}_param)\"\n",
    "\n",
    "\n",
    "linear_params = {\n",
    "    format_key(feature): coef_dict[\"Coefficient (Original)\"][i]\n",
    "    for i, feature in enumerate(coef_dict[\"Feature\"].values())\n",
    "}\n",
    "linear_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63804c9b",
   "metadata": {},
   "source": [
    "We run the evaluation script on a set of unseen test queries, and get the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdcd76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the project root.\n",
    "from vespa.evaluation import VespaEvaluator\n",
    "from vespa.application import Vespa\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def rank_first_phase_query_fn(query_text: str, top_k: int) -> dict:\n",
    "    return {\n",
    "        \"yql\": str(\n",
    "            qb.select(\"*\")\n",
    "            .from_(VESPA_SCHEMA_NAME)\n",
    "            .where(\n",
    "                qb.nearestNeighbor(\n",
    "                    field=\"title_embedding\",\n",
    "                    query_vector=\"embedding\",\n",
    "                    annotations={\"targetHits\": 100},\n",
    "                )\n",
    "                | qb.nearestNeighbor(\n",
    "                    field=\"chunk_embeddings\",\n",
    "                    query_vector=\"embedding\",\n",
    "                    annotations={\"targetHits\": 100},\n",
    "                )\n",
    "                | qb.userQuery(\n",
    "                    query_text,\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        \"hits\": top_k,\n",
    "        \"query\": query_text,\n",
    "        \"ranking.profile\": \"learned-linear\",\n",
    "        \"input.query(embedding)\": f\"embed({query_text})\",\n",
    "        \"input.query(float_embedding)\": f\"embed({query_text})\",\n",
    "        \"presentation.summary\": \"no-chunks\",\n",
    "    } | linear_params\n",
    "\n",
    "\n",
    "first_phase_evaluator = VespaEvaluator(\n",
    "    queries=test_ids_to_query,\n",
    "    relevant_docs=test_relevant_docs,\n",
    "    vespa_query_fn=rank_first_phase_query_fn,\n",
    "    id_field=\"id\",\n",
    "    app=app,\n",
    "    name=\"first-phase-evaluation\",\n",
    "    write_csv=False,\n",
    "    precision_recall_at_k=[10, 20],\n",
    ")\n",
    "\n",
    "first_phase_results = first_phase_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c789bfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy@1': 1.0,\n",
       " 'accuracy@3': 1.0,\n",
       " 'accuracy@5': 1.0,\n",
       " 'accuracy@10': 1.0,\n",
       " 'precision@10': 0.23500000000000001,\n",
       " 'recall@10': 0.9405303030303032,\n",
       " 'precision@20': 0.1275,\n",
       " 'recall@20': 0.990909090909091,\n",
       " 'mrr@10': 1.0,\n",
       " 'ndcg@10': 0.8893451868887793,\n",
       " 'map@100': 0.8183245416199961,\n",
       " 'searchtime_avg': 0.04085000000000001,\n",
       " 'searchtime_q50': 0.0425,\n",
       " 'searchtime_q90': 0.06040000000000004,\n",
       " 'searchtime_q95': 0.08305000000000001}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_phase_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0c61b0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy@1</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy@3</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy@5</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy@10</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision@10</th>\n",
       "      <td>0.235000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall@10</th>\n",
       "      <td>0.940530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision@20</th>\n",
       "      <td>0.127500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall@20</th>\n",
       "      <td>0.990909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrr@10</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndcg@10</th>\n",
       "      <td>0.889345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map@100</th>\n",
       "      <td>0.818325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_avg</th>\n",
       "      <td>0.040850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q50</th>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q90</th>\n",
       "      <td>0.060400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q95</th>\n",
       "      <td>0.083050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   value\n",
       "accuracy@1      1.000000\n",
       "accuracy@3      1.000000\n",
       "accuracy@5      1.000000\n",
       "accuracy@10     1.000000\n",
       "precision@10    0.235000\n",
       "recall@10       0.940530\n",
       "precision@20    0.127500\n",
       "recall@20       0.990909\n",
       "mrr@10          1.000000\n",
       "ndcg@10         0.889345\n",
       "map@100         0.818325\n",
       "searchtime_avg  0.040850\n",
       "searchtime_q50  0.042500\n",
       "searchtime_q90  0.060400\n",
       "searchtime_q95  0.083050"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_phase_df = pd.DataFrame(first_phase_results, index=[\"value\"]).T\n",
    "first_phase_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e796fc4",
   "metadata": {},
   "source": [
    "For the first phase ranking, we care most about recall, as we just want to make sure that the candidate documents are ranked high enough to be included in the second-phase ranking. (the default number of documents that will be exposed to second-phase is 10 000, but can be controlled by the `rerank-count` parameter).\n",
    "\n",
    "We can see that our results are already very good. This is of course due to the fact that we have a small,synthetic dataset. In reality, you should align the metric expectations with your dataset and test queries.\n",
    "\n",
    "We can also see that our search time is quite fast, with an average of 22ms. You should consider whether this is well within your latency budget, as you want some headroom for second-phase ranking.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b25115",
   "metadata": {},
   "source": [
    "## Second-phase ranking\n",
    "\n",
    "For the second-phase ranking, we can afford to use a more expensive ranking expression, since we will only run it on the top-k documents from the first-phase ranking (defined by the `rerank-count` parameter, which defaults to 10,000 documents).\n",
    "\n",
    "This is where we can significantly improve ranking quality by using more sophisticated models and features that would be too expensive to compute for all matched documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a4cb65",
   "metadata": {},
   "source": [
    "### Collecting features for second-phase ranking\n",
    "\n",
    "For second-phase ranking, we request Vespa's default set of rank features, which includes a comprehensive set of text features. See the [rank features documentation](https://docs.vespa.ai/en/reference/rank-features.html) for complete details.\n",
    "\n",
    "We can collect both match features and rank features by running the same code as we did for first-phase ranking, with  some additional parameters to collect rank features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1671a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_phase_collector = VespaFeatureCollector(\n",
    "    queries=ids_to_query,\n",
    "    relevant_docs=relevant_docs,\n",
    "    vespa_query_fn=feature_collection_second_phase_query_fn,\n",
    "    app=app,\n",
    "    name=\"second-phase\",\n",
    "    id_field=\"id\",\n",
    "    collect_matchfeatures=True,\n",
    "    collect_summaryfeatures=False,\n",
    "    collect_rankfeatures=True,\n",
    "    write_csv=False,\n",
    "    random_hits_strategy=\"ratio\",\n",
    "    random_hits_value=1,\n",
    ")\n",
    "second_phase_features = second_phase_collector.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4294a010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>relevance_label</th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>match_avg_top_3_chunk_sim_scores</th>\n",
       "      <th>match_avg_top_3_chunk_text_scores</th>\n",
       "      <th>match_bm25(chunks)</th>\n",
       "      <th>match_bm25(title)</th>\n",
       "      <th>match_is_favorite</th>\n",
       "      <th>match_max_chunk_sim_scores</th>\n",
       "      <th>...</th>\n",
       "      <th>rank_term(3).significance</th>\n",
       "      <th>rank_term(3).weight</th>\n",
       "      <th>rank_term(4).connectedness</th>\n",
       "      <th>rank_term(4).significance</th>\n",
       "      <th>rank_term(4).weight</th>\n",
       "      <th>rank_textSimilarity(title).fieldCoverage</th>\n",
       "      <th>rank_textSimilarity(title).order</th>\n",
       "      <th>rank_textSimilarity(title).proximity</th>\n",
       "      <th>rank_textSimilarity(title).queryCoverage</th>\n",
       "      <th>rank_textSimilarity(title).score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.928815</td>\n",
       "      <td>0.358027</td>\n",
       "      <td>15.100841</td>\n",
       "      <td>23.010389</td>\n",
       "      <td>4.333828</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791824</td>\n",
       "      <td>0.248329</td>\n",
       "      <td>8.444725</td>\n",
       "      <td>7.717984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.268457</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.271836</td>\n",
       "      <td>0.225300</td>\n",
       "      <td>12.327676</td>\n",
       "      <td>18.611592</td>\n",
       "      <td>2.453409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.258905</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.322500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.982272</td>\n",
       "      <td>0.231970</td>\n",
       "      <td>5.111429</td>\n",
       "      <td>7.128779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alex_q_01</td>\n",
       "      <td>24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.975659</td>\n",
       "      <td>0.201503</td>\n",
       "      <td>2.404518</td>\n",
       "      <td>2.680087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.201503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.560104</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>alex_q_19</td>\n",
       "      <td>58</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.990156</td>\n",
       "      <td>0.136911</td>\n",
       "      <td>2.231116</td>\n",
       "      <td>2.606189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.136911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548752</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.558248</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>alex_q_20</td>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.618527</td>\n",
       "      <td>0.337411</td>\n",
       "      <td>8.959117</td>\n",
       "      <td>12.534452</td>\n",
       "      <td>9.865092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558248</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>alex_q_20</td>\n",
       "      <td>35</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.617958</td>\n",
       "      <td>0.227978</td>\n",
       "      <td>8.462585</td>\n",
       "      <td>13.478890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.239757</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558248</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>alex_q_20</td>\n",
       "      <td>63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.979987</td>\n",
       "      <td>0.182378</td>\n",
       "      <td>3.131521</td>\n",
       "      <td>5.032468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.183292</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558248</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>alex_q_20</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.977501</td>\n",
       "      <td>0.157868</td>\n",
       "      <td>2.246247</td>\n",
       "      <td>2.442976</td>\n",
       "      <td>1.388680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558248</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.524369</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.335833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      query_id doc_id  relevance_label  relevance_score  \\\n",
       "0    alex_q_01      1              1.0         0.928815   \n",
       "1    alex_q_01     50              1.0         0.791824   \n",
       "2    alex_q_01     82              1.0         0.271836   \n",
       "3    alex_q_01     34              0.0         0.982272   \n",
       "4    alex_q_01     24              0.0         0.975659   \n",
       "..         ...    ...              ...              ...   \n",
       "97   alex_q_19     58              0.0         0.990156   \n",
       "98   alex_q_20     20              1.0         0.618527   \n",
       "99   alex_q_20     35              1.0         0.617958   \n",
       "100  alex_q_20     63              0.0         0.979987   \n",
       "101  alex_q_20     32              0.0         0.977501   \n",
       "\n",
       "     match_avg_top_3_chunk_sim_scores  match_avg_top_3_chunk_text_scores  \\\n",
       "0                            0.358027                          15.100841   \n",
       "1                            0.248329                           8.444725   \n",
       "2                            0.225300                          12.327676   \n",
       "3                            0.231970                           5.111429   \n",
       "4                            0.201503                           2.404518   \n",
       "..                                ...                                ...   \n",
       "97                           0.136911                           2.231116   \n",
       "98                           0.337411                           8.959117   \n",
       "99                           0.227978                           8.462585   \n",
       "100                          0.182378                           3.131521   \n",
       "101                          0.157868                           2.246247   \n",
       "\n",
       "     match_bm25(chunks)  match_bm25(title)  match_is_favorite  \\\n",
       "0             23.010389           4.333828                1.0   \n",
       "1              7.717984           0.000000                0.0   \n",
       "2             18.611592           2.453409                1.0   \n",
       "3              7.128779           0.000000                0.0   \n",
       "4              2.680087           0.000000                1.0   \n",
       "..                  ...                ...                ...   \n",
       "97             2.606189           0.000000                0.0   \n",
       "98            12.534452           9.865092                0.0   \n",
       "99            13.478890           0.000000                0.0   \n",
       "100            5.032468           0.000000                1.0   \n",
       "101            2.442976           1.388680                0.0   \n",
       "\n",
       "     match_max_chunk_sim_scores  ...  rank_term(3).significance  \\\n",
       "0                      0.391143  ...                   0.524369   \n",
       "1                      0.268457  ...                   0.524369   \n",
       "2                      0.258905  ...                   0.524369   \n",
       "3                      0.257180  ...                   0.524369   \n",
       "4                      0.201503  ...                   0.524369   \n",
       "..                          ...  ...                        ...   \n",
       "97                     0.136911  ...                   0.548752   \n",
       "98                     0.402615  ...                   0.558248   \n",
       "99                     0.239757  ...                   0.558248   \n",
       "100                    0.183292  ...                   0.558248   \n",
       "101                    0.157868  ...                   0.558248   \n",
       "\n",
       "     rank_term(3).weight  rank_term(4).connectedness  \\\n",
       "0                  100.0                         0.1   \n",
       "1                  100.0                         0.1   \n",
       "2                  100.0                         0.1   \n",
       "3                  100.0                         0.1   \n",
       "4                  100.0                         0.1   \n",
       "..                   ...                         ...   \n",
       "97                 100.0                         0.1   \n",
       "98                 100.0                         0.1   \n",
       "99                 100.0                         0.1   \n",
       "100                100.0                         0.1   \n",
       "101                100.0                         0.1   \n",
       "\n",
       "     rank_term(4).significance  rank_term(4).weight  \\\n",
       "0                     0.560104                100.0   \n",
       "1                     0.560104                100.0   \n",
       "2                     0.560104                100.0   \n",
       "3                     0.560104                100.0   \n",
       "4                     0.560104                100.0   \n",
       "..                         ...                  ...   \n",
       "97                    0.558248                100.0   \n",
       "98                    0.524369                100.0   \n",
       "99                    0.524369                100.0   \n",
       "100                   0.524369                100.0   \n",
       "101                   0.524369                100.0   \n",
       "\n",
       "     rank_textSimilarity(title).fieldCoverage  \\\n",
       "0                                    0.400000   \n",
       "1                                    0.000000   \n",
       "2                                    0.200000   \n",
       "3                                    0.000000   \n",
       "4                                    0.000000   \n",
       "..                                        ...   \n",
       "97                                   0.000000   \n",
       "98                                   0.833333   \n",
       "99                                   0.000000   \n",
       "100                                  0.000000   \n",
       "101                                  0.200000   \n",
       "\n",
       "     rank_textSimilarity(title).order  rank_textSimilarity(title).proximity  \\\n",
       "0                                 1.0                                  1.00   \n",
       "1                                 0.0                                  0.00   \n",
       "2                                 0.0                                  0.75   \n",
       "3                                 0.0                                  0.00   \n",
       "4                                 0.0                                  0.00   \n",
       "..                                ...                                   ...   \n",
       "97                                0.0                                  0.00   \n",
       "98                                1.0                                  1.00   \n",
       "99                                0.0                                  0.00   \n",
       "100                               0.0                                  0.00   \n",
       "101                               0.0                                  0.75   \n",
       "\n",
       "     rank_textSimilarity(title).queryCoverage  \\\n",
       "0                                    0.133333   \n",
       "1                                    0.000000   \n",
       "2                                    0.066667   \n",
       "3                                    0.000000   \n",
       "4                                    0.000000   \n",
       "..                                        ...   \n",
       "97                                   0.000000   \n",
       "98                                   0.555556   \n",
       "99                                   0.000000   \n",
       "100                                  0.000000   \n",
       "101                                  0.111111   \n",
       "\n",
       "     rank_textSimilarity(title).score  \n",
       "0                            0.620000  \n",
       "1                            0.000000  \n",
       "2                            0.322500  \n",
       "3                            0.000000  \n",
       "4                            0.000000  \n",
       "..                                ...  \n",
       "97                           0.000000  \n",
       "98                           0.833333  \n",
       "99                           0.000000  \n",
       "100                          0.000000  \n",
       "101                          0.335833  \n",
       "\n",
       "[102 rows x 198 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_phase_df = pd.DataFrame(second_phase_features[\"results\"])\n",
    "second_phase_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af595dc",
   "metadata": {},
   "source": [
    "This collects 195 features (excluding ids and labels), providing a rich feature set for training more sophisticated ranking models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe59fa1",
   "metadata": {},
   "source": [
    "### Training a GBDT model for second-phase ranking\n",
    "\n",
    "With the expanded feature set, we can train a Gradient Boosted Decision Tree (GBDT) model to predict document relevance. We use [LightGBM](https://docs.vespa.ai/en/lightgbm.html) for this purpose. \n",
    "\n",
    "Vespa also supports [XGBoost](https://docs.vespa.ai/en/xgboost.html) and [ONNX](https://docs.vespa.ai/en/onnx.html) models.\n",
    "\n",
    "To train the model, run the following command ([link to training script](https://github.com/vespa-engine/sample-apps/blob/master/rag-blueprint/eval/train_lightgbm.py)):\n",
    "\n",
    "The training process includes several important considerations:\n",
    "\n",
    "* **Cross-validation**: We use 5-fold stratified cross-validation to evaluate model performance and prevent overfitting\n",
    "* **Hyperparameter tuning**: We set conservative hyperparameters to prevent growing overly large and deep trees, especially important for smaller datasets\n",
    "* **Feature selection**: Features with zero importance during cross-validation are excluded from the final model\n",
    "* **Early stopping**: Training stops when validation scores don't improve for 50 rounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2df1117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 5-Fold Stratified Cross-Validation...\n",
      "Fold 1: AUC = 0.9727, ACC = 0.8095\n",
      "Fold 2: AUC = 0.9636, ACC = 0.8571\n",
      "Fold 3: AUC = 0.9798, ACC = 0.9000\n",
      "Fold 4: AUC = 0.9798, ACC = 0.8500\n",
      "Fold 5: AUC = 1.0000, ACC = 0.8000\n",
      "\n",
      "Training final model on 14 features with non-zero importance.\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def strip_feature_prefix(feature_name: str) -> str:\n",
    "    \"\"\"Strips 'rank_' or 'match_' prefix from a feature name.\"\"\"\n",
    "    return re.sub(r\"^(rank_|match_)\", \"\", feature_name)\n",
    "\n",
    "\n",
    "def calculate_mean_importance(\n",
    "    importance_frames: list,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Calculates and returns the mean feature importance from all folds.\"\"\"\n",
    "    if not importance_frames:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"gain\"])\n",
    "    imp_all = pd.concat(importance_frames, axis=0)\n",
    "    imp_mean = (\n",
    "        imp_all.groupby(\"feature\")[\"gain\"]\n",
    "        .mean()\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    return imp_mean\n",
    "\n",
    "\n",
    "def perform_cross_validation(\n",
    "    df: pd.DataFrame, args: Dict[str, Any]\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Performs stratified cross-validation with LightGBM on a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: Input pandas DataFrame containing features and the target column.\n",
    "        args: A dictionary of parameters for the training process.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - cv_results_df: DataFrame with the cross-validation metrics (Mean and Std Dev).\n",
    "        - feature_importance_df: DataFrame with the mean feature importance (gain).\n",
    "        - final_model_dict: The final trained LightGBM model, exported as a dictionary.\n",
    "    \"\"\"\n",
    "    # --- Parameter setup ---\n",
    "    target_col = args.get(\"target\", \"relevance_label\")\n",
    "    drop_cols = args.get(\"drop_cols\", [\"query_id\", \"doc_id\", \"relevance_score\"])\n",
    "    folds = args.get(\"folds\", 5)\n",
    "    seed = args.get(\"seed\", 42)\n",
    "    max_rounds = args.get(\"max_rounds\", 1000)\n",
    "    early_stop = args.get(\"early_stop\", 50)\n",
    "    learning_rate = args.get(\"learning_rate\", 0.05)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # --- Data Cleaning ---\n",
    "    df = df.copy()\n",
    "    constant_cols = [c for c in df.columns if df[c].nunique(dropna=False) <= 1]\n",
    "    cols_to_drop = [c for c in drop_cols if c in df.columns]\n",
    "    feature_cols = df.columns.difference(\n",
    "        constant_cols + cols_to_drop + [target_col]\n",
    "    ).tolist()\n",
    "\n",
    "    # Strip prefixes from feature names and rename columns\n",
    "    stripped_feature_mapping = {\n",
    "        original_col: strip_feature_prefix(original_col)\n",
    "        for original_col in feature_cols\n",
    "    }\n",
    "    df = df.rename(columns=stripped_feature_mapping)\n",
    "    feature_cols = list(stripped_feature_mapping.values())\n",
    "\n",
    "    # --- Handle Categorical Variables ---\n",
    "    cat_cols = [\n",
    "        c\n",
    "        for c in df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "        if c in feature_cols\n",
    "    ]\n",
    "    for c in cat_cols:\n",
    "        df[c] = df[c].astype(str)\n",
    "        df[c] = LabelEncoder().fit_transform(df[c])\n",
    "    categorical_feature_idx = [feature_cols.index(c) for c in cat_cols]\n",
    "\n",
    "    # --- Prepare X and y ---\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col].astype(int)\n",
    "\n",
    "    # Store original names and rename columns for LightGBM compatibility\n",
    "    original_feature_names = X.columns.tolist()\n",
    "    X.columns = [f\"feature_{i}\" for i in range(len(X.columns))]\n",
    "    feature_name_mapping = dict(zip(X.columns, original_feature_names))\n",
    "\n",
    "    # --- Stratified K-Fold Cross-Validation ---\n",
    "    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    oof_pred = np.zeros(len(df))\n",
    "    importance_frames = []\n",
    "    fold_metrics = {\"Accuracy\": [], \"ROC AUC\": []}\n",
    "    best_iterations = []\n",
    "\n",
    "    print(f\"Performing {folds}-Fold Stratified Cross-Validation...\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y), 1):\n",
    "        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "        lgb_train = lgb.Dataset(\n",
    "            X_train, y_train, categorical_feature=categorical_feature_idx\n",
    "        )\n",
    "        lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "        params = dict(\n",
    "            objective=\"binary\",\n",
    "            metric=\"auc\",\n",
    "            seed=seed,\n",
    "            verbose=-1,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=10,\n",
    "            max_depth=3,\n",
    "            feature_fraction=0.8,\n",
    "            bagging_fraction=0.8,\n",
    "            bagging_freq=5,\n",
    "        )\n",
    "        callbacks = [lgb.early_stopping(early_stop, verbose=False)]\n",
    "\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgb_train,\n",
    "            num_boost_round=max_rounds,\n",
    "            valid_sets=[lgb_val],\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "\n",
    "        best_iterations.append(model.best_iteration)\n",
    "        val_preds = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        oof_pred[val_idx] = val_preds\n",
    "\n",
    "        fold_metrics[\"ROC AUC\"].append(roc_auc_score(y_val, val_preds))\n",
    "        fold_metrics[\"Accuracy\"].append(\n",
    "            accuracy_score(y_val, (val_preds > 0.5).astype(int))\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold}: AUC = {fold_metrics['ROC AUC'][-1]:.4f}, ACC = {fold_metrics['Accuracy'][-1]:.4f}\"\n",
    "        )\n",
    "\n",
    "        importance_frames.append(\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"feature\": original_feature_names,\n",
    "                    \"gain\": model.feature_importance(importance_type=\"gain\"),\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # --- Compile Results ---\n",
    "    cv_results_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Metric\": list(fold_metrics.keys()),\n",
    "            \"Mean\": [np.mean(v) for v in fold_metrics.values()],\n",
    "            \"Std Dev\": [np.std(v) for v in fold_metrics.values()],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    feature_importance_df = calculate_mean_importance(importance_frames)\n",
    "\n",
    "    # --- Train Final Model ---\n",
    "    final_features = feature_importance_df[feature_importance_df[\"gain\"] > 0][\n",
    "        \"feature\"\n",
    "    ].tolist()\n",
    "    print(\n",
    "        f\"\\nTraining final model on {len(final_features)} features with non-zero importance.\"\n",
    "    )\n",
    "\n",
    "    # Map selected original names back to 'feature_i' names\n",
    "    final_feature_indices = [\n",
    "        key for key, val in feature_name_mapping.items() if val in final_features\n",
    "    ]\n",
    "    X_final = X[final_feature_indices]\n",
    "\n",
    "    final_categorical_idx = [\n",
    "        X_final.columns.get_loc(c)\n",
    "        for c in X_final.columns\n",
    "        if feature_name_mapping[c] in cat_cols\n",
    "    ]\n",
    "\n",
    "    full_dataset = lgb.Dataset(X_final, y, categorical_feature=final_categorical_idx)\n",
    "    final_boost_rounds = int(np.mean(best_iterations))\n",
    "\n",
    "    final_model = lgb.train(params, full_dataset, num_boost_round=final_boost_rounds)\n",
    "\n",
    "    # Export model with original feature names\n",
    "    model_json = final_model.dump_model()\n",
    "    model_json_str = json.dumps(model_json)\n",
    "    for renamed_feature, original_feature in feature_name_mapping.items():\n",
    "        model_json_str = model_json_str.replace(\n",
    "            f'\"{renamed_feature}\"', f'\"{original_feature}\"'\n",
    "        )\n",
    "    final_model_dict = json.loads(model_json_str)\n",
    "\n",
    "    print(\"Training completed successfully!\")\n",
    "    return cv_results_df, feature_importance_df, final_model_dict\n",
    "\n",
    "\n",
    "# 2. Define arguments as a dictionary\n",
    "training_args = {\n",
    "    \"target\": \"relevance_label\",\n",
    "    \"drop_cols\": [\"query_id\", \"doc_id\", \"relevance_score\"],\n",
    "    \"folds\": 5,\n",
    "    \"seed\": 42,\n",
    "    \"max_rounds\": 500,\n",
    "    \"early_stop\": 25,\n",
    "    \"learning_rate\": 0.05,\n",
    "}\n",
    "\n",
    "# 3. Run the cross-validation and get the results\n",
    "cv_results, feature_importance, final_model = perform_cross_validation(\n",
    "    df=second_phase_df, args=training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "224f2658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std Dev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.843333</td>\n",
       "      <td>0.035964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ROC AUC</td>\n",
       "      <td>0.979192</td>\n",
       "      <td>0.011979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Metric      Mean   Std Dev\n",
       "0  Accuracy  0.843333  0.035964\n",
       "1   ROC AUC  0.979192  0.011979"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0fbe618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nativeProximity</td>\n",
       "      <td>183.686466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>firstPhase</td>\n",
       "      <td>131.138263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>avg_top_3_chunk_sim_scores</td>\n",
       "      <td>58.646572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max_chunk_sim_scores</td>\n",
       "      <td>40.141040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>elementCompleteness(chunks).queryCompleteness</td>\n",
       "      <td>37.331087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>nativeRank</td>\n",
       "      <td>13.850518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>avg_top_3_chunk_text_scores</td>\n",
       "      <td>1.838134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bm25(chunks)</td>\n",
       "      <td>0.463590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>modified_freshness</td>\n",
       "      <td>0.386416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fieldMatch(title).absoluteProximity</td>\n",
       "      <td>0.374392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>fieldMatch(title).orderness</td>\n",
       "      <td>0.363286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>elementSimilarity(chunks)</td>\n",
       "      <td>0.214760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max_chunk_text_scores</td>\n",
       "      <td>0.183127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nativeFieldMatch</td>\n",
       "      <td>0.119759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>fieldTermMatch(title,3).weight</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          feature        gain\n",
       "0                                 nativeProximity  183.686466\n",
       "1                                      firstPhase  131.138263\n",
       "2                      avg_top_3_chunk_sim_scores   58.646572\n",
       "3                            max_chunk_sim_scores   40.141040\n",
       "4   elementCompleteness(chunks).queryCompleteness   37.331087\n",
       "5                                      nativeRank   13.850518\n",
       "6                     avg_top_3_chunk_text_scores    1.838134\n",
       "7                                    bm25(chunks)    0.463590\n",
       "8                              modified_freshness    0.386416\n",
       "9             fieldMatch(title).absoluteProximity    0.374392\n",
       "10                    fieldMatch(title).orderness    0.363286\n",
       "11                      elementSimilarity(chunks)    0.214760\n",
       "12                          max_chunk_text_scores    0.183127\n",
       "13                               nativeFieldMatch    0.119759\n",
       "14                 fieldTermMatch(title,3).weight    0.000000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9638cb9",
   "metadata": {},
   "source": [
    "### Feature importance analysis\n",
    "\n",
    "The trained model reveals which features are most important for ranking quality. \n",
    "(As this notebook runs in CI, and not everything from data_collection and training is deterministic, the exact feature importances may vary, but we _expect_ the observations below to hold for most runs.)\n",
    "\n",
    "Key observations:\n",
    "\n",
    "* **Text proximity features** ([nativeProximity](https://docs.vespa.ai/en/reference/nativerank.html#nativeProximity)) are highly valuable for understanding query-document relevance\n",
    "* **First-phase score** (`firstPhase`) being important validates that our first-phase ranking provides a good foundation\n",
    "* **Chunk-level features** (both text and semantic) contribute significantly to ranking quality\n",
    "* **Traditional text features** like [nativeRank](https://docs.vespa.ai/en/reference/nativerank.html#nativeRank) and [bm25](https://docs.vespa.ai/en/reference/bm25.html#ranking-function) remain important\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f501bd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'tree',\n",
       " 'version': 'v4',\n",
       " 'num_class': 1,\n",
       " 'num_tree_per_iteration': 1,\n",
       " 'label_index': 0,\n",
       " 'max_feature_idx': 16,\n",
       " 'objective': 'binary sigmoid:1',\n",
       " 'average_output': False,\n",
       " 'feature_names': ['avg_top_3_chunk_sim_scores',\n",
       "  'avg_top_3_chunk_text_scores',\n",
       "  'bm25(chunks)',\n",
       "  'bm25(chunks)',\n",
       "  'max_chunk_sim_scores',\n",
       "  'max_chunk_text_scores',\n",
       "  'modified_freshness',\n",
       "  'bm25(chunks)',\n",
       "  'bm25(chunks)',\n",
       "  'elementCompleteness(chunks).queryCompleteness',\n",
       "  'elementSimilarity(chunks)',\n",
       "  'fieldMatch(title).absoluteProximity',\n",
       "  'fieldMatch(title).orderness',\n",
       "  'firstPhase',\n",
       "  'nativeFieldMatch',\n",
       "  'nativeProximity',\n",
       "  'nativeRank'],\n",
       " 'monotone_constraints': [],\n",
       " 'feature_infos': {'avg_top_3_chunk_sim_scores': {'min_value': 0.08106629550457,\n",
       "   'max_value': 0.4134707450866699,\n",
       "   'values': []},\n",
       "  'avg_top_3_chunk_text_scores': {'min_value': 0,\n",
       "   'max_value': 20.105823516845703,\n",
       "   'values': []},\n",
       "  'bm25(chunks)': {'min_value': 0,\n",
       "   'max_value': 25.04552896302937,\n",
       "   'values': []},\n",
       "  'max_chunk_sim_scores': {'min_value': 0.08106629550457,\n",
       "   'max_value': 0.4462931454181671,\n",
       "   'values': []},\n",
       "  'max_chunk_text_scores': {'min_value': 0,\n",
       "   'max_value': 21.62700843811035,\n",
       "   'values': []},\n",
       "  'modified_freshness': {'min_value': 0,\n",
       "   'max_value': 0.5671891292958484,\n",
       "   'values': []},\n",
       "  'elementCompleteness(chunks).queryCompleteness': {'min_value': 0,\n",
       "   'max_value': 0.7777777777777778,\n",
       "   'values': []},\n",
       "  'elementSimilarity(chunks)': {'min_value': 0,\n",
       "   'max_value': 0.7162878787878787,\n",
       "   'values': []},\n",
       "  'fieldMatch(title).absoluteProximity': {'min_value': 0,\n",
       "   'max_value': 0.10000000149011612,\n",
       "   'values': []},\n",
       "  'fieldMatch(title).orderness': {'min_value': 0,\n",
       "   'max_value': 1,\n",
       "   'values': []},\n",
       "  'firstPhase': {'min_value': -5.438998465840945,\n",
       "   'max_value': 14.07283096376979,\n",
       "   'values': []},\n",
       "  'nativeFieldMatch': {'min_value': 0,\n",
       "   'max_value': 0.3354072940571937,\n",
       "   'values': []},\n",
       "  'nativeProximity': {'min_value': 0,\n",
       "   'max_value': 0.1963793884211417,\n",
       "   'values': []},\n",
       "  'nativeRank': {'min_value': 0.0017429193899782137,\n",
       "   'max_value': 0.17263275990663562,\n",
       "   'values': []}},\n",
       " 'tree_info': [{'tree_index': 0,\n",
       "   'num_leaves': 2,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 1,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 50.4098014831543,\n",
       "    'threshold': 0.02084435169178268,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.165181,\n",
       "    'internal_weight': 18.8831,\n",
       "    'internal_count': 76,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': 0.08130811914532406,\n",
       "     'leaf_weight': 9.193098649382593,\n",
       "     'leaf_count': 37},\n",
       "    'right_child': {'leaf_index': 1,\n",
       "     'leaf_value': 0.24475291179584288,\n",
       "     'leaf_weight': 9.690022900700567,\n",
       "     'leaf_count': 39}}},\n",
       "  {'tree_index': 1,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 0,\n",
       "    'split_gain': 44.23429870605469,\n",
       "    'threshold': 0.18672376126050952,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.00762683,\n",
       "    'internal_weight': 18.8402,\n",
       "    'internal_count': 76,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.10463142349527131,\n",
       "     'leaf_weight': 5.986800223588946,\n",
       "     'leaf_count': 24},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 9,\n",
       "     'split_gain': 7.076389789581299,\n",
       "     'threshold': 0.44949494949494956,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0599142,\n",
       "     'internal_weight': 12.8534,\n",
       "     'internal_count': 52,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.013179562064110115,\n",
       "      'leaf_weight': 4.968685954809187,\n",
       "      'leaf_count': 20},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.08936491628319639,\n",
       "      'leaf_weight': 7.884672373533249,\n",
       "      'leaf_count': 32}}}},\n",
       "  {'tree_index': 2,\n",
       "   'num_leaves': 2,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 42.20650100708008,\n",
       "    'threshold': 0.02084435169178268,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.00729477,\n",
       "    'internal_weight': 18.7478,\n",
       "    'internal_count': 76,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.06880462126513588,\n",
       "     'leaf_weight': 9.240163266658785,\n",
       "     'leaf_count': 37},\n",
       "    'right_child': {'leaf_index': 1,\n",
       "     'leaf_value': 0.08125312744778718,\n",
       "     'leaf_weight': 9.507659405469893,\n",
       "     'leaf_count': 39}}},\n",
       "  {'tree_index': 3,\n",
       "   'num_leaves': 2,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 38.436100006103516,\n",
       "    'threshold': 0.02084435169178268,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.00699584,\n",
       "    'internal_weight': 18.6093,\n",
       "    'internal_count': 76,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.06538935309867093,\n",
       "     'leaf_weight': 9.236633136868479,\n",
       "     'leaf_count': 37},\n",
       "    'right_child': {'leaf_index': 1,\n",
       "     'leaf_value': 0.07833036395826393,\n",
       "     'leaf_weight': 9.372678577899931,\n",
       "     'leaf_count': 39}}},\n",
       "  {'tree_index': 4,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 0,\n",
       "    'split_gain': 35.5458984375,\n",
       "    'threshold': 0.18672376126050952,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.00672514,\n",
       "    'internal_weight': 18.4298,\n",
       "    'internal_count': 76,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.09372889424381685,\n",
       "     'leaf_weight': 5.958949193358424,\n",
       "     'leaf_count': 24},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 9,\n",
       "     'split_gain': 5.318920135498047,\n",
       "     'threshold': 0.44949494949494956,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0547252,\n",
       "     'internal_weight': 12.4708,\n",
       "     'internal_count': 52,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.014303727398432995,\n",
       "      'leaf_weight': 4.924616768956183,\n",
       "      'leaf_count': 20},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.08110403985734628,\n",
       "      'leaf_weight': 7.546211168169975,\n",
       "      'leaf_count': 32}}}},\n",
       "  {'tree_index': 5,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 0,\n",
       "    'split_gain': 38.138301849365234,\n",
       "    'threshold': 0.18672376126050952,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.00466505,\n",
       "    'internal_weight': 17.5394,\n",
       "    'internal_count': 73,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.08973068432306786,\n",
       "     'leaf_weight': 6.64585913717747,\n",
       "     'leaf_count': 27},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 9,\n",
       "     'split_gain': 1.3554699420928955,\n",
       "     'threshold': 0.4641025641025642,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0622534,\n",
       "     'internal_weight': 10.8935,\n",
       "     'internal_count': 46,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.04350337739463364,\n",
       "      'leaf_weight': 5.113931432366369,\n",
       "      'leaf_count': 21},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.07884389694057212,\n",
       "      'leaf_weight': 5.779602885246277,\n",
       "      'leaf_count': 25}}}},\n",
       "  {'tree_index': 6,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 0,\n",
       "    'split_gain': 34.902099609375,\n",
       "    'threshold': 0.18672376126050952,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.004498,\n",
       "    'internal_weight': 17.3039,\n",
       "    'internal_count': 73,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.08633609429142271,\n",
       "     'leaf_weight': 6.563828170299533,\n",
       "     'leaf_count': 27},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 15,\n",
       "     'split_gain': 1.338919997215271,\n",
       "     'threshold': 0.04231842199421151,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0600115,\n",
       "     'internal_weight': 10.7401,\n",
       "     'internal_count': 46,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.04135593626110073,\n",
       "      'leaf_weight': 5.074008285999296,\n",
       "      'leaf_count': 21},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.07671780288029927,\n",
       "      'leaf_weight': 5.66606205701828,\n",
       "      'leaf_count': 25}}}},\n",
       "  {'tree_index': 7,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 0,\n",
       "    'split_gain': 32.02009963989258,\n",
       "    'threshold': 0.18672376126050952,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.00434441,\n",
       "    'internal_weight': 17.0374,\n",
       "    'internal_count': 73,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.08334419516313175,\n",
       "     'leaf_weight': 6.4620268940925625,\n",
       "     'leaf_count': 27},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 13,\n",
       "     'split_gain': 1.350219964981079,\n",
       "     'threshold': 2.3306006116972546,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0579262,\n",
       "     'internal_weight': 10.5754,\n",
       "     'internal_count': 46,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.039874616438302576,\n",
       "      'leaf_weight': 5.23301127552986,\n",
       "      'leaf_count': 22},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.075608344236657,\n",
       "      'leaf_weight': 5.342339798808098,\n",
       "      'leaf_count': 24}}}},\n",
       "  {'tree_index': 8,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 0,\n",
       "    'split_gain': 29.436899185180664,\n",
       "    'threshold': 0.18672376126050952,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.00420139,\n",
       "    'internal_weight': 16.7481,\n",
       "    'internal_count': 73,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.08069001048178517,\n",
       "     'leaf_weight': 6.343828111886981,\n",
       "     'leaf_count': 27},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 9,\n",
       "     'split_gain': 1.3577200174331665,\n",
       "     'threshold': 0.4641025641025642,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0559624,\n",
       "     'internal_weight': 10.4043,\n",
       "     'internal_count': 46,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.03721400081314201,\n",
       "      'leaf_weight': 5.008224830031393,\n",
       "      'leaf_count': 21},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.07336338756704952,\n",
       "      'leaf_weight': 5.396055206656456,\n",
       "      'leaf_count': 25}}}},\n",
       "  {'tree_index': 9,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 0,\n",
       "    'split_gain': 27.117399215698242,\n",
       "    'threshold': 0.18672376126050952,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': 0.00406947,\n",
       "    'internal_weight': 16.4361,\n",
       "    'internal_count': 73,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.0783218588683625,\n",
       "     'leaf_weight': 6.212180107831958,\n",
       "     'leaf_count': 27},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 13,\n",
       "     'split_gain': 1.3397400379180908,\n",
       "     'threshold': 2.3306006116972546,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0541313,\n",
       "     'internal_weight': 10.2239,\n",
       "     'internal_count': 46,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.03614212999194114,\n",
       "      'leaf_weight': 5.143270537257193,\n",
       "      'leaf_count': 22},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.07234219952515168,\n",
       "      'leaf_weight': 5.080672308802605,\n",
       "      'leaf_count': 24}}}},\n",
       "  {'tree_index': 10,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 24.532800674438477,\n",
       "    'threshold': 0.02681743703534994,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.0040159,\n",
       "    'internal_weight': 17.9796,\n",
       "    'internal_count': 81,\n",
       "    'left_child': {'split_index': 1,\n",
       "     'split_feature': 1,\n",
       "     'split_gain': 7.316380023956299,\n",
       "     'threshold': 3.092608213424683,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': -0.0496308,\n",
       "     'internal_weight': 11.1677,\n",
       "     'internal_count': 48,\n",
       "     'left_child': {'leaf_index': 0,\n",
       "      'leaf_value': -0.0856005281817455,\n",
       "      'leaf_weight': 6.239090889692308,\n",
       "      'leaf_count': 27},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': -0.004096688964982691,\n",
       "      'leaf_weight': 4.92857152223587,\n",
       "      'leaf_count': 21}},\n",
       "    'right_child': {'leaf_index': 1,\n",
       "     'leaf_value': 0.07076665519154234,\n",
       "     'leaf_weight': 6.811910331249236,\n",
       "     'leaf_count': 33}}},\n",
       "  {'tree_index': 11,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 13,\n",
       "    'split_gain': 23.044300079345703,\n",
       "    'threshold': -0.9175117702774908,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.00387752,\n",
       "    'internal_weight': 17.6602,\n",
       "    'internal_count': 81,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.07470333072738213,\n",
       "     'leaf_weight': 6.959094658493998,\n",
       "     'leaf_count': 31},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 13,\n",
       "     'split_gain': 3.699049949645996,\n",
       "     'threshold': 1.8772808596672073,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0421818,\n",
       "     'internal_weight': 10.7011,\n",
       "     'internal_count': 50,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.011210025880369016,\n",
       "      'leaf_weight': 5.071562081575392,\n",
       "      'leaf_count': 22},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.07008390819526038,\n",
       "      'leaf_weight': 5.629503101110458,\n",
       "      'leaf_count': 28}}}},\n",
       "  {'tree_index': 12,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 21.399799346923828,\n",
       "    'threshold': 0.02681743703534994,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.00374963,\n",
       "    'internal_weight': 17.3372,\n",
       "    'internal_count': 81,\n",
       "    'left_child': {'split_index': 1,\n",
       "     'split_feature': 2,\n",
       "     'split_gain': 5.836999893188477,\n",
       "     'threshold': 3.5472756680480115,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': -0.046492,\n",
       "     'internal_weight': 10.89,\n",
       "     'internal_count': 48,\n",
       "     'left_child': {'leaf_index': 0,\n",
       "      'leaf_value': -0.08218247103000542,\n",
       "      'leaf_weight': 5.5828584283590335,\n",
       "      'leaf_count': 25},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': -0.008947176009566292,\n",
       "      'leaf_weight': 5.307131439447403,\n",
       "      'leaf_count': 23}},\n",
       "    'right_child': {'leaf_index': 1,\n",
       "     'leaf_value': 0.06844637890571116,\n",
       "     'leaf_weight': 6.447218477725982,\n",
       "     'leaf_count': 33}}},\n",
       "  {'tree_index': 13,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 13,\n",
       "    'split_gain': 19.988399505615234,\n",
       "    'threshold': -0.9175117702774908,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.00362511,\n",
       "    'internal_weight': 17.0069,\n",
       "    'internal_count': 81,\n",
       "    'left_child': {'leaf_index': 0,\n",
       "     'leaf_value': -0.07099178346638545,\n",
       "     'leaf_weight': 6.683696135878566,\n",
       "     'leaf_count': 31},\n",
       "    'right_child': {'split_index': 1,\n",
       "     'split_feature': 13,\n",
       "     'split_gain': 3.370919942855835,\n",
       "     'threshold': 1.8772808596672073,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0399912,\n",
       "     'internal_weight': 10.3232,\n",
       "     'internal_count': 50,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.010651145320500731,\n",
       "      'leaf_weight': 5.024654343724249,\n",
       "      'leaf_count': 22},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': 0.06781494678857775,\n",
       "      'leaf_weight': 5.298499584197998,\n",
       "      'leaf_count': 28}}}},\n",
       "  {'tree_index': 14,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 18.75670051574707,\n",
       "    'threshold': 0.02681743703534994,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.00351166,\n",
       "    'internal_weight': 16.6706,\n",
       "    'internal_count': 81,\n",
       "    'left_child': {'split_index': 1,\n",
       "     'split_feature': 1,\n",
       "     'split_gain': 5.915229797363281,\n",
       "     'threshold': 3.092608213424683,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': -0.0436897,\n",
       "     'internal_weight': 10.592,\n",
       "     'internal_count': 48,\n",
       "     'left_child': {'leaf_index': 0,\n",
       "      'leaf_value': -0.07794227861102893,\n",
       "      'leaf_weight': 5.755450502038004,\n",
       "      'leaf_count': 27},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': -0.002928969366291567,\n",
       "      'leaf_weight': 4.836504548788071,\n",
       "      'leaf_count': 21}},\n",
       "    'right_child': {'leaf_index': 1,\n",
       "     'leaf_value': 0.06649753931977596,\n",
       "     'leaf_weight': 6.0786804407835,\n",
       "     'leaf_count': 33}}},\n",
       "  {'tree_index': 15,\n",
       "   'num_leaves': 3,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 9,\n",
       "    'split_gain': 19.521400451660156,\n",
       "    'threshold': 0.44949494949494956,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.00670763,\n",
       "    'internal_weight': 16.4224,\n",
       "    'internal_count': 83,\n",
       "    'left_child': {'split_index': 1,\n",
       "     'split_feature': 1,\n",
       "     'split_gain': 2.7174599170684814,\n",
       "     'threshold': 2.4830845594406132,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': -0.0505802,\n",
       "     'internal_weight': 9.96688,\n",
       "     'internal_count': 47,\n",
       "     'left_child': {'leaf_index': 0,\n",
       "      'leaf_value': -0.0748234090211124,\n",
       "      'leaf_weight': 5.352049484848978,\n",
       "      'leaf_count': 26},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': -0.022464201444285303,\n",
       "      'leaf_weight': 4.614828139543533,\n",
       "      'leaf_count': 21}},\n",
       "    'right_child': {'leaf_index': 1,\n",
       "     'leaf_value': 0.06102882167971128,\n",
       "     'leaf_weight': 6.455503240227698,\n",
       "     'leaf_count': 36}}},\n",
       "  {'tree_index': 16,\n",
       "   'num_leaves': 4,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 20.915599822998047,\n",
       "    'threshold': 0.02084435169178268,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.00650951,\n",
       "    'internal_weight': 16.0734,\n",
       "    'internal_count': 83,\n",
       "    'left_child': {'split_index': 1,\n",
       "     'split_feature': 1,\n",
       "     'split_gain': 0.7167580127716064,\n",
       "     'threshold': 2.181384921073914,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': -0.0584402,\n",
       "     'internal_weight': 8.78815,\n",
       "     'internal_count': 42,\n",
       "     'left_child': {'leaf_index': 0,\n",
       "      'leaf_value': -0.07283105883520614,\n",
       "      'leaf_weight': 4.359884411096575,\n",
       "      'leaf_count': 22},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': -0.044271557748743806,\n",
       "      'leaf_weight': 4.428262785077095,\n",
       "      'leaf_count': 20}},\n",
       "    'right_child': {'split_index': 2,\n",
       "     'split_feature': 6,\n",
       "     'split_gain': 0.27922600507736206,\n",
       "     'threshold': 0.48491415168876384,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0561343,\n",
       "     'internal_weight': 7.28523,\n",
       "     'internal_count': 41,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.046566645885268335,\n",
       "      'leaf_weight': 3.725804477930068,\n",
       "      'leaf_count': 21},\n",
       "     'right_child': {'leaf_index': 3,\n",
       "      'leaf_value': 0.06614921330100301,\n",
       "      'leaf_weight': 3.559424474835396,\n",
       "      'leaf_count': 20}}}},\n",
       "  {'tree_index': 17,\n",
       "   'num_leaves': 4,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 19.341999053955078,\n",
       "    'threshold': 0.02084435169178268,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.0063281,\n",
       "    'internal_weight': 15.7046,\n",
       "    'internal_count': 83,\n",
       "    'left_child': {'split_index': 1,\n",
       "     'split_feature': 1,\n",
       "     'split_gain': 0.7211930155754089,\n",
       "     'threshold': 2.181384921073914,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': -0.0566291,\n",
       "     'internal_weight': 8.62062,\n",
       "     'internal_count': 42,\n",
       "     'left_child': {'leaf_index': 0,\n",
       "      'leaf_value': -0.07136146887938256,\n",
       "      'leaf_weight': 4.2304699271917325,\n",
       "      'leaf_count': 22},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': -0.04243262371555604,\n",
       "      'leaf_weight': 4.390146732330322,\n",
       "      'leaf_count': 20}},\n",
       "    'right_child': {'split_index': 2,\n",
       "     'split_feature': 4,\n",
       "     'split_gain': 0.17738600075244904,\n",
       "     'threshold': 0.3187254816293717,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.054884,\n",
       "     'internal_weight': 7.08399,\n",
       "     'internal_count': 41,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.04723412069233138,\n",
       "      'leaf_weight': 3.6613249629735956,\n",
       "      'leaf_count': 20},\n",
       "     'right_child': {'leaf_index': 3,\n",
       "      'leaf_value': 0.06306728950350501,\n",
       "      'leaf_weight': 3.4226654171943665,\n",
       "      'leaf_count': 21}}}},\n",
       "  {'tree_index': 18,\n",
       "   'num_leaves': 4,\n",
       "   'num_cat': 0,\n",
       "   'shrinkage': 0.05,\n",
       "   'tree_structure': {'split_index': 0,\n",
       "    'split_feature': 15,\n",
       "    'split_gain': 17.89940071105957,\n",
       "    'threshold': 0.02084435169178268,\n",
       "    'decision_type': '<=',\n",
       "    'default_left': True,\n",
       "    'missing_type': 'None',\n",
       "    'internal_value': -0.00615586,\n",
       "    'internal_weight': 15.3347,\n",
       "    'internal_count': 83,\n",
       "    'left_child': {'split_index': 1,\n",
       "     'split_feature': 9,\n",
       "     'split_gain': 0.660440981388092,\n",
       "     'threshold': 0.22649572649572652,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': -0.0549116,\n",
       "     'internal_weight': 8.45071,\n",
       "     'internal_count': 42,\n",
       "     'left_child': {'leaf_index': 0,\n",
       "      'leaf_value': -0.06930617045321565,\n",
       "      'leaf_weight': 4.101271376013754,\n",
       "      'leaf_count': 22},\n",
       "     'right_child': {'leaf_index': 2,\n",
       "      'leaf_value': -0.04133840308087882,\n",
       "      'leaf_weight': 4.349441319704056,\n",
       "      'leaf_count': 20}},\n",
       "    'right_child': {'split_index': 2,\n",
       "     'split_feature': 15,\n",
       "     'split_gain': 0.189178004860878,\n",
       "     'threshold': 0.05606487282356567,\n",
       "     'decision_type': '<=',\n",
       "     'default_left': True,\n",
       "     'missing_type': 'None',\n",
       "     'internal_value': 0.0536959,\n",
       "     'internal_weight': 6.88402,\n",
       "     'internal_count': 41,\n",
       "     'left_child': {'leaf_index': 1,\n",
       "      'leaf_value': 0.04578324148730414,\n",
       "      'leaf_weight': 3.6016914695501336,\n",
       "      'leaf_count': 20},\n",
       "     'right_child': {'leaf_index': 3,\n",
       "      'leaf_value': 0.062378436439081024,\n",
       "      'leaf_weight': 3.282333254814148,\n",
       "      'leaf_count': 21}}}}],\n",
       " 'feature_importances': {'avg_top_3_chunk_sim_scores': 7,\n",
       "  'avg_top_3_chunk_text_scores': 5,\n",
       "  'bm25(chunks)': 1,\n",
       "  'max_chunk_sim_scores': 1,\n",
       "  'modified_freshness': 1,\n",
       "  'elementCompleteness(chunks).queryCompleteness': 6,\n",
       "  'firstPhase': 6,\n",
       "  'nativeProximity': 11},\n",
       " 'pandas_categorical': []}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6922467",
   "metadata": {},
   "source": [
    "### Integrating the GBDT model into Vespa\n",
    "\n",
    "The trained LightGBM model can be exported and added to your Vespa application package:\n",
    "\n",
    "```txt\n",
    "app/\n",
    "├── models/\n",
    "│   └── lightgbm_model.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7001d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the final model to a file\n",
    "model_file = repo_root / \"app\" / \"models\" / \"lightgbm_model.json\"\n",
    "with open(model_file, \"w\") as f:\n",
    "    json.dump(final_model, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5164ca",
   "metadata": {},
   "source": [
    "Create a new rank profile that uses this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "100c5b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```txt\n",
       "rank-profile second-with-gbdt inherits collect-second-phase {\n",
       "    match-features {\n",
       "        max_chunk_sim_scores\n",
       "        max_chunk_text_scores\n",
       "        avg_top_3_chunk_text_scores\n",
       "        avg_top_3_chunk_sim_scores\n",
       "        bm25(title)\n",
       "        modified_freshness\n",
       "        open_count\n",
       "        firstPhase\n",
       "    }\n",
       "    # nativeProximity,168.84977385997772\n",
       "    # firstPhase,151.73823466300965\n",
       "    # max_chunk_sim_scores,69.43774781227111\n",
       "    # avg_top_3_chunk_text_scores,56.507930064201354\n",
       "    # avg_top_3_chunk_sim_scores,31.87002867460251\n",
       "    # nativeRank,20.071615393646063\n",
       "    # nativeFieldMatch,15.991393876075744\n",
       "    # elementSimilarity(chunks),9.700291919708253\n",
       "    # bm25(chunks),3.8777143508195877\n",
       "    # max_chunk_text_scores,3.6405647873878477\n",
       "    # \"fieldTermMatch(chunks,4).firstPosition\",1.2615019798278808\n",
       "    # \"fieldTermMatch(chunks,4).occurrences\",1.0542740106582642\n",
       "    # \"fieldTermMatch(chunks,4).weight\",0.7263560056686401\n",
       "    # term(3).significance,0.5077840089797974\n",
       "    rank-features {\n",
       "        nativeProximity\n",
       "        nativeFieldMatch\n",
       "        nativeRank\n",
       "        elementSimilarity(chunks)\n",
       "        fieldTermMatch(chunks, 4).firstPosition\n",
       "        fieldTermMatch(chunks, 4).occurrences\n",
       "        fieldTermMatch(chunks, 4).weight\n",
       "        term(3).significance\n",
       "    }\n",
       "    second-phase {\n",
       "        expression: lightgbm(\"lightgbm_model.json\")\n",
       "    }\n",
       "\n",
       "    summary-features: top_3_chunk_sim_scores\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "second_gbdt_rp = (\n",
    "    repo_root / \"app\" / \"schemas\" / \"doc\" / \"second-with-gbdt.profile\"\n",
    ").read_text()\n",
    "display_md(second_gbdt_rp, tag=\"txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfed570",
   "metadata": {},
   "source": [
    "And redeploy your application. We add a try/except block to this in case your authentication token has expired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "462e7c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment started in run 87 of dev-aws-us-east-1c for vespa-team.rag-blueprint. This may take a few minutes the first time.\n",
      "INFO    [09:43:43]  Deploying platform version 8.586.25 and application dev build 87 for dev-aws-us-east-1c of default ...\n",
      "INFO    [09:43:43]  Using CA signed certificate version 5\n",
      "INFO    [09:43:52]  Session 379708 for tenant 'vespa-team' prepared and activated.\n",
      "INFO    [09:43:52]  ######## Details for all nodes ########\n",
      "INFO    [09:43:52]  h125699b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:43:52]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:43:52]  --- storagenode on port 19102 has config generation 379705, wanted is 379708\n",
      "INFO    [09:43:52]  --- searchnode on port 19107 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:52]  --- distributor on port 19111 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:52]  --- metricsproxy-container on port 19092 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:52]  h125755a.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:43:52]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:43:52]  --- container on port 4080 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:52]  --- metricsproxy-container on port 19092 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:52]  h97530b.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:43:52]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:43:52]  --- logserver-container on port 4080 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:52]  --- metricsproxy-container on port 19092 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:52]  h119190c.dev.us-east-1c.aws.vespa-cloud.net: expected to be UP\n",
      "INFO    [09:43:52]  --- platform vespa/cloud-tenant-rhel8:8.586.25\n",
      "INFO    [09:43:52]  --- container-clustercontroller on port 19050 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:52]  --- metricsproxy-container on port 19092 has config generation 379708, wanted is 379708\n",
      "INFO    [09:43:59]  Found endpoints:\n",
      "INFO    [09:43:59]  - dev.aws-us-east-1c\n",
      "INFO    [09:43:59]   |-- https://fe5fe13c.fe19121d.z.vespa-app.cloud/ (cluster 'default')\n",
      "INFO    [09:43:59]  Deployment of new application revision complete!\n",
      "Only region: aws-us-east-1c available in dev environment.\n",
      "Found mtls endpoint for default\n",
      "URL: https://fe5fe13c.fe19121d.z.vespa-app.cloud/\n",
      "Application is up!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    app: Vespa = vespa_cloud.deploy(disk_folder=application_root)\n",
    "except Exception:\n",
    "    vespa_cloud = VespaCloud(\n",
    "        tenant=VESPA_TENANT_NAME,\n",
    "        application=VESPA_APPLICATION_NAME,\n",
    "        key_content=VESPA_TEAM_API_KEY,\n",
    "        application_root=application_root,\n",
    "    )\n",
    "    app: Vespa = vespa_cloud.deploy(disk_folder=application_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c53b0a",
   "metadata": {},
   "source": [
    "### Evaluating second-phase ranking performance\n",
    "\n",
    "Let us run the ranking evaluation to evaluate the GBDT-powered second-phase ranking on unseen test queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3796d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_second_phase_query_fn(query_text: str, top_k: int) -> dict:\n",
    "    return {\n",
    "        \"yql\": str(\n",
    "            qb.select(\"*\")\n",
    "            .from_(VESPA_SCHEMA_NAME)\n",
    "            .where(\n",
    "                qb.nearestNeighbor(\n",
    "                    field=\"title_embedding\",\n",
    "                    query_vector=\"embedding\",\n",
    "                    annotations={\"targetHits\": 100},\n",
    "                )\n",
    "                | qb.nearestNeighbor(\n",
    "                    field=\"chunk_embeddings\",\n",
    "                    query_vector=\"embedding\",\n",
    "                    annotations={\"targetHits\": 100},\n",
    "                )\n",
    "                | qb.userQuery(\n",
    "                    query_text,\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        \"hits\": top_k,\n",
    "        \"query\": query_text,\n",
    "        \"ranking\": \"second-with-gbdt\",\n",
    "        \"input.query(embedding)\": f\"embed({query_text})\",\n",
    "        \"input.query(float_embedding)\": f\"embed({query_text})\",\n",
    "        \"presentation.summary\": \"no-chunks\",\n",
    "    }\n",
    "\n",
    "\n",
    "second_phase_evaluator = VespaEvaluator(\n",
    "    queries=test_ids_to_query,\n",
    "    relevant_docs=test_relevant_docs,\n",
    "    vespa_query_fn=rank_second_phase_query_fn,\n",
    "    id_field=\"id\",\n",
    "    app=app,\n",
    "    name=\"second-phase-evaluation\",\n",
    "    write_csv=False,\n",
    "    precision_recall_at_k=[10, 20],\n",
    ")\n",
    "\n",
    "second_phase_results = second_phase_evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "17005a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy@1': 0.75,\n",
       " 'accuracy@3': 0.95,\n",
       " 'accuracy@5': 0.95,\n",
       " 'accuracy@10': 1.0,\n",
       " 'precision@10': 0.24000000000000005,\n",
       " 'recall@10': 0.9651515151515152,\n",
       " 'precision@20': 0.12999999999999998,\n",
       " 'recall@20': 0.9954545454545455,\n",
       " 'mrr@10': 0.8404761904761905,\n",
       " 'ndcg@10': 0.8391408637111896,\n",
       " 'map@100': 0.7673197781750414,\n",
       " 'searchtime_avg': 0.03360000000000001,\n",
       " 'searchtime_q50': 0.0285,\n",
       " 'searchtime_q90': 0.05120000000000001,\n",
       " 'searchtime_q95': 0.0534}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_phase_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1ca34212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy@1</th>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy@3</th>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy@5</th>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy@10</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision@10</th>\n",
       "      <td>0.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall@10</th>\n",
       "      <td>0.965152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision@20</th>\n",
       "      <td>0.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall@20</th>\n",
       "      <td>0.995455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mrr@10</th>\n",
       "      <td>0.840476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ndcg@10</th>\n",
       "      <td>0.839141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>map@100</th>\n",
       "      <td>0.767320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_avg</th>\n",
       "      <td>0.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q50</th>\n",
       "      <td>0.028500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q90</th>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>searchtime_q95</th>\n",
       "      <td>0.053400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   value\n",
       "accuracy@1      0.750000\n",
       "accuracy@3      0.950000\n",
       "accuracy@5      0.950000\n",
       "accuracy@10     1.000000\n",
       "precision@10    0.240000\n",
       "recall@10       0.965152\n",
       "precision@20    0.130000\n",
       "recall@20       0.995455\n",
       "mrr@10          0.840476\n",
       "ndcg@10         0.839141\n",
       "map@100         0.767320\n",
       "searchtime_avg  0.033600\n",
       "searchtime_q50  0.028500\n",
       "searchtime_q90  0.051200\n",
       "searchtime_q95  0.053400"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_phase_df = pd.DataFrame(second_phase_results, index=[\"value\"]).T\n",
    "second_phase_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657a4f83",
   "metadata": {},
   "source": [
    "Expected results show significant improvement over first-phase ranking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4e094851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_85d02_row5_col0, #T_85d02_row5_col1, #T_85d02_row5_col2, #T_85d02_row7_col0, #T_85d02_row7_col1, #T_85d02_row7_col2 {\n",
       "  background-color: lightblue;\n",
       "  color: black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_85d02\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_85d02_level0_col0\" class=\"col_heading level0 col0\" >first_phase</th>\n",
       "      <th id=\"T_85d02_level0_col1\" class=\"col_heading level0 col1\" >second_phase</th>\n",
       "      <th id=\"T_85d02_level0_col2\" class=\"col_heading level0 col2\" >diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row0\" class=\"row_heading level0 row0\" >accuracy@1</th>\n",
       "      <td id=\"T_85d02_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_85d02_row0_col1\" class=\"data row0 col1\" >0.750000</td>\n",
       "      <td id=\"T_85d02_row0_col2\" class=\"data row0 col2\" >-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row1\" class=\"row_heading level0 row1\" >accuracy@3</th>\n",
       "      <td id=\"T_85d02_row1_col0\" class=\"data row1 col0\" >1.000000</td>\n",
       "      <td id=\"T_85d02_row1_col1\" class=\"data row1 col1\" >0.950000</td>\n",
       "      <td id=\"T_85d02_row1_col2\" class=\"data row1 col2\" >-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row2\" class=\"row_heading level0 row2\" >accuracy@5</th>\n",
       "      <td id=\"T_85d02_row2_col0\" class=\"data row2 col0\" >1.000000</td>\n",
       "      <td id=\"T_85d02_row2_col1\" class=\"data row2 col1\" >0.950000</td>\n",
       "      <td id=\"T_85d02_row2_col2\" class=\"data row2 col2\" >-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row3\" class=\"row_heading level0 row3\" >accuracy@10</th>\n",
       "      <td id=\"T_85d02_row3_col0\" class=\"data row3 col0\" >1.000000</td>\n",
       "      <td id=\"T_85d02_row3_col1\" class=\"data row3 col1\" >1.000000</td>\n",
       "      <td id=\"T_85d02_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row4\" class=\"row_heading level0 row4\" >precision@10</th>\n",
       "      <td id=\"T_85d02_row4_col0\" class=\"data row4 col0\" >0.235000</td>\n",
       "      <td id=\"T_85d02_row4_col1\" class=\"data row4 col1\" >0.240000</td>\n",
       "      <td id=\"T_85d02_row4_col2\" class=\"data row4 col2\" >0.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row5\" class=\"row_heading level0 row5\" >recall@10</th>\n",
       "      <td id=\"T_85d02_row5_col0\" class=\"data row5 col0\" >0.940500</td>\n",
       "      <td id=\"T_85d02_row5_col1\" class=\"data row5 col1\" >0.965200</td>\n",
       "      <td id=\"T_85d02_row5_col2\" class=\"data row5 col2\" >0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row6\" class=\"row_heading level0 row6\" >precision@20</th>\n",
       "      <td id=\"T_85d02_row6_col0\" class=\"data row6 col0\" >0.127500</td>\n",
       "      <td id=\"T_85d02_row6_col1\" class=\"data row6 col1\" >0.130000</td>\n",
       "      <td id=\"T_85d02_row6_col2\" class=\"data row6 col2\" >0.002500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row7\" class=\"row_heading level0 row7\" >recall@20</th>\n",
       "      <td id=\"T_85d02_row7_col0\" class=\"data row7 col0\" >0.990900</td>\n",
       "      <td id=\"T_85d02_row7_col1\" class=\"data row7 col1\" >0.995500</td>\n",
       "      <td id=\"T_85d02_row7_col2\" class=\"data row7 col2\" >0.004500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row8\" class=\"row_heading level0 row8\" >mrr@10</th>\n",
       "      <td id=\"T_85d02_row8_col0\" class=\"data row8 col0\" >1.000000</td>\n",
       "      <td id=\"T_85d02_row8_col1\" class=\"data row8 col1\" >0.840500</td>\n",
       "      <td id=\"T_85d02_row8_col2\" class=\"data row8 col2\" >-0.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row9\" class=\"row_heading level0 row9\" >ndcg@10</th>\n",
       "      <td id=\"T_85d02_row9_col0\" class=\"data row9 col0\" >0.889300</td>\n",
       "      <td id=\"T_85d02_row9_col1\" class=\"data row9 col1\" >0.839100</td>\n",
       "      <td id=\"T_85d02_row9_col2\" class=\"data row9 col2\" >-0.050200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row10\" class=\"row_heading level0 row10\" >map@100</th>\n",
       "      <td id=\"T_85d02_row10_col0\" class=\"data row10 col0\" >0.818300</td>\n",
       "      <td id=\"T_85d02_row10_col1\" class=\"data row10 col1\" >0.767300</td>\n",
       "      <td id=\"T_85d02_row10_col2\" class=\"data row10 col2\" >-0.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row11\" class=\"row_heading level0 row11\" >searchtime_avg</th>\n",
       "      <td id=\"T_85d02_row11_col0\" class=\"data row11 col0\" >0.040900</td>\n",
       "      <td id=\"T_85d02_row11_col1\" class=\"data row11 col1\" >0.033600</td>\n",
       "      <td id=\"T_85d02_row11_col2\" class=\"data row11 col2\" >-0.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row12\" class=\"row_heading level0 row12\" >searchtime_q50</th>\n",
       "      <td id=\"T_85d02_row12_col0\" class=\"data row12 col0\" >0.042500</td>\n",
       "      <td id=\"T_85d02_row12_col1\" class=\"data row12 col1\" >0.028500</td>\n",
       "      <td id=\"T_85d02_row12_col2\" class=\"data row12 col2\" >-0.014000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row13\" class=\"row_heading level0 row13\" >searchtime_q90</th>\n",
       "      <td id=\"T_85d02_row13_col0\" class=\"data row13 col0\" >0.060400</td>\n",
       "      <td id=\"T_85d02_row13_col1\" class=\"data row13 col1\" >0.051200</td>\n",
       "      <td id=\"T_85d02_row13_col2\" class=\"data row13 col2\" >-0.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_85d02_level0_row14\" class=\"row_heading level0 row14\" >searchtime_q95</th>\n",
       "      <td id=\"T_85d02_row14_col0\" class=\"data row14 col0\" >0.083100</td>\n",
       "      <td id=\"T_85d02_row14_col1\" class=\"data row14 col1\" >0.053400</td>\n",
       "      <td id=\"T_85d02_row14_col2\" class=\"data row14 col2\" >-0.029700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x32717ae10>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df = pd.concat(\n",
    "    [\n",
    "        first_phase_df.rename(columns={\"value\": \"first_phase\"}),\n",
    "        second_phase_df.rename(columns={\"value\": \"second_phase\"}),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "# Add diff\n",
    "total_df[\"diff\"] = total_df[\"second_phase\"] - total_df[\"first_phase\"]\n",
    "total_df = total_df.round(4)\n",
    "\n",
    "\n",
    "# highlight recall@10 row and recall@20 row\n",
    "# Define a function to apply the style\n",
    "def highlight_rows_by_index(row, indices_to_highlight):\n",
    "    if row.name in indices_to_highlight:\n",
    "        return [\"background-color: lightblue; color: black\"] * len(row)\n",
    "    return [\"\"] * len(row)\n",
    "\n",
    "\n",
    "total_df.style.apply(\n",
    "    highlight_rows_by_index,\n",
    "    indices_to_highlight=[\"recall@10\", \"recall@20\"],\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e899ee35",
   "metadata": {},
   "source": [
    "For a larger dataset, we would expect to see significant improvement over first-phase ranking.\n",
    "Since our first-phase ranking is already quite good, we can not see this here, but we will leave the comparison code for you to run on a real-world dataset.\n",
    "\n",
    "We also observe a slight increase in search time (from 22ms to 35ms average), which is expected due to the additional complexity of the GBDT model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465790d",
   "metadata": {},
   "source": [
    "### Query profiles with GBDT ranking\n",
    "\n",
    "Create new query profiles that leverage the improved ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ca8ded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<query-profile id=\"hybrid-with-gbdt\" inherits=\"hybrid\">\n",
       "  <field name=\"hits\">20</field>\n",
       "  <field name=\"ranking.profile\">second-with-gbdt</field>\n",
       "  <field name=\"presentation.summary\">top_3_chunks</field>\n",
       "</query-profile>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hybrid_with_gbdt_qp = (qp_dir / \"hybrid-with-gbdt.xml\").read_text()\n",
    "display_md(hybrid_with_gbdt_qp, tag=\"xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "94f23a61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```xml\n",
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<!-- Copyright Vespa.ai. Licensed under the terms of the Apache 2.0 license. See LICENSE in the\n",
       "project root. -->\n",
       "<query-profile id=\"rag-with-gbdt\" inherits=\"hybrid-with-gbdt\">\n",
       "  <field name=\"hits\">50</field>\n",
       "  <field name=\"searchChain\">openai</field>\n",
       "  <field name=\"presentation.format\">sse</field>\n",
       "</query-profile>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rag_with_gbdt_qp = (qp_dir / \"rag-with-gbdt.xml\").read_text()\n",
    "display_md(rag_with_gbdt_qp, tag=\"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b20f2c9",
   "metadata": {},
   "source": [
    "Test the improved ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e77692c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'index:content/0/a3f390d8c35680335e3aebe1',\n",
       " 'relevance': 0.8034803261636057,\n",
       " 'source': 'content',\n",
       " 'fields': {'matchfeatures': {'bm25(title)': 0.0,\n",
       "   'firstPhase': 1.9722333906160157,\n",
       "   'avg_top_3_chunk_sim_scores': 0.2565740570425987,\n",
       "   'avg_top_3_chunk_text_scores': 4.844822406768799,\n",
       "   'max_chunk_sim_scores': 0.2736895978450775,\n",
       "   'max_chunk_text_scores': 7.804652690887451,\n",
       "   'modified_freshness': 0.5275786815220422,\n",
       "   'open_count': 7.0},\n",
       "  'sddocname': 'doc',\n",
       "  'chunks_top3': [\"# Parameter-Efficient Fine-Tuning (PEFT) Techniques - Overview\\n\\n**Goal:** Fine-tune large pre-trained models with significantly fewer trainable parameters, reducing computational cost and memory footprint.\\n\\n**Key Techniques I've Researched/Used:**\\n\\n1.  **LoRA (Low-Rank Adaptation):**\\n    * Freezes pre-trained model weights.\\n    * Injects trainable rank decomposition matrices into Transformer layers.\\n    * Significantly reduces trainable parameters.\\n    * My default starting point for LLM fine-tuning (see `llm_finetuning_pitfalls_best_practices.md`).\\n\\n2.  **QLoRA:**\\n    * Builds on LoRA.\\n    * Quantizes pre-trained model to 4-bit.\\n    * Uses LoRA for fine-tuning the quantized model.\\n    * Further reduces memory usage, enabling fine-tuning of larger models on \",\n",
       "   'consumer GPUs.\\n\\n3.  **Adapter Modules:**\\n    * Inserts small, trainable neural network modules (adapters) between existing layers of the pre-trained model.\\n    * Only adapters are trained.\\n\\n4.  **Prompt Tuning / Prefix Tuning:**\\n    * Keeps model parameters frozen.\\n    * Learns a small set of continuous prompt embeddings (virtual tokens) that are prepended to the input sequence.\\n\\n**Benefits for SynapseFlow (Internal Model Dev):**\\n- Faster iteration on fine-tuning tasks.\\n- Ability to experiment with larger models on available hardware.\\n- Easier to manage multiple fine-tuned model versions (smaller delta to store).\\n\\n## <MORE_TEXT:HERE> (Links to papers, Hugging Face PEFT library notes)'],\n",
       "  'summaryfeatures': {'top_3_chunk_sim_scores': {'type': 'tensor<float>(chunk{})',\n",
       "    'cells': {'0': 0.2736895978450775, '1': 0.23945851624011993}},\n",
       "   'vespa.summaryFeatures.cached': 0.0}}}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what are key points learned for finetuning llms?\"\n",
    "query_profile = \"hybrid-with-gbdt\"\n",
    "\n",
    "body = {\n",
    "    \"query\": query,\n",
    "    \"queryProfile\": query_profile,\n",
    "}\n",
    "with app.syncio() as sess:\n",
    "    result = sess.query(body=body)\n",
    "result.hits[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d69ff",
   "metadata": {},
   "source": [
    "Let us summarize our best practices for second-phase ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ecfd4c",
   "metadata": {},
   "source": [
    "### Best practices for second-phase ranking\n",
    "\n",
    "**Model complexity considerations:**\n",
    "\n",
    "* Use more sophisticated models (GBDT, neural networks) that would be too expensive for first-phase\n",
    "* Take advantage of the reduced candidate set (typically 100-10,000 documents)\n",
    "* Include expensive text features like `nativeProximity` and `fieldMatch`\n",
    "\n",
    "**Feature engineering:**\n",
    "\n",
    "* Combine first-phase scores with additional text and semantic features\n",
    "* Use chunk-level aggregations (max, average, top-k) to capture document structure\n",
    "* Include metadata signals\n",
    "\n",
    "**Training data quality:**\n",
    "\n",
    "* Use the first-phase ranking to generate better training data\n",
    "* Consider having LLMs generate relevance judgments for top-k results\n",
    "* Iteratively improve with user interaction data when available\n",
    "\n",
    "**Performance monitoring:**\n",
    "\n",
    "* Monitor latency impact of second-phase ranking\n",
    "* Adjust `rerank-count` based on quality vs. performance trade-offs\n",
    "* Consider using different models for different query types or use cases\n",
    "\n",
    "The second-phase ranking represents a crucial step in building high-quality RAG applications, providing the precision needed for effective LLM context while maintaining reasonable query latencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1581b4ba",
   "metadata": {},
   "source": [
    "## (Optional) Global-phase ranking\n",
    "\n",
    "We also have the option of configuring [global-phase](https://docs.vespa.ai/en/reference/schema-reference.html#globalphase-rank) ranking, which can rerank the top k (as set by `rerank-count` parameter) documents from the second-phase ranking.\n",
    "\n",
    "Common options for global-phase are [cross-encoders](https://docs.vespa.ai/en/cross-encoders.html) or another GBDT model, trained for better separating top ranked documents on objectives such as [LambdaMart](https://xgboost.readthedocs.io/en/latest/tutorials/learning_to_rank.html). For RAG applications, we consider this less important than for search applications where the results are mainly consumed by an human, as LLMs don't care that much about the ordering of the results.\n",
    "\n",
    "See also our notebook on using [cross-encoders for global reranking](https://vespa-engine.github.io/pyvespa/examples/cross-encoders-for-global-reranking.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea47e3",
   "metadata": {},
   "source": [
    "## Further improvements\n",
    "\n",
    "Finally, we will sketch out some opportunities for further improvements.\n",
    "As you have seen, we started out with only binary relevance labels for a few queries, and trained a model based on the relevant docs and a set of random documents.\n",
    "\n",
    "As you may have noted, we have not discussed what most people think about when discussing RAG evals, evaluating the \"Generation\"-step. There are several tools available to do this, for example [ragas](https://docs.ragas.io/en/stable/) and [ARES](https://github.com/stanford-futuredata/ARES). We refer to other sources for details on this, as this tutorial is probably enough to digest as it is.\n",
    "\n",
    "This was useful initially, as we had no better way to retrieve the candidate documents.\n",
    "Now, that we have a reasonably good second-phase ranking, we could potentially generate a new set of relevance labels for queries that we did not have labels for by having an LLM do relevance judgments of the top k returned hits. This training dataset would likely be even better in separating the top documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110100af",
   "metadata": {},
   "source": [
    "## Structured output from the LLM\n",
    "\n",
    "Let us also show how we can request structured JSON output from the LLM, which can be useful for several reasons, the most common probably being citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "30774dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"SynapseFlow's strategy focuses on simplifying the deployment, management, and scaling of machine learning models for developers and small teams. The key components of their strategy include:\\n\\n1. **Target Audience**: They target individual developers, startups, and SMEs with a particular emphasis on those new to MLOps, allowing them to leverage AI deployment without needing deep Ops knowledge.\\n\\n2. **Customer Pain Points**: SynapseFlow aims to address common challenges such as complex deployment processes, reliance on DevOps teams for model deployment, and slow, bureaucratic workflows. They provide a solution that minimizes infrastructure overhead and streamlines the journey from model experimentation to production.\\n\\n3. **Developer-First Approach**: Offering a developer-first API and intuitive UI, they ensure that users can deploy models quickly, focusing on easing the operational burden of MLOps.\\n\\n4. **Marketing and Outreach**: Their go-to-market strategy includes content marketing to educate potential users, leveraging developer communities, and building relationships through the YC network. They're also focused on SEO for high visibility within relevant search terms.\\n\\n5. **Feature Differentiators**: The platform differentiates itself through ease of deployment, a simple user interface, and a transparent pricing model tailored for startups and small businesses, making it more accessible than traditional MLOps solutions like SageMaker or Vertex AI.\\n\\n6. **Feedback and Iteration**: SynapseFlow is committed to continuous improvement based on user feedback, refining their offerings, and iteratively enhancing their product based on real-world user experiences and needs.  \\n\\n7. **Future Growth**: Plans for future growth include targeting additional user segments and functionalities, such as integrating advanced monitoring solutions and data drift detection.\\n\\nOverall, SynapseFlow's strategy is to be the go-to platform for AI deployment, with a focus on simplifying processes for those who may not have extensive technical resources, thereby enabling more teams to harness the power of AI effectively.\",\n",
       " 'citations': ['1', '4', '5', '8', '9']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vespa.io import VespaResponse\n",
    "import json\n",
    "\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"answer\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The answer to the query if it is contained in the documents. If not, it say that you are not allowed to answer based on the documents.\",\n",
    "        },\n",
    "        \"citations\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List of returned and cited document IDs\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"answer\", \"citations\"],\n",
    "    \"additionalProperties\": False,\n",
    "}\n",
    "\n",
    "query = \"What is SynapseFlows strategy\"\n",
    "body = {\n",
    "    \"query\": query,\n",
    "    \"queryProfile\": \"hybrid\",\n",
    "    \"searchChain\": \"openai\",\n",
    "    \"llm.json_schema\": json.dumps(schema),\n",
    "    \"presentation.format\": \"json\",\n",
    "}\n",
    "\n",
    "with app.syncio() as sess:\n",
    "    resp = sess.query(body=body)\n",
    "\n",
    "\n",
    "def response_to_string(response: VespaResponse):\n",
    "    \"\"\"\n",
    "    Convert a Vespa response to a string of the returned tokens.\n",
    "    \"\"\"\n",
    "    children = response.json.get(\"root\", {}).get(\"children\", [])\n",
    "    tokens = \"\"\n",
    "    for child in children:\n",
    "        if child.get(\"id\") == \"event_stream\":\n",
    "            for stream_child in child.get(\"children\", []):\n",
    "                tokens += stream_child.get(\"fields\", {}).get(\"token\", \"\")\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokens = response_to_string(resp)\n",
    "json.loads(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c840a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we have built a complete RAG application using Vespa, providing our recommendations for how to approach both retrieval phase with binary vectors and text matching, first-phase ranking with a linear combination of relatively cheap features to a more sophisticated second-phase ranking system with more expensive features and a GBDT model.\n",
    "\n",
    "We hope that this tutorial, along with the provided code in our [sample-apps repository](https://github.com/vespa-engine/sample-apps/tree/master/rag-blueprint), will serve as a useful reference for building your own RAG applications, with an evaluation-driven approach.\n",
    "\n",
    "By using the principles demonstrated in this tutorial, you are empowered to build high-quality RAG applications that can scale to any dataset size, and any query load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdb9cc8",
   "metadata": {},
   "source": [
    "![rag meme](../_static/rag-meme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56242b",
   "metadata": {},
   "source": [
    "## FAQ\n",
    "\n",
    "* **Q: Which embedding models can I use with Vespa?**\n",
    "  A: Vespa supports a variety of embedding models. For a list of vespa provided models on Vespa Cloud, see [Model hub](https://docs.vespa.ai/en/cloud/model-hub.html). See also [embedding reference](https://docs.vespa.ai/en/embedding.html#provided-embedders) for how to use embedders. You can also use private models (gated by authentication with Bearer token from Vespa Cloud secret store).\n",
    "\n",
    "* **Q: Why don't you use ColBERT for ranking?**\n",
    "  A: We love ColBERT, and it has shown great performance. We do support ColBERT-style models in Vespa. The challenge is the added cost in memory storage, especially for large-scale applications. If you use it, we recommend consider binarizing the vectors to reduce memory usage 32x compared to float. If you want to improve the ranking quality and accept the additional cost, we encourage you to evaluate and try.\n",
    "  Here are some resources if you want to learn more about using ColBERT with Vespa:\n",
    "\n",
    "  * [Announcing ColBERT embedder](https://blog.vespa.ai/announcing-colbert-embedder-in-vespa/#what-is-colbert?)\n",
    "  * [Long context ColBERT](https://blog.vespa.ai/announcing-long-context-colbert-in-vespa/)\n",
    "  * [Long context ColBERT sample app](https://github.com/vespa-engine/sample-apps/tree/master/colbert-long/#vespa-sample-applications---long-context-colbert)\n",
    "  * [ColBERT sample app](https://github.com/vespa-engine/sample-apps/tree/master/colbert)\n",
    "  * [ColBERT embedder reference](https://docs.vespa.ai/en/embedding.html#colbert-embedder)\n",
    "  * [ColBERT standalone python example notebook](https://vespa-engine.github.io/pyvespa/examples/colbert_standalone_Vespa-cloud.html)\n",
    "  * [ColBERT standalone long context example notebook](https://vespa-engine.github.io/pyvespa/examples/colbert_standalone_long_context_Vespa-cloud.html)\n",
    "\n",
    "* **Q: Do I need to use an LLM with Vespa?**\n",
    "  A: No, you are free to use Vespa as a search engine. We provide the option of calling out to LLMs from within a Vespa application for reduced latency compared to sending large search results sets several times over network as well as the option to deploy Local LLMs, optionally in your own infrastructure if you prefer. See [Vespa Cloud Enclave](https://docs.vespa.ai/en/cloud/enclave/enclave.html)\n",
    "\n",
    "* **Q: Why do we use binary vectors for the document embeddings?**\n",
    "  A: Binary vectors takes up a lot less memory and are faster to compute distances on, with only a slight reduction in quality. See blog [post](https://blog.vespa.ai/combining-matryoshka-with-binary-quantization-using-embedder/) for details.\n",
    "  \n",
    "* **Q: How can you say that Vespa can scale to any data and query load?**\n",
    "  A: Vespa can scale both the stateless container nodes and content nodes of your application. See [overview](https://docs.vespa.ai/en/overview.html) and [elasticity](https://docs.vespa.ai/en/elasticity.html) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75204cc9",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "As this tutorial is running in a CI environment, we will clean up the resources created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09e94299",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.getenv(\"CI\", \"false\") == \"true\":\n",
    "    vespa_cloud.delete()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pyvespa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
