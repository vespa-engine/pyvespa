{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a429e148",
   "metadata": {},
   "source": [
    "![Vespa logo](https://vespa.ai/assets/vespa-logo-color.png)\n",
    "\n",
    "# Multi-vector indexing with HNSW\n",
    "\n",
    "This is the pyvespa steps of the multi-vector-indexing sample application.\n",
    "Go to the [source](https://github.com/vespa-engine/sample-apps/tree/master/multi-vector-indexing)\n",
    "for a full description and prerequisites,\n",
    "and read the [blog post](https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/).\n",
    "Highlighted features:\n",
    "\n",
    "* Approximate Nearest Neighbor Search - using HNSW or exact\n",
    "* Use a Component to configure the Huggingface embedder.\n",
    "* Using synthetic fields with auto-generated\n",
    "  [embeddings](https://docs.vespa.ai/en/embedding.html) in data and query flow.\n",
    "* Application package file export, model files in the application package, deployment from files.\n",
    "* [Multiphased ranking](https://docs.vespa.ai/en/phased-ranking.html).\n",
    "* How to control text search result highlighting.\n",
    "\n",
    "For simpler examples, see [text search](https://pyvespa.readthedocs.io/en/latest/getting-started-pyvespa.html)\n",
    "and [pyvespa examples](https://pyvespa.readthedocs.io/en/latest/examples/pyvespa-examples.html).\n",
    "\n",
    "Pyvespa is an add-on to Vespa, and this guide will export the application package containing `services.xml` and `wiki.sd`. The latter is the schema file for this application - knowing services.xml and schema files is useful when reading Vespa documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f78eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Refer to <a href=\"https://pyvespa.readthedocs.io/en/latest/troubleshooting.html\">troubleshooting</a>\n",
    "    for any problem when running this guide.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fea8a2",
   "metadata": {},
   "source": [
    "This notebook requires [pyvespa >= 0.37.1](https://pyvespa.readthedocs.io/en/latest/index.html#requirements),\n",
    "ZSTD, and the [Vespa CLI](https://docs.vespa.ai/en/vespa-cli.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5137e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install pyvespa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d35f3da",
   "metadata": {},
   "source": [
    "## Create the application\n",
    "\n",
    "Configure the Vespa instance with a component loading the E5-small model.\n",
    "Components are used to plug in code and models to a Vespa application -\n",
    "[read more](https://docs.vespa.ai/en/jdisc/container-components.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7baafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.package import *\n",
    "from pathlib import Path\n",
    "\n",
    "app_package = ApplicationPackage(name=\"wiki\",\n",
    "              components=[Component(id=\"e5-small-q\", type=\"hugging-face-embedder\",\n",
    "                  parameters=[\n",
    "                      Parameter(\"transformer-model\", {\"path\": \"model/e5-small-v2-int8.onnx\"}),\n",
    "                      Parameter(\"tokenizer-model\", {\"path\": \"model/tokenizer.json\"})\n",
    "              ])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb9a7d9",
   "metadata": {},
   "source": [
    "## Configure fields\n",
    "\n",
    "Vespa has a variety of basic and complex\n",
    "[field types](https://docs.vespa.ai/en/reference/schema-reference.html#field).\n",
    "This application uses a combination of integer, text and tensor fields,\n",
    "making it easy to implement hybrid ranking use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9106a6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_package.schema.add_fields(\n",
    "    Field(name=\"id\", type=\"int\", indexing=[\"attribute\", \"summary\"]),\n",
    "    Field(name=\"title\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"),\n",
    "    Field(name=\"url\", type=\"string\", indexing=[\"index\", \"summary\"], index=\"enable-bm25\"),\n",
    "    Field(name=\"paragraphs\", type=\"array<string>\", indexing=[\"index\", \"summary\"],\n",
    "          index=\"enable-bm25\", bolding=True),\n",
    "    Field(name=\"paragraph_embeddings\", type=\"tensor<float>(p{},x[384])\",\n",
    "          indexing=[\"input paragraphs\", \"embed\", \"index\", \"attribute\"],\n",
    "          ann=HNSW(distance_metric=\"angular\"),\n",
    "          is_document_field=False)\n",
    "    #\n",
    "    # Alteratively, for exact distance calculation not using HNSW:\n",
    "    # \n",
    "    # Field(name=\"paragraph_embeddings\", type=\"tensor<float>(p{},x[384])\",\n",
    "    #       indexing=[\"input paragraphs\", \"embed\", \"attribute\"],\n",
    "    #       attribute=[\"distance-metric: angular\"],\n",
    "    #       is_document_field=False)\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88be785",
   "metadata": {},
   "source": [
    "One field of particular interest is `paragraph_embeddings`.\n",
    "Note that we are _not_ feeding embeddings to this instance.\n",
    "Instead, the embeddings are generated by using the [embed](https://docs.vespa.ai/en/embedding.html)\n",
    "feature, using the model configured at start.\n",
    "Read more in [Text embedding made simple](https://blog.vespa.ai/text-embedding-made-simple/).\n",
    "\n",
    "Looking closely at the code, `paragraph_embeddings` uses `is_document_field=False`, meaning it will read another field as input (here `paragraph`), and run `embed` on it.\n",
    "\n",
    "As only one model is configured, `embed` will use that one -\n",
    "it is possible to configure mode models and use `embed model-id` as well.\n",
    "\n",
    "As the code comment illustrates, there can be different distrance metrics used,\n",
    "as well as using an _exact_ or _approximate_ nearest neighbor search.\n",
    "\n",
    "\n",
    "## Configure rank profiles\n",
    "\n",
    "A rank profile defines the computation for the ranking,\n",
    "with a wide range of possible features as input.\n",
    "Below you will find `first_phase` ranking using text ranking (`bm`),\n",
    "semantic ranking using vector distance (consider a tensor a vector here),\n",
    "and combinations of the two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4048480",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_package.schema.add_rank_profile(RankProfile(\n",
    "    name=\"semantic\",\n",
    "    inputs=[(\"query(q)\", \"tensor<float>(x[384])\")],\n",
    "    inherits=\"default\",\n",
    "    first_phase=\"cos(distance(field,paragraph_embeddings))\",\n",
    "    match_features=[\"closest(paragraph_embeddings)\"])\n",
    ")\n",
    "\n",
    "app_package.schema.add_rank_profile(RankProfile(\n",
    "        name = \"bm25\",\n",
    "        first_phase = \"2*bm25(title) + bm25(paragraphs)\")\n",
    ")\n",
    "\n",
    "app_package.schema.add_rank_profile(RankProfile(\n",
    "    name=\"hybrid\",\n",
    "    inherits=\"semantic\",\n",
    "    functions=[\n",
    "        Function(name=\"avg_paragraph_similarity\",\n",
    "            expression=\"\"\"reduce(\n",
    "                              sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),\n",
    "                              avg,\n",
    "                              p\n",
    "                          )\"\"\"),\n",
    "        Function(name=\"max_paragraph_similarity\",\n",
    "            expression=\"\"\"reduce(\n",
    "                              sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x),\n",
    "                              max,\n",
    "                              p\n",
    "                          )\"\"\"),\n",
    "        Function(name=\"all_paragraph_similarities\",\n",
    "            expression=\"sum(l2_normalize(query(q),x) * l2_normalize(attribute(paragraph_embeddings),x),x)\")\n",
    "    ],\n",
    "    first_phase=FirstPhaseRanking(\n",
    "        expression=\"cos(distance(field,paragraph_embeddings))\"),\n",
    "    second_phase=SecondPhaseRanking(\n",
    "        expression=\"firstPhase + avg_paragraph_similarity() + log( bm25(title) + bm25(paragraphs) + bm25(url))\"),\n",
    "    match_features=[\"closest(paragraph_embeddings)\",\n",
    "                    \"firstPhase\",\n",
    "                    \"bm25(title)\",\n",
    "                    \"bm25(paragraphs)\",\n",
    "                    \"avg_paragraph_similarity\",\n",
    "                    \"max_paragraph_similarity\",\n",
    "                    \"all_paragraph_similarities\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0048dc",
   "metadata": {},
   "source": [
    "## Configure fieldset\n",
    "\n",
    "A [fieldset](https://docs.vespa.ai/en/reference/schema-reference.html#fieldset)\n",
    "is a way to configure search in multiple fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e14283",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_package.schema.add_field_set(FieldSet(name=\"default\", fields=[\"title\", \"url\", \"paragraphs\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a1db4",
   "metadata": {},
   "source": [
    "## Configure document summary\n",
    "\n",
    "A [document summary](https://docs.vespa.ai/en/document-summaries.html)\n",
    "is the collection of fields to return in query results -\n",
    "the default summary is used unless other specified in the query.\n",
    "Here we configure a `minimal` fieldset without the larger paragraph text/embedding fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293dac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_package.schema.add_document_summary(DocumentSummary(name=\"minimal\",\n",
    "                                        summary_fields=[Summary(\"id\", \"int\"),\n",
    "                                                        Summary(\"title\", \"string\")]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c6ef2",
   "metadata": {},
   "source": [
    "## Export the configuration\n",
    "\n",
    "At this point, the application is well defined.\n",
    "Remember that the Component configuration at start configures model files to be found in a `model` directory.\n",
    "We must therefore export the configuration and add the models, before we can deploy to the Vespa instance.\n",
    "Export the [application package](https://docs.vespa.ai/en/application-packages.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"pkg\").mkdir(parents=True, exist_ok=True)\n",
    "app_package.to_files(\"pkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4febd6b",
   "metadata": {},
   "source": [
    "It is a good idea to inspect the files exported into `pkg` - these are files referred to in the\n",
    "[Vespa Documentation](https://docs.vespa.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5316d",
   "metadata": {},
   "source": [
    "## Download model files\n",
    "\n",
    "At this point, we can save the model files into the application package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b87bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! mkdir -p pkg/model\n",
    "! curl -L -o pkg/model/tokenizer.json \\\n",
    "  https://raw.githubusercontent.com/vespa-engine/sample-apps/master/simple-semantic-search/model/tokenizer.json\n",
    "\n",
    "! curl -L -o pkg/model/e5-small-v2-int8.onnx \\\n",
    "  https://github.com/vespa-engine/sample-apps/raw/master/simple-semantic-search/model/e5-small-v2-int8.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d5ed0",
   "metadata": {},
   "source": [
    "## Deploy the application\n",
    "\n",
    "As all the files in the app package are ready, we can start a Vespa instance - here using Docker.\n",
    "Deploy the app package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02821efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vespa.deployment import VespaDocker\n",
    "\n",
    "vespa_docker = VespaDocker()\n",
    "app = vespa_docker.deploy_from_disk(application_name=\"wiki\", application_root=\"pkg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c1060b",
   "metadata": {},
   "source": [
    "## Feed documents\n",
    "\n",
    "Download the Wikipedia articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a74eff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -s -H \"Accept:application/vnd.github.v3.raw\" \\\n",
    "  https://api.github.com/repos/vespa-engine/sample-apps/contents/multi-vector-indexing/ext/articles.jsonl.zst | \\\n",
    "  zstdcat - > articles.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8e0395",
   "metadata": {},
   "source": [
    "I you do not have ZSTD install, get `articles.jsonl.zip` and unzip it instead.\n",
    "\n",
    "Feed and index the Wikipedia articles using the [Vespa CLI](https://docs.vespa.ai/en/vespa-cli.html).\n",
    "As part of feeding, `embed` is called on each article,\n",
    "and the output of this is stored in the `paragraph_embeddings` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130fdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "! vespa config set target local\n",
    "! vespa feed articles.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7461f9a",
   "metadata": {},
   "source": [
    "Note that creating embeddings is computationally expensive, but this is a small dataset with only 8 articles, so will be done in a few seconds.\n",
    "\n",
    "The Vespa instance is now populated with the Wikipedia articles, with generated embeddings, and ready for queries.\n",
    "The next sections have examples of various kinds of queries to run on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9e58c",
   "metadata": {},
   "source": [
    "## Simple retrieve all articles with undefined ranking\n",
    "\n",
    "Run a query selecting _all_ documents, returning two of them.\n",
    "The rank profile is the built-in `unranked` which means no ranking calculations are done,\n",
    "the results are returned in random order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd6e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.query(body={\n",
    "  'yql': 'select * from wiki where true',\n",
    "  'ranking.profile': 'unranked',\n",
    "  'hits': 2\n",
    "})\n",
    "result.hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72937dc2",
   "metadata": {},
   "source": [
    "## Traditional keyword search with BM25 ranking on the article level\n",
    "\n",
    "Run a text-search query and use the [bm25](https://docs.vespa.ai/en/reference/bm25.html)\n",
    "ranking profile configured at the start of this guide: `2*bm25(title) + bm25(paragraphs)`.\n",
    "Here, we use BM25 on the `title` and `paragraph` text fields, giving more weight to matches in title:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.query(body={\n",
    "  'yql': 'select * from wiki where userQuery()',\n",
    "  'query': 24,\n",
    "  'ranking.profile': 'bm25',\n",
    "  'hits': 2\n",
    "})\n",
    "result.hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716cb42",
   "metadata": {},
   "source": [
    "## Semantic vector search on the paragraph level\n",
    "\n",
    "This query creates an embedding of the query \"what does 24 mean in the context of railways\"\n",
    "and specifies the `semantic` ranking profile: `cos(distance(field,paragraph_embeddings))`.\n",
    "This will hence compute the distance between the vector in the query\n",
    "and the vectors computed when indexing: `\"input paragraphs\", \"embed\", \"index\", \"attribute\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bccdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.query(body={\n",
    "  'yql': 'select * from wiki where {targetHits:1}nearestNeighbor(paragraph_embeddings,q)',\n",
    "  'input.query(q)': 'embed(what does 24 mean in the context of railways)',\n",
    "  'ranking.profile': 'semantic',\n",
    "  'hits': 2\n",
    "})\n",
    "result.hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c134faa6",
   "metadata": {},
   "source": [
    "An interesting question then is, of the paragraphs in the document, which one was the closest?\n",
    "When analysing ranking,\n",
    "using [match-features](https://docs.vespa.ai/en/reference/schema-reference.html#match-features)\n",
    "lets you export the scores used in the ranking calculations, see\n",
    "[closest](https://docs.vespa.ai/en/reference/rank-features.html#closest(name)) - from the result above:\n",
    "```\n",
    "'matchfeatures': {\n",
    "    'closest(paragraph_embeddings)': {\n",
    "        'type': 'tensor<float>(p{})',\n",
    "        'cells': {'4': 1.0}\n",
    "    }\n",
    "}\n",
    "```\n",
    "This means, the tensor of index 4 has the closest match. With this, it is straight forward to feed articles with an array of paragraphs and highlight the best matching paragraph in the document!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c9fc99",
   "metadata": {},
   "source": [
    "## Hybrid search and ranking\n",
    "\n",
    "Hybrid combining keyword search on the article level with vector search in the paragraph index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0d11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.query(body={\n",
    "  'yql': 'select * from wiki where userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))',\n",
    "  'input.query(q)': 'embed(what does 24 mean in the context of railways)',\n",
    "  'query': 'what does 24 mean in the context of railways',\n",
    "  'ranking.profile': 'hybrid',\n",
    "  'hits': 1\n",
    "})\n",
    "result.hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6dd1f4",
   "metadata": {},
   "source": [
    "This case combines exact search with nearestNeighbor search. The `hybrid` rank-profile above\n",
    "also calculates several additional features using\n",
    "[tensor expressions](https://docs.vespa.ai/en/tensor-user-guide.html):\n",
    "\n",
    "- `firstPhase` is the score of the first ranking phase, configured in the hybrid\n",
    "  profile as `cos(distance(field, paragraph_embeddings))`.\n",
    "- `all_paragraph_similarities` returns all the similarity scores for all paragraphs.\n",
    "- `avg_paragraph_similarity` is the average similarity score across all the paragraphs.\n",
    "- `max_paragraph_similarity` is the same as `firstPhase`, but computed using a tensor expression.\n",
    "\n",
    "These additional features are calculated during [second-phase ranking](https://docs.vespa.ai/en/phased-ranking.html) \n",
    "to limit the number of vector computations.\n",
    "\n",
    "The [Tensor Playground](https://docs.vespa.ai/playground/) is useful to play with tensor expressions.\n",
    "\n",
    "The [Hybrid Search](https://blog.vespa.ai/improving-zero-shot-ranking-with-vespa/) blog post series\n",
    "is a good read to learn more about hybrid ranking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa9e8d",
   "metadata": {},
   "source": [
    "## Hybrid search and filter\n",
    "\n",
    "YQL is a structured query langauge.\n",
    "In the query examples, the user input is fed as-is using the `userQuery()` operator.\n",
    "\n",
    "Filters are normally separate from the user input,\n",
    "below is an example of adding a filter `url contains \"9985\"` to the YQL string.\n",
    "\n",
    "Finally, the use the [Query API](https://docs.vespa.ai/en/query-api.html) for other options, like highlighting -\n",
    "here disable [bolding](https://docs.vespa.ai/en/reference/schema-reference.html#bolding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250f2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.query(body={\n",
    "  'yql': 'select * from wiki where url contains \"9985\" and userQuery() or ({targetHits:1}nearestNeighbor(paragraph_embeddings,q))',\n",
    "  'input.query(q)': 'embed(what does 24 mean in the context of railways)',\n",
    "  'query': 'what does 24 mean in the context of railways',\n",
    "  'ranking.profile': 'hybrid',\n",
    "  'bolding': False\n",
    "})\n",
    "result.hits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6558f3",
   "metadata": {},
   "source": [
    "In short, the above query demonstrates how easy it is to combine various ranking strategies,\n",
    "and also combine with filters.\n",
    "\n",
    "To learn more about pre-filtering vs post-filtering,\n",
    "read [Filtering strategies and serving performance](https://blog.vespa.ai/constrained-approximate-nearest-neighbor-search/).\n",
    "[Semantic search with multi-vector indexing](https://blog.vespa.ai/semantic-search-with-multi-vector-indexing/)\n",
    "is a great read overall for this domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfe782e",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ca46387",
   "metadata": {},
   "outputs": [],
   "source": [
    "vespa_docker.container.stop()\n",
    "vespa_docker.container.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
